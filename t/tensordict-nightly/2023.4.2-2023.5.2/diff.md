# Comparing `tmp/tensordict_nightly-2023.4.2-py39-none-any.whl.zip` & `tmp/tensordict_nightly-2023.5.2-py39-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,28 +1,29 @@
-Zip file size: 114504 bytes, number of entries: 26
--rw-r--r--  2.0 unx     1088 b- defN 23-Apr-02 11:18 tensordict/__init__.py
--rw-r--r--  2.0 unx    29601 b- defN 23-Apr-02 11:18 tensordict/memmap.py
--rw-r--r--  2.0 unx    31903 b- defN 23-Apr-02 11:18 tensordict/persistent.py
--rw-r--r--  2.0 unx    30447 b- defN 23-Apr-02 11:18 tensordict/tensorclass.py
--rw-r--r--  2.0 unx   241935 b- defN 23-Apr-02 11:18 tensordict/tensordict.py
--rw-r--r--  2.0 unx    25528 b- defN 23-Apr-02 11:18 tensordict/utils.py
--rw-r--r--  2.0 unx       84 b- defN 23-Apr-02 11:18 tensordict/version.py
--rw-r--r--  2.0 unx     1074 b- defN 23-Apr-02 11:18 tensordict/nn/__init__.py
--rw-r--r--  2.0 unx    18697 b- defN 23-Apr-02 11:18 tensordict/nn/common.py
--rw-r--r--  2.0 unx    17837 b- defN 23-Apr-02 11:18 tensordict/nn/functional_modules.py
--rw-r--r--  2.0 unx    19304 b- defN 23-Apr-02 11:18 tensordict/nn/probabilistic.py
--rw-r--r--  2.0 unx    11616 b- defN 23-Apr-02 11:18 tensordict/nn/sequence.py
--rw-r--r--  2.0 unx     3198 b- defN 23-Apr-02 11:18 tensordict/nn/utils.py
--rw-r--r--  2.0 unx      499 b- defN 23-Apr-02 11:18 tensordict/nn/distributions/__init__.py
--rw-r--r--  2.0 unx     7073 b- defN 23-Apr-02 11:18 tensordict/nn/distributions/continuous.py
--rw-r--r--  2.0 unx     2580 b- defN 23-Apr-02 11:18 tensordict/nn/distributions/discrete.py
--rw-r--r--  2.0 unx     6504 b- defN 23-Apr-02 11:18 tensordict/nn/distributions/truncated_normal.py
--rw-r--r--  2.0 unx     1226 b- defN 23-Apr-02 11:18 tensordict/nn/distributions/utils.py
--rw-r--r--  2.0 unx      381 b- defN 23-Apr-02 11:18 tensordict/prototype/__init__.py
--rw-r--r--  2.0 unx     7507 b- defN 23-Apr-02 11:18 tensordict/prototype/fx.py
--rw-r--r--  2.0 unx      663 b- defN 23-Apr-02 11:18 tensordict/prototype/tensorclass.py
--rw-r--r--  2.0 unx     1098 b- defN 23-Apr-02 11:18 tensordict_nightly-2023.4.2.dist-info/LICENSE
--rw-r--r--  2.0 unx    14192 b- defN 23-Apr-02 11:18 tensordict_nightly-2023.4.2.dist-info/METADATA
--rw-r--r--  2.0 unx       93 b- defN 23-Apr-02 11:18 tensordict_nightly-2023.4.2.dist-info/WHEEL
--rw-r--r--  2.0 unx       11 b- defN 23-Apr-02 11:18 tensordict_nightly-2023.4.2.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     2269 b- defN 23-Apr-02 11:18 tensordict_nightly-2023.4.2.dist-info/RECORD
-26 files, 476408 bytes uncompressed, 110828 bytes compressed:  76.7%
+Zip file size: 128815 bytes, number of entries: 27
+-rw-r--r--  2.0 unx     1088 b- defN 23-May-02 16:42 tensordict/__init__.py
+-rw-r--r--  2.0 unx     6000 b- defN 23-May-02 16:42 tensordict/_contextlib.py
+-rw-r--r--  2.0 unx    29601 b- defN 23-May-02 16:42 tensordict/memmap.py
+-rw-r--r--  2.0 unx    33049 b- defN 23-May-02 16:42 tensordict/persistent.py
+-rw-r--r--  2.0 unx    30420 b- defN 23-May-02 16:42 tensordict/tensorclass.py
+-rw-r--r--  2.0 unx   264665 b- defN 23-May-02 16:42 tensordict/tensordict.py
+-rw-r--r--  2.0 unx    26209 b- defN 23-May-02 16:42 tensordict/utils.py
+-rw-r--r--  2.0 unx       84 b- defN 23-May-02 16:43 tensordict/version.py
+-rw-r--r--  2.0 unx     1314 b- defN 23-May-02 16:42 tensordict/nn/__init__.py
+-rw-r--r--  2.0 unx    35195 b- defN 23-May-02 16:42 tensordict/nn/common.py
+-rw-r--r--  2.0 unx    17891 b- defN 23-May-02 16:42 tensordict/nn/functional_modules.py
+-rw-r--r--  2.0 unx    22301 b- defN 23-May-02 16:42 tensordict/nn/probabilistic.py
+-rw-r--r--  2.0 unx    19564 b- defN 23-May-02 16:42 tensordict/nn/sequence.py
+-rw-r--r--  2.0 unx    10621 b- defN 23-May-02 16:42 tensordict/nn/utils.py
+-rw-r--r--  2.0 unx      499 b- defN 23-May-02 16:42 tensordict/nn/distributions/__init__.py
+-rw-r--r--  2.0 unx     7073 b- defN 23-May-02 16:42 tensordict/nn/distributions/continuous.py
+-rw-r--r--  2.0 unx     2580 b- defN 23-May-02 16:42 tensordict/nn/distributions/discrete.py
+-rw-r--r--  2.0 unx     6504 b- defN 23-May-02 16:42 tensordict/nn/distributions/truncated_normal.py
+-rw-r--r--  2.0 unx     1226 b- defN 23-May-02 16:42 tensordict/nn/distributions/utils.py
+-rw-r--r--  2.0 unx      381 b- defN 23-May-02 16:42 tensordict/prototype/__init__.py
+-rw-r--r--  2.0 unx     7507 b- defN 23-May-02 16:42 tensordict/prototype/fx.py
+-rw-r--r--  2.0 unx      739 b- defN 23-May-02 16:42 tensordict/prototype/tensorclass.py
+-rw-r--r--  2.0 unx     1098 b- defN 23-May-02 16:43 tensordict_nightly-2023.5.2.dist-info/LICENSE
+-rw-r--r--  2.0 unx    15208 b- defN 23-May-02 16:43 tensordict_nightly-2023.5.2.dist-info/METADATA
+-rw-r--r--  2.0 unx       93 b- defN 23-May-02 16:43 tensordict_nightly-2023.5.2.dist-info/WHEEL
+-rw-r--r--  2.0 unx       11 b- defN 23-May-02 16:43 tensordict_nightly-2023.5.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     2352 b- defN 23-May-02 16:43 tensordict_nightly-2023.5.2.dist-info/RECORD
+27 files, 543273 bytes uncompressed, 125013 bytes compressed:  77.0%
```

## zipnote {}

```diff
@@ -1,10 +1,13 @@
 Filename: tensordict/__init__.py
 Comment: 
 
+Filename: tensordict/_contextlib.py
+Comment: 
+
 Filename: tensordict/memmap.py
 Comment: 
 
 Filename: tensordict/persistent.py
 Comment: 
 
 Filename: tensordict/tensorclass.py
@@ -57,23 +60,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2023.4.2.dist-info/LICENSE
+Filename: tensordict_nightly-2023.5.2.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2023.4.2.dist-info/METADATA
+Filename: tensordict_nightly-2023.5.2.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2023.4.2.dist-info/WHEEL
+Filename: tensordict_nightly-2023.5.2.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2023.4.2.dist-info/top_level.txt
+Filename: tensordict_nightly-2023.5.2.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2023.4.2.dist-info/RECORD
+Filename: tensordict_nightly-2023.5.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/persistent.py

```diff
@@ -102,16 +102,31 @@
         group (h5py.Group, optional): a file or a group that contains data. Exclusive with ``filename``.
         mode (str, optional): Reading mode. Defaults to ``"r"``.
         backend (str, optional): storage backend. Currently only ``"h5"`` is supported.
         device (torch.device or compatible, optional): device of the tensordict.
             Defaults to ``None`` (ie. default PyTorch device).
         **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
 
+    .. note::
+      Currently, PersistentTensorDict instances are not closed when getting out-of-scope.
+      This means that it is the responsibility of the user to close them if necessary.
+
+    Examples:
+        >>> import tempfile
+        >>> with tempfile.NamedTemporaryFile() as f:
+        ...     data = PersistentTensorDict(file=f, batch_size=[3], mode="w")
+        ...     data["a", "b"] = torch.randn(3, 4)
+        ...     print(data)
+
     """
 
+    def __new__(cls, *args, **kwargs):
+        cls._names = None
+        return super().__new__(cls, *args, **kwargs)
+
     def __init__(
         self,
         *,
         batch_size,
         filename=None,
         group=None,
         mode="r",
@@ -197,14 +212,18 @@
             out.update(input_dict)
         else:
             out.update(TensorDict(input_dict, batch_size=batch_size))
         if not _has_batch_size:
             _set_max_batch_size(out)
         return out
 
+    def close(self):
+        """Closes the persistent tensordict."""
+        self.file.close()
+
     def _process_key(self, key):
         if isinstance(key, str):
             return key
         else:
             return "/".join(key)
 
     def _check_batch_size(self, batch_size) -> None:
@@ -272,15 +291,15 @@
                     # and convert to torch tensor first.
                     warnings.warn(
                         "Indexing an h5py.Dataset object with a boolean mask "
                         "that needs broadcasting does not work directly. "
                         "tensordict will cast the entire array in memory and index it using the mask. "
                         "This is suboptimal and may lead to performance issue."
                     )
-                    out = torch.as_tensor(array, device=device)[idx]
+                    out = torch.as_tensor(np.asarray(array), device=device)[idx]
                 else:
                     raise err
             if self._pin_mem:
                 return out.pin_memory()
             return out
         elif array is not default:
             out = self._nested_tensordicts.get(key, None)
@@ -411,14 +430,22 @@
         _batch_size = self._batch_size
         try:
             self._batch_size = torch.Size(value)
             self._check_batch_size(self._batch_size)
         except ValueError:
             self._batch_size = _batch_size
 
+    def _rename_subtds(self, names):
+        if names is None:
+            names = [None] * self.ndim
+        for item in self._nested_tensordicts.values():
+            if is_tensor_collection(item):
+                td_names = list(names) + [None] * (item.ndim - self.ndim)
+                item.rename_(*td_names)
+
     def contiguous(self):
         """Materializes a PersistentTensorDict on a regular TensorDict."""
         return self.to_tensordict()
 
     def del_(self, key):
         del self.file[key]
         return self
@@ -661,14 +688,15 @@
                 key, subkey = key[0], key[1:]
             else:
                 key, subkey = key, []
             target_td = self.get(key, default=None)
             if target_td is None:
                 self.file.create_group(key)
                 target_td = self.get(key)
+                target_td.batch_size = value.batch_size
             elif not is_tensor_collection(target_td):
                 raise RuntimeError(
                     f"cannot set a tensor collection in place of a non-tensor collection in {self.__class__.__name__}. "
                     f"Got self.get({key})={target_td} and value={value}."
                 )
             if idx is None:
                 if len(subkey):
@@ -715,15 +743,16 @@
 
         else:
             try:
                 self.file.create_dataset(key, data=value, **self.kwargs)
             except (ValueError, OSError) as err:
                 if "name already exists" in str(err):
                     warnings.warn(
-                        "Replacing an array with another one is inefficient. Consider using different names or populating in-place using `inplace=True`."
+                        "Replacing an array with another one is inefficient. "
+                        "Consider using different names or populating in-place using `inplace=True`."
                     )
                     del self.file[key]
                     self.file.create_dataset(key, data=value, **self.kwargs)
         return self
 
     def set(
         self,
@@ -790,14 +819,15 @@
                 # f_src.copy(f_src[key],  f_dest[key], "DataSet")
             # create a non-recursive copy and update the file
             # this way, we can keep the batch-size of every nested tensordict
             clone = self.clone(False)
             clone.file = f_src
             clone.filename = newfile
             clone._pin_mem = False
+            clone.names = self._names
             return clone
         else:
             # we need to keep the batch-size of nested tds, which we do manually
             nested_tds = {
                 key: td.clone(False) for key, td in self._nested_tensordicts.items()
             }
             filename = self.filename
@@ -808,14 +838,15 @@
                 mode=self.mode,
                 backend="h5",
                 device=self.device,
                 batch_size=self.batch_size,
             )
             clone._nested_tensordicts = nested_tds
             clone._pin_mem = False
+            clone.names = self._names
             return clone
 
     def __getstate__(self):
         state = self.__dict__.copy()
         filename = state["file"].file.filename
         group_name = state["file"].name
         state["file"] = None
```

## tensordict/tensorclass.py

```diff
@@ -259,19 +259,19 @@
             raise TypeError(
                 f"{self.__class__.__name__}.__init__() missing {n_missing} "
                 f"required positional argument{'' if n_missing == 1 else 's'}: "
                 f"""{", ".join(f"'{name}'" for name in missing_params)}"""
             )
 
         self._tensordict = TensorDict(
-            {}, batch_size=batch_size, device=device, _run_checks=False
+            {}, batch_size=torch.Size(batch_size), device=device, _run_checks=False
         )
         # To save non tensor data (Nested tensor classes also go here)
         self._non_tensordict = {}
-        init(self, **{key: value for key, value in kwargs.items()})
+        init(self, **kwargs)
 
     new_params = [
         inspect.Parameter("batch_size", inspect.Parameter.KEYWORD_ONLY),
         inspect.Parameter("device", inspect.Parameter.KEYWORD_ONLY, default=None),
     ]
     wrapper.__signature__ = init_sig.replace(parameters=params + new_params)
```

## tensordict/tensordict.py

```diff
@@ -356,14 +356,160 @@
 
         Returns:
             a torch.Size object describing the TensorDict batch size.
 
         """
         raise NotImplementedError
 
+    @property
+    def names(self):
+        names = self._names
+        if names is None:
+            return [None for _ in range(self.batch_dims)]
+        return names
+
+    @names.setter
+    def names(self, value):
+        # we don't run checks on types for efficiency purposes
+        if value is None:
+            self._names = None
+            return
+        num_none = sum(v is None for v in value)
+        if num_none:
+            num_none -= 1
+        if len(set(value)) != len(value) - num_none:
+            raise ValueError(f"Some dimension names are non-unique: {value}.")
+        if len(value) != self.batch_dims:
+            raise ValueError(
+                "the length of the dimension names must equate the tensordict batch_dims attribute. "
+                f"Got {value} for batch_dims {self.batch_dims}."
+            )
+        self._rename_subtds(value)
+        self._names = list(value)
+
+    @abc.abstractmethod
+    def _rename_subtds(self, value):
+        # renames all the sub-tensordicts dimension according to value.
+        # If value has less dimensions than the TD, the rest is just assumed to be None
+        raise NotImplementedError
+
+    def _check_dim_name(self, name):
+        if name is None:
+            return False
+        if self._names is not None and name in self._names:
+            return True
+        for key in self.keys():
+            if is_tensor_collection(self.entry_class(key)):
+                if self.get(key)._check_dim_name(name):
+                    return True
+        else:
+            return False
+
+    def refine_names(self, *names):
+        """Refines the dimension names of self according to names.
+
+        Refining is a special case of renaming that “lifts” unnamed dimensions.
+        A None dim can be refined to have any name; a named dim can only be
+        refined to have the same name.
+
+        Because named tensors can coexist with unnamed tensors, refining names
+        gives a nice way to write named-tensor-aware code that works with both
+        named and unnamed tensors.
+
+        names may contain up to one Ellipsis (...). The Ellipsis is expanded
+        greedily; it is expanded in-place to fill names to the same length as
+        self.dim() using names from the corresponding indices of self.names.
+
+        Returns: the tensordict with dimensions named accordingly.
+
+        """
+        # replace ellipsis if any
+        names_copy = copy(names)
+        if any(name is Ellipsis for name in names):
+            ellipsis_name = [NO_DEFAULT for _ in range(self.ndim - len(names) + 1)]
+            names = []
+            for name in names_copy:
+                if name is Ellipsis:
+                    names += ellipsis_name
+                else:
+                    names.append(name)
+        # check that the names that are set are either None or identical
+        curr_names = self.names
+        for i, name in enumerate(names):
+            if name is NO_DEFAULT:
+                # whatever value is ok
+                names[i] = curr_names[i]
+                continue
+            else:
+                if curr_names[i] is None:
+                    continue
+                if self.names[i] == name:
+                    continue
+                else:
+                    raise RuntimeError(
+                        f"refine_names: cannot coerce TensorDict names {self.names} with {names_copy}."
+                    )
+        self.names = names
+        # we also need to rename the sub-tensordicts
+        self._rename_subtds(self.names)
+        return self
+
+    def rename(self, *names, **rename_map):
+        clone = self.clone(recurse=False)
+        if len(names) == 1 and names[0] is None:
+            clone.names = None
+        if rename_map and names:
+            raise ValueError(
+                "Passed both a name map and a name list. Only one is accepted."
+            )
+        elif not rename_map and not names:
+            raise ValueError(
+                "Neither a name map nor a name list was passed. "
+                "Only one is accepted."
+            )
+        elif rename_map:
+            for i, name in enumerate(clone.names):
+                new_name = rename_map.pop(name, NO_DEFAULT)
+                if new_name is not NO_DEFAULT:
+                    clone._names[i] = new_name
+            if rename_map:
+                raise ValueError(
+                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
+                )
+        else:
+            clone.names = names
+        return clone
+
+    def rename_(self, *names, **rename_map):
+        if len(names) == 1 and names[0] is None:
+            self.names = None
+        if rename_map and names:
+            raise ValueError(
+                "Passed both a name map and a name list. " "Only one is accepted."
+            )
+        elif not rename_map and not names and self.batch_dims:
+            raise ValueError(
+                "Neither a name map nor a name list was passed. "
+                "Only one is accepted."
+            )
+        elif rename_map:
+            _names = copy(self.names)
+            for i, name in enumerate(_names):
+                new_name = rename_map.pop(name, NO_DEFAULT)
+                if new_name is not NO_DEFAULT:
+                    _names[i] = new_name
+            if rename_map:
+                raise ValueError(
+                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
+                )
+            self.names = _names
+        else:
+            self.names = names
+        return self
+
     def size(self, dim: int | None = None) -> torch.Size | int:
         """Returns the size of the dimension indicated by :obj:`dim`.
 
         If dim is not specified, returns the batch_size (or shape) of the TensorDict.
 
         """
         if dim is None:
@@ -380,27 +526,32 @@
         if self._lazy:
             raise RuntimeError(
                 "modifying the batch size of a lazy repesentation of a "
                 "tensordict is not permitted. Consider instantiating the "
                 "tensordict first by calling `td = td.to_tensordict()` before "
                 "resetting the batch size."
             )
-        if self.batch_size == new_batch_size:
-            return
         if not isinstance(new_batch_size, torch.Size):
             new_batch_size = torch.Size(new_batch_size)
         for key in self.keys():
             if is_tensor_collection(self.entry_class(key)):
                 tensordict = self.get(key)
                 if len(tensordict.batch_size) < len(new_batch_size):
                     # document as edge case
                     tensordict.batch_size = new_batch_size
                     self._set(key, tensordict)
         self._check_new_batch_size(new_batch_size)
         self._change_batch_size(new_batch_size)
+        if self._names is not None:
+            if len(self._names) < len(new_batch_size):
+                self.names = self._names + [None] * (
+                    len(new_batch_size) - len(self._names)
+                )
+            else:
+                self.names = self._names[: self.batch_dims]
 
     @property
     def batch_dims(self) -> int:
         """Length of the tensordict batch size.
 
         Returns:
             int describing the number of dimensions of the tensordict.
@@ -1104,24 +1255,25 @@
             >>> assert (td_2["b", "c"] == 2).all()
         """
         if inplace:
             out = self
         elif batch_size is not None:
             out = TensorDict(
                 {},
-                batch_size=batch_size,
+                batch_size=torch.Size(batch_size),
                 device=self.device,
                 _run_checks=False,
                 **constructor_kwargs,
             )
         else:
             out = TensorDict(
                 {},
                 batch_size=self.batch_size,
                 device=self.device,
+                names=self._names,
                 _run_checks=False,
                 **constructor_kwargs,
             )
 
         is_locked = out.is_locked
         if not inplace and is_locked:
             out.unlock_()
@@ -1146,14 +1298,29 @@
                 else:
                     out._set(key, item_trsf, inplace=inplace)
 
         if not inplace and is_locked:
             out.lock_()
         return out
 
+    def as_tensor(self):
+        """Calls as_tensor on all the tensors contained in the object.
+
+        This is reserved to classes that contain exclusively MemmapTensors,
+        and will raise an exception in all other cases.
+
+        """
+        try:
+            return self.apply(lambda x: x.as_tensor())
+        except AttributeError as err:
+            raise AttributeError(
+                f"{self.__class__.__name__} does not have an 'as_tensor' method "
+                f"because at least one of its tensors does not support this method."
+            ) from err
+
     def update(
         self,
         input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
         clone: bool = False,
         inplace: bool = False,
     ) -> TensorDictBase:
         """Updates the TensorDict with values from either a dictionary or another TensorDict.
@@ -1282,14 +1449,16 @@
                 )
             if clone:
                 value = value.clone()
             self.set_at_(key, value, idx)
         return self
 
     def _convert_to_tensor(self, array: np.ndarray) -> Tensor | MemmapTensor:
+        if isinstance(array, np.bool_):
+            array = array.item()
         return torch.as_tensor(array, device=self.device)
 
     def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> TensorDictBase:
         return TensorDict(
             dict_value,
             batch_size=self.batch_size,
             device=self.device,
@@ -1333,14 +1502,28 @@
                 value.batch_size = self.batch_size
             else:
                 raise RuntimeError(
                     f"batch dimension mismatch, got self.batch_size"
                     f"={self.batch_size} and value.shape[:self.batch_dims]"
                     f"={_shape(value)[: self.batch_dims]} with value {value}"
                 )
+        if (
+            self._names is not None
+            and is_tensor_collection(value)
+            and check_shape
+            and value.names[: self.ndim] != self.names
+        ):
+            value = value.clone(False).refine_names(*self.names)
+        elif (
+            self._names is None
+            and check_shape
+            and is_tensor_collection(value)
+            and value._names is not None
+        ):
+            self.names = value.names[: self.batch_dims]
 
         return value
 
     @abc.abstractmethod
     def pin_memory(self) -> TensorDictBase:
         """Calls :obj:`pin_memory` on the stored tensors."""
         raise NotImplementedError(f"{self.__class__.__name__}")
@@ -1428,19 +1611,125 @@
             last_n_dims = tensor_dims - tensordict_dims
             if last_n_dims > 0:
                 d[key] = value.expand((*shape, *value.shape[-last_n_dims:]))
             else:
                 d[key] = value.expand(shape)
         return TensorDict(
             source=d,
-            batch_size=shape,
+            batch_size=torch.Size(shape),
             device=self.device,
             _run_checks=False,
         )
 
+    def flatten(self, start_dim=0, end_dim=-1):
+        """Flattens all the tensors of a tensordict.
+
+        Args:
+            start_dim (int) – the first dim to flatten
+            end_dim (int) – the last dim to flatten
+
+        Examples:
+            >>> td = TensorDict({"a": torch.arange(60).view(3, 4, 5), "b": torch.arange(12).view(3, 4)}, [3, 4])
+            >>> td_flat = td.flatten(0, 1)
+            >>> td_flat.batch_size
+            torch.Size([12])
+            >>> td_flat["a"]
+            tensor([[ 0,  1,  2,  3,  4],
+                    [ 5,  6,  7,  8,  9],
+                    [10, 11, 12, 13, 14],
+                    [15, 16, 17, 18, 19],
+                    [20, 21, 22, 23, 24],
+                    [25, 26, 27, 28, 29],
+                    [30, 31, 32, 33, 34],
+                    [35, 36, 37, 38, 39],
+                    [40, 41, 42, 43, 44],
+                    [45, 46, 47, 48, 49],
+                    [50, 51, 52, 53, 54],
+                    [55, 56, 57, 58, 59]])
+            >>> td_flat["b"]
+            tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
+
+        """
+        if end_dim < 0:
+            end_dim = self.ndim + end_dim
+            if end_dim < 0:
+                raise ValueError(
+                    f"Incompatible end_dim {end_dim} for tensordict with shape {self.shape}."
+                )
+        if end_dim <= start_dim:
+            raise ValueError(
+                "The end dimension must be strictly greater than the start dim."
+            )
+
+        def flatten(tensor):
+            return torch.flatten(tensor, start_dim, end_dim)
+
+        nelt = prod(self.batch_size[start_dim : end_dim + 1])
+        if start_dim > 0:
+            batch_size = (
+                list(self.batch_size)[:start_dim]
+                + [nelt]
+                + list(self.batch_size[end_dim + 1 :])
+            )
+        else:
+            batch_size = [nelt] + list(self.batch_size[end_dim + 1 :])
+        out = self.apply(flatten, batch_size=batch_size)
+        if self._names is not None:
+            names = [
+                name
+                for i, name in enumerate(self._names)
+                if (i < start_dim or i > end_dim)
+            ]
+            names.insert(start_dim, None)
+            out.names = names
+        return out
+
+    def unflatten(self, dim, unflattened_size):
+        """Unflattens a tensordict dim expanding it to a desired shape.
+
+        Args:
+            dim (int): specifies the dimension of the input tensor to be unflattened.
+            unflattened_size (shape): is the new shape of the unflattened dimension of the tensordict.
+
+        Examples:
+            >>> td = TensorDict({"a": torch.arange(60).view(3, 4, 5), "b": torch.arange(12).view(3, 4)}, [3, 4])
+            >>> td_flat = td.flatten(0, 1)
+            >>> td_unflat = td_flat.unflatten(0, [3, 4])
+            >>> assert (td == td_unflat).all()
+        """
+        if dim < 0:
+            dim = self.ndim + dim
+            if dim < 0:
+                raise ValueError(
+                    f"Incompatible dim {dim} for tensordict with shape {self.shape}."
+                )
+
+        def unflatten(tensor):
+            return torch.unflatten(
+                tensor,
+                dim,
+                unflattened_size,
+            )
+
+        if dim > 0:
+            batch_size = (
+                list(self.batch_size)[:dim]
+                + list(unflattened_size)
+                + list(self.batch_size[dim + 1 :])
+            )
+        else:
+            batch_size = list(unflattened_size) + list(self.batch_size[1:])
+        out = self.apply(unflatten, batch_size=batch_size)
+        if self._names is not None:
+            names = copy(self._names)
+            for _ in range(len(unflattened_size) - 1):
+                names.insert(dim, None)
+            out.names = names
+        return out
+
     def __bool__(self) -> bool:
         raise ValueError("Converting a tensordict to boolean value is not permitted")
 
     def __ne__(self, other: object) -> TensorDictBase:
         """XOR operation over two tensordicts, for evey key.
 
         The two tensordicts must have the same key set.
@@ -1673,15 +1962,17 @@
                 prefix / "meta.pt",
             )
         if not self.keys():
             raise Exception(
                 "memmap_like() must be called when the TensorDict is (partially) "
                 "populated. Set a tensor first."
             )
-        tensordict = TensorDict({}, self.batch_size, device=self.device)
+        tensordict = TensorDict(
+            {}, self.batch_size, device=self.device, names=self._names
+        )
         for key, value in self.items():
             if is_tensor_collection(value):
                 if prefix is not None:
                     # ensure subdirectory exists
                     (prefix / key).mkdir(exist_ok=True)
                     tensordict[key] = value.memmap_like(
                         prefix=prefix / key,
@@ -1730,14 +2021,15 @@
             a new tensordict with no tensor requiring gradient.
 
         """
         return TensorDict(
             {key: item.detach() for key, item in self.items()},
             batch_size=self.batch_size,
             device=self.device,
+            names=self._names,
             _run_checks=False,
         )
 
     def to_h5(
         self,
         filename,
         **kwargs,
@@ -1780,19 +2072,22 @@
                 device=None,
                 is_shared=False)
 
 
         """
         from .persistent import PersistentTensorDict
 
-        return PersistentTensorDict.from_dict(
+        out = PersistentTensorDict.from_dict(
             self,
             filename=filename,
             **kwargs,
         )
+        if self._names is not None:
+            out.names = self._names
+        return out
 
     def to_tensordict(self):
         """Returns a regular TensorDict instance from the TensorDictBase.
 
         Returns:
             a new TensorDict object containing the same values.
 
@@ -1802,14 +2097,15 @@
                 key: value.clone()
                 if not is_tensor_collection(value)
                 else value.to_tensordict()
                 for key, value in self.items()
             },
             device=self.device,
             batch_size=self.batch_size,
+            names=self._names,
         )
 
     def zero_(self) -> TensorDictBase:
         """Zeros all tensors in the tensordict in-place."""
         for key in self.keys():
             self.fill_(key, 0)
         return self
@@ -1823,22 +2119,29 @@
         idx = [
             ((*tuple(slice(None) for _ in range(dim)), i))
             for i in range(self.shape[dim])
         ]
         if dim < 0:
             dim = self.batch_dims + dim
         batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
+        names = self._names
+        if names is not None:
+            names = copy(names)
+            names = [name for i, name in enumerate(names) if i != dim]
         out = []
         for _idx in idx:
             out.append(
                 self.apply(
                     lambda tensor, idx=_idx: tensor[idx],
                     batch_size=batch_size,
                 )
             )
+            if names is not None:
+                for item in out:
+                    item.names = names
             if self.is_shared():
                 out[-1].share_memory_()
             elif self.is_memmap():
                 out[-1].memmap_()
         return tuple(out)
 
     def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
@@ -1878,14 +2181,15 @@
                 TensorDict will be copied too. Default is `True`.
 
         """
         return TensorDict(
             source={key: _clone_value(value, recurse) for key, value in self.items()},
             batch_size=self.batch_size,
             device=self.device,
+            names=copy(self._names),
             _run_checks=False,
             _is_shared=self.is_shared() if not recurse else False,
             _is_memmap=self.is_memmap() if not recurse else False,
         )
 
     @classmethod
     def __torch_function__(
@@ -2013,15 +2317,18 @@
             while mask.ndimension() > self.batch_dims:
                 mask_expand = mask.squeeze(-1)
             else:
                 mask_expand = mask
             value_select = value[mask_expand]
             d[key] = value_select
         dim = int(mask.sum().item())
-        return TensorDict(device=self.device, source=d, batch_size=torch.Size([dim]))
+        other_dim = self.shape[mask.ndim :]
+        return TensorDict(
+            device=self.device, source=d, batch_size=torch.Size([dim, *other_dim])
+        )
 
     @abc.abstractmethod
     def is_contiguous(self) -> bool:
         """Returns a boolean indicating if all the tensors are contiguous."""
         raise NotImplementedError
 
     @abc.abstractmethod
@@ -2130,15 +2437,15 @@
         if d:
             batch_size = d[key].shape[: len(shape)]
         else:
             if any(not isinstance(i, int) or i < 0 for i in shape):
                 raise RuntimeError(
                     "Implicit reshaping is not permitted with empty " "tensordicts"
                 )
-            batch_size = shape
+            batch_size = torch.Size(shape)
         return TensorDict(d, batch_size, device=self.device, _run_checks=False)
 
     def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
         """Splits each tensor in the TensorDict with the specified size in the given dimension, like `torch.split`.
 
         Returns a list of TensorDict with the view of split chunks of items. Nested TensorDicts will remain nested.
 
@@ -2159,54 +2466,64 @@
             raise IndexError(
                 f"Dimension out of range (expected to be in range of [-{self.batch_dims}, {self.batch_dims - 1}], but got {dim})"
             )
         if dim < 0:
             dim += self.batch_dims
         if isinstance(split_size, int):
             rep, remainder = divmod(self.batch_size[dim], split_size)
-            rep_shape = [
-                split_size if idx == dim else size
-                for (idx, size) in enumerate(self.batch_size)
-            ]
+            rep_shape = torch.Size(
+                [
+                    split_size if idx == dim else size
+                    for (idx, size) in enumerate(self.batch_size)
+                ]
+            )
             batch_sizes = [rep_shape for _ in range(rep)]
             if remainder:
                 batch_sizes.append(
-                    [
-                        remainder if dim_idx == dim else dim_size
-                        for (dim_idx, dim_size) in enumerate(self.batch_size)
-                    ]
+                    torch.Size(
+                        [
+                            remainder if dim_idx == dim else dim_size
+                            for (dim_idx, dim_size) in enumerate(self.batch_size)
+                        ]
+                    )
                 )
         elif isinstance(split_size, list) and all(
             isinstance(element, int) for element in split_size
         ):
             if sum(split_size) != self.batch_size[dim]:
                 raise RuntimeError(
                     f"Split method expects split_size to sum exactly to {self.batch_size[dim]} (tensor's size at dimension {dim}), but got split_size={split_size}"
                 )
             for i in split_size:
                 batch_sizes.append(
-                    [
-                        i if dim_idx == dim else dim_size
-                        for (dim_idx, dim_size) in enumerate(self.batch_size)
-                    ]
+                    torch.Size(
+                        [
+                            i if dim_idx == dim else dim_size
+                            for (dim_idx, dim_size) in enumerate(self.batch_size)
+                        ]
+                    )
                 )
         else:
             raise TypeError(
                 "split(): argument 'split_size' must be int or list of ints"
             )
         dictionaries = [{} for _ in range(len(batch_sizes))]
         for key, item in self.items():
             split_tensors = torch.split(item, split_size, dim)
             for idx, split_tensor in enumerate(split_tensors):
                 dictionaries[idx][key] = split_tensor
+        names = self._names
+        if names is not None:
+            names = copy(names)
         return [
             TensorDict(
                 dictionaries[i],
                 batch_sizes[i],
                 device=self.device,
+                names=names,
                 _run_checks=False,
                 _is_shared=self.is_shared(),
                 _is_memmap=self.is_memmap(),
             )
             for i in range(len(dictionaries))
         ]
 
@@ -2241,14 +2558,21 @@
                         batch_size=torch.Size([3, 2, 5]),
                         device=None,
                         is_shared=False)},
                 batch_size=torch.Size([3, 2]),
                 device=None,
                 is_shared=False)
 
+        Gather keeps the dimension names.
+
+        Examples:
+            >>> td.names = ["a", "b"]
+            >>> td_gather = td.gather(dim=1, index=index)
+            >>> td_gather.names
+            ["a", "b"]
         """
         return torch.gather(self, dim, index, out=out)
 
     def view(
         self,
         *shape: int,
         size: list | tuple | torch.Size | None = None,
@@ -2391,18 +2715,25 @@
             raise RuntimeError(
                 "dim must be greater than or equal to -tensordict.batch_dims and "
                 "smaller than tensordict.batch_dims"
             )
         if dim is not None:
             if dim < 0:
                 dim = self.batch_dims + dim
+
+            names = self._names
+            if names is not None:
+                names = copy(names)
+                names = [name for i, name in enumerate(names) if i != dim]
+
             return TensorDict(
                 source={key: value.all(dim=dim) for key, value in self.items()},
                 batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
                 device=self.device,
+                names=names,
             )
         return all(value.all() for value in self.values())
 
     def any(self, dim: int = None) -> bool | TensorDictBase:
         """Checks if any value is True/non-null in the tensordict.
 
         Args:
@@ -2417,18 +2748,25 @@
             raise RuntimeError(
                 "dim must be greater than or equal to -tensordict.batch_dims and "
                 "smaller than tensordict.batch_dims"
             )
         if dim is not None:
             if dim < 0:
                 dim = self.batch_dims + dim
+
+            names = self._names
+            if names is not None:
+                names = copy(names)
+                names = [name for i, name in enumerate(names) if i != dim]
+
             return TensorDict(
                 source={key: value.any(dim=dim) for key, value in self.items()},
                 batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
                 device=self.device,
+                names=names,
             )
         return any([value.any() for value in self.values()])
 
     def get_sub_tensordict(self, idx: IndexType) -> TensorDictBase:
         """Returns a SubTensorDict with the desired index."""
         return SubTensorDict(source=self, idx=idx)
 
@@ -2543,19 +2881,64 @@
         # direct the user to use TensorDict.keys() instead
         raise NotImplementedError(
             "TensorDict does not support membership checks with the `in` keyword. If "
             "you want to check if a particular key is in your TensorDict, please use "
             "`key in tensordict.keys()` instead."
         )
 
+    def _get_names_idx(self, idx):
+        if self._names is None:
+            names = None
+        else:
+
+            def is_boolean(idx):
+                if isinstance(idx, tuple) and len(idx) == 1:
+                    return is_boolean(idx[0])
+                if hasattr(idx, "dtype") and idx.dtype is torch.bool:
+                    return idx.ndim
+                return None
+
+            num_boolean_dim = is_boolean(idx)
+            if num_boolean_dim:
+                names = [None] + self._names[num_boolean_dim:]
+            else:
+
+                def is_int(subidx):
+                    if isinstance(subidx, Number):
+                        return True
+                    if isinstance(subidx, Tensor) and len(subidx.shape) == 0:
+                        return True
+                    return False
+
+                if not isinstance(idx, tuple):
+                    idx = (idx,)
+                if len(idx) < self.ndim:
+                    idx = (*idx, Ellipsis)
+                idx_names = convert_ellipsis_to_idx(idx, self.batch_size)
+                # this will convert a [None, :, :, 0, None, 0] in [None, 0, 1, None, 3]
+                count = 0
+                idx_to_take = []
+                for _idx in idx_names:
+                    if _idx is None:
+                        idx_to_take.append(None)
+                    elif _is_number(_idx):
+                        count += 1
+                    else:
+                        idx_to_take.append(count)
+                        count += 1
+                names = [self._names[i] if i is not None else None for i in idx_to_take]
+        return names
+
     def _index_tensordict(self, idx: IndexType) -> TensorDictBase:
+        names = self._get_names_idx(idx)
         return TensorDict(
             source={key: _get_item(item, idx) for key, item in self.items()},
             batch_size=_getitem_batch_size(self.batch_size, idx),
             device=self.device,
+            names=names,
             _run_checks=False,
             _is_shared=self.is_shared(),
             _is_memmap=self.is_memmap(),
         )
 
     def __getitem__(self, idx: IndexType) -> TensorDictBase:
         """Indexes all tensors according to the provided index.
@@ -2573,25 +2956,27 @@
             >>> subtd.set("a", torch.ones(1,4,5))
             >>> print(td.get("a"))  # first row is full of 1
             >>> # Warning: this will not work as expected
             >>> subtd.get("a")[:] = 2.0
             >>> print(td.get("a"))  # values have not changed
 
         """
+        if isinstance(idx, tuple) and len(idx) == 1:
+            idx = idx[0]
         if isinstance(idx, str) or (
             isinstance(idx, tuple) and all(isinstance(sub_idx, str) for sub_idx in idx)
         ):
             return self.get(idx)
 
         if not self.batch_size:
             raise RuntimeError(
                 "indexing a tensordict with td.batch_dims==0 is not permitted"
             )
 
-        if isinstance(idx, Number):
+        if _is_number(idx):
             return self._index_tensordict((idx,))
 
         if isinstance(idx, list):
             idx = torch.tensor(idx, device=self.device)
             return self._index_tensordict(idx)
 
         if isinstance(idx, np.ndarray):
@@ -2868,14 +3253,18 @@
         batch_size (iterable of int, optional): a batch size for the
             tensordict. The batch size is immutable and can only be modified
             by calling operations that create a new TensorDict. Unless the
             source is another TensorDict, the batch_size argument must be
             provided as it won't be inferred from the data.
         device (torch.device or compatible type, optional): a device for the
             TensorDict.
+        names (lsit of str, optional): the names of the dimensions of the
+            tensordict. If provided, its length must match the one of the
+            ``batch_size``. Defaults to ``None`` (no dimension name, or ``None``
+            for every dimension).
 
     Examples:
         >>> import torch
         >>> from tensordict import TensorDict
         >>> source = {'random': torch.randn(3, 4),
         ...     'zeros': torch.zeros(3, 4, 5)}
         >>> batch_size = [3]
@@ -2890,60 +3279,75 @@
         >>> print(td_unqueeze.view(-1).shape)
         torch.Size([3])
         >>> print((td.clone()==td).all())
         True
 
     """
 
+    __slots__ = (
+        "_tensordict",
+        "_batch_size",
+        "_is_shared",
+        "_is_memmap",
+        "_device",
+        "_is_locked",
+        "_names",
+    )
+
     def __new__(cls, *args: Any, **kwargs: Any) -> TensorDict:
         cls._is_shared = False
         cls._is_memmap = False
+        cls._names = None
         return super().__new__(cls, *args, _safe=True, _lazy=False, **kwargs)
 
     def __init__(
         self,
         source: TensorDictBase | dict[str, CompatibleType],
         batch_size: Sequence[int] | torch.Size | int | None = None,
         device: DeviceType | None = None,
+        names: Sequence[str] | None = None,
         _run_checks: bool = True,
         _is_shared: bool | None = False,
         _is_memmap: bool | None = False,
     ) -> None:
         self._is_shared = _is_shared
         self._is_memmap = _is_memmap
         if device is not None:
             device = torch.device(device)
         self._device = device
 
         if not _run_checks:
             self._tensordict: dict = dict(source)
-            self._batch_size = torch.Size(batch_size)
+            self._batch_size = batch_size
             upd_dict = {}
             for key, value in self._tensordict.items():
                 if isinstance(value, dict):
                     value = TensorDict(
                         value,
                         batch_size=self._batch_size,
                         device=self._device,
                         _run_checks=_run_checks,
                         _is_shared=_is_shared,
                         _is_memmap=_is_memmap,
                     )
                     upd_dict[key] = value
             if upd_dict:
                 self._tensordict.update(upd_dict)
+            self._names = names
         else:
             self._tensordict = {}
             if not isinstance(source, (TensorDictBase, dict)):
                 raise ValueError(
                     "A TensorDict source is expected to be a TensorDictBase "
                     f"sub-type or a dictionary, found type(source)={type(source)}."
                 )
             self._batch_size = self._parse_batch_size(source, batch_size)
 
+            self.names = names
+
             if source is not None:
                 for key, value in source.items():
                     self.set(key, value)
 
     @classmethod
     def from_dict(cls, input_dict, batch_size=None, device=None):
         """Returns a TensorDict created from a dictionary or another :class:`TensorDict`.
@@ -3019,36 +3423,46 @@
         return out
 
     @staticmethod
     def _parse_batch_size(
         source: TensorDictBase | dict,
         batch_size: Sequence[int] | torch.Size | int | None = None,
     ) -> torch.Size:
-        if isinstance(batch_size, Sequence):
+        try:
             return torch.Size(batch_size)
-        elif isinstance(batch_size, Number):
-            return torch.Size([batch_size])
-        elif isinstance(source, TensorDictBase):
-            return source.batch_size
-        raise ValueError(
-            "batch size was not specified when creating the TensorDict "
-            "instance and it could not be retrieved from source."
-        )
+        except Exception as err:
+            if isinstance(batch_size, Number):
+                return torch.Size([batch_size])
+            elif isinstance(source, TensorDictBase):
+                return source.batch_size
+            raise ValueError(
+                "batch size was not specified when creating the TensorDict "
+                "instance and it could not be retrieved from source."
+            ) from err
 
     @property
     def batch_dims(self) -> int:
         return len(self.batch_size)
 
     @batch_dims.setter
     def batch_dims(self, value: int) -> None:
         raise RuntimeError(
             f"Setting batch dims on {self.__class__.__name__} instances is "
             f"not allowed."
         )
 
+    def _rename_subtds(self, names):
+        if names is None:
+            names = [None] * self.ndim
+        for item in self._tensordict.values():
+            if is_tensor_collection(item):
+                item_names = item.names
+                td_names = list(names) + item_names[len(names) :]
+                item.rename_(*td_names)
+
     @property
     def device(self) -> torch.device | None:
         """Device of the tensordict.
 
         Returns `None` if device hasn't been provided in the constructor or set via `tensordict.to(device)`.
 
         """
@@ -3111,20 +3525,23 @@
             raise RuntimeError(
                 f"TensorDict.device is {self._device}, but elements have "
                 f"device values {devices}. If TensorDict.device is set then "
                 "all elements must share that device."
             )
 
     def _index_tensordict(self, idx: IndexType) -> TensorDictBase:
+        names = self._get_names_idx(idx)
         self_copy = copy(self)
+        # self_copy = self.clone(False)
         self_copy._tensordict = {
             key: _get_item(item, idx) for key, item in self.items()
         }
         self_copy._batch_size = _getitem_batch_size(self_copy.batch_size, idx)
         self_copy._device = self.device
+        self_copy.names = names
         return self_copy
 
     def pin_memory(self) -> TensorDictBase:
         def pin_mem(tensor):
             return tensor.pin_memory()
 
         return self.apply(pin_mem)
@@ -3163,20 +3580,23 @@
         for key, value in self.items():
             tensor_dims = len(value.shape)
             last_n_dims = tensor_dims - tensordict_dims
             if last_n_dims > 0:
                 d[key] = value.expand(*shape, *value.shape[-last_n_dims:])
             else:
                 d[key] = value.expand(*shape)
-        return TensorDict(
+        out = TensorDict(
             source=d,
-            batch_size=[*shape],
+            batch_size=torch.Size(shape),
             device=self.device,
             _run_checks=False,
         )
+        if self._names is not None:
+            out.refine_names(..., *self.names)
+        return out
 
     def _set(self, key: str, value, inplace: bool = False) -> TensorDictBase:
         if isinstance(key, tuple):
             td, subkey = _get_leaf_tensordict(
                 self, key, _default_hook if not inplace else None
             )
         else:
@@ -3187,16 +3607,15 @@
             except KeyError as err:
                 raise err
             except Exception as err:
                 raise ValueError(
                     f"Failed to update '{subkey}' in tensordict {td}"
                 ) from err
         else:
-            if td._tensordict.get(subkey, None) is not value:
-                td._tensordict[subkey] = value
+            td._tensordict[subkey] = value
 
         return self
 
     def set(
         self,
         key: NestedKey,
         value: dict[str, CompatibleType] | CompatibleType,
@@ -3509,25 +3928,28 @@
         return out
 
     def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             td = dest(source=self, **kwargs)
+            if self._names is not None:
+                td.names = self._names
             return td
         elif isinstance(dest, (torch.device, str, int)):
             # must be device
             dest = torch.device(dest)
             if self.device is not None and dest == self.device:
                 return self
 
             self_copy = TensorDict(
                 {key: value.to(dest, **kwargs) for key, value in self.items()},
                 batch_size=self.batch_size,
                 device=dest,
+                names=self._names,
             )
             return self_copy
         elif isinstance(dest, torch.Size):
             self.batch_size = dest
             return self
         else:
             raise NotImplementedError(
@@ -3582,14 +4004,15 @@
                         *val, strict=strict, inplace=inplace
                     )
 
         out = TensorDict(
             device=self.device,
             batch_size=self.batch_size,
             source=source,
+            names=self._names,
             _run_checks=False,
             _is_memmap=self._is_memmap,
             _is_shared=self._is_shared,
         )
         if inplace:
             self._tensordict = out._tensordict
             return self
@@ -3598,14 +4021,21 @@
     def keys(
         self, include_nested: bool = False, leaves_only: bool = False
     ) -> _TensorDictKeysView:
         return _TensorDictKeysView(
             self, include_nested=include_nested, leaves_only=leaves_only
         )
 
+    def __getstate__(self):
+        return {slot: getattr(self, slot) for slot in self.__slots__}
+
+    def __setstate__(self, state):
+        for slot, value in state.items():
+            setattr(self, slot, value)
+
 
 class _ErrorInteceptor:
     """Context manager for catching errors and modifying message.
 
     Intended for use with stacking / concatenation operations applied to TensorDicts.
 
     """
@@ -3779,17 +4209,21 @@
         target_shape = list(tensor.shape)
         target_shape[dim] = index_expand.shape[dim]
         index_expand = index_expand.expand(target_shape)
         out = torch.gather(tensor, dim, index_expand, out=dest)
         return out
 
     if out is None:
+
+        names = input._names
+
         return TensorDict(
             {key: _gather_tensor(value) for key, value in input.items()},
             batch_size=index.shape,
+            names=names,
         )
     TensorDict(
         {key: _gather_tensor(value, out[key]) for key, value in input.items()},
         batch_size=index.shape,
     )
     return out
 
@@ -3873,21 +4307,18 @@
     list_of_tensordicts: Sequence[TensorDictBase],
     dim: int = 0,
     device: DeviceType | None = None,
     out: TensorDictBase | None = None,
 ) -> TensorDictBase:
     if not list_of_tensordicts:
         raise RuntimeError("list_of_tensordicts cannot be empty")
-    if dim < 0:
-        raise RuntimeError(
-            f"negative dim in torch.dim(list_of_tensordicts, dim=dim) not "
-            f"allowed, got dim={dim}"
-        )
 
     batch_size = list(list_of_tensordicts[0].batch_size)
+    if dim < 0:
+        dim = len(batch_size) + dim
     if dim >= len(batch_size):
         raise RuntimeError(
             f"dim must be in the range 0 <= dim < len(batch_size), got dim"
             f"={dim} and batch_size={batch_size}"
         )
     batch_size[dim] = sum([td.batch_size[dim] for td in list_of_tensordicts])
     batch_size = torch.Size(batch_size)
@@ -3905,15 +4336,18 @@
             device = list_of_tensordicts[0].device
             for td in list_of_tensordicts[1:]:
                 if device == td.device:
                     continue
                 else:
                     device = None
                     break
-        return TensorDict(out, device=device, batch_size=batch_size, _run_checks=False)
+        names = list_of_tensordicts[0]._names
+        return TensorDict(
+            out, device=device, batch_size=batch_size, _run_checks=False, names=names
+        )
     else:
         if out.batch_size != batch_size:
             raise RuntimeError(
                 "out.batch_size and cat batch size must match, "
                 f"got out.batch_size={out.batch_size} and batch_size"
                 f"={batch_size}"
             )
@@ -4089,15 +4523,17 @@
     for i in range(len(pad_size)):
         new_batch_size[i // 2] += pad_size[i]
 
     reverse_pad = pad_size[::-1]
     for i in range(0, len(reverse_pad), 2):
         reverse_pad[i], reverse_pad[i + 1] = reverse_pad[i + 1], reverse_pad[i]
 
-    out = TensorDict({}, new_batch_size, device=tensordict.device, _run_checks=False)
+    out = TensorDict(
+        {}, torch.Size(new_batch_size), device=tensordict.device, _run_checks=False
+    )
     for key, tensor in tensordict.items():
         cur_pad = reverse_pad
         if len(pad_size) < len(_shape(tensor)) * 2:
             cur_pad = [0] * (len(_shape(tensor)) * 2 - len(pad_size)) + reverse_pad
 
         if is_tensor_collection(tensor):
             padded = pad(tensor, pad_size, value)
@@ -4159,15 +4595,17 @@
             len(list_of_tensordicts),
         ]
     elif batch_first:
         shape = [len(list_of_tensordicts), shape]
     else:
         shape = [shape, len(list_of_tensordicts)]
     if out is None:
-        out = TensorDict({}, shape, device=device, _run_checks=False)
+        out = TensorDict(
+            {}, batch_size=torch.Size(shape), device=device, _run_checks=False
+        )
         for key in keys:
             try:
                 out.set(
                     key,
                     torch.nn.utils.rnn.pad_sequence(
                         [td.get(key) for td in list_of_tensordicts],
                         batch_first=batch_first,
@@ -4276,19 +4714,19 @@
                 (
                     tuple,
                     list,
                 ),
             )
             else tuple(idx)
         )
-        # we msut convert ellipsis into slices
-        idx = self._convert_ellipsis(idx, self._source._batch_size)
-        # idx = self._convert_range(idx)
+        self._batch_size = _getitem_batch_size(self._source.batch_size, idx)
+        if any(item is Ellipsis for item in idx):
+            idx = convert_ellipsis_to_idx(idx, self._source.batch_size)
         self.idx = idx
-        self._batch_size = _getitem_batch_size(self._source.batch_size, self.idx)
+
         if batch_size is not None and batch_size != self.batch_size:
             raise RuntimeError("batch_size does not match self.batch_size.")
 
     # @staticmethod
     # def _convert_range(idx):
     #     return tuple(list(_idx) if isinstance(_idx, range) else _idx for _idx in idx)
 
@@ -4314,28 +4752,51 @@
     def exclude(self, *keys: str, inplace: bool = False) -> TensorDictBase:
         if inplace:
             return super().exclude(*keys, inplace=True)
         return TensorDict(
             {key: value for key, value in self.items()},
             batch_size=self.batch_size,
             device=self.device,
+            names=self._names,
             _run_checks=False,
             _is_memmap=self.is_memmap(),
             _is_shared=self.is_shared(),
         ).exclude(*keys, inplace=True)
 
     @property
     def batch_size(self) -> torch.Size:
         return self._batch_size
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
         self._batch_size_setter(new_size)
 
     @property
+    def names(self):
+        names = self._source._get_names_idx(self.idx)
+        if names is None:
+            return [None] * self.batch_dims
+        return names
+
+    @property
+    def _names(self):
+        return self.names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a subtensordict cannot be modified. Instantiate the tensordict first."
+        )
+
+    def _rename_subtds(self, names):
+        for key in self.keys():
+            if is_tensor_collection(self.entry_class(key)):
+                raise RuntimeError("Cannot rename nested sub-tensordict dimensions.")
+
+    @property
     def device(self) -> None | torch.device:
         return self._source.device
 
     @device.setter
     def device(self, value: DeviceType) -> None:
         self._source.device = value
 
@@ -4459,17 +4920,20 @@
         self._source._stack_onto_at_(key, list_item, dim=dim, idx=self.idx)
         return self
 
     def to(self, dest: DeviceType | torch.Size | type, **kwargs: Any) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
-            return dest(
+            out = dest(
                 source=self.clone(),
             )
+            if self._names is not None:
+                out.names = self._names
+            return out
         elif isinstance(dest, (torch.device, str, int)):
             dest = torch.device(dest)
             # try:
             if self.device is not None and dest == self.device:
                 return self
             td = self.to_tensordict().to(dest, **kwargs)
             # must be device
@@ -4633,14 +5097,15 @@
     def contiguous(self) -> TensorDictBase:
         if self.is_contiguous():
             return self
         return TensorDict(
             batch_size=self.batch_size,
             source={key: value for key, value in self.items()},
             device=self.device,
+            names=self.names,
             _run_checks=False,
         )
 
     def select(
         self, *keys: str, inplace: bool = False, strict: bool = True
     ) -> TensorDictBase:
         if inplace:
@@ -4747,14 +5212,15 @@
         torch.Size([3, 10, 4])
         >>> print(td_stack[:, 0] is tds[0])
         True
 
     """
 
     def __new__(cls, *args: Any, **kwargs: Any) -> LazyStackedTensorDict:
+        cls._names = None
         return super().__new__(cls, *args, _safe=False, _lazy=True, **kwargs)
 
     def __init__(
         self,
         *tensordicts: TensorDictBase,
         stack_dim: int = 0,
         batch_size: Sequence[int] | None = None,  # TODO: remove
@@ -4824,14 +5290,46 @@
     def batch_size(self) -> torch.Size:
         return self._batch_size
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
         return self._batch_size_setter(new_size)
 
+    @property
+    def names(self):
+        if self._names is None:
+            names = copy(self.tensordicts[0].names)
+            names.insert(self.stack_dim, None)
+            self._names = names
+        return self._names
+
+    @names.setter
+    def names(self, value):
+        names_c = list(value)
+        if value is None:
+            for td in self.tensordicts:
+                td.names = None
+            self._names = None
+        else:
+            self._names = copy(names_c)
+            name = names_c[self.stack_dim]
+            names_c = list(names_c)
+            del names_c[self.stack_dim]
+            for td in self.tensordicts:
+                if td._check_dim_name(name):
+                    raise ValueError(f"The dimension name {name} is already taken.")
+                td.rename_(*names_c)
+
+    def _rename_subtds(self, names):
+        # remove the name of the stack dim
+        names = list(names)
+        del names[self.stack_dim]
+        for td in self.tensordicts:
+            td.names = names
+
     def is_shared(self) -> bool:
         are_shared = [td.is_shared() for td in self.tensordicts]
         are_shared = [value for value in are_shared if value is not None]
         if not len(are_shared):
             return None
         if any(are_shared) and not all(are_shared):
             raise RuntimeError(
@@ -4889,16 +5387,17 @@
     def set(
         self,
         key: NestedKey,
         tensor: dict[str, CompatibleType] | CompatibleType,
         inplace: bool = False,
     ) -> TensorDictBase:
         key = self._validate_key(key)
-        if self.is_locked:
-            raise RuntimeError(TensorDictBase.LOCK_ERROR)
+        # we don't need this as locked lazy stacks have locked nested tds so the error will be captured in the loop
+        # if self.is_locked:
+        #     raise RuntimeError(TensorDictBase.LOCK_ERROR)
 
         tensor = self._validate_value(tensor)
         for td, _item in zip(self.tensordicts, tensor.unbind(self.stack_dim)):
             td.set(key, _item, inplace=inplace)
 
         first_key = key if (isinstance(key, str)) else key[0]
         if key not in self._valid_keys:
@@ -4927,16 +5426,18 @@
             return tuple(self.tensordicts)
         else:
             return super().unbind(dim)
 
     def set_at_(
         self, key: str, value: dict | CompatibleType, idx: IndexType
     ) -> TensorDictBase:
-        sub_td = self[idx]
-        sub_td.set_(key, value)
+        # this generalizes across all types of indices
+        item = self.get(key)
+        item[idx] = self._validate_value(value, check_shape=False)
+        self.set(key, item, inplace=True)
         return self
 
     def _stack_onto_(
         self,
         key: str,
         list_item: list[CompatibleType],
         dim: int,
@@ -4976,29 +5477,41 @@
             keys = self.valid_keys
 
         if key not in keys:
             return self._default_get(key, default)
 
         tensors = [td.get(key, default=default) for td in self.tensordicts]
         try:
-            return torch.stack(tensors, self.stack_dim)
+            out = torch.stack(tensors, self.stack_dim)
+            if is_tensor_collection(out) and self._names is not None:
+                out.refine_names(*self.names, *out.names[self.ndim :])
+            return out
         except RuntimeError as err:
             if "stack expects each tensor to be equal size" in str(err):
                 shapes = {_shape(tensor) for tensor in tensors}
                 raise RuntimeError(
                     f"Found more than one unique shape in the tensors to be "
                     f"stacked ({shapes}). This is likely due to a modification "
                     f"of one of the stacked TensorDicts, where a key has been "
                     f"updated/created with an uncompatible shape. If the entries "
                     f"are intended to have a different shape, use the get_nestedtensor "
                     f"method instead."
                 )
             else:
                 raise err
 
+    def get_at(self, key, index, default=NO_DEFAULT):
+        item = self.get(key, default=default)
+        if item is default and default is not NO_DEFAULT:
+            return item
+        if isinstance(item, TensorDictBase):
+            return SubTensorDict(item, index)
+        else:
+            return item[index]
+
     def get_nestedtensor(
         self,
         key: NestedKey,
         default: str | CompatibleType = NO_DEFAULT,
     ) -> CompatibleType:
         # disallow getting nested tensor if the stacking dimension is not 0
         if self.stack_dim != 0:
@@ -5039,42 +5552,50 @@
         source = {key: value.contiguous() for key, value in self.items()}
         batch_size = self.batch_size
         device = self.device
         out = TensorDict(
             source=source,
             batch_size=batch_size,
             device=device,
+            names=self.names,
             _run_checks=False,
         )
         return out
 
     def clone(self, recurse: bool = True) -> TensorDictBase:
         if recurse:
             # This could be optimized using copy but we must be careful with
             # metadata (_is_shared etc)
-            return LazyStackedTensorDict(
+            out = LazyStackedTensorDict(
                 *[td.clone() for td in self.tensordicts],
                 stack_dim=self.stack_dim,
             )
-        return LazyStackedTensorDict(
-            *[td.clone(recurse=False) for td in self.tensordicts],
-            stack_dim=self.stack_dim,
-        )
+        else:
+            out = LazyStackedTensorDict(
+                *[td.clone(recurse=False) for td in self.tensordicts],
+                stack_dim=self.stack_dim,
+            )
+        if self._names is not None:
+            out.names = self.names
+        return out
 
     def pin_memory(self) -> TensorDictBase:
         for td in self.tensordicts:
             td.pin_memory()
         return self
 
     def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
             kwargs.update({"batch_size": self.batch_size})
-            return dest(source=self, **kwargs)
+            out = dest(source=self, **kwargs)
+            if self._names is not None:
+                out.names = self._names
+            return out
         elif isinstance(dest, (torch.device, str, int)):
             dest = torch.device(dest)
             if self.device is not None and dest == self.device:
                 return self
             td = self.to_tensordict().to(dest, **kwargs)
             return td
 
@@ -5182,93 +5703,125 @@
         return super().__setitem__(item, value)
 
     def __contains__(self, item: IndexType) -> bool:
         if isinstance(item, TensorDictBase):
             return any(item is td for td in self.tensordicts)
         return super().__contains__(item)
 
-    def __getitem__(self, item: IndexType) -> TensorDictBase:
-        if item is Ellipsis or (isinstance(item, tuple) and Ellipsis in item):
-            item = convert_ellipsis_to_idx(item, self.batch_size)
-        if isinstance(item, tuple) and sum(
-            isinstance(_item, str) for _item in item
+    def __getitem__(self, index: IndexType) -> TensorDictBase:
+        if isinstance(index, tuple) and len(index) == 1:
+            index = index[0]
+        if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
+            index = convert_ellipsis_to_idx(index, self.batch_size)
+        if index is None:
+            return self.unsqueeze(0)
+        if isinstance(index, tuple) and sum(
+            isinstance(_item, str) for _item in index
         ) not in [
-            len(item),
+            len(index),
             0,
         ]:
             raise IndexError(_STR_MIXED_INDEX_ERROR)
-        if isinstance(item, (list, range)):
-            item = torch.tensor(item, device=self.device)
-        if isinstance(item, tuple) and any(
-            isinstance(sub_index, (list, range)) for sub_index in item
+        if isinstance(index, (list, range)):
+            index = torch.tensor(index, device=self.device)
+        if isinstance(index, tuple) and any(
+            isinstance(sub_index, (list, range)) for sub_index in index
         ):
-            item = tuple(
+            index = tuple(
                 torch.tensor(sub_index, device=self.device)
                 if isinstance(sub_index, (list, range))
                 else sub_index
-                for sub_index in item
+                for sub_index in index
             )
-        if isinstance(item, str):
-            return self.get(item)
-        elif isinstance(item, tuple) and all(
-            isinstance(sub_item, str) for sub_item in item
+        if isinstance(index, str):
+            return self.get(index)
+        elif isinstance(index, tuple) and all(
+            isinstance(sub_item, str) for sub_item in index
         ):
-            out = self.get(item[0])
-            if len(item) > 1:
+            out = self.get(index[0])
+            if len(index) > 1:
                 if not isinstance(out, TensorDictBase):
                     raise RuntimeError(
                         f"Got a {type(out)} when a TensorDictBase instance was expected."
                     )
-                return out.get(item[1:])
+                return out.get(index[1:])
             else:
                 return out
-        elif isinstance(item, Tensor) and item.dtype == torch.bool:
-            return self.masked_select(item)
-        elif (
-            isinstance(item, (Number,))
-            or (isinstance(item, Tensor) and item.ndimension() == 0)
-        ) and self.stack_dim == 0:
-            return self.tensordicts[item]
-        elif isinstance(item, (Tensor, list)) and self.stack_dim == 0:
+        elif isinstance(index, Tensor) and index.dtype == torch.bool:
+            return self.masked_select(index)
+        elif _is_number(index) and self.stack_dim == 0:
+            return self.tensordicts[index]
+        elif isinstance(index, (Tensor, list)) and self.stack_dim == 0:
             out = LazyStackedTensorDict(
-                *[self.tensordicts[_item] for _item in item],
+                *[self.tensordicts[_item] for _item in index],
                 stack_dim=self.stack_dim,
             )
             return out
-        elif isinstance(item, (Tensor, list)) and self.stack_dim != 0:
+        elif isinstance(index, (Tensor, list)) and self.stack_dim != 0:
+            tds = [tensordict[index] for tensordict in self.tensordicts]
+            dim_drop = self.tensordicts[0].ndim - tds[0].ndim
             out = LazyStackedTensorDict(
-                *[tensordict[item] for tensordict in self.tensordicts],
-                stack_dim=self.stack_dim,
+                *tds,
+                stack_dim=self.stack_dim - dim_drop,
             )
+            if self._names is not None:
+                out.names = [
+                    name if i != out.stack_dim else self.names[self.stack_dim]
+                    for i, name in enumerate(out.names)
+                ]
             return out
-        elif isinstance(item, slice) and self.stack_dim == 0:
-            return LazyStackedTensorDict(
-                *self.tensordicts[item], stack_dim=self.stack_dim
-            )
-        elif isinstance(item, slice) and self.stack_dim != 0:
-            return LazyStackedTensorDict(
-                *[tensordict[item] for tensordict in self.tensordicts],
+        elif isinstance(index, slice) and self.stack_dim == 0:
+            out = LazyStackedTensorDict(
+                *self.tensordicts[index], stack_dim=self.stack_dim
+            )
+            if self._names is not None:
+                out.names = [
+                    name if i != out.stack_dim else self.names[self.stack_dim]
+                    for i, name in enumerate(out.names)
+                ]
+            return out
+        elif isinstance(index, slice) and self.stack_dim != 0:
+            out = LazyStackedTensorDict(
+                *[tensordict[index] for tensordict in self.tensordicts],
                 stack_dim=self.stack_dim,
             )
-        elif isinstance(item, (slice, Number)):
+            if self._names is not None:
+                out.names = [
+                    name if i != out.stack_dim else self.names[self.stack_dim]
+                    for i, name in enumerate(out.names)
+                ]
+            return out
+        elif isinstance(index, (slice, Number)):
             new_stack_dim = (
-                self.stack_dim - 1 if isinstance(item, Number) else self.stack_dim
+                self.stack_dim - 1 if isinstance(index, Number) else self.stack_dim
             )
-            return LazyStackedTensorDict(
-                *[td[item] for td in self.tensordicts],
+            out = LazyStackedTensorDict(
+                *[td[index] for td in self.tensordicts],
                 stack_dim=new_stack_dim,
             )
-        elif isinstance(item, tuple):
+            if self._names is not None:
+                out.names = [
+                    name if i != out.stack_dim else self.names[self.stack_dim]
+                    for i, name in enumerate(out.names)
+                ]
+            return out
+        elif isinstance(index, tuple):
+            for i, item in enumerate(index):
+                if item is None:
+                    truncated = tuple(
+                        item if j != i else slice(None) for j, item in enumerate(index)
+                    )
+                    return self.unsqueeze(i).__getitem__(truncated)
             # select sub tensordicts
             _sub_item = tuple(
-                _item for i, _item in enumerate(item) if i != self.stack_dim
+                _item for i, _item in enumerate(index) if i != self.stack_dim
             )
 
-            if self.stack_dim < len(item):
-                idx = item[self.stack_dim]
+            if self.stack_dim < len(index):
+                idx = index[self.stack_dim]
                 if isinstance(idx, (Number, slice)):
                     tensordicts = self.tensordicts[idx]
                 elif isinstance(idx, Tensor):
                     tensordicts = [self.tensordicts[i] for i in idx]
                 else:
                     raise TypeError(
                         "Invalid index used for stack dimension. Expected number, "
@@ -5279,22 +5832,38 @@
                         return tensordicts[_sub_item]
                     return tensordicts
             else:
                 tensordicts = self.tensordicts
 
             if len(_sub_item):
                 tensordicts = [td[_sub_item] for td in tensordicts]
+            index_to_stack = []
+            count = 0
+            for item in index:
+                if item is None:
+                    index_to_stack += [item]
+                if count == self.stack_dim:
+                    break
+                count += 1
+                if item is not None:
+                    index_to_stack += [item]
             new_stack_dim = self.stack_dim - sum(
-                [isinstance(_item, Number) for _item in item[: self.stack_dim]]
+                int(_is_number(_item)) - int(_item is None) for _item in index_to_stack
             )
-            return torch.stack(list(tensordicts), dim=new_stack_dim)
+            out = torch.stack(list(tensordicts), dim=new_stack_dim)
+            if self._names is not None:
+                out.names = [
+                    name if i != out.stack_dim else self.names[self.stack_dim]
+                    for i, name in enumerate(out.names)
+                ]
+            return out
         else:
             raise NotImplementedError(
                 f"selecting StackedTensorDicts with type "
-                f"{item.__class__.__name__} is not supported yet"
+                f"{index.__class__.__name__} is not supported yet"
             )
 
     def __eq__(self, other):
         # avoiding circular imports
         from tensordict.tensorclass import is_tensorclass
 
         if is_tensorclass(other):
@@ -5826,14 +6395,23 @@
             )(**self.custom_op_kwargs).shape
         return self._batch_size
 
     @batch_size.setter
     def batch_size(self, new_size: torch.Size) -> None:
         self._batch_size_setter(new_size)
 
+    def _rename_subtds(self, names):
+        for key in self.keys():
+            if is_tensor_collection(self.entry_class(key)):
+                raise RuntimeError(
+                    "Cannot rename dimensions of a lazy TensorDict with "
+                    "nested collections. Convert the instance to a regular "
+                    "tensordict by using the `to_tensordict()` method first."
+                )
+
     def _change_batch_size(self, new_size: torch.Size) -> None:
         if not hasattr(self, "_orig_batch_size"):
             self._orig_batch_size = self.batch_size
         elif self._orig_batch_size == new_size:
             del self._orig_batch_size
         self._batch_size = new_size
 
@@ -5976,14 +6554,15 @@
     def clone(self, recurse: bool = True) -> TensorDictBase:
         if not recurse:
             return copy(self)
         return TensorDict(
             source=self.to_dict(),
             batch_size=self.batch_size,
             device=self.device,
+            names=self._names,
             _run_checks=False,
         )
 
     def is_contiguous(self) -> bool:
         return all([value.is_contiguous() for _, value in self.items()])
 
     def contiguous(self) -> TensorDictBase:
@@ -6003,15 +6582,18 @@
         self._source = self._source.del_(key)
         return self
 
     def to(self, dest: DeviceType | type, **kwargs) -> TensorDictBase:
         if isinstance(dest, type) and issubclass(dest, TensorDictBase):
             if isinstance(self, dest):
                 return self
-            return dest(source=self)
+            out = dest(source=self)
+            if self._names is not None:
+                out.names = self._names
+            return out
         elif isinstance(dest, (torch.device, str, int)):
             if self.device is not None and torch.device(dest) == self.device:
                 return self
             td = self._source.to(dest, **kwargs)
             self_copy = copy(self)
             self_copy._source = td
             return self_copy
@@ -6078,14 +6660,21 @@
 
     def share_memory_(self) -> _CustomOpTensorDict:
         self._source.share_memory_()
         self._is_shared = True
         self.lock_()
         return self
 
+    @property
+    def _names(self):
+        # we also want for _names to be accurate
+        if self._source._names is None:
+            return None
+        return self.names
+
 
 class _UnsqueezedTensorDict(_CustomOpTensorDict):
     """A lazy view on an unsqueezed TensorDict.
 
     When calling `tensordict.unsqueeze(dim)`, a lazy view of this operation is
     returned such that the following code snippet works without raising an
     exception:
@@ -6119,14 +6708,27 @@
         unsqueezed_dim = self.custom_op_kwargs["dim"]
         diff_to_apply = 1 if dim < unsqueezed_dim else 0
         list_item_unsqueeze = [
             item.squeeze(unsqueezed_dim - diff_to_apply) for item in list_item
         ]
         return self._source._stack_onto_(key, list_item_unsqueeze, dim)
 
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim = self.custom_op_kwargs.get("dim")
+        names.insert(dim, None)
+        return names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
 
 class _SqueezedTensorDict(_CustomOpTensorDict):
     """A lazy view on a squeezed TensorDict.
 
     See the `UnsqueezedTensorDict` class documentation for more information.
 
     """
@@ -6153,14 +6755,28 @@
         # dim=2, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 4], [3, 4], ...] => unsq 2
         diff_to_apply = 1 if dim < squeezed_dim else 0
         list_item_unsqueeze = [
             item.unsqueeze(squeezed_dim - diff_to_apply) for item in list_item
         ]
         return self._source._stack_onto_(key, list_item_unsqueeze, dim)
 
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim = self.custom_op_kwargs["dim"]
+        if self._source.batch_size[dim] == 1:
+            del names[dim]
+        return names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
 
 class _ViewedTensorDict(_CustomOpTensorDict):
     def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
         new_dim_list = list(self.custom_op_kwargs.get("size"))
         new_dim_list += list(source_tensor.shape[self._source.batch_dims :])
         new_dim = torch.Size(new_dim_list)
         new_dict = deepcopy(self.custom_op_kwargs)
@@ -6185,14 +6801,24 @@
         elif not isinstance(shape, torch.Size):
             shape = infer_size_impl(shape, self.numel())
             shape = torch.Size(shape)
         if shape == self._source.batch_size:
             return self._source
         return super().view(*shape)
 
+    @property
+    def names(self):
+        return [None] * self.ndim
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
 
 class _PermutedTensorDict(_CustomOpTensorDict):
     """A lazy view on a TensorDict with the batch dimensions permuted.
 
     When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
     returned such that the following code snippet works without raising an
     exception:
@@ -6277,14 +6903,25 @@
             perm = list(inv_permute_dims) + list(
                 range(self.batch_dims - 1, item.ndimension())
             )
             list_permuted_items.append(item.permute(*perm))
         self._source._stack_onto_(key, list_permuted_items, new_dim)
         return self
 
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        return [names[i] for i in self.custom_op_kwargs["dims"]]
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
 
 def _get_repr(tensor: Tensor) -> str:
     s = ", ".join(
         [
             f"shape={_shape(tensor)}",
             f"device={_device(tensor)}",
             f"dtype={_dtype(tensor)}",
@@ -6483,7 +7120,15 @@
 def _clone_value(value: CompatibleType, recurse: bool) -> CompatibleType:
     if recurse:
         return value.clone()
     elif is_tensor_collection(value):
         return value.clone(recurse=False)
     else:
         return value
+
+
+def _is_number(item):
+    if isinstance(item, Number):
+        return True
+    if isinstance(item, Tensor) and item.ndim == 0:
+        return True
+    return False
```

### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

## tensordict/utils.py

```diff
@@ -71,33 +71,45 @@
     expensive operation.
     Args:
         shape (torch.Size): Input shape
         items (index): Index of the hypothetical tensor
 
     Returns:
         Size of the resulting object (tensor or tensordict)
+
+    Examples:
+        >>> idx = (None, ..., None)
+        >>> torch.zeros(4, 3, 2, 1)[idx].shape
+        torch.Size([1, 4, 3, 2, 1, 1])
+        >>> _getitem_batch_size([4, 3, 2, 1], idx)
+        torch.Size([1, 4, 3, 2, 1, 1])
     """
     # let's start with simple cases
     if isinstance(items, tuple) and len(items) == 1:
         items = items[0]
     if isinstance(items, int):
         return shape[1:]
     if isinstance(items, torch.Tensor) and items.dtype is torch.bool:
         return torch.Size([items.sum(), *shape[items.ndimension() :]])
     if (
         isinstance(items, (torch.Tensor, np.ndarray)) and len(items.shape) <= 1
     ) or isinstance(items, list):
+        if isinstance(items, torch.Tensor) and not items.shape:
+            return shape[1:]
         if len(items):
             return torch.Size([len(items), *shape[1:]])
         else:
             return shape[1:]
 
     if not isinstance(items, tuple):
         items = (items,)
 
+    if any(item is Ellipsis for item in items):
+        items = convert_ellipsis_to_idx(items, shape)
+
     sanitized_items = []
     for _item in items:
         if isinstance(_item, (list, np.ndarray)):
             _item = torch.tensor(_item)
         elif isinstance(_item, torch.Tensor):
             # np.broadcast will complain if we give it CUDA tensors
             _item = _item.cpu()
@@ -116,15 +128,14 @@
     # to extract diagonal entries of the array.
     # if the tensor indices are contiguous, or separated by scalars, they are replaced
     # in-place by the broadcast shape. if they are separated by non-scalar indices, the
     # broadcast shape is prepended to the new batch size
     # https://numpy.org/doc/stable/user/basics.indexing.html#integer-array-indexing
     tensor_indices = []
     contiguous, prev = True, None
-
     for i, _item in enumerate(sanitized_items):
         if isinstance(_item, torch.Tensor):
             tensor_indices.append(_item)
             if prev is not None and i != prev + 1:
                 contiguous = False
             prev = i
         elif isinstance(_item, Number) and prev is not None and i == prev + 1:
@@ -211,30 +222,37 @@
         new_index (tuple): Output index
     """
     new_index = ()
     num_dims = len(batch_size)
 
     if idx is Ellipsis:
         idx = (...,)
+
     num_ellipsis = sum(_idx is Ellipsis for _idx in idx)
-    if num_dims < (len(idx) - num_ellipsis):
+    if num_dims < (len(idx) - num_ellipsis - sum(item is None for item in idx)):
         raise RuntimeError("Not enough dimensions in TensorDict for index provided.")
 
     start_pos, after_ellipsis_length = None, 0
     for i, item in enumerate(idx):
         if item is Ellipsis:
             if start_pos is not None:
                 raise RuntimeError("An index can only have one ellipsis at most.")
             else:
                 start_pos = i
         if item is not Ellipsis and start_pos is not None:
             after_ellipsis_length += 1
+        if item is None:
+            # unsqueeze
+            num_dims += 1
 
     before_ellipsis_length = start_pos
-    ellipsis_length = num_dims - after_ellipsis_length - before_ellipsis_length
+    if start_pos is None:
+        return idx
+    else:
+        ellipsis_length = num_dims - after_ellipsis_length - before_ellipsis_length
 
     new_index += idx[:start_pos]
 
     ellipsis_start = start_pos
     ellipsis_end = start_pos + ellipsis_length
     new_index += (slice(None),) * (ellipsis_end - ellipsis_start)
 
@@ -407,14 +425,17 @@
     return False
 
 
 def is_seq_of_nested_key(seq: Sequence[NestedKey]) -> bool:
     """Returns True if seq is a Sequence[NestedKey]."""
     if seq and isinstance(seq, Sequence):
         return all(is_nested_key(k) for k in seq)
+    elif isinstance(seq, Sequence):
+        # we allow empty inputs
+        return True
     return False
 
 
 def _seq_of_nested_key_check(seq: Sequence[NestedKey]) -> None:
     if not is_seq_of_nested_key(seq):
         raise ValueError(f"seq should be a Sequence[NestedKey]. Got {seq}")
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2023.04.02'
-git_version = 'a25f5d7e38c5c418301c1523bd6cb74c03379b05'
+__version__ = '2023.05.02'
+git_version = 'ef49f8e8c8f38cdbc3bce73e99283309ae8cd9af'
```

## tensordict/nn/__init__.py

```diff
@@ -3,39 +3,50 @@
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 from tensordict.nn.common import (
     dispatch,
     make_tensordict,
     TensorDictModule,
+    TensorDictModuleBase,
     TensorDictModuleWrapper,
 )
+from tensordict.nn.distributions import NormalParamExtractor
 from tensordict.nn.functional_modules import (
     get_functional,
     is_functional,
     make_functional,
     repopulate_module,
 )
 from tensordict.nn.probabilistic import (
+    InteractionType,
     ProbabilisticTensorDictModule,
     ProbabilisticTensorDictSequential,
     set_interaction_mode,
+    set_interaction_type,
 )
 from tensordict.nn.sequence import TensorDictSequential
-from tensordict.nn.utils import biased_softplus, inv_softplus
+from tensordict.nn.utils import (
+    biased_softplus,
+    inv_softplus,
+    set_skip_existing,
+    skip_existing,
+)
 
 __all__ = [
     "dispatch",
     "TensorDictModule",
     "TensorDictModuleWrapper",
     "get_functional",
     "make_functional",
     "repopulate_module",
+    "InteractionType",
     "ProbabilisticTensorDictModule",
     "ProbabilisticTensorDictSequential",
     "set_interaction_mode",
+    "set_interaction_type",
     "TensorDictSequential",
     "make_tensordict",
     "biased_softplus",
     "inv_softplus",
     "is_functional",
 ]
```

## tensordict/nn/common.py

```diff
@@ -5,19 +5,20 @@
 
 from __future__ import annotations
 
 import functools
 import inspect
 import warnings
 from textwrap import indent
-from typing import Any, Callable, Iterable, Sequence
+from typing import Any, Callable, Iterable, List, Sequence, Tuple, Union
 
 import torch
-
 from tensordict.nn.functional_modules import make_functional
+
+from tensordict.nn.utils import set_skip_existing
 from tensordict.tensordict import make_tensordict, TensorDictBase
 from tensordict.utils import _normalize_key, _seq_of_nested_key_check, NestedKey
 from torch import nn, Tensor
 
 try:
     from functorch import FunctionalModule, FunctionalModuleWithBuffers
 
@@ -271,15 +272,301 @@
                 out = tuple(out[key] for key in dest)
                 return out[0] if len(out) == 1 else out
             return func(_self, tensordict, *args, **kwargs)
 
         return wrapper
 
 
-class TensorDictModule(nn.Module):
+class _OutKeysSelect:
+    def __init__(self, out_keys):
+        self.out_keys = out_keys
+        self._initialized = False
+
+    def _init(self, module):
+        if self._initialized:
+            return
+        self._initialized = True
+        self.module = module
+        module.out_keys = list(self.out_keys)
+
+    def __call__(
+        self,
+        module: TensorDictModuleBase,
+        tensordict_in: TensorDictBase,
+        tensordict_out: TensorDictBase,
+    ):
+        # detect dispatch calls
+        in_keys = module.in_keys
+        is_dispatched = self._detect_dispatch(tensordict_in, in_keys)
+        out_keys = self.out_keys
+        # if dispatch filtered the out keys as they should we're happy
+        if is_dispatched:
+            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
+                len(out_keys) == len(tensordict_out)
+            ):
+                return tensordict_out
+        self._init(module)
+        if is_dispatched:
+            # it might be the case that dispatch was not aware of what the out-keys were.
+            if isinstance(tensordict_out, tuple):
+                out = tuple(
+                    item
+                    for i, item in enumerate(tensordict_out)
+                    if module._out_keys[i] in module.out_keys
+                )
+                if len(out) == 1:
+                    return out[0]
+                return out
+            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
+                return tensordict_out
+            elif (
+                module._out_keys[0] not in module.out_keys
+                and len(module._out_keys) == 1
+            ):
+                return ()
+            else:
+                raise RuntimeError(
+                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
+                )
+        if tensordict_out is tensordict_in:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+            )
+        else:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+                strict=False,
+            )
+
+    def _detect_dispatch(self, tensordict_in, in_keys):
+        if isinstance(tensordict_in, TensorDictBase) and all(
+            key in tensordict_in.keys() for key in in_keys
+        ):
+            return False
+        elif isinstance(tensordict_in, tuple):
+            if len(tensordict_in):
+                if isinstance(tensordict_in[0], TensorDictBase):
+                    return self._detect_dispatch(tensordict_in[0], in_keys)
+                return True
+            return not len(in_keys)
+        # not a TDBase: must be True
+        return True
+
+    def remove(self):
+        # reset ground truth
+        self.module.out_keys = self.module._out_keys
+
+    def __del__(self):
+        self.remove()
+
+
+class TensorDictModuleBase(nn.Module):
+    """Base class to TensorDict modules.
+
+    TensorDictModule subclasses are characterized by ``in_keys`` and ``out_keys``
+    key-lists that indicate what input entries are to be read and what output
+    entries should be expected to be written.
+
+    The forward method input/output signature should always follow the
+    convention:
+
+        >>> tensordict_out = module.forward(tensordict_in)
+
+    """
+
+    def __new__(cls, *args, **kwargs):
+        # check the out_keys and in_keys in the dict
+        if "in_keys" in cls.__dict__ and not isinstance(
+            cls.__dict__.get("in_keys"), property
+        ):
+            in_keys = cls.__dict__.get("in_keys")
+            # now let's remove it
+            delattr(cls, "in_keys")
+            cls._in_keys = in_keys
+            cls.in_keys = TensorDictModuleBase.in_keys
+        if "out_keys" in cls.__dict__ and not isinstance(
+            cls.__dict__.get("out_keys"), property
+        ):
+            out_keys = cls.__dict__.get("out_keys")
+            # now let's remove it
+            delattr(cls, "out_keys")
+            cls._out_keys = out_keys
+            cls._out_keys_apparent = out_keys
+            cls.out_keys = TensorDictModuleBase.out_keys
+        out = super().__new__(cls)
+        return out
+
+    @property
+    def in_keys(self):
+        return self._in_keys
+
+    @in_keys.setter
+    def in_keys(self, value: List[Union[str, Tuple[str]]]):
+        self._in_keys = value
+
+    @property
+    def out_keys(self):
+        return self._out_keys_apparent
+
+    @property
+    def out_keys_source(self):
+        return self._out_keys
+
+    @out_keys.setter
+    def out_keys(self, value: List[Union[str, Tuple[str]]]):
+        # the first time out_keys are set, they are marked as ground truth
+        if not hasattr(self, "_out_keys"):
+            self._out_keys = value
+        self._out_keys_apparent = value
+
+    def select_out_keys(self, *out_keys):
+        """Selects the keys that will be found in the output tensordict.
+
+        This is useful whenever one wants to get rid of intermediate keys in a
+        complicated graph, or when the presence of these keys may trigger unexpected
+        behaviours.
+
+        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
+
+        Args:
+            *out_keys (a sequence of strings or tuples of strings): the
+                out_keys that should be found in the output tensordict.
+
+        Returns: the same module, modified in-place with updated ``out_keys``.
+
+        The simplest usage is with :class:`~.TensorDictModule`:
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This feature will also work with dispatched arguments:
+        Examples:
+            >>> mod(torch.zeros(()), torch.ones(()))
+            tensor(2.)
+
+        This change will occur in-place (ie the same module will be returned
+        with an updated list of out_keys). It can be reverted using the
+        :meth:`TensorDictModuleBase.reset_out_keys` method.
+
+        Examples:
+            >>> mod.reset_out_keys()
+            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This will work with other classes too, such as Sequential:
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential
+            >>> seq = TensorDictSequential(
+            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
+            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
+            ... )
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> seq.select_out_keys("z")
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        self.register_forward_hook(_OutKeysSelect(out_keys))
+        for hook in self._forward_hooks.values():
+            hook._init(self)
+        return self
+
+    def reset_out_keys(self):
+        """Resets the ``out_keys`` attribute to its orignal value.
+
+        Returns: the same module, with its original ``out_keys`` values.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.reset_out_keys()
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+        """
+        for i, hook in list(self._forward_hooks.items()):
+            if isinstance(hook, _OutKeysSelect):
+                del self._forward_hooks[i]
+        return self
+
+
+class TensorDictModule(TensorDictModuleBase):
     """A TensorDictModule, is a python wrapper around a :obj:`nn.Module` that reads and writes to a TensorDict.
 
     By default, :class:`TensorDictModule` subclasses are always functional,
     meaning that they support the ``td_module(input, params=params)`` function
     call signature.
 
     Args:
@@ -291,62 +578,130 @@
 
     Embedding a neural network in a TensorDictModule only requires to specify the input
     and output keys. TensorDictModule support functional and regular :obj:`nn.Module`
     objects. In the functional case, the 'params' (and 'buffers') keyword argument must
     be specified:
 
     Examples:
+        >>> from tensordict import TensorDict
+        >>> # one can wrap regular nn.Module
+        >>> module = TensorDictModule(nn.Transformer(128), in_keys=["input", "tgt"], out_keys=["out"])
+        >>> input = torch.ones(2, 3, 128)
+        >>> tgt = torch.zeros(2, 3, 128)
+        >>> data = TensorDict({"input": input, "tgt": tgt}, batch_size=[2, 3])
+        >>> data = module(data)
+        >>> print(data)
+        TensorDict(
+            fields={
+                input: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
+                out: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
+                tgt: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([2, 3]),
+            device=None,
+            is_shared=False)
+        >>> # we can also pass directly the tensors
+        >>> out = module(input, tgt)
+        >>> assert out.shape == input.shape
+        >>> # we can also wrap regular functions
+        >>> module = TensorDictModule(lambda x: (x-1, x+1), in_keys=[("input", "x")], out_keys=[("output", "x-1"), ("output", "x+1")])
+        >>> module(TensorDict({("input", "x"): torch.zeros(())}, batch_size=[]))
+        TensorDict(
+            fields={
+                input: TensorDict(
+                    fields={
+                        x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False),
+                output: TensorDict(
+                    fields={
+                        x+1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                        x-1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+        >>> # we can use TensorDictModule to populate a tensordict
+        >>> module = TensorDictModule(lambda: torch.randn(3), in_keys=[], out_keys=["x"])
+        >>> print(module(TensorDict({}, batch_size=[])))
+        TensorDict(
+            fields={
+                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    Functional calls to a tensordict module is easy:
+    Examples:
         >>> import torch
         >>> from tensordict import TensorDict
         >>> from tensordict.nn import TensorDictModule
         >>> from tensordict.nn.functional_modules import make_functional
         >>> td = TensorDict({"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3,])
         >>> module = torch.nn.GRUCell(4, 8)
-        >>> fmodule, params, buffers = functorch.make_functional_with_buffers(module)
-        >>> td_fmodule = TensorDictModule(
-        ...    module=torch.nn.GRUCell(4, 8), in_keys=["input", "hidden"], out_keys=["output"]
+        >>> td_module = TensorDictModule(
+        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
         ... )
-        >>> params = make_functional(td_fmodule)
-        >>> td_functional = td_fmodule(td.clone(), params=params)
+        >>> params = make_functional(td_module)
+        >>> td_functional = td_module(td.clone(), params=params)
         >>> print(td_functional)
         TensorDict(
             fields={
-                hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),
-                input: Tensor(torch.Size([3, 4]), dtype=torch.float32),
-                output: Tensor(torch.Size([3, 8]), dtype=torch.float32)},
-            shared=False,
+                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
             batch_size=torch.Size([3]),
             device=None,
             is_shared=False)
 
     In the stateful case:
+        >>> module = torch.nn.GRUCell(4, 8)
         >>> td_module = TensorDictModule(
-        ...    module=torch.nn.GRUCell(4, 8), in_keys=["input", "hidden"], out_keys=["output"]
+        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
         ... )
         >>> td_stateful = td_module(td.clone())
         >>> print(td_stateful)
         TensorDict(
             fields={
-                hidden: Tensor(torch.Size([3, 8]), dtype=torch.float32),
-                input: Tensor(torch.Size([3, 4]), dtype=torch.float32),
-                output: Tensor(torch.Size([3, 8]), dtype=torch.float32)},
+                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
             batch_size=torch.Size([3]),
             device=None,
             is_shared=False)
 
     One can use a vmap operator to call the functional module.
-        >>> from functorch import vmap
+        >>> from torch import vmap
+        >>> from tensordict.nn.functional_modules import extract_weights_and_buffers
+        >>> params = extract_weights_and_buffers(td_module)
         >>> params_repeat = params.expand(4)
-        >>> td_vmap = vmap(td_fmodule, (None, 0))(td.clone(), params_repeat)
+        >>> print(params_repeat)
+        TensorDict(
+            fields={
+                module: TensorDict(
+                    fields={
+                        bias_hh: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
+                        bias_ih: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight_hh: Tensor(shape=torch.Size([4, 24, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight_ih: Tensor(shape=torch.Size([4, 24, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([4]),
+            device=None,
+            is_shared=False)
+        >>> td_vmap = vmap(td_module, (None, 0))(td.clone(), params_repeat)
         >>> print(td_vmap)
         TensorDict(
             fields={
-                hidden: Tensor(torch.Size([4, 3, 8]), dtype=torch.float32),
-                input: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
-                output: Tensor(torch.Size([4, 3, 8]), dtype=torch.float32)},
+                hidden: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
             batch_size=torch.Size([4, 3]),
             device=None,
             is_shared=False)
 
     """
 
     def __init__(
@@ -388,36 +743,55 @@
         self,
         tensordict: TensorDictBase,
         tensors: list[Tensor],
         tensordict_out: TensorDictBase | None = None,
         out_keys: Iterable[NestedKey] | None = None,
     ) -> TensorDictBase:
         if out_keys is None:
-            out_keys = self.out_keys
+            out_keys = self.out_keys_source
         if tensordict_out is None:
             tensordict_out = tensordict
         for _out_key, _tensor in zip(out_keys, tensors):
             if _out_key != "_":
                 tensordict_out.set(_out_key, _tensor)
         return tensordict_out
 
     def _call_module(
         self, tensors: Sequence[Tensor], **kwargs: Any
     ) -> Tensor | Sequence[Tensor]:
         out = self.module(*tensors, **kwargs)
         return out
 
     @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
     def forward(
         self,
         tensordict: TensorDictBase,
+        *args,
         tensordict_out: TensorDictBase | None = None,
         **kwargs: Any,
     ) -> TensorDictBase:
         """When the tensordict parameter is not set, kwargs are used to create an instance of TensorDict."""
+        if len(args):
+            tensordict_out = args[0]
+            args = args[1:]
+            # we will get rid of tensordict_out as a regular arg, because it
+            # blocks us when using vmap
+            # with stateful but functional modules: the functional module checks if
+            # it still contains parameters. If so it considers that only a "params" kwarg
+            # is indicative of what the params are, when we could potentially make a
+            # special rule for TensorDictModule that states that the second arg is
+            # likely to be the module params.
+            warnings.warn(
+                "tensordict_out will be deprecated soon.", category=DeprecationWarning
+            )
+        if len(args):
+            raise ValueError(
+                "Got a non-empty list of extra agruments, when none was expected."
+            )
         tensors = tuple(tensordict.get(in_key, None) for in_key in self.in_keys)
         try:
             tensors = self._call_module(tensors, **kwargs)
         except Exception as err:
             if any(tensor is None for tensor in tensors) and "None" in str(err):
                 none_set = {
                     key for key, tensor in zip(self.in_keys, tensors) if tensor is None
@@ -457,18 +831,19 @@
         except AttributeError as err1:
             try:
                 return getattr(super().__getattr__("module"), name)
             except Exception as err2:
                 raise err2 from err1
 
 
-class TensorDictModuleWrapper(nn.Module):
+class TensorDictModuleWrapper(TensorDictModuleBase):
     """Wrapper class for TensorDictModule objects.
 
-    Once created, a TensorDictModuleWrapper will behave exactly as the TensorDictModule it contains except for the methods that are
+    Once created, a TensorDictModuleWrapper will behave exactly as the
+    TensorDictModule it contains except for the methods that are
     overwritten.
 
     Args:
         td_module (TensorDictModule): operator to be wrapped.
 
     """
```

## tensordict/nn/functional_modules.py

```diff
@@ -256,16 +256,16 @@
         setattr(model, name, None)
         tensordict[name] = param
 
     for name, module in model.named_children():
         module_tensordict = extract_weights_and_buffers(module)
         if module_tensordict is not None:
             tensordict[name] = module_tensordict
-
-    return TensorDict(tensordict, batch_size=[], _run_checks=False)
+    model.__dict__["_is_stateless"] = True
+    return TensorDict(tensordict, batch_size=torch.Size([]), _run_checks=False)
 
 
 def _swap_state(
     model: nn.Module,
     tensordict: TensorDict,
     is_stateless: bool,
     return_old_tensordict: bool = False,
```

## tensordict/nn/probabilistic.py

```diff
@@ -2,70 +2,135 @@
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 from __future__ import annotations
 
 import re
+from enum import auto, Enum
 from textwrap import indent
-from typing import Any, Sequence
+from typing import Any, Callable, Sequence
+from warnings import warn
 
-import torch.nn as nn
-from tensordict.nn.common import TensorDictModule
+from tensordict._contextlib import _DecoratorContextManager
+
+from tensordict.nn.common import dispatch, TensorDictModule, TensorDictModuleBase
 from tensordict.nn.distributions import Delta, distributions_maps
 from tensordict.nn.sequence import TensorDictSequential
+
+from tensordict.nn.utils import set_skip_existing
 from tensordict.tensordict import TensorDictBase
 from tensordict.utils import _seq_of_nested_key_check
 from torch import distributions as D, Tensor
-from torch.autograd.grad_mode import _DecoratorContextManager
 
 __all__ = ["ProbabilisticTensorDictModule", "ProbabilisticTensorDictSequential"]
 
 
-_INTERACTION_MODE = None
+class InteractionType(Enum):
+    MODE = auto()
+    MEDIAN = auto()
+    MEAN = auto()
+    RANDOM = auto()
+
+    @classmethod
+    def from_str(cls, type_str: str) -> InteractionType:
+        """Return the interaction_type with name matched to the provided string (case insensitive)."""
+        for member_type in cls:
+            if member_type.name == type_str.upper():
+                return member_type
+        raise ValueError(f"The provided interaction type {type_str} is unsupported!")
+
+
+_INTERACTION_TYPE: InteractionType | None = None
+
+
+def _insert_interaction_mode_deprecation_warning(
+    prefix: str = "",
+) -> Callable[[str, Warning, int], None]:
+    return warn(
+        (
+            f"{prefix}interaction_mode is deprecated for naming clarity. "
+            f"Please use {prefix}interaction_type with InteractionType enum instead."
+        ),
+        DeprecationWarning,
+        stacklevel=2,
+    )
+
+
+def interaction_type() -> InteractionType | None:
+    """Returns the current sampling type."""
+    return _INTERACTION_TYPE
 
 
 def interaction_mode() -> str | None:
-    """Returns the current sampling mode."""
-    return _INTERACTION_MODE
+    """*Deprecated* Returns the current sampling mode."""
+    _insert_interaction_mode_deprecation_warning()
+    type = interaction_type()
+    return type.name.lower() if type else None
 
 
 class set_interaction_mode(_DecoratorContextManager):
-    """Sets the sampling mode of all ProbabilisticTDModules to the desired mode.
+    """*Deprecated* Sets the sampling mode of all ProbabilisticTDModules to the desired mode.
 
     Args:
         mode (str): mode to use when the policy is being called.
     """
 
-    def __init__(self, mode: str = "mode") -> None:
+    def __init__(self, mode: str | None = "mode") -> None:
+        _insert_interaction_mode_deprecation_warning("set_")
         super().__init__()
-        self.mode = mode
+        self.mode = InteractionType.from_str(mode) if mode else None
 
     def clone(self) -> set_interaction_mode:
         # override this method if your children class takes __init__ parameters
         return self.__class__(self.mode)
 
     def __enter__(self) -> None:
-        global _INTERACTION_MODE
-        self.prev = _INTERACTION_MODE
-        _INTERACTION_MODE = self.mode
+        global _INTERACTION_TYPE
+        self.prev = _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.mode
 
     def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _INTERACTION_MODE
-        _INTERACTION_MODE = self.prev
+        global _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.prev
+
+
+class set_interaction_type(_DecoratorContextManager):
+    """Sets all ProbabilisticTDModules sampling to the desired type.
+
+    Args:
+        type (InteractionType): sampling type to use when the policy is being called.
+    """
+
+    def __init__(self, type: InteractionType | None = InteractionType.MODE) -> None:
+        super().__init__()
+        self.type = type
 
+    def clone(self) -> set_interaction_type:
+        # override this method if your children class takes __init__ parameters
+        return self.__class__(self.type)
+
+    def __enter__(self) -> None:
+        global _INTERACTION_TYPE
+        self.prev = _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.type
 
-class ProbabilisticTensorDictModule(nn.Module):
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.prev
+
+
+class ProbabilisticTensorDictModule(TensorDictModuleBase):
     """A probabilistic TD Module.
 
     `ProbabilisticTensorDictModule` is a non-parametric module representing a
     probability distribution. It reads the distribution parameters from an input
     TensorDict using the specified `in_keys`. The output is sampled given some rule,
-    specified by the input :obj:`default_interaction_mode` argument and the
-    :obj:`interaction_mode()` global function.
+    specified by the input :obj:`default_interaction_type` argument and the
+    :obj:`interaction_type()` global function.
 
     :obj:`ProbabilisticTensorDictModule` can be used to construct the distribution
     (through the :obj:`get_dist()` method) and/or sampling from this distribution
     (through a regular :obj:`__call__()` to the module).
 
     A :obj:`ProbabilisticTensorDictModule` instance has two main features:
     - It reads and writes TensorDict objects
@@ -88,25 +153,27 @@
             the distribution class of interest, e.g. :obj:`"loc"` and :obj:`"scale"` for
             the Normal distribution and similar. If in_keys is a dictionary,, the keys
             are the keys of the distribution and the values are the keys in the
             tensordict that will get match to the corresponding distribution keys.
         out_keys (str or iterable of str): keys where the sampled values will be
             written. Importantly, if these keys are found in the input TensorDict, the
             sampling step will be skipped.
-        default_interaction_mode (str, optional): keyword-only argument.
+        default_interaction_mode (str, optional): *Deprecated* keyword-only argument.
+            Please use default_interaction_type instead.
+        default_interaction_type (InteractionType, optional): keyword-only argument.
             Default method to be used to retrieve
-            the output value. Should be one of: 'mode', 'median', 'mean' or 'random'
+            the output value. Should be one of InteractionType: MODE, MEDIAN, MEAN or RANDOM
             (in which case the value is sampled randomly from the distribution). Default
-            is 'mode'.
+            is MODE.
             Note: When a sample is drawn, the :obj:`ProbabilisticTDModule` instance will
-            first look for the interaction mode dictated by the `interaction_mode()`
+            first look for the interaction mode dictated by the `interaction_type()`
             global function. If this returns `None` (its default value), then the
-            `default_interaction_mode` of the `ProbabilisticTDModule` instance will be
-            used. Note that DataCollector instances will use `set_interaction_mode` to
-            `"random"` by default.
+            `default_interaction_type` of the `ProbabilisticTDModule` instance will be
+            used. Note that DataCollector instances will use `set_interaction_type` to
+            `RANDOM` by default.
         distribution_class (Type, optional): keyword-only argument.
             A :class:`torch.distributions.Distribution` class to
             be used for sampling.
             Default is :class:`tensordict.nn.distributions.Delta`.
         distribution_kwargs (dict, optional): keyword-only argument.
             Keyword-argument pairs to be passed to the distribution.
         return_log_prob (bool, optional): keyword-only argument.
@@ -169,15 +236,15 @@
             batch_size=torch.Size([3]),
             device=None,
             is_shared=False)
         >>> dist = td_module.get_dist(td, params=params)
         >>> print(dist)
         Normal(loc: torch.Size([3, 4]), scale: torch.Size([3, 4]))
         >>> # we can also apply the module to the TensorDict with vmap
-        >>> from functorch import vmap
+        >>> from torch import vmap
         >>> params = params.expand(4)
         >>> td_vmap = vmap(td_module, (None, 0))(td, params)
         >>> print(td_vmap)
         TensorDict(
             fields={
                 action: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32),
                 hidden: Tensor(torch.Size([4, 3, 8]), dtype=torch.float32),
@@ -188,20 +255,23 @@
                 scale: Tensor(torch.Size([4, 3, 4]), dtype=torch.float32)},
             batch_size=torch.Size([4, 3]),
             device=None,
             is_shared=False)
 
     """
 
+    SAMPLE_LOG_PROB_KEY = "sample_log_prob"
+
     def __init__(
         self,
         in_keys: str | Sequence[str] | dict,
         out_keys: str | Sequence[str] | None = None,
         *,
-        default_interaction_mode: str = "mode",
+        default_interaction_mode: str | None = None,
+        default_interaction_type: InteractionType = InteractionType.MODE,
         distribution_class: type = Delta,
         distribution_kwargs: dict | None = None,
         return_log_prob: bool = False,
         cache_dist: bool = False,
         n_empirical_estimate: int = 1000,
     ) -> None:
         super().__init__()
@@ -222,25 +292,34 @@
                 )
 
         _seq_of_nested_key_check(out_keys)
         _seq_of_nested_key_check(tuple(in_keys.values()))
         self.out_keys = out_keys
         self.in_keys = in_keys
 
-        self.default_interaction_mode = default_interaction_mode
+        if default_interaction_mode is not None:
+            _insert_interaction_mode_deprecation_warning("default_")
+            self.default_interaction_type = InteractionType.from_str(
+                default_interaction_mode
+            )
+        else:
+            self.default_interaction_type = default_interaction_type
+
         if isinstance(distribution_class, str):
             distribution_class = distributions_maps.get(distribution_class.lower())
         self.distribution_class = distribution_class
         self.distribution_kwargs = (
             distribution_kwargs if distribution_kwargs is not None else {}
         )
         self.n_empirical_estimate = n_empirical_estimate
         self._dist = None
         self.cache_dist = cache_dist if hasattr(distribution_class, "update") else False
         self.return_log_prob = return_log_prob
+        if self.return_log_prob:
+            self.out_keys.append(self.SAMPLE_LOG_PROB_KEY)
 
     def get_dist(self, tensordict: TensorDictBase) -> D.Distribution:
         try:
             dist_kwargs = {
                 dist_key: tensordict[td_key]
                 for dist_key, td_key in self.in_keys.items()
             }
@@ -255,88 +334,92 @@
                 raise TypeError(
                     f"TensorDict with keys {tensordict.keys()} does not match the distribution {self.distribution_class} keywords."
                 )
             else:
                 raise err
         return dist
 
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
     def forward(
         self,
         tensordict: TensorDictBase,
         tensordict_out: TensorDictBase | None = None,
         _requires_sample: bool = True,
     ) -> TensorDictBase:
         if tensordict_out is None:
             tensordict_out = tensordict
 
         dist = self.get_dist(tensordict)
         if _requires_sample:
-            out_tensors = self._dist_sample(dist, interaction_mode=interaction_mode())
+            out_tensors = self._dist_sample(dist, interaction_type=interaction_type())
             if isinstance(out_tensors, Tensor):
                 out_tensors = (out_tensors,)
             tensordict_out.update(
                 {key: value for key, value in zip(self.out_keys, out_tensors)}
             )
             if self.return_log_prob:
                 log_prob = dist.log_prob(*out_tensors)
-                tensordict_out.set("sample_log_prob", log_prob)
+                tensordict_out.set(self.SAMPLE_LOG_PROB_KEY, log_prob)
         elif self.return_log_prob:
-            out_tensors = [tensordict.get(key) for key in self.out_keys]
+            out_tensors = [
+                tensordict.get(key)
+                for key in self.out_keys
+                if key != self.SAMPLE_LOG_PROB_KEY
+            ]
             log_prob = dist.log_prob(*out_tensors)
-            tensordict_out.set("sample_log_prob", log_prob)
+            tensordict_out.set(self.SAMPLE_LOG_PROB_KEY, log_prob)
             # raise RuntimeError(
             #     "ProbabilisticTensorDictModule.return_log_prob = True is incompatible with settings in which "
             #     "the submodule is responsible for sampling. To manually gather the log-probability, call first "
             #     "\n>>> dist, tensordict = tensordict_module.get_dist(tensordict)"
             #     "\n>>> tensordict.set('sample_log_prob', dist.log_prob(tensordict.get(sample_key))"
             # )
         return tensordict_out
 
     def _dist_sample(
         self,
         dist: D.Distribution,
-        interaction_mode: bool = None,
+        interaction_type: InteractionType | None = None,
     ) -> tuple[Tensor, ...] | Tensor:
-        if interaction_mode is None or interaction_mode == "":
-            interaction_mode = self.default_interaction_mode
-        if not isinstance(dist, D.Distribution):
-            raise TypeError(f"type {type(dist)} not recognised by _dist_sample")
+        if interaction_type is None:
+            interaction_type = self.default_interaction_type
 
-        if interaction_mode == "mode":
-            if hasattr(dist, "mode"):
+        if interaction_type is InteractionType.MODE:
+            try:
                 return dist.mode
-            else:
+            except AttributeError:
                 raise NotImplementedError(
                     f"method {type(dist)}.mode is not implemented"
                 )
 
-        elif interaction_mode == "median":
-            if hasattr(dist, "median"):
+        elif interaction_type is InteractionType.MEDIAN:
+            try:
                 return dist.median
-            else:
+            except AttributeError:
                 raise NotImplementedError(
                     f"method {type(dist)}.median is not implemented"
                 )
 
-        elif interaction_mode == "mean":
+        elif interaction_type is InteractionType.MEAN:
             try:
                 return dist.mean
             except (AttributeError, NotImplementedError):
                 if dist.has_rsample:
                     return dist.rsample((self.n_empirical_estimate,)).mean(0)
                 else:
                     return dist.sample((self.n_empirical_estimate,)).mean(0)
 
-        elif interaction_mode == "random":
+        elif interaction_type is InteractionType.RANDOM:
             if dist.has_rsample:
                 return dist.rsample()
             else:
                 return dist.sample()
         else:
-            raise NotImplementedError(f"unknown interaction_mode {interaction_mode}")
+            raise NotImplementedError(f"unknown interaction_type {interaction_type}")
 
 
 class ProbabilisticTensorDictSequential(TensorDictSequential):
     """A sequence of TensorDictModules ending in a ProbabilistictTensorDictModule.
 
     Similarly to :obj:`TensorDictSequential`, but enforces that the final module in the
     sequence is an :obj:`ProbabilisticTensorDictModule` and also exposes ``get_dist``
@@ -393,18 +476,18 @@
     def get_dist_params(
         self,
         tensordict: TensorDictBase,
         tensordict_out: TensorDictBase | None = None,
         **kwargs,
     ) -> tuple[D.Distribution, TensorDictBase]:
         tds = self.det_part
-        mode = interaction_mode()
-        if mode is None:
-            mode = self.module[-1].default_interaction_mode
-        with set_interaction_mode(mode):
+        type = interaction_type()
+        if type is None:
+            type = self.module[-1].default_interaction_type
+        with set_interaction_type(type):
             return tds(tensordict, tensordict_out, **kwargs)
 
     def get_dist(
         self,
         tensordict: TensorDictBase,
         tensordict_out: TensorDictBase | None = None,
         **kwargs,
@@ -413,14 +496,16 @@
         tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
         return self.build_dist_from_params(tensordict_out)
 
     def build_dist_from_params(self, tensordict: TensorDictBase) -> D.Distribution:
         """Construct a distribution from the input parameters. Other modules in the sequence are not evaluated."""
         return self.module[-1].get_dist(tensordict)
 
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
     def forward(
         self,
         tensordict: TensorDictBase,
         tensordict_out: TensorDictBase | None = None,
         **kwargs,
     ) -> TensorDictBase:
         tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
```

## tensordict/nn/sequence.py

```diff
@@ -4,14 +4,16 @@
 # LICENSE file in the root directory of this source tree.
 
 from __future__ import annotations
 
 from copy import deepcopy
 from typing import Any, Iterable
 
+from tensordict.nn.utils import set_skip_existing
+
 _has_functorch = False
 try:
     import functorch
 
     _has_functorch = True
 except ImportError:
     print(
@@ -47,14 +49,39 @@
          partial_tolerant (bool, optional): if True, the input tensordict can miss some of the input keys.
             If so, the only module that will be executed are those who can be executed given the keys that
             are present.
             Also, if the input tensordict is a lazy stack of tensordicts AND if partial_tolerant is :obj:`True` AND if the
             stack does not have the required keys, then TensorDictSequential will scan through the sub-tensordicts
             looking for those that have the required keys, if any.
 
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+        >>> torch.manual_seed(0)
+        >>> module = TensorDictSequential(
+        ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["x+1"]),
+        ...     TensorDictModule(nn.Linear(3, 4), in_keys=["x+1"], out_keys=["w*(x+1)+b"]),
+        ... )
+        >>> # with tensordict input
+        >>> print(module(TensorDict({"x": torch.zeros(3)}, [])))
+        TensorDict(
+            fields={
+                w*(x+1)+b: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                x+1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+        >>> # with tensor input: returns all the output keys in the order of the modules, ie "x+1" and "w*(x+1)+b"
+        >>> module(x=torch.zeros(3))
+        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
+        >>> module(torch.zeros(3))
+        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
+
     TensorDictSequence supports functional, modular and vmap coding:
     Examples:
         >>> import torch
         >>> from tensordict import TensorDict
         >>> from tensordict.nn import (
         ...     ProbabilisticTensorDictModule,
         ...     ProbabilisticTensorDictSequential,
@@ -178,83 +205,233 @@
         self,
         in_keys: Iterable[NestedKey] | None = None,
         out_keys: Iterable[NestedKey] | None = None,
     ) -> TensorDictSequential:
         """Returns a new TensorDictSequential with only the modules that are necessary to compute the given output keys with the given input keys.
 
         Args:
-            in_keys: input keys of the subsequence we want to select
-            out_keys: output keys of the subsequence we want to select
+            in_keys: input keys of the subsequence we want to select.
+                All the keys absent from ``in_keys`` will be considered as
+                non-relevant, and modules that *just* take these keys as inputs
+                will be discarded.
+                The resulting sequential module will follow the pattern "all
+                the modules which output will be affected by a different value
+                for any in <in_keys>".
+                If none is provided, the module's ``in_keys`` are assumed.
+            out_keys: output keys of the subsequence we want to select.
+                Only the modules that are necessary to get the ``out_keys``
+                will be found in the resulting sequence.
+                The resulting sequential module will follow the pattern "all
+                the modules that condition the value or <out_keys> entries."
+                If none is provided, the module's ``out_keys`` are assumed.
 
         Returns:
             A new TensorDictSequential with only the modules that are necessary acording to the given input and output keys.
+
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential as Seq, TensorDictModule as Mod
+            >>> idn = lambda x: x
+            >>> module = Seq(
+            ...     Mod(idn, in_keys=["a"], out_keys=["b"]),
+            ...     Mod(idn, in_keys=["b"], out_keys=["c"]),
+            ...     Mod(idn, in_keys=["c"], out_keys=["d"]),
+            ...     Mod(idn, in_keys=["a"], out_keys=["e"]),
+            ... )
+            >>> # select all modules whose output depend on "a"
+            >>> module.select_subsequence(in_keys=["a"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['b'])
+                  (1): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['c'])
+                  (2): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['c'],
+                      out_keys=['d'])
+                  (3): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['e'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['b', 'c', 'd', 'e'])
+            >>> # select all modules whose output depend on "c"
+            >>> module.select_subsequence(in_keys=["c"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['c'],
+                      out_keys=['d'])
+                ),
+                device=cpu,
+                in_keys=['c'],
+                out_keys=['d'])
+            >>> # select all modules that affect the value of "c"
+            >>> module.select_subsequence(out_keys=["c"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['b'])
+                  (1): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['c'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['b', 'c'])
+            >>> # select all modules that affect the value of "e"
+            >>> module.select_subsequence(out_keys=["e"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['e'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['e'])
+
+        This method propagates to nested sequential:
+
+            >>> module = Seq(
+            ...     Seq(
+            ...         Mod(idn, in_keys=["a"], out_keys=["b"]),
+            ...         Mod(idn, in_keys=["b"], out_keys=["c"]),
+            ...     ),
+            ...     Seq(
+            ...         Mod(idn, in_keys=["b"], out_keys=["d"]),
+            ...         Mod(idn, in_keys=["d"], out_keys=["e"]),
+            ...     ),
+            ... )
+            >>> # select submodules whose output will be affected by a change in "b" or "d" AND which output is "e"
+            >>> module.select_subsequence(in_keys=["b", "d"], out_keys=["e"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictSequential(
+                      module=ModuleList(
+                        (0): TensorDictModule(
+                            module=<function <lambda> at 0x129efae50>,
+                            device=cpu,
+                            in_keys=['b'],
+                            out_keys=['d'])
+                        (1): TensorDictModule(
+                            module=<function <lambda> at 0x129efae50>,
+                            device=cpu,
+                            in_keys=['d'],
+                            out_keys=['e'])
+                      ),
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['d', 'e'])
+                ),
+                device=cpu,
+                in_keys=['b'],
+                out_keys=['d', 'e'])
+
         """
         if in_keys is None:
             in_keys = deepcopy(self.in_keys)
         else:
             in_keys = [_normalize_key(key) for key in in_keys]
         if out_keys is None:
             out_keys = deepcopy(self.out_keys)
         else:
             out_keys = [_normalize_key(key) for key in out_keys]
-        id_to_keep = set(range(len(self.module)))
-        for i, module in enumerate(self.module):
+        module_list = list(self.module)
+        id_to_keep = set(range(len(module_list)))
+        for i, module in enumerate(module_list):
+            if (
+                type(module) is TensorDictSequential
+            ):  # no isinstance because we don't want to mess up subclasses
+                try:
+                    module = module_list[i] = module.select_subsequence(in_keys=in_keys)
+                except ValueError:
+                    # then the module can be removed
+                    id_to_keep.remove(i)
+                    continue
+
             if all(key in in_keys for key in module.in_keys):
                 in_keys.extend(module.out_keys)
             else:
                 id_to_keep.remove(i)
-        for i, module in reversed(list(enumerate(self.module))):
+        for i, module in reversed(list(enumerate(module_list))):
             if i in id_to_keep:
                 if any(key in out_keys for key in module.out_keys):
+                    if (
+                        type(module) is TensorDictSequential
+                    ):  # no isinstance because we don't want to mess up subclasses
+                        module = module_list[i] = module.select_subsequence(
+                            out_keys=out_keys
+                        )
                     out_keys.extend(module.in_keys)
                 else:
                     id_to_keep.remove(i)
         id_to_keep = sorted(id_to_keep)
 
-        modules = [self.module[i] for i in id_to_keep]
+        modules = [module_list[i] for i in id_to_keep]
 
         if modules == []:
             raise ValueError(
                 "No modules left after selection. Make sure that in_keys and out_keys are coherent."
             )
 
         return self.__class__(*modules)
 
     def _run_module(
         self,
         module: TensorDictModule,
         tensordict: TensorDictBase,
         **kwargs: Any,
     ) -> Any:
-        tensordict_keys = set(tensordict.keys(include_nested=True))
         if not self.partial_tolerant or all(
-            key in tensordict_keys for key in module.in_keys
+            key in tensordict.keys(include_nested=True) for key in module.in_keys
         ):
             tensordict = module(tensordict, **kwargs)
         elif self.partial_tolerant and isinstance(tensordict, LazyStackedTensorDict):
             for sub_td in tensordict.tensordicts:
-                tensordict_keys = set(sub_td.keys(include_nested=True))
-                if all(key in tensordict_keys for key in module.in_keys):
+                if all(
+                    key in sub_td.keys(include_nested=True) for key in module.in_keys
+                ):
                     module(sub_td, **kwargs)
             tensordict._update_valid_keys()
         return tensordict
 
     @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
     def forward(
         self,
         tensordict: TensorDictBase,
         tensordict_out: TensorDictBase | None = None,
         **kwargs: Any,
     ) -> TensorDictBase:
         if not len(kwargs):
             for module in self.module:
                 tensordict = self._run_module(module, tensordict, **kwargs)
         else:
             raise RuntimeError(
-                "TensorDictSequential does not support keyword arguments other than 'tensordict_out', 'in_keys' and 'out_keys'"
+                f"TensorDictSequential does not support keyword arguments other than 'tensordict_out' or in_keys: {self.in_keys}. Got {kwargs.keys()} instead."
             )
         if tensordict_out is not None:
             tensordict_out.update(tensordict, inplace=True)
             return tensordict_out
         return tensordict
 
     def __len__(self) -> int:
```

## tensordict/nn/utils.py

```diff
@@ -1,20 +1,25 @@
 # Copyright (c) Meta Platforms, Inc. and affiliates.
 #
 # This source code is licensed under the MIT license found in the
 # LICENSE file in the root directory of this source tree.
 
 from __future__ import annotations
 
-from typing import Callable
+import functools
+import inspect
+from typing import Any, Callable
 
 import torch
 from torch import nn
 
 __all__ = ["mappings", "inv_softplus", "biased_softplus"]
+_SKIP_EXISTING = False
+
+from tensordict._contextlib import _DecoratorContextManager
 
 
 def inv_softplus(bias: float | torch.Tensor) -> float | torch.Tensor:
     """Inverse softplus function.
 
     Args:
         bias (float or tensor): the value to be softplus-inverted.
@@ -84,7 +89,187 @@
                 float(stripped_key[-2]), min_val=float(stripped_key[-1])
             )
         else:
             raise ValueError(f"Invalid number of args in  {key}")
 
     else:
         raise NotImplementedError(f"Unknown mapping {key}")
+
+
+class set_skip_existing(_DecoratorContextManager):
+    """A context manager for skipping existing nodes in a TensorDict graph.
+
+    When used as a context manager, it will set the `skip_existing()` value
+    to the ``mode`` indicated, leaving the user able to code up methods that
+    will check the global value and execute the code accordingly.
+
+    When used as a method decorator, it will check the tensordict input keys
+    and if the ``skip_existing()`` call returns ``True``, it will skip the method
+    if all the output keys are already present.
+    This not not expected to be used as a decorator for methods that do not
+    respect the following signature: ``def fun(self, tensordict, *args, **kwargs)``.
+
+    Args:
+        mode (bool, optional):
+            If ``True``, it indicates that existing entries in the graph
+            won't be overwritten, unless they are only partially present. :func:`~.skip_existing`
+            will return ``True``.
+            If ``False``, no check will be performed.
+            If ``None``, the value of :func:`~.skip_existing` will not be
+            changed. This is intended to be used exclusively for decorating
+            methods and allow their behaviour to depend on the same class
+            when used as a context manager (see example below).
+            Defaults to ``True``.
+        in_key_attr (str, optional): the name of the input key list attribute
+            in the module's method being decorated. Defaults to ``in_keys``.
+        out_key_attr (str, optional): the name of the output key list attribute
+            in the module's method being decorated. Defaults to ``out_keys``.
+
+    Examples:
+        >>> with set_skip_existing():
+        ...     if skip_existing():
+        ...         print("True")
+        ...     else:
+        ...         print("False")
+        ...
+        True
+        >>> print("calling from outside:", skip_existing())
+        calling from outside: False
+
+    This class can also be used as a decorator:
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
+        >>> class MyModule(TensorDictModuleBase):
+        ...     in_keys = []
+        ...     out_keys = ["out"]
+        ...     @set_skip_existing()
+        ...     def forward(self, tensordict):
+        ...         print("hello")
+        ...         tensordict.set("out", torch.zeros(()))
+        ...         return tensordict
+        >>> module = MyModule()
+        >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
+        TensorDict(
+            fields={
+                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+        >>> module(TensorDict({}, []))  # prints hello
+        hello
+        TensorDict(
+            fields={
+                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    Decorating a method with the mode set to ``None`` is useful whenever one
+    wants ot let the context manager take care of skipping things from the outside:
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
+        >>> class MyModule(TensorDictModuleBase):
+        ...     in_keys = []
+        ...     out_keys = ["out"]
+        ...     @set_skip_existing(None)
+        ...     def forward(self, tensordict):
+        ...         print("hello")
+        ...         tensordict.set("out", torch.zeros(()))
+        ...         return tensordict
+        >>> module = MyModule()
+        >>> _ = module(TensorDict({"out": torch.zeros(())}, []))  # prints "hello"
+        hello
+        >>> with set_skip_existing(True):
+        ...     _ = module(TensorDict({"out": torch.zeros(())}, []))  # no print
+
+
+    .. note::
+        To allow for modules to have the same input and output keys and not
+        mistakenly ignoring subgraphs, ``@set_skip_existing(True)`` will be
+        deactivated whenever the output keys are also the input keys:
+
+            >>> class MyModule(TensorDictModuleBase):
+            ...     in_keys = ["out"]
+            ...     out_keys = ["out"]
+            ...     @set_skip_existing()
+            ...     def forward(self, tensordict):
+            ...         print("calling the method!")
+            ...         return tensordict
+            ...
+            >>> module = MyModule()
+            >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
+            calling the method!
+            TensorDict(
+                fields={
+                    out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+
+    """
+
+    def __init__(
+        self, mode: bool | None = True, in_key_attr="in_keys", out_key_attr="out_keys"
+    ):
+        self.mode = mode
+        self.in_key_attr = in_key_attr
+        self.out_key_attr = out_key_attr
+        self._called = False
+
+    def clone(self) -> set_skip_existing:
+        # override this method if your children class takes __init__ parameters
+        out = self.__class__(self.mode)
+        out._called = self._called
+        return out
+
+    def __call__(self, func: Callable):
+
+        self._called = True
+
+        # sanity check
+        for i, key in enumerate(inspect.signature(func).parameters):
+            if i == 0:
+                # skip self
+                continue
+            if key != "tensordict":
+                raise RuntimeError(
+                    "the first argument of the wrapped function must be "
+                    "named 'tensordict'."
+                )
+            break
+
+        @functools.wraps(func)
+        def wrapper(_self, tensordict, *args: Any, **kwargs: Any) -> Any:
+            in_keys = getattr(_self, self.in_key_attr)
+            out_keys = getattr(_self, self.out_key_attr)
+            # we use skip_existing to allow users to override the mode internally
+            if (
+                skip_existing()
+                and all(key in tensordict.keys(True) for key in out_keys)
+                and not any(key in out_keys for key in in_keys)
+            ):
+                return tensordict
+            return func(_self, tensordict, *args, **kwargs)
+
+        return super().__call__(wrapper)
+
+    def __enter__(self) -> None:
+        global _SKIP_EXISTING
+        self.prev = _SKIP_EXISTING
+        if self.mode is not None:
+            _SKIP_EXISTING = self.mode
+        elif not self._called:
+            raise RuntimeError(
+                f"It seems you are using {self.__class__.__name__} as a decorator with ``None`` input. "
+                f"This behaviour is not allowed."
+            )
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _SKIP_EXISTING
+        _SKIP_EXISTING = self.prev
+
+
+def skip_existing():
+    """Returns whether or not existing entries in a tensordict should be re-computed by a module."""
+    return _SKIP_EXISTING
```

## tensordict/prototype/tensorclass.py

```diff
@@ -6,18 +6,20 @@
     tensorclass as tensorclass_true,
 )
 
 
 @wraps(tensorclass_true)
 def tensorclass(*args, **kwargs):  # noqa: D103
     warnings.warn(
-        "tensorclass is not a prototype anymore and can be imported directly from tensordict root."
+        "tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
+        category=DeprecationWarning,
     )
     return tensorclass_true(*args, **kwargs)
 
 
 @wraps(is_tensorclass_true)
 def is_tensorclass(*args, **kwargs):  # noqa: D103
     warnings.warn(
-        "is_tensorclass is not a prototype anymore and can be imported directly from tensordict root."
+        "is_tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
+        category=DeprecationWarning,
     )
     return is_tensorclass_true(*args, **kwargs)
```

## Comparing `tensordict_nightly-2023.4.2.dist-info/LICENSE` & `tensordict_nightly-2023.5.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `tensordict_nightly-2023.4.2.dist-info/METADATA` & `tensordict_nightly-2023.5.2.dist-info/METADATA`

 * *Files 10% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: tensordict-nightly
-Version: 2023.4.2
+Version: 2023.5.2
 Home-page: https://github.com/pytorch-labs/tensordict
 Author: tensordict contributors
 Author-email: vmoens@fb.com
 License: BSD
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
@@ -20,25 +20,43 @@
 Provides-Extra: h5
 Requires-Dist: h5py (>=3.8) ; extra == 'h5'
 Provides-Extra: tests
 Requires-Dist: pytest ; extra == 'tests'
 Requires-Dist: pyyaml ; extra == 'tests'
 Requires-Dist: pytest-instafail ; extra == 'tests'
 Requires-Dist: pytest-rerunfailures ; extra == 'tests'
+Requires-Dist: pytest-benchmark ; extra == 'tests'
 
+<!--- BADGES: START --->
+<!---
 [![Documentation](https://img.shields.io/badge/Documentation-blue.svg?style=flat)](https://pytorch-labs.github.io/tensordict/)
-[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)](https://pytorch-labs.github.io/tensordict/dev/bench/)
+--->
+[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-package]
+[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)][#docs-package-benchmark]
 [![Python version](https://img.shields.io/pypi/pyversions/tensordict.svg)](https://www.python.org/downloads/)
-[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)](https://github.com/pytorch-labs/tensordict/blob/main/LICENSE)
+[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
 <a href="https://pypi.org/project/tensordict"><img src="https://img.shields.io/pypi/v/tensordict" alt="pypi version"></a>
 <a href="https://pypi.org/project/tensordict-nightly"><img src="https://img.shields.io/pypi/v/tensordict-nightly?label=nightly" alt="pypi nightly version"></a>
-[![Downloads](https://static.pepy.tech/personalized-badge/tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)](https://pepy.tech/project/tensordict)
-[![Downloads](https://static.pepy.tech/personalized-badge/tensordict-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))](https://pepy.tech/project/tensordict-nightly)
-[![codecov](https://codecov.io/gh/pytorch-labs/tensordict/branch/main/graph/badge.svg?token=9QTUG6NAGQ)](https://codecov.io/gh/pytorch-labs/tensordict)
-[![pytorch](https://circleci.com/gh/pytorch-labs/tensordict.svg?style=shield)](https://circleci.com/gh/pytorch-labs/tensordict)
+[![Downloads](https://static.pepy.tech/personalized-badge/tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)][#pepy-package]
+[![Downloads](https://static.pepy.tech/personalized-badge/tensordict-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))][#pepy-package-nightly]
+[![codecov](https://codecov.io/gh/pytorch-labs/tensordict/branch/main/graph/badge.svg?token=9QTUG6NAGQ)][#codecov-package]
+[![circleci](https://circleci.com/gh/pytorch-labs/tensordict.svg?style=shield)][#circleci-package]
+[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/tensordict?logo=anaconda&style=flat)][#conda-forge-package]
+[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/tensordict?logo=anaconda&style=flat&color=orange)][#conda-forge-package]
+
+[#docs-package]: https://pytorch-labs.github.io/tensordict/
+[#docs-package-benchmark]: https://pytorch-labs.github.io/tensordict/dev/bench/
+[#github-license]: https://github.com/pytorch-labs/tensordict/blob/main/LICENSE
+[#pepy-package]: https://pepy.tech/project/tensordict
+[#pepy-package-nightly]: https://pepy.tech/project/tensordict-nightly
+[#codecov-package]: https://codecov.io/gh/pytorch-labs/tensordict
+[#circleci-package]: https://circleci.com/gh/pytorch-labs/tensordict
+[#conda-forge-package]: https://anaconda.org/conda-forge/tensordict
+
+<!--- BADGES: END --->
 
 # TensorDict
 
 [**Installation**](#installation) | [**General features**](#general) |
 [**Tensor-like features**](#tensor-like-features) |  [**Distributed capabilities**](#distributed-capabilities) |
 [**TensorDict for functional programming using FuncTorch**](#tensordict-for-functional-programming-using-functorch) |
 [**Lazy preallocation**](#lazy-preallocation) | [**Nesting TensorDicts**](#nesting-tensordicts) | [**TensorClass**](#tensorclass)
@@ -161,15 +179,15 @@
 We also provide an API to use TensorDict in conjunction with [FuncTorch](https://pytorch.org/functorch).
 For instance, TensorDict makes it easy to concatenate model weights to do model ensembling:
 ```python
 >>> from torch import nn
 >>> from tensordict import TensorDict
 >>> from tensordict.nn import make_functional
 >>> import torch
->>> from functorch import vmap
+>>> from torch import vmap
 >>> layer1 = nn.Linear(3, 4)
 >>> layer2 = nn.Linear(4, 4)
 >>> model = nn.Sequential(layer1, layer2)
 >>> # we represent the weights hierarchically
 >>> weights1 = TensorDict(layer1.state_dict(), []).unflatten_keys(".")
 >>> weights2 = TensorDict(layer2.state_dict(), []).unflatten_keys(".")
 >>> params = make_functional(model)
@@ -307,25 +325,39 @@
 arbitrary functions through the `apply` method (and many more).
 
 Tensorclasses support nesting and, in fact, all the TensorDict features.
 
 
 ## Installation
 
+**With Pip**:
+
 To install the latest stable version of tensordict, simply run
+
 ```bash
 pip install tensordict
 ```
+
 This will work with Python 3.7 and upward as well as PyTorch 1.12 and upward.
 
 To enjoy the latest features, one can use
+
 ```bash
 pip install tensordict-nightly
 ```
 
+**With Conda**:
+
+Install `tensordict` from `conda-forge` channel.
+
+```sh
+conda install -c conda-forge tensordict
+```
+
+
 ## Citation
 
 If you're using TensorDict, please refer to this BibTeX entry to cite this work:
 ```
 @software{TensorDict,
   author = {Moens, Vincent},
   title = {{TensorDict: your PyTorch universal data carrier}},
```

### html2text {}

```diff
@@ -1,114 +1,124 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.4.2 Home-page:
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2023.5.2 Home-page:
 https://github.com/pytorch-labs/tensordict Author: tensordict contributors
 Author-email: vmoens@fb.com License: BSD Classifier: Programming Language ::
 Python :: 3.7 Classifier: Programming Language :: Python :: 3.8 Classifier:
 Programming Language :: Python :: 3.9 Classifier: Programming Language ::
 Python :: 3.10 Classifier: Development Status :: 4 - Beta Description-Content-
 Type: text/markdown License-File: LICENSE Requires-Dist: torch Requires-Dist:
 numpy Requires-Dist: cloudpickle Provides-Extra: checkpointing Requires-Dist:
 torchsnapshot-nightly ; extra == 'checkpointing' Provides-Extra: h5 Requires-
 Dist: h5py (>=3.8) ; extra == 'h5' Provides-Extra: tests Requires-Dist: pytest
 ; extra == 'tests' Requires-Dist: pyyaml ; extra == 'tests' Requires-Dist:
 pytest-instafail ; extra == 'tests' Requires-Dist: pytest-rerunfailures ; extra
-== 'tests' [![Documentation](https://img.shields.io/badge/Documentation-
-blue.svg?style=flat)](https://pytorch-labs.github.io/tensordict/) [!
-[Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)](https://
-pytorch-labs.github.io/tensordict/dev/bench/) [![Python version](https://
-img.shields.io/pypi/pyversions/tensordict.svg)](https://www.python.org/
-downloads/) [![GitHub license](https://img.shields.io/badge/license-MIT-
-blue.svg)](https://github.com/pytorch-labs/tensordict/blob/main/LICENSE) [pypi
-version] [pypi_nightly_version] [![Downloads](https://static.pepy.tech/
+== 'tests' Requires-Dist: pytest-benchmark ; extra == 'tests'   [![Docs -
+GitHub.io](https://img.shields.io/static/
+v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-
+package] [![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)]
+[#docs-package-benchmark] [![Python version](https://img.shields.io/pypi/
+pyversions/tensordict.svg)](https://www.python.org/downloads/) [![GitHub
+license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
+[pypi_version] [pypi_nightly_version] [![Downloads](https://static.pepy.tech/
 personalized-badge/
 tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)]
-(https://pepy.tech/project/tensordict) [![Downloads](https://static.pepy.tech/
-personalized-badge/tensordict-
+[#pepy-package] [![Downloads](https://static.pepy.tech/personalized-badge/
+tensordict-
 nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20
-(nightly))](https://pepy.tech/project/tensordict-nightly) [![codecov](https://
-codecov.io/gh/pytorch-labs/tensordict/branch/main/graph/
-badge.svg?token=9QTUG6NAGQ)](https://codecov.io/gh/pytorch-labs/tensordict) [!
-[pytorch](https://circleci.com/gh/pytorch-labs/tensordict.svg?style=shield)]
-(https://circleci.com/gh/pytorch-labs/tensordict) # TensorDict
-[**Installation**](#installation) | [**General features**](#general) |
-[**Tensor-like features**](#tensor-like-features) | [**Distributed
-capabilities**](#distributed-capabilities) | [**TensorDict for functional
-programming using FuncTorch**](#tensordict-for-functional-programming-using-
-functorch) | [**Lazy preallocation**](#lazy-preallocation) | [**Nesting
-TensorDicts**](#nesting-tensordicts) | [**TensorClass**](#tensorclass)
-`TensorDict` is a dictionary-like class that inherits properties from tensors,
-such as indexing, shape operations, casting to device or point-to-point
-communication in distributed settings. The main purpose of TensorDict is to
-make code-bases more _readable_ and _modular_ by abstracting away tailored
-operations: ```python for i, tensordict in enumerate(dataset): # the model
-reads and writes tensordicts tensordict = model(tensordict) loss = loss_module
-(tensordict) loss.backward() optimizer.step() optimizer.zero_grad() ``` With
-this level of abstraction, one can recycle a training loop for highly
-heterogeneous task. Each individual step of the training loop (data collection
-and transform, model prediction, loss computation etc.) can be tailored to the
-use case at hand without impacting the others. For instance, the above example
-can be easily used across classification and segmentation tasks, among many
-others. ## Features ### General A tensordict is primarily defined by its
-`batch_size` (or `shape`) and its key-value pairs: ```python >>> from
-tensordict import TensorDict >>> import torch >>> tensordict = TensorDict({ ...
-"key 1": torch.ones(3, 4, 5), ... "key 2": torch.zeros(3, 4, 5,
-dtype=torch.bool), ... }, batch_size=[3, 4]) ``` The `batch_size` and the first
-dimensions of each of the tensors must be compliant. The tensors can be of any
-dtype and device. Optionally, one can restrict a tensordict to live on a
-dedicated device, which will send each tensor that is written there: ```python
->>> tensordict = TensorDict({ ... "key 1": torch.ones(3, 4, 5), ... "key 2":
-torch.zeros(3, 4, 5, dtype=torch.bool), ... }, batch_size=[3, 4], device="cuda:
-0") >>> tensordict["key 3"] = torch.randn(3, 4, device="cpu") >>> assert
-tensordict["key 3"].device is torch.device("cuda:0") ``` ### Tensor-like
-features TensorDict objects can be indexed exactly like tensors. The resulting
-of indexing a TensorDict is another TensorDict containing tensors indexed along
-the required dimension: ```python >>> tensordict = TensorDict({ ... "key 1":
-torch.ones(3, 4, 5), ... "key 2": torch.zeros(3, 4, 5, dtype=torch.bool), ...
-}, batch_size=[3, 4]) >>> sub_tensordict = tensordict[..., :2] >>> assert
-sub_tensordict.shape == torch.Size([3, 2]) >>> assert sub_tensordict["key
-1"].shape == torch.Size([3, 2, 5]) ``` Similarly, one can build tensordicts by
-stacking or concatenating single tensordicts: ```python >>> tensordicts =
-[TensorDict({ ... "key 1": torch.ones(3, 4, 5), ... "key 2": torch.zeros(3, 4,
-5, dtype=torch.bool), ... }, batch_size=[3, 4]) for _ in range(2)] >>>
-stack_tensordict = torch.stack(tensordicts, 1) >>> assert
-stack_tensordict.shape == torch.Size([3, 2, 4]) >>> assert stack_tensordict
-["key 1"].shape == torch.Size([3, 2, 4, 5]) >>> cat_tensordict = torch.cat
-(tensordicts, 0) >>> assert cat_tensordict.shape == torch.Size([6, 4]) >>>
-assert cat_tensordict["key 1"].shape == torch.Size([6, 4, 5]) ``` TensorDict
-instances can also be reshaped, viewed, squeezed and unsqueezed: ```python >>>
+(nightly))][#pepy-package-nightly] [![codecov](https://codecov.io/gh/pytorch-
+labs/tensordict/branch/main/graph/badge.svg?token=9QTUG6NAGQ)][#codecov-
+package] [![circleci](https://circleci.com/gh/pytorch-labs/
+tensordict.svg?style=shield)][#circleci-package] [![Conda - Platform](https://
+img.shields.io/conda/pn/conda-forge/tensordict?logo=anaconda&style=flat)]
+[#conda-forge-package] [![Conda (channel only)](https://img.shields.io/conda/
+vn/conda-forge/tensordict?logo=anaconda&style=flat&color=orange)][#conda-forge-
+package] [#docs-package]: https://pytorch-labs.github.io/tensordict/ [#docs-
+package-benchmark]: https://pytorch-labs.github.io/tensordict/dev/bench/
+[#github-license]: https://github.com/pytorch-labs/tensordict/blob/main/LICENSE
+[#pepy-package]: https://pepy.tech/project/tensordict [#pepy-package-nightly]:
+https://pepy.tech/project/tensordict-nightly [#codecov-package]: https://
+codecov.io/gh/pytorch-labs/tensordict [#circleci-package]: https://
+circleci.com/gh/pytorch-labs/tensordict [#conda-forge-package]: https://
+anaconda.org/conda-forge/tensordict  # TensorDict [**Installation**]
+(#installation) | [**General features**](#general) | [**Tensor-like features**]
+(#tensor-like-features) | [**Distributed capabilities**](#distributed-
+capabilities) | [**TensorDict for functional programming using FuncTorch**]
+(#tensordict-for-functional-programming-using-functorch) | [**Lazy
+preallocation**](#lazy-preallocation) | [**Nesting TensorDicts**](#nesting-
+tensordicts) | [**TensorClass**](#tensorclass) `TensorDict` is a dictionary-
+like class that inherits properties from tensors, such as indexing, shape
+operations, casting to device or point-to-point communication in distributed
+settings. The main purpose of TensorDict is to make code-bases more _readable_
+and _modular_ by abstracting away tailored operations: ```python for i,
+tensordict in enumerate(dataset): # the model reads and writes tensordicts
+tensordict = model(tensordict) loss = loss_module(tensordict) loss.backward()
+optimizer.step() optimizer.zero_grad() ``` With this level of abstraction, one
+can recycle a training loop for highly heterogeneous task. Each individual step
+of the training loop (data collection and transform, model prediction, loss
+computation etc.) can be tailored to the use case at hand without impacting the
+others. For instance, the above example can be easily used across
+classification and segmentation tasks, among many others. ## Features ###
+General A tensordict is primarily defined by its `batch_size` (or `shape`) and
+its key-value pairs: ```python >>> from tensordict import TensorDict >>> import
+torch >>> tensordict = TensorDict({ ... "key 1": torch.ones(3, 4, 5), ... "key
+2": torch.zeros(3, 4, 5, dtype=torch.bool), ... }, batch_size=[3, 4]) ``` The
+`batch_size` and the first dimensions of each of the tensors must be compliant.
+The tensors can be of any dtype and device. Optionally, one can restrict a
+tensordict to live on a dedicated device, which will send each tensor that is
+written there: ```python >>> tensordict = TensorDict({ ... "key 1": torch.ones
+(3, 4, 5), ... "key 2": torch.zeros(3, 4, 5, dtype=torch.bool), ... },
+batch_size=[3, 4], device="cuda:0") >>> tensordict["key 3"] = torch.randn(3, 4,
+device="cpu") >>> assert tensordict["key 3"].device is torch.device("cuda:0")
+``` ### Tensor-like features TensorDict objects can be indexed exactly like
+tensors. The resulting of indexing a TensorDict is another TensorDict
+containing tensors indexed along the required dimension: ```python >>>
 tensordict = TensorDict({ ... "key 1": torch.ones(3, 4, 5), ... "key 2":
-torch.zeros(3, 4, 5, dtype=torch.bool), ... }, batch_size=[3, 4]) >>> print
-(tensordict.view(-1)) torch.Size([12]) >>> print(tensordict.reshape(-1))
-torch.Size([12]) >>> print(tensordict.unsqueeze(-1)) torch.Size([3, 4, 1]) ```
-One can also send tensordict from device to device, place them in shared
-memory, clone them, update them in-place or not, split them, unbind them,
-expand them etc. If a functionality is missing, it is easy to call it using
-`apply()` or `apply_()`: ```python tensordict_uniform = tensordict.apply(lambda
-tensor: tensor.uniform_()) ``` ### Distributed capabilities Complex data
-structures can be cumbersome to synchronize in distributed settings.
-`tensordict` solves that problem with synchronous and asynchronous helper
-methods such as `recv`, `irecv`, `send` and `isend` that behave like their
-`torch.distributed` counterparts: ```python >>> # on all workers >>> data =
-TensorDict({"a": torch.zeros(()), ("b", "c"): torch.ones(())}, []) >>> # on
-worker 1 >>> data.isend(dst=0) >>> # on worker 0 >>> data.irecv(src=1) ``` When
-nodes share a common scratch space, the [`MemmapTensor` backend](https://
-pytorch-labs.github.io/tensordict/tutorials/tensordict_memory.html) can be used
-to seamlessly send, receive and read a huge amount of data. ### TensorDict for
-functional programming using FuncTorch We also provide an API to use TensorDict
-in conjunction with [FuncTorch](https://pytorch.org/functorch). For instance,
-TensorDict makes it easy to concatenate model weights to do model ensembling:
-```python >>> from torch import nn >>> from tensordict import TensorDict >>>
-from tensordict.nn import make_functional >>> import torch >>> from functorch
-import vmap >>> layer1 = nn.Linear(3, 4) >>> layer2 = nn.Linear(4, 4) >>> model
-= nn.Sequential(layer1, layer2) >>> # we represent the weights hierarchically
->>> weights1 = TensorDict(layer1.state_dict(), []).unflatten_keys(".") >>>
-weights2 = TensorDict(layer2.state_dict(), []).unflatten_keys(".") >>> params =
-make_functional(model) >>> assert (params == TensorDict({"0": weights1, "1":
-weights2}, [])).all() >>> # Let's use our functional module >>> x = torch.randn
-(10, 3) >>> out = model(x, params=params) # params is the last arg (or kwarg)
->>> # an ensemble of models: we stack params along the first dimension... >>>
+torch.zeros(3, 4, 5, dtype=torch.bool), ... }, batch_size=[3, 4]) >>>
+sub_tensordict = tensordict[..., :2] >>> assert sub_tensordict.shape ==
+torch.Size([3, 2]) >>> assert sub_tensordict["key 1"].shape == torch.Size([3,
+2, 5]) ``` Similarly, one can build tensordicts by stacking or concatenating
+single tensordicts: ```python >>> tensordicts = [TensorDict({ ... "key 1":
+torch.ones(3, 4, 5), ... "key 2": torch.zeros(3, 4, 5, dtype=torch.bool), ...
+}, batch_size=[3, 4]) for _ in range(2)] >>> stack_tensordict = torch.stack
+(tensordicts, 1) >>> assert stack_tensordict.shape == torch.Size([3, 2, 4]) >>>
+assert stack_tensordict["key 1"].shape == torch.Size([3, 2, 4, 5]) >>>
+cat_tensordict = torch.cat(tensordicts, 0) >>> assert cat_tensordict.shape ==
+torch.Size([6, 4]) >>> assert cat_tensordict["key 1"].shape == torch.Size([6,
+4, 5]) ``` TensorDict instances can also be reshaped, viewed, squeezed and
+unsqueezed: ```python >>> tensordict = TensorDict({ ... "key 1": torch.ones(3,
+4, 5), ... "key 2": torch.zeros(3, 4, 5, dtype=torch.bool), ... }, batch_size=
+[3, 4]) >>> print(tensordict.view(-1)) torch.Size([12]) >>> print
+(tensordict.reshape(-1)) torch.Size([12]) >>> print(tensordict.unsqueeze(-1))
+torch.Size([3, 4, 1]) ``` One can also send tensordict from device to device,
+place them in shared memory, clone them, update them in-place or not, split
+them, unbind them, expand them etc. If a functionality is missing, it is easy
+to call it using `apply()` or `apply_()`: ```python tensordict_uniform =
+tensordict.apply(lambda tensor: tensor.uniform_()) ``` ### Distributed
+capabilities Complex data structures can be cumbersome to synchronize in
+distributed settings. `tensordict` solves that problem with synchronous and
+asynchronous helper methods such as `recv`, `irecv`, `send` and `isend` that
+behave like their `torch.distributed` counterparts: ```python >>> # on all
+workers >>> data = TensorDict({"a": torch.zeros(()), ("b", "c"): torch.ones(
+())}, []) >>> # on worker 1 >>> data.isend(dst=0) >>> # on worker 0 >>>
+data.irecv(src=1) ``` When nodes share a common scratch space, the
+[`MemmapTensor` backend](https://pytorch-labs.github.io/tensordict/tutorials/
+tensordict_memory.html) can be used to seamlessly send, receive and read a huge
+amount of data. ### TensorDict for functional programming using FuncTorch We
+also provide an API to use TensorDict in conjunction with [FuncTorch](https://
+pytorch.org/functorch). For instance, TensorDict makes it easy to concatenate
+model weights to do model ensembling: ```python >>> from torch import nn >>>
+from tensordict import TensorDict >>> from tensordict.nn import make_functional
+>>> import torch >>> from torch import vmap >>> layer1 = nn.Linear(3, 4) >>>
+layer2 = nn.Linear(4, 4) >>> model = nn.Sequential(layer1, layer2) >>> # we
+represent the weights hierarchically >>> weights1 = TensorDict
+(layer1.state_dict(), []).unflatten_keys(".") >>> weights2 = TensorDict
+(layer2.state_dict(), []).unflatten_keys(".") >>> params = make_functional
+(model) >>> assert (params == TensorDict({"0": weights1, "1": weights2},
+[])).all() >>> # Let's use our functional module >>> x = torch.randn(10, 3) >>>
+out = model(x, params=params) # params is the last arg (or kwarg) >>> # an
+ensemble of models: we stack params along the first dimension... >>>
 params_stack = torch.stack([params, params], 0) >>> # ... and use it as an
 input we'd like to pass through the model >>> y = vmap(model, (None, 0))(x,
 params_stack) >>> print(y.shape) torch.Size([2, 10, 4]) ``` Moreover,
 tensordict modules are compatible with `torch.fx` and `torch.compile`, which
 means that you can get the best of both worlds: a codebase that is both
 readable and future-proof as well as efficient and portable! ### Lazy
 preallocation Pre-allocating tensors can be cumbersome and hard to scale if the
@@ -162,21 +172,22 @@
 dtype=torch.int64), mask=Tensor(torch.Size([10, 10, 1, 64, 64]),
 dtype=torch.bool), batch_size=torch.Size([10, 10]), device=None,
 is_shared=False) ``` As this example shows, one can write a specific data
 structures with dedicated methods while still enjoying the TensorDict artifacts
 such as shape operations (e.g. reshape or permutations), data manipulation
 (indexing, `cat` and `stack`) or calling arbitrary functions through the
 `apply` method (and many more). Tensorclasses support nesting and, in fact, all
-the TensorDict features. ## Installation To install the latest stable version
-of tensordict, simply run ```bash pip install tensordict ``` This will work
-with Python 3.7 and upward as well as PyTorch 1.12 and upward. To enjoy the
-latest features, one can use ```bash pip install tensordict-nightly ``` ##
-Citation If you're using TensorDict, please refer to this BibTeX entry to cite
-this work: ``` @software{TensorDict, author = {Moens, Vincent}, title = {
-{TensorDict: your PyTorch universal data carrier}}, url = {https://github.com/
-pytorch-labs/tensordict}, version = {0.1.0}, year = {2023} } ``` ## Disclaimer
-TensorDict is at the *beta*-stage, meaning that there may be bc-breaking
-changes introduced, but they should come with a warranty. Hopefully these
-should not happen too often, as the current roadmap mostly involves adding new
-features and building compatibility with the broader PyTorch ecosystem. ##
-License TensorDict is licensed under the MIT License. See [LICENSE](LICENSE)
-for details.
+the TensorDict features. ## Installation **With Pip**: To install the latest
+stable version of tensordict, simply run ```bash pip install tensordict ```
+This will work with Python 3.7 and upward as well as PyTorch 1.12 and upward.
+To enjoy the latest features, one can use ```bash pip install tensordict-
+nightly ``` **With Conda**: Install `tensordict` from `conda-forge` channel.
+```sh conda install -c conda-forge tensordict ``` ## Citation If you're using
+TensorDict, please refer to this BibTeX entry to cite this work: ``` @software
+{TensorDict, author = {Moens, Vincent}, title = {{TensorDict: your PyTorch
+universal data carrier}}, url = {https://github.com/pytorch-labs/tensordict},
+version = {0.1.0}, year = {2023} } ``` ## Disclaimer TensorDict is at the
+*beta*-stage, meaning that there may be bc-breaking changes introduced, but
+they should come with a warranty. Hopefully these should not happen too often,
+as the current roadmap mostly involves adding new features and building
+compatibility with the broader PyTorch ecosystem. ## License TensorDict is
+licensed under the MIT License. See [LICENSE](LICENSE) for details.
```

## Comparing `tensordict_nightly-2023.4.2.dist-info/RECORD` & `tensordict_nightly-2023.5.2.dist-info/RECORD`

 * *Files 20% similar despite different names*

```diff
@@ -1,26 +1,27 @@
 tensordict/__init__.py,sha256=dDQidc208swLADInUxPlfvgfERJr2xeVTNC5NRvwN4A,1088
+tensordict/_contextlib.py,sha256=F4b0KH4FSeKsY9dkRuIXgHEn1dL_aNk7lFs1DonDYn4,6000
 tensordict/memmap.py,sha256=sBIRRJVadGiXlT5A8UfxoQqn6EEE1TBUDUaCyz65Yws,29601
-tensordict/persistent.py,sha256=qml2n4IQkPLVImuSOGSMDWqQL39hz4L2uArsRBELX7o,31903
-tensordict/tensorclass.py,sha256=JJjGuWXYi9SM21meEmSg99c1BtztqlRyfT_nzpSiBmk,30447
-tensordict/tensordict.py,sha256=Nu1U9L5IH1vXVUzodpfvY534YTY6VR7kddllb4lidtI,241935
-tensordict/utils.py,sha256=2WPzIw-eluw0MbO39SIkXGevYGZfVEaJwrmsd5s1Nxo,25528
-tensordict/version.py,sha256=O0j3hEtifgokRkChOlhfAT1pFY9FoAxmzDRfcv5hW44,84
-tensordict/nn/__init__.py,sha256=CAwIhG88AHZr9CuT7RnXsqidMIK8HIF9q6EgCGuO2WY,1074
-tensordict/nn/common.py,sha256=mm5nG3SAAvjtdgFDQ1r1ge4fS02Xp8b0tbGgYGlbnwI,18697
-tensordict/nn/functional_modules.py,sha256=p7GmQ0fBYE6_FG21ZeIqp_a9Zn9uoB1UkrFr8Ua1sRA,17837
-tensordict/nn/probabilistic.py,sha256=ExwiGs4Na_GieADaSSJDcE7O5sFHPTvaeTDSI0Bd5kA,19304
-tensordict/nn/sequence.py,sha256=88U9wunww1K82DfoqfTIKZYw3P_5QIMNCm7SMLUq5oI,11616
-tensordict/nn/utils.py,sha256=klUTQV7ixDmcmUB6v2eFr4hbO0StxLZY-c8C4wRpeQI,3198
+tensordict/persistent.py,sha256=CkF6hVsKAO21Le-7fuUChsLczoUr7Eflj6A6H7Z1XEo,33049
+tensordict/tensorclass.py,sha256=6eGhArep4GwEOeMFrdvv9MyVuy6oc4G58mhI2AVPh8M,30420
+tensordict/tensordict.py,sha256=g1nZs6TsTwb2xYaxu08QIWcuHCTYvrKOvs2G8DeWTdc,264665
+tensordict/utils.py,sha256=rZjY_b8mGys_YDWyWIQbZ1xeiJrXvG-u7sUxaSsjqIc,26209
+tensordict/version.py,sha256=uI1lmSuyHM-zE-ooE5krMgGvRcL76q6U47jYk9mhYE4,84
+tensordict/nn/__init__.py,sha256=gjahPjJuC1uGPppYt7yZ8Ifusf2hTnOJHZGf8IutgoU,1314
+tensordict/nn/common.py,sha256=E7x13Ft194FD4jhF2VMDy80p-K9W5R7oZeZtlKqCvJo,35195
+tensordict/nn/functional_modules.py,sha256=c01QAbWQBVMn2CULMymDwq9VK78ejz70fw9nkvxND5s,17891
+tensordict/nn/probabilistic.py,sha256=ZYa7f2PzWqgwTw3TJAcd125HRpD4p6aPc1HD5o709WM,22301
+tensordict/nn/sequence.py,sha256=l5nuYu0Ja11IO9qWSlijvRZH4XDLbUy8lwkFmyAHrHE,19564
+tensordict/nn/utils.py,sha256=Hb8zIWL_wGs8OSaGjqQJ4DAadLqA9d4GARR7EboKOFU,10621
 tensordict/nn/distributions/__init__.py,sha256=sD3yMuMceDJ8EqtptE4HEaS9Q_3keskCB9npPNFKXUo,499
 tensordict/nn/distributions/continuous.py,sha256=Y31G7ozOnlGUgzdqYZaKeXVjDOVMoSW1hIQWtcbyDO0,7073
 tensordict/nn/distributions/discrete.py,sha256=gUzTKs3yO8GkSY0ghBWVaZdHdmFyMMLWd5LV52CfrXY,2580
 tensordict/nn/distributions/truncated_normal.py,sha256=d1ontYbG_Q-W5gcVnWLadvZ1OBBeZGmBb5EUmr3ekak,6504
 tensordict/nn/distributions/utils.py,sha256=fX6NUeNnWKK6kDdOp8NTM43Lls3vMCF5h-S2teVZw3E,1226
 tensordict/prototype/__init__.py,sha256=b9Wmi1tbh2Em_wmKa52IXwxuudd6CUqdLgaob0bSWqQ,381
 tensordict/prototype/fx.py,sha256=Z3X821miRKcrOYwApvBDYt2_hST3IXNVe4v03tVvQKo,7507
-tensordict/prototype/tensorclass.py,sha256=sS-Sm8UKqLOa9Bxp49ysN7u4yWXy9Osssgr8RUn7vag,663
-tensordict_nightly-2023.4.2.dist-info/LICENSE,sha256=xdjS4_xk-IwnLuIFCvTYTl9Y8aXRejqpmke3dGam_nI,1098
-tensordict_nightly-2023.4.2.dist-info/METADATA,sha256=6f7TmfUPPJ-O3gwldomUt_5HRdzNgFHxXEQxE6L65s4,14192
-tensordict_nightly-2023.4.2.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
-tensordict_nightly-2023.4.2.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
-tensordict_nightly-2023.4.2.dist-info/RECORD,,
+tensordict/prototype/tensorclass.py,sha256=pE53rWFqcPmuJOyOkI7cBZL9sMVfPx8bdQR1hNgYPvU,739
+tensordict_nightly-2023.5.2.dist-info/LICENSE,sha256=xdjS4_xk-IwnLuIFCvTYTl9Y8aXRejqpmke3dGam_nI,1098
+tensordict_nightly-2023.5.2.dist-info/METADATA,sha256=X6pCDowZzeOhapxWdKgrJ5fM9RVNb2vnIFlkeAeOTjY,15208
+tensordict_nightly-2023.5.2.dist-info/WHEEL,sha256=ns_9KNZvwSNZtRgVV_clzMUG_fXjGc5Z8Tx4hxQ0gkw,93
+tensordict_nightly-2023.5.2.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
+tensordict_nightly-2023.5.2.dist-info/RECORD,,
```

