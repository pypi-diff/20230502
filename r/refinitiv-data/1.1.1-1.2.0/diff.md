# Comparing `tmp/refinitiv-data-1.1.1.tar.gz` & `tmp/refinitiv-data-1.2.0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "refinitiv-data-1.1.1.tar", last modified: Thu Feb 23 14:40:45 2023, max compression
+gzip compressed data, was "refinitiv-data-1.2.0.tar", last modified: Tue May  2 10:11:28 2023, max compression
```

## Comparing `refinitiv-data-1.1.1.tar` & `refinitiv-data-1.2.0.tar`

### file list

```diff
@@ -1,992 +1,1033 @@
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.635148 refinitiv-data-1.1.1/
--rw-rw-rw-   0 root         (0) root         (0)    11358 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/LICENSE
--rw-r--r--   0 root         (0) root         (0)     9778 2023-02-23 14:40:45.635148 refinitiv-data-1.1.1/PKG-INFO
--rw-rw-rw-   0 root         (0) root         (0)     9061 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/README.md
--rw-rw-rw-   0 root         (0) root         (0)     2578 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/pyproject.toml
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.439148 refinitiv-data-1.1.1/refinitiv/
--rw-rw-rw-   0 root         (0) root         (0)       76 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.443148 refinitiv-data-1.1.1/refinitiv/data/
--rw-rw-rw-   0 root         (0) root         (0)     1096 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    16457 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_config_defaults.py
--rw-rw-rw-   0 root         (0) root         (0)     1165 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_config_functions.py
--rw-rw-rw-   0 root         (0) root         (0)    13460 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_configure.py
--rw-rw-rw-   0 root         (0) root         (0)     3871 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_content_type.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.443148 refinitiv-data-1.1.1/refinitiv/data/_core/
--rw-rw-rw-   0 root         (0) root         (0)        0 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1572 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/log_reporter.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.447148 refinitiv-data-1.1.1/refinitiv/data/_core/session/
--rw-rw-rw-   0 root         (0) root         (0)      719 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      245 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_dacs_params.py
--rw-rw-rw-   0 root         (0) root         (0)     3997 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_default_session_manager.py
--rw-rw-rw-   0 root         (0) root         (0)     5220 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_desktop_session.py
--rw-rw-rw-   0 root         (0) root         (0)     9167 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_platform_session.py
--rw-rw-rw-   0 root         (0) root         (0)    12196 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_retry_transport.py
--rw-rw-rw-   0 root         (0) root         (0)   680348 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_scope_map.py
--rw-rw-rw-   0 root         (0) root         (0)    14783 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_session.py
--rw-rw-rw-   0 root         (0) root         (0)     1112 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_cxn_factory.py
--rw-rw-rw-   0 root         (0) root         (0)      188 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_cxn_type.py
--rw-rw-rw-   0 root         (0) root         (0)     2538 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    10341 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_provider.py
--rw-rw-rw-   0 root         (0) root         (0)       95 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_type.py
--rw-rw-rw-   0 root         (0) root         (0)     3778 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/access_token_updater.py
--rw-rw-rw-   0 root         (0) root         (0)    12000 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/auth_manager.py
--rw-rw-rw-   0 root         (0) root         (0)    10942 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/connection.py
--rw-rw-rw-   0 root         (0) root         (0)      660 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/event.py
--rw-rw-rw-   0 root         (0) root         (0)     1469 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/event_code.py
--rw-rw-rw-   0 root         (0) root         (0)      489 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/global_settings.py
--rw-rw-rw-   0 root         (0) root         (0)     1475 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/grant_password.py
--rw-rw-rw-   0 root         (0) root         (0)    12746 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/http_service.py
--rw-rw-rw-   0 root         (0) root         (0)      620 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/null_session.py
--rw-rw-rw-   0 root         (0) root         (0)     4353 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/refresh_token_updater.py
--rw-rw-rw-   0 root         (0) root         (0)     2487 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/tools.py
--rw-rw-rw-   0 root         (0) root         (0)     3990 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_core/session/updater.py
--rw-rw-rw-   0 root         (0) root         (0)     2315 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_errors.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.447148 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/
--rw-rw-rw-   0 root         (0) root         (0)        0 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.451148 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/
--rw-rw-rw-   0 root         (0) root         (0)     1071 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/LICENSE
--rw-rw-rw-   0 root         (0) root         (0)    26502 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    12473 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/configuration.py
--rw-rw-rw-   0 root         (0) root         (0)     7392 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/configuration_set.py
--rw-rw-rw-   0 root         (0) root         (0)     5965 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/helpers.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.451148 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/
--rw-rw-rw-   0 root         (0) root         (0)      165 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     4573 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_containers.py
--rw-rw-rw-   0 root         (0) root         (0)     3794 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     7104 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_history_provider.py
--rw-rw-rw-   0 root         (0) root         (0)      705 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_history_provider_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     1500 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_intervals_consts.py
--rw-rw-rw-   0 root         (0) root         (0)     4269 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_mixed_streams.py
--rw-rw-rw-   0 root         (0) root         (0)     5072 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_ohlc_builder.py
--rw-rw-rw-   0 root         (0) root         (0)    12475 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_pricing_recorder.py
--rw-rw-rw-   0 root         (0) root         (0)     5372 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_recording_control.py
--rw-rw-rw-   0 root         (0) root         (0)     3158 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_stream_update_handler.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.455148 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/
--rw-rw-rw-   0 root         (0) root         (0)      246 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     4776 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_adc_context.py
--rw-rw-rw-   0 root         (0) root         (0)     2606 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_adc_rdp_context.py
--rw-rw-rw-   0 root         (0) root         (0)     1615 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_adc_udf_context.py
--rw-rw-rw-   0 root         (0) root         (0)     2878 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_context.py
--rw-rw-rw-   0 root         (0) root         (0)     2920 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_context_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     5903 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_context.py
--rw-rw-rw-   0 root         (0) root         (0)     2878 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_rdp_context.py
--rw-rw-rw-   0 root         (0) root         (0)     2354 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_udf_context.py
--rw-rw-rw-   0 root         (0) root         (0)      436 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_hp_and_cust_inst_context.py
--rw-rw-rw-   0 root         (0) root         (0)     4117 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_hp_context.py
--rw-rw-rw-   0 root         (0) root         (0)      758 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_hp_rdp_context.py
--rw-rw-rw-   0 root         (0) root         (0)      712 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_hp_udf_context.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.459148 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/
--rw-rw-rw-   0 root         (0) root         (0)      621 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2397 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_add_periods.py
--rw-rw-rw-   0 root         (0) root         (0)     2539 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_count_periods.py
--rw-rw-rw-   0 root         (0) root         (0)     3593 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_date_schedule.py
--rw-rw-rw-   0 root         (0) root         (0)     4651 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_holidays.py
--rw-rw-rw-   0 root         (0) root         (0)     1418 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_is_working_day.py
--rw-rw-rw-   0 root         (0) root         (0)     7218 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/get_data.py
--rw-rw-rw-   0 root         (0) root         (0)    11513 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/get_history.py
--rw-rw-rw-   0 root         (0) root         (0)     3439 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/get_stream.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.459148 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/news/
--rw-rw-rw-   0 root         (0) root         (0)      247 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/news/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2661 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/news/_news.py
--rw-rw-rw-   0 root         (0) root         (0)     2839 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/session.py
--rw-rw-rw-   0 root         (0) root         (0)    13028 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_log.py
--rw-rw-rw-   0 root         (0) root         (0)      338 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_open_state.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.459148 refinitiv-data-1.1.1/refinitiv/data/_qpl/
--rw-rw-rw-   0 root         (0) root         (0)      313 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      641 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_bid_ask.py
--rw-rw-rw-   0 root         (0) root         (0)     2781 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)      766 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_fx_spot_quote.py
--rw-rw-rw-   0 root         (0) root         (0)     2045 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_fx_swap_points.py
--rw-rw-rw-   0 root         (0) root         (0)     3720 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_fx_swp_to_swp.py
--rw-rw-rw-   0 root         (0) root         (0)      932 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_serializer.py
--rw-rw-rw-   0 root         (0) root         (0)      760 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_tenor_bid_ask.py
--rw-rw-rw-   0 root         (0) root         (0)      257 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_qpl/_tenor_types.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.463148 refinitiv-data-1.1.1/refinitiv/data/_tools/
--rw-rw-rw-   0 root         (0) root         (0)      187 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    13354 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_common.py
--rw-rw-rw-   0 root         (0) root         (0)     1214 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_converter.py
--rw-rw-rw-   0 root         (0) root         (0)     5864 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_dataframe.py
--rw-rw-rw-   0 root         (0) root         (0)     4061 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_datetime.py
--rw-rw-rw-   0 root         (0) root         (0)       53 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_debug.py
--rw-rw-rw-   0 root         (0) root         (0)     1768 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_params.py
--rw-rw-rw-   0 root         (0) root         (0)      835 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_patterns.py
--rw-rw-rw-   0 root         (0) root         (0)     1083 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_repr.py
--rw-rw-rw-   0 root         (0) root         (0)     1361 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_specification.py
--rw-rw-rw-   0 root         (0) root         (0)     2105 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/_utils.py
--rw-rw-rw-   0 root         (0) root         (0)     5285 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_tools/templates.py
--rw-rw-rw-   0 root         (0) root         (0)      630 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/_types.py
--rw-r--r--   0 root         (0) root         (0)      160 2023-02-23 14:40:45.000000 refinitiv-data-1.1.1/refinitiv/data/_version.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.467148 refinitiv-data-1.1.1/refinitiv/data/content/
--rw-rw-rw-   0 root         (0) root         (0)      572 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      609 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_content_data.py
--rw-rw-rw-   0 root         (0) root         (0)     1092 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_content_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     2502 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_content_provider_layer.py
--rw-rw-rw-   0 root         (0) root         (0)     1543 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_content_response_factory.py
--rw-rw-rw-   0 root         (0) root         (0)      101 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_df_build_type.py
--rw-rw-rw-   0 root         (0) root         (0)    17066 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_df_builder.py
--rw-rw-rw-   0 root         (0) root         (0)     8555 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_df_builder_factory.py
--rw-rw-rw-   0 root         (0) root         (0)    16915 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_entire_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)      829 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_error_parser.py
--rw-rw-rw-   0 root         (0) root         (0)     1976 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_historical_content_validator.py
--rw-rw-rw-   0 root         (0) root         (0)     6886 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_historical_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     8135 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_historical_df_builder.py
--rw-rw-rw-   0 root         (0) root         (0)     1337 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_historical_join_responses.py
--rw-rw-rw-   0 root         (0) root         (0)     2031 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_historical_response_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     3040 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_intervals.py
--rw-rw-rw-   0 root         (0) root         (0)     2009 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_join_responses.py
--rw-rw-rw-   0 root         (0) root         (0)     1593 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_universe_content_validator.py
--rw-rw-rw-   0 root         (0) root         (0)     8570 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_universe_stream.py
--rw-rw-rw-   0 root         (0) root         (0)    18586 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/_universe_streams.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.471148 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/
--rw-rw-rw-   0 root         (0) root         (0)      466 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    15555 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_custom_instruments_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     2160 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      777 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)     2265 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_events.py
--rw-rw-rw-   0 root         (0) root         (0)     6061 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_instrument_class.py
--rw-rw-rw-   0 root         (0) root         (0)    13185 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_instrument_prop_classes.py
--rw-rw-rw-   0 root         (0) root         (0)    18101 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_manage.py
--rw-rw-rw-   0 root         (0) root         (0)     1329 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_search.py
--rw-rw-rw-   0 root         (0) root         (0)     8912 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_stream_facade.py
--rw-rw-rw-   0 root         (0) root         (0)     2787 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_summaries.py
--rw-rw-rw-   0 root         (0) root         (0)       59 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/events.py
--rw-rw-rw-   0 root         (0) root         (0)      662 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/manage.py
--rw-rw-rw-   0 root         (0) root         (0)       59 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/search.py
--rw-rw-rw-   0 root         (0) root         (0)       62 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/summaries.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.475148 refinitiv-data-1.1.1/refinitiv/data/content/esg/
--rw-rw-rw-   0 root         (0) root         (0)      295 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1478 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_base_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1785 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_basic_overview_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1519 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_esg_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     1881 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_full_measures_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1916 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_full_scores_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1911 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_standard_measures_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1902 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_standard_scores_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1188 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/_universe_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       78 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/basic_overview.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.475148 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/
--rw-rw-rw-   0 root         (0) root         (0)      143 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3888 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_actions.py
--rw-rw-rw-   0 root         (0) root         (0)     9767 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_db_manager.py
--rw-rw-rw-   0 root         (0) root         (0)     1908 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      371 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_df_builder.py
--rw-rw-rw-   0 root         (0) root         (0)      309 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_errors.py
--rw-rw-rw-   0 root         (0) root         (0)     7633 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_file_manager.py
--rw-rw-rw-   0 root         (0) root         (0)     3264 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_package_manager.py
--rw-rw-rw-   0 root         (0) root         (0)     1263 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_package_manager_facade.py
--rw-rw-rw-   0 root         (0) root         (0)     2620 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_tools.py
--rw-rw-rw-   0 root         (0) root         (0)       77 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/full_measures.py
--rw-rw-rw-   0 root         (0) root         (0)       75 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/full_scores.py
--rw-rw-rw-   0 root         (0) root         (0)       81 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/standard_measures.py
--rw-rw-rw-   0 root         (0) root         (0)       79 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/standard_scores.py
--rw-rw-rw-   0 root         (0) root         (0)       72 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/esg/universe.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.475148 refinitiv-data-1.1.1/refinitiv/data/content/estimates/
--rw-rw-rw-   0 root         (0) root         (0)      223 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1290 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)      444 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/_enums.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.479148 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/
--rw-rw-rw-   0 root         (0) root         (0)       63 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2235 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/_annual_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2204 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/_interim_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/annual.py
--rw-rw-rw-   0 root         (0) root         (0)       71 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/interim.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.479148 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/
--rw-rw-rw-   0 root         (0) root         (0)       63 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1790 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/_annual_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1794 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/_interim_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/annual.py
--rw-rw-rw-   0 root         (0) root         (0)       71 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/interim.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.483148 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/
--rw-rw-rw-   0 root         (0) root         (0)      581 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2214 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_annual_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2375 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_non_periodic_measures_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2385 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_annual_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2389 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_interim_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2327 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_recommendations_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2218 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_interim_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2248 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_non_periodic_measures_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2198 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_recommendations_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/annual.py
--rw-rw-rw-   0 root         (0) root         (0)      106 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/historical_snapshots_non_periodic_measures.py
--rw-rw-rw-   0 root         (0) root         (0)      109 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/historical_snapshots_periodic_measures_annual.py
--rw-rw-rw-   0 root         (0) root         (0)      110 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/historical_snapshots_periodic_measures_interim.py
--rw-rw-rw-   0 root         (0) root         (0)      100 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/historical_snapshots_recommendations.py
--rw-rw-rw-   0 root         (0) root         (0)       71 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/interim.py
--rw-rw-rw-   0 root         (0) root         (0)       85 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/non_periodic_measures.py
--rw-rw-rw-   0 root         (0) root         (0)       79 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/recommendations.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.483148 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/
--rw-rw-rw-   0 root         (0) root         (0)      117 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1780 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/_annual_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1864 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/_historical_snapshots_kpi_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1785 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/_interim_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/annual.py
--rw-rw-rw-   0 root         (0) root         (0)       88 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/historical_snapshots_kpi.py
--rw-rw-rw-   0 root         (0) root         (0)       71 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/interim.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.487148 refinitiv-data-1.1.1/refinitiv/data/content/filings/
--rw-rw-rw-   0 root         (0) root         (0)       80 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)       75 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/_errors.py
--rw-rw-rw-   0 root         (0) root         (0)    20498 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/_retrieval_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     2778 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/_retrieval_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     7287 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/_search_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     2448 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/_search_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       73 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/retrieval.py
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/filings/search.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.487148 refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/
--rw-rw-rw-   0 root         (0) root         (0)      193 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2044 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/_data_grid_type.py
--rw-rw-rw-   0 root         (0) root         (0)    11297 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)    10195 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.487148 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/
--rw-rw-rw-   0 root         (0) root         (0)      247 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3615 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)     2815 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_events_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      762 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_historical_pricing_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     3752 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_historical_pricing_request_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     3234 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_summaries_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/events.py
--rw-rw-rw-   0 root         (0) root         (0)       73 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/summaries.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.491148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/
--rw-rw-rw-   0 root         (0) root         (0)      192 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     5727 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_content_provider.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.491148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/
--rw-rw-rw-   0 root         (0) root         (0)      389 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.491148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.495148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/
--rw-rw-rw-   0 root         (0) root         (0)      104 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    11506 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    28284 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     2397 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_request.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.499148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/
--rw-rw-rw-   0 root         (0) root         (0)      565 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      277 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_basis_spline_smooth_model.py
--rw-rw-rw-   0 root         (0) root         (0)     1830 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_business_sector.py
--rw-rw-rw-   0 root         (0) root         (0)      142 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_calendar_adjustment.py
--rw-rw-rw-   0 root         (0) root         (0)      183 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_calibration_model.py
--rw-rw-rw-   0 root         (0) root         (0)     2908 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_curve_sub_type.py
--rw-rw-rw-   0 root         (0) root         (0)      636 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_economic_sector.py
--rw-rw-rw-   0 root         (0) root         (0)     8994 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry.py
--rw-rw-rw-   0 root         (0) root         (0)     3896 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry_group.py
--rw-rw-rw-   0 root         (0) root         (0)     1188 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interest_calculation_method.py
--rw-rw-rw-   0 root         (0) root         (0)      564 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interpolation_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      235 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_issuer_type.py
--rw-rw-rw-   0 root         (0) root         (0)      143 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_main_constituent_asset_class.py
--rw-rw-rw-   0 root         (0) root         (0)      111 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_price_side.py
--rw-rw-rw-   0 root         (0) root         (0)     1248 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_rating.py
--rw-rw-rw-   0 root         (0) root         (0)      181 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_rating_scale_source.py
--rw-rw-rw-   0 root         (0) root         (0)      213 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_reference_entity_type.py
--rw-rw-rw-   0 root         (0) root         (0)      203 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_seniority.py
--rw-rw-rw-   0 root         (0) root         (0)      268 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_types.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.499148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/
--rw-rw-rw-   0 root         (0) root         (0)       85 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.503148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/
--rw-rw-rw-   0 root         (0) root         (0)      505 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     4482 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_butterfly_shift.py
--rw-rw-rw-   0 root         (0) root         (0)     6770 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_combined_shift.py
--rw-rw-rw-   0 root         (0) root         (0)     9765 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_curve_definition_pricing.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.503148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/
--rw-rw-rw-   0 root         (0) root         (0)      179 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      150 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_array_main_constituent_asset_class.py
--rw-rw-rw-   0 root         (0) root         (0)      102 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_array_risk_type.py
--rw-rw-rw-   0 root         (0) root         (0)      172 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_constituent_override_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      178 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_shift_type.py
--rw-rw-rw-   0 root         (0) root         (0)      127 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_shift_unit.py
--rw-rw-rw-   0 root         (0) root         (0)     3924 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_flattening_shift.py
--rw-rw-rw-   0 root         (0) root         (0)     5564 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     5772 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     1830 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_shift_scenario.py
--rw-rw-rw-   0 root         (0) root         (0)     3920 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_long_end_shift.py
--rw-rw-rw-   0 root         (0) root         (0)     3921 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_parallel_shift.py
--rw-rw-rw-   0 root         (0) root         (0)     3158 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_request.py
--rw-rw-rw-   0 root         (0) root         (0)     7749 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_shift_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3921 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_short_end_shift.py
--rw-rw-rw-   0 root         (0) root         (0)     6772 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_time_bucket_shift.py
--rw-rw-rw-   0 root         (0) root         (0)     4477 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_twist_shift.py
--rw-rw-rw-   0 root         (0) root         (0)      374 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_types.py
--rw-rw-rw-   0 root         (0) root         (0)     3087 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_valuation_time.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.511148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/
--rw-rw-rw-   0 root         (0) root         (0)      799 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2982 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_base_definition_mixin.py
--rw-rw-rw-   0 root         (0) root         (0)     1236 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_description.py
--rw-rw-rw-   0 root         (0) root         (0)     1158 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_formula_description.py
--rw-rw-rw-   0 root         (0) root         (0)     3747 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_constituents_description.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.511148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/
--rw-rw-rw-   0 root         (0) root         (0)       69 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3721 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_curve_definition_description.py
--rw-rw-rw-   0 root         (0) root         (0)     1556 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_request.py
--rw-rw-rw-   0 root         (0) root         (0)     1997 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_curve_parameters.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.511148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/
--rw-rw-rw-   0 root         (0) root         (0)       24 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      547 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/_request.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.511148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/
--rw-rw-rw-   0 root         (0) root         (0)       68 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      122 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/_quotation_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      150 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/_standard_turn_period.py
--rw-rw-rw-   0 root         (0) root         (0)     1716 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_description.py
--rw-rw-rw-   0 root         (0) root         (0)     2176 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_formula_description.py
--rw-rw-rw-   0 root         (0) root         (0)     2164 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_formula_parameter_description.py
--rw-rw-rw-   0 root         (0) root         (0)     3621 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2656 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_description.py
--rw-rw-rw-   0 root         (0) root         (0)      940 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_turn_fields.py
--rw-rw-rw-   0 root         (0) root         (0)     1899 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2711 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_description.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.511148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/
--rw-rw-rw-   0 root         (0) root         (0)       62 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     7088 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_curve_definition_keys.py
--rw-rw-rw-   0 root         (0) root         (0)      959 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_request.py
--rw-rw-rw-   0 root         (0) root         (0)     2871 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2752 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_description.py
--rw-rw-rw-   0 root         (0) root         (0)     2115 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instruments_segment.py
--rw-rw-rw-   0 root         (0) root         (0)     1193 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_mixin_request.py
--rw-rw-rw-   0 root         (0) root         (0)     1689 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask.py
--rw-rw-rw-   0 root         (0) root         (0)     1038 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask_fields.py
--rw-rw-rw-   0 root         (0) root         (0)     2176 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_fx_forward_turn.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.511148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/
--rw-rw-rw-   0 root         (0) root         (0)       42 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     8786 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/_curve_get_definition_item.py
--rw-rw-rw-   0 root         (0) root         (0)      900 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_turn_adjustment.py
--rw-rw-rw-   0 root         (0) root         (0)      667 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_types.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.511148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/
--rw-rw-rw-   0 root         (0) root         (0)       64 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3845 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_curve_update_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1538 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_request.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.515148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/
--rw-rw-rw-   0 root         (0) root         (0)      106 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      142 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_interpolation_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      126 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_main_constituent_asset_class.py
--rw-rw-rw-   0 root         (0) root         (0)       99 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_risk_type.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.515148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/
--rw-rw-rw-   0 root         (0) root         (0)       50 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3135 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/_request.py
--rw-rw-rw-   0 root         (0) root         (0)      186 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_types.py
--rw-rw-rw-   0 root         (0) root         (0)     8532 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_curves_builder_df.py
--rw-rw-rw-   0 root         (0) root         (0)    22446 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_curves_data_provider.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.515148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/
--rw-rw-rw-   0 root         (0) root         (0)      532 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      172 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_calendar_adjustment.py
--rw-rw-rw-   0 root         (0) root         (0)      208 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_compounding_type.py
--rw-rw-rw-   0 root         (0) root         (0)      124 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_forward_curves_outputs.py
--rw-rw-rw-   0 root         (0) root         (0)      546 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_instrument_type.py
--rw-rw-rw-   0 root         (0) root         (0)      137 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_main_constituent_asset_class.py
--rw-rw-rw-   0 root         (0) root         (0)      220 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_market_data_access_denied_fallback.py
--rw-rw-rw-   0 root         (0) root         (0)      131 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_swap_price_side.py
--rw-rw-rw-   0 root         (0) root         (0)      167 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_zc_curves_outputs.py
--rw-rw-rw-   0 root         (0) root         (0)      582 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_zc_interpolation_mode.py
--rw-rw-rw-   0 root         (0) root         (0)     2774 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_forward_curve_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3049 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_forward_curve_request_item.py
--rw-rw-rw-   0 root         (0) root         (0)      830 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_forward_curve_types.py
--rw-rw-rw-   0 root         (0) root         (0)     3378 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_interest_rate_curve_get_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.519148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/
--rw-rw-rw-   0 root         (0) root         (0)      172 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      158 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_constituents.py
--rw-rw-rw-   0 root         (0) root         (0)     1567 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_convexity.py
--rw-rw-rw-   0 root         (0) root         (0)     3881 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_curve.py
--rw-rw-rw-   0 root         (0) root         (0)      261 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_curve_point.py
--rw-rw-rw-   0 root         (0) root         (0)      746 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_instrument.py
--rw-rw-rw-   0 root         (0) root         (0)    13784 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_interest_rate_curve_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)      990 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_step.py
--rw-rw-rw-   0 root         (0) root         (0)     1426 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_turn.py
--rw-rw-rw-   0 root         (0) root         (0)     4669 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_swap_zc_curve_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    13044 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_swap_zc_curve_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     3593 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     5674 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_definitions.py
--rw-rw-rw-   0 root         (0) root         (0)    16595 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     2307 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_request_item.py
--rw-rw-rw-   0 root         (0) root         (0)      632 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_types.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.539148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/
--rw-rw-rw-   0 root         (0) root         (0)     3130 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      153 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_adjust_interest_to_payment_date.py
--rw-rw-rw-   0 root         (0) root         (0)      165 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_american_monte_carlo_method.py
--rw-rw-rw-   0 root         (0) root         (0)      300 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_amortization_frequency.py
--rw-rw-rw-   0 root         (0) root         (0)      176 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_amortization_type.py
--rw-rw-rw-   0 root         (0) root         (0)      122 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_asset_class.py
--rw-rw-rw-   0 root         (0) root         (0)      241 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_average_type.py
--rw-rw-rw-   0 root         (0) root         (0)      333 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_axis.py
--rw-rw-rw-   0 root         (0) root         (0)      249 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_barrier_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      152 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_barrier_style.py
--rw-rw-rw-   0 root         (0) root         (0)      219 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_barrier_type.py
--rw-rw-rw-   0 root         (0) root         (0)      153 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_benchmark_yield_selection_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      174 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_binary_type.py
--rw-rw-rw-   0 root         (0) root         (0)      359 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_business_day_convention.py
--rw-rw-rw-   0 root         (0) root         (0)      111 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_buy_sell.py
--rw-rw-rw-   0 root         (0) root         (0)      234 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_calibration_strategy.py
--rw-rw-rw-   0 root         (0) root         (0)      129 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_call_put.py
--rw-rw-rw-   0 root         (0) root         (0)      134 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_cds_convention.py
--rw-rw-rw-   0 root         (0) root         (0)      763 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_common_tools.py
--rw-rw-rw-   0 root         (0) root         (0)      154 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_credit_spread_type.py
--rw-rw-rw-   0 root         (0) root         (0)     1052 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_date_moving_convention.py
--rw-rw-rw-   0 root         (0) root         (0)      171 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_date_rolling_convention.py
--rw-rw-rw-   0 root         (0) root         (0)      420 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_dates_calendars_frequency.py
--rw-rw-rw-   0 root         (0) root         (0)     1192 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_day_count_basis.py
--rw-rw-rw-   0 root         (0) root         (0)      694 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_day_of_week.py
--rw-rw-rw-   0 root         (0) root         (0)      123 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_direction.py
--rw-rw-rw-   0 root         (0) root         (0)      168 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_discounting_type.py
--rw-rw-rw-   0 root         (0) root         (0)      217 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_dividend_extrapolation.py
--rw-rw-rw-   0 root         (0) root         (0)      309 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_dividend_type.py
--rw-rw-rw-   0 root         (0) root         (0)      423 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_doc_clause.py
--rw-rw-rw-   0 root         (0) root         (0)      142 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_double_binary_type.py
--rw-rw-rw-   0 root         (0) root         (0)      349 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_end_of_month_convention.py
--rw-rw-rw-   0 root         (0) root         (0)      159 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_equity_dividend_type.py
--rw-rw-rw-   0 root         (0) root         (0)      161 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_eti_input_volatility_type.py
--rw-rw-rw-   0 root         (0) root         (0)      177 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_exercise_schedule_type.py
--rw-rw-rw-   0 root         (0) root         (0)      137 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_exercise_style.py
--rw-rw-rw-   0 root         (0) root         (0)      154 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_extrapolation_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      257 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_fixing_frequency.py
--rw-rw-rw-   0 root         (0) root         (0)      249 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_format.py
--rw-rw-rw-   0 root         (0) root         (0)      237 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_forward_compute_method.py
--rw-rw-rw-   0 root         (0) root         (0)      266 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_forward_extrapolation.py
--rw-rw-rw-   0 root         (0) root         (0)      862 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_frequency.py
--rw-rw-rw-   0 root         (0) root         (0)      360 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_funding_spread_method.py
--rw-rw-rw-   0 root         (0) root         (0)      239 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_fx_binary_type.py
--rw-rw-rw-   0 root         (0) root         (0)      266 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_fx_cross_type.py
--rw-rw-rw-   0 root         (0) root         (0)      242 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_fx_leg_type.py
--rw-rw-rw-   0 root         (0) root         (0)      178 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_fx_swap_calculation_method.py
--rw-rw-rw-   0 root         (0) root         (0)      192 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_fx_volatility_model.py
--rw-rw-rw-   0 root         (0) root         (0)      796 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_holiday_outupts.py
--rw-rw-rw-   0 root         (0) root         (0)      200 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_implied_deposit_date_convention.py
--rw-rw-rw-   0 root         (0) root         (0)      107 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_in_or_out.py
--rw-rw-rw-   0 root         (0) root         (0)      286 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_average_method.py
--rw-rw-rw-   0 root         (0) root         (0)      264 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_compounding_method.py
--rw-rw-rw-   0 root         (0) root         (0)      173 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_convexity_adjustment_integration_method.py
--rw-rw-rw-   0 root         (0) root         (0)      227 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_convexity_adjustment_method.py
--rw-rw-rw-   0 root         (0) root         (0)      226 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_convexity_adjustment_type.py
--rw-rw-rw-   0 root         (0) root         (0)      171 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_observation_method.py
--rw-rw-rw-   0 root         (0) root         (0)      142 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_reset_type.py
--rw-rw-rw-   0 root         (0) root         (0)      226 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_index_spread_compounding_method.py
--rw-rw-rw-   0 root         (0) root         (0)      163 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_inflation_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      215 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_input_volatility_type.py
--rw-rw-rw-   0 root         (0) root         (0)      179 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_interest_calculation_convention.py
--rw-rw-rw-   0 root         (0) root         (0)      146 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_interest_type.py
--rw-rw-rw-   0 root         (0) root         (0)      279 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_interpolation_mode.py
--rw-rw-rw-   0 root         (0) root         (0)      348 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_local_volatility_method.py
--rw-rw-rw-   0 root         (0) root         (0)      216 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_method.py
--rw-rw-rw-   0 root         (0) root         (0)      195 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_notional_exchange.py
--rw-rw-rw-   0 root         (0) root         (0)      249 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_numeraire_type.py
--rw-rw-rw-   0 root         (0) root         (0)      175 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_option_volatility_type.py
--rw-rw-rw-   0 root         (0) root         (0)      592 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_period_type.py
--rw-rw-rw-   0 root         (0) root         (0)      159 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_premium_settlement_type.py
--rw-rw-rw-   0 root         (0) root         (0)      145 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_price_side.py
--rw-rw-rw-   0 root         (0) root         (0)      269 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_pricing_model_type.py
--rw-rw-rw-   0 root         (0) root         (0)      227 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_projected_index_calculation_method.py
--rw-rw-rw-   0 root         (0) root         (0)      135 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_quote_fallback_logic.py
--rw-rw-rw-   0 root         (0) root         (0)      956 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_redemption_date_type.py
--rw-rw-rw-   0 root         (0) root         (0)      180 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_repo_curve_type.py
--rw-rw-rw-   0 root         (0) root         (0)      114 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_risk_type.py
--rw-rw-rw-   0 root         (0) root         (0)      292 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_rounding.py
--rw-rw-rw-   0 root         (0) root         (0)      271 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_rounding_type.py
--rw-rw-rw-   0 root         (0) root         (0)      273 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_seniority.py
--rw-rw-rw-   0 root         (0) root         (0)      176 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_settlement_type.py
--rw-rw-rw-   0 root         (0) root         (0)      214 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_status.py
--rw-rw-rw-   0 root         (0) root         (0)      294 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_stub_rule.py
--rw-rw-rw-   0 root         (0) root         (0)      152 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_swaption_settlement_type.py
--rw-rw-rw-   0 root         (0) root         (0)      128 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_swaption_type.py
--rw-rw-rw-   0 root         (0) root         (0)      172 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_swaption_volatility_type.py
--rw-rw-rw-   0 root         (0) root         (0)      152 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_tenor_reference_date.py
--rw-rw-rw-   0 root         (0) root         (0)      284 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_time_stamp.py
--rw-rw-rw-   0 root         (0) root         (0)      156 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_underlying_type.py
--rw-rw-rw-   0 root         (0) root         (0)      110 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_up_or_down.py
--rw-rw-rw-   0 root         (0) root         (0)      112 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_vol_type.py
--rw-rw-rw-   0 root         (0) root         (0)      311 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_volatility_adjustment_type.py
--rw-rw-rw-   0 root         (0) root         (0)      189 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_volatility_model.py
--rw-rw-rw-   0 root         (0) root         (0)      151 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_volatility_term_structure_type.py
--rw-rw-rw-   0 root         (0) root         (0)      139 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_volatility_type.py
--rw-rw-rw-   0 root         (0) root         (0)      880 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_yield_type.py
--rw-rw-rw-   0 root         (0) root         (0)    10829 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_ipa_content_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     1459 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_ipa_content_validator.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.543148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/
--rw-rw-rw-   0 root         (0) root         (0)      433 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1942 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_american_monte_carlo_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     3841 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_amortization_item.py
--rw-rw-rw-   0 root         (0) root         (0)     2188 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_barrier_definition_element.py
--rw-rw-rw-   0 root         (0) root         (0)      917 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_basket_item.py
--rw-rw-rw-   0 root         (0) root         (0)     1163 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_bid_ask_mid.py
--rw-rw-rw-   0 root         (0) root         (0)     5046 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_bond_rounding_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)      691 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_day_weight.py
--rw-rw-rw-   0 root         (0) root         (0)     2295 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_fx_point.py
--rw-rw-rw-   0 root         (0) root         (0)     2084 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_input_flow.py
--rw-rw-rw-   0 root         (0) root         (0)     1846 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_interpolation_weight.py
--rw-rw-rw-   0 root         (0) root         (0)     1589 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_numerical_method.py
--rw-rw-rw-   0 root         (0) root         (0)      948 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_payout_scaling.py
--rw-rw-rw-   0 root         (0) root         (0)     1359 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_pde_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     4546 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_object_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.543148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/
--rw-rw-rw-   0 root         (0) root         (0)       23 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1883 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_cap_surface_request_item.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.547148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_enums/
--rw-rw-rw-   0 root         (0) root         (0)      330 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      137 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_enums/_moneyness_type.py
--rw-rw-rw-   0 root         (0) root         (0)      442 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_enums/_outputs.py
--rw-rw-rw-   0 root         (0) root         (0)     1897 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_eti_surface_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     6007 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_eti_surface_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     1640 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_eti_surface_request_item.py
--rw-rw-rw-   0 root         (0) root         (0)      567 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_fx_surface_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     9627 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_fx_surface_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     1681 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_fx_surface_request_item.py
--rw-rw-rw-   0 root         (0) root         (0)     1117 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_i_ir_vol_model_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     4557 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_i_ir_vol_model_pricing_parameters.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.547148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/
--rw-rw-rw-   0 root         (0) root         (0)      351 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1628 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_maturity_filter.py
--rw-rw-rw-   0 root         (0) root         (0)     1095 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_moneyness_weight.py
--rw-rw-rw-   0 root         (0) root         (0)     1214 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_strike_filter.py
--rw-rw-rw-   0 root         (0) root         (0)     1379 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_strike_filter_range.py
--rw-rw-rw-   0 root         (0) root         (0)     5184 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_surface.py
--rw-rw-rw-   0 root         (0) root         (0)     4844 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_surface_filters.py
--rw-rw-rw-   0 root         (0) root         (0)     5754 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_surface_output.py
--rw-rw-rw-   0 root         (0) root         (0)      294 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_surface_point.py
--rw-rw-rw-   0 root         (0) root         (0)      784 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_volatility_surface_point.py
--rw-rw-rw-   0 root         (0) root         (0)     1592 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_surface_request_item.py
--rw-rw-rw-   0 root         (0) root         (0)      446 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_surface_types.py
--rw-rw-rw-   0 root         (0) root         (0)    10001 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_surfaces_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     2062 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_swaption_surface_request_item.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.547148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/
--rw-rw-rw-   0 root         (0) root         (0)      284 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.547148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/
--rw-rw-rw-   0 root         (0) root         (0)       44 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.547148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/curves/
--rw-rw-rw-   0 root         (0) root         (0)     1085 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/curves/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2529 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/curves/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      470 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/curves/_enums.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.551148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/
--rw-rw-rw-   0 root         (0) root         (0)      164 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      472 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/_arg_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       84 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/_base_data_class.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.551148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/
--rw-rw-rw-   0 root         (0) root         (0)     1192 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2806 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      307 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/_enums.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.551148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/
--rw-rw-rw-   0 root         (0) root         (0)     1942 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     7447 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_data_classes.py
--rw-rw-rw-   0 root         (0) root         (0)      240 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)    11287 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_manage.py
--rw-rw-rw-   0 root         (0) root         (0)     4903 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_search.py
--rw-rw-rw-   0 root         (0) root         (0)      116 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/manage.py
--rw-rw-rw-   0 root         (0) root         (0)       59 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/search.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.551148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/
--rw-rw-rw-   0 root         (0) root         (0)       44 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     5996 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     2922 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_search.py
--rw-rw-rw-   0 root         (0) root         (0)       59 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/search.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.555148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/forward_curves/
--rw-rw-rw-   0 root         (0) root         (0)      895 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/forward_curves/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     6060 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/forward_curves/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      189 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/forward_curves/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       80 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/forward_curves/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.555148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curve_definitions/
--rw-rw-rw-   0 root         (0) root         (0)      189 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curve_definitions/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     4653 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curve_definitions/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       51 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curve_definitions/_enums.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.555148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curves/
--rw-rw-rw-   0 root         (0) root         (0)      750 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curves/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3998 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curves/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      227 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curves/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)      131 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curves/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.555148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/
--rw-rw-rw-   0 root         (0) root         (0)      196 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1829 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/_base_request_items.py
--rw-rw-rw-   0 root         (0) root         (0)      976 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/_content_data_validator.py
--rw-rw-rw-   0 root         (0) root         (0)      592 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/_request_factory.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.555148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/add_periods/
--rw-rw-rw-   0 root         (0) root         (0)      260 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/add_periods/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2896 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_add_periods_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)    10060 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.555148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/count_periods/
--rw-rw-rw-   0 root         (0) root         (0)      172 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/count_periods/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     8317 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods.py
--rw-rw-rw-   0 root         (0) root         (0)     2507 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods_data_provider.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.559148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/
--rw-rw-rw-   0 root         (0) root         (0)      158 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     6951 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule.py
--rw-rw-rw-   0 root         (0) root         (0)      842 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule_data_provider.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.559148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/holidays/
--rw-rw-rw-   0 root         (0) root         (0)      143 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/holidays/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     7363 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays.py
--rw-rw-rw-   0 root         (0) root         (0)     3725 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays_data_provider.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.559148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/
--rw-rw-rw-   0 root         (0) root         (0)      149 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     7002 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/_is_working_day.py
--rw-rw-rw-   0 root         (0) root         (0)     3259 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/_is_working_day_data_provider.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.559148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/
--rw-rw-rw-   0 root         (0) root         (0)      483 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2463 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_base_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     8502 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_contracts_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     3681 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1052 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_instrument_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3723 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_quantitative_data_stream.py
--rw-rw-rw-   0 root         (0) root         (0)     3615 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_stream_facade.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.563148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/
--rw-rw-rw-   0 root         (0) root         (0)     1510 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    31443 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_bond_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    92041 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_bond_pricing_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)    13067 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      587 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       64 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.563148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/
--rw-rw-rw-   0 root         (0) root         (0)     1148 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    28094 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     6671 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_pricing_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)    14140 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      420 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       94 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.567148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/
--rw-rw-rw-   0 root         (0) root         (0)      621 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    13571 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_cds_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     7555 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_cds_pricing_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     8711 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      192 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)      122 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_models.py
--rw-rw-rw-   0 root         (0) root         (0)    15899 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_premium_leg_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    10575 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_protection_leg_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.567148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/
--rw-rw-rw-   0 root         (0) root         (0)      554 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     6146 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      149 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)     8406 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     7379 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_leg_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    10761 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_pricing_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)       31 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.567148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/
--rw-rw-rw-   0 root         (0) root         (0)     2190 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.571148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_base/
--rw-rw-rw-   0 root         (0) root         (0)      181 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_base/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      127 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_base/_barrier_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      126 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_base/_binary_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      114 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_base/_info.py
--rw-rw-rw-   0 root         (0) root         (0)      130 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_base/_underlying_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     9738 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.571148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_enums/
--rw-rw-rw-   0 root         (0) root         (0)      557 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_enums/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      142 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_enums/_double_binary_type.py
--rw-rw-rw-   0 root         (0) root         (0)      239 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_enums/_fx_binary_type.py
--rw-rw-rw-   0 root         (0) root         (0)      150 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_enums/_settlement_type.py
--rw-rw-rw-   0 root         (0) root         (0)      214 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_enums/_underlying_type.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.571148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/
--rw-rw-rw-   0 root         (0) root         (0)      390 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1865 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_barrier_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1836 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_binary_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      989 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_cbbc_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    14038 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      875 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_double_barriers_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     4937 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_fixing_info.py
--rw-rw-rw-   0 root         (0) root         (0)      708 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_underlying_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.575148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/
--rw-rw-rw-   0 root         (0) root         (0)      563 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     4457 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_average_info.py
--rw-rw-rw-   0 root         (0) root         (0)     3643 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_barrier_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3131 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_binary_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    18535 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2148 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1166 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_info.py
--rw-rw-rw-   0 root         (0) root         (0)     4073 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_binary_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1307 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_dual_currency_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1807 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_forward_start.py
--rw-rw-rw-   0 root         (0) root         (0)      779 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_underlying_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      116 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_models.py
--rw-rw-rw-   0 root         (0) root         (0)     4182 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_option_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3671 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_option_instrument_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    32717 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_option_pricing_parameters.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.575148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/
--rw-rw-rw-   0 root         (0) root         (0)      545 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     5850 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       77 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)     8042 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     4865 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     4366 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_pricing_parameters.py
--rw-rw-rw-   0 root         (0) root         (0)     3028 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_contract.py
--rw-rw-rw-   0 root         (0) root         (0)     4988 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_pricing_parameters.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.579148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/
--rw-rw-rw-   0 root         (0) root         (0)     1529 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1779 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      601 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       51 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_models.py
--rw-rw-rw-   0 root         (0) root         (0)     5229 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_swap_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    34670 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_swap_leg_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    10008 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_swap_pricing_parameters.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.579148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/
--rw-rw-rw-   0 root         (0) root         (0)      757 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1670 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_bermudan_swaption_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     7528 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      185 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       33 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_models.py
--rw-rw-rw-   0 root         (0) root         (0)    11733 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      717 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_market_data_rule.py
--rw-rw-rw-   0 root         (0) root         (0)     6250 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_pricing_parameters.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.583148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/
--rw-rw-rw-   0 root         (0) root         (0)      366 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     5909 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      114 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)    10277 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     4201 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_pricing_parameters.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.583148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/
--rw-rw-rw-   0 root         (0) root         (0)      338 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      734 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.583148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/cap/
--rw-rw-rw-   0 root         (0) root         (0)      640 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/cap/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1141 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/cap/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      134 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/cap/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       64 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/cap/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.583148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/eti/
--rw-rw-rw-   0 root         (0) root         (0)      855 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/eti/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1070 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/eti/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      156 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/eti/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)      175 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/eti/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.583148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/fx/
--rw-rw-rw-   0 root         (0) root         (0)      737 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/fx/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2486 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/fx/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      140 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/fx/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)      174 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/fx/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.587148 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/swaption/
--rw-rw-rw-   0 root         (0) root         (0)      660 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/swaption/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1013 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/swaption/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      134 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/swaption/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)       64 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/swaption/_models.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.587148 refinitiv-data-1.1.1/refinitiv/data/content/news/
--rw-rw-rw-   0 root         (0) root         (0)      168 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     6236 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_data_classes.py
--rw-rw-rw-   0 root         (0) root         (0)     5000 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_headlines_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    14325 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_news_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     2550 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_news_data_provider_layer.py
--rw-rw-rw-   0 root         (0) root         (0)      171 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_sort_order.py
--rw-rw-rw-   0 root         (0) root         (0)      883 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_story_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3428 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_tools.py
--rw-rw-rw-   0 root         (0) root         (0)     1383 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_udf_html_parser.py
--rw-rw-rw-   0 root         (0) root         (0)      125 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/_urgency.py
--rw-rw-rw-   0 root         (0) root         (0)      120 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/headlines.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.587148 refinitiv-data-1.1.1/refinitiv/data/content/news/images/
--rw-rw-rw-   0 root         (0) root         (0)       63 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/images/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3462 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/images/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     1392 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/images/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2821 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/images/_image.py
--rw-rw-rw-   0 root         (0) root         (0)      143 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/story.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.591148 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/
--rw-rw-rw-   0 root         (0) root         (0)       99 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1146 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     1051 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      871 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_df_builder.py
--rw-rw-rw-   0 root         (0) root         (0)      876 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_top_news_headline.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.591148 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/
--rw-rw-rw-   0 root         (0) root         (0)      111 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1094 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     1001 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      764 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_df_builder.py
--rw-rw-rw-   0 root         (0) root         (0)      470 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_subcategory.py
--rw-rw-rw-   0 root         (0) root         (0)      107 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_top_news_id.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.591148 refinitiv-data-1.1.1/refinitiv/data/content/ownership/
--rw-rw-rw-   0 root         (0) root         (0)      259 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      354 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/_enums.py
--rw-rw-rw-   0 root         (0) root         (0)     1721 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/_org_info_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     4980 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/_ownership_data_provider.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.595148 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/
--rw-rw-rw-   0 root         (0) root         (0)      353 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2432 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_breakdown_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1788 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_concentration_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2129 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_investors_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2043 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_recent_activity_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3285 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_shareholders_history_report_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2095 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_shareholders_report_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1977 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_top_n_concentration_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       73 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/breakdown.py
--rw-rw-rw-   0 root         (0) root         (0)       77 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/concentration.py
--rw-rw-rw-   0 root         (0) root         (0)       73 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/investors.py
--rw-rw-rw-   0 root         (0) root         (0)       79 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/recent_activity.py
--rw-rw-rw-   0 root         (0) root         (0)       91 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/shareholders_history_report.py
--rw-rw-rw-   0 root         (0) root         (0)       83 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/shareholders_report.py
--rw-rw-rw-   0 root         (0) root         (0)       83 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/top_n_concentration.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.599148 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/
--rw-rw-rw-   0 root         (0) root         (0)      383 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2410 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_breakdown_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1771 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_concentration_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2024 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_holdings_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2156 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_investors_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2043 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_recent_activity_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3197 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_shareholders_history_report_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2084 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_shareholders_report_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1945 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_top_n_concentration_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       73 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/breakdown.py
--rw-rw-rw-   0 root         (0) root         (0)       77 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/concentration.py
--rw-rw-rw-   0 root         (0) root         (0)       72 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/holdings.py
--rw-rw-rw-   0 root         (0) root         (0)       73 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/investors.py
--rw-rw-rw-   0 root         (0) root         (0)       79 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/recent_activity.py
--rw-rw-rw-   0 root         (0) root         (0)       91 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/shareholders_history_report.py
--rw-rw-rw-   0 root         (0) root         (0)       83 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/shareholders_report.py
--rw-rw-rw-   0 root         (0) root         (0)       83 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/top_n_concentration.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.599148 refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/
--rw-rw-rw-   0 root         (0) root         (0)      111 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2140 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/_shareholders_report_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3238 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/_transaction_report_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       83 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/shareholders_report.py
--rw-rw-rw-   0 root         (0) root         (0)       82 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/transaction_report.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.603148 refinitiv-data-1.1.1/refinitiv/data/content/ownership/investor/
--rw-rw-rw-   0 root         (0) root         (0)       48 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/investor/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2116 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/investor/_holdings_definition.py
--rw-rw-rw-   0 root         (0) root         (0)       72 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/investor/holdings.py
--rw-rw-rw-   0 root         (0) root         (0)       72 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/ownership/org_info.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.603148 refinitiv-data-1.1.1/refinitiv/data/content/pricing/
--rw-rw-rw-   0 root         (0) root         (0)      260 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3430 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     5682 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/_pricing_content_provider.py
--rw-rw-rw-   0 root         (0) root         (0)    18568 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/_stream_facade.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.603148 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/
--rw-rw-rw-   0 root         (0) root         (0)       63 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     8205 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_chain_record.py
--rw-rw-rw-   0 root         (0) root         (0)     5219 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_chain_records.py
--rw-rw-rw-   0 root         (0) root         (0)     1776 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_chains_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     3806 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)    55563 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_display_template.py
--rw-rw-rw-   0 root         (0) root         (0)     8614 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_stream.py
--rw-rw-rw-   0 root         (0) root         (0)     4789 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_stream_facade.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.607148 refinitiv-data-1.1.1/refinitiv/data/content/search/
--rw-rw-rw-   0 root         (0) root         (0)      169 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    12002 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     4249 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2022 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/_lookup_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1059 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/_metadata_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1969 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/_views.py
--rw-rw-rw-   0 root         (0) root         (0)       70 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/lookup.py
--rw-rw-rw-   0 root         (0) root         (0)       72 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/search/metadata.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.607148 refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/
--rw-rw-rw-   0 root         (0) root         (0)      294 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2845 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_asset_class.py
--rw-rw-rw-   0 root         (0) root         (0)      521 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_asset_state.py
--rw-rw-rw-   0 root         (0) root         (0)     4413 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_country_code.py
--rw-rw-rw-   0 root         (0) root         (0)     7399 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      639 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_symbol_type.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.607148 refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/
--rw-rw-rw-   0 root         (0) root         (0)      227 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3659 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     7814 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/_stream.py
--rw-rw-rw-   0 root         (0) root         (0)    10797 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/_stream_facade.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.611148 refinitiv-data-1.1.1/refinitiv/data/delivery/
--rw-rw-rw-   0 root         (0) root         (0)      146 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/__init__.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.615148 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/
--rw-rw-rw-   0 root         (0) root         (0)       72 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      685 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_api_type.py
--rw-rw-rw-   0 root         (0) root         (0)      625 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_connection.py
--rw-rw-rw-   0 root         (0) root         (0)     1826 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     2768 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)    26171 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_provider_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     5775 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_provider_layer.py
--rw-rw-rw-   0 root         (0) root         (0)      199 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_type.py
--rw-rw-rw-   0 root         (0) root         (0)     1109 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_endpoint_data.py
--rw-rw-rw-   0 root         (0) root         (0)      914 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_endpoint_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     5884 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_endpoint_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2091 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_parsed_data.py
--rw-rw-rw-   0 root         (0) root         (0)     4035 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_raw_data_parser.py
--rw-rw-rw-   0 root         (0) root         (0)      559 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_request.py
--rw-rw-rw-   0 root         (0) root         (0)     5788 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_request_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     2217 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_response.py
--rw-rw-rw-   0 root         (0) root         (0)     1652 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_response_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     3641 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_validators.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.619148 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/
--rw-rw-rw-   0 root         (0) root         (0)      651 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    11692 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_omm_stream.py
--rw-rw-rw-   0 root         (0) root         (0)      137 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_protocol_type.py
--rw-rw-rw-   0 root         (0) root         (0)     4956 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_rdp_stream.py
--rw-rw-rw-   0 root         (0) root         (0)    10271 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_cxn_cache.py
--rw-rw-rw-   0 root         (0) root         (0)     5423 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_cxn_config_data.py
--rw-rw-rw-   0 root         (0) root         (0)    12213 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_cxn_config_provider.py
--rw-rw-rw-   0 root         (0) root         (0)    11005 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_factory.py
--rw-rw-rw-   0 root         (0) root         (0)     9559 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_listener.py
--rw-rw-rw-   0 root         (0) root         (0)      104 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_type.py
--rw-rw-rw-   0 root         (0) root         (0)     2590 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/base_stream.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.623148 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/
--rw-rw-rw-   0 root         (0) root         (0)      149 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     5527 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_funcs.py
--rw-rw-rw-   0 root         (0) root         (0)     2687 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_offstream.py
--rw-rw-rw-   0 root         (0) root         (0)     2278 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_response.py
--rw-rw-rw-   0 root         (0) root         (0)      185 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_stream_connection.py
--rw-rw-rw-   0 root         (0) root         (0)      334 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_type.py
--rw-rw-rw-   0 root         (0) root         (0)     2351 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/event.py
--rw-rw-rw-   0 root         (0) root         (0)    15946 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/omm_stream.py
--rw-rw-rw-   0 root         (0) root         (0)     7024 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/omm_stream_connection.py
--rw-rw-rw-   0 root         (0) root         (0)     2773 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/omm_stream_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     4163 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/proxy_info.py
--rw-rw-rw-   0 root         (0) root         (0)    10741 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rdp_stream.py
--rw-rw-rw-   0 root         (0) root         (0)     3135 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rdp_stream_connection.py
--rw-rw-rw-   0 root         (0) root         (0)     2116 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rdp_stream_definition.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.623148 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/
--rw-rw-rw-   0 root         (0) root         (0)        0 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      834 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/conversion.py
--rw-rw-rw-   0 root         (0) root         (0)     5546 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/ema.py
--rw-rw-rw-   0 root         (0) root         (0)     8842 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/socket.py
--rw-rw-rw-   0 root         (0) root         (0)     4680 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream.py
--rw-rw-rw-   0 root         (0) root         (0)     4265 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_cache.py
--rw-rw-rw-   0 root         (0) root         (0)    16651 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_connection.py
--rw-rw-rw-   0 root         (0) root         (0)      225 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_cxn_state.py
--rw-rw-rw-   0 root         (0) root         (0)      175 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_state.py
--rw-rw-rw-   0 root         (0) root         (0)     3508 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_state_manager.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.623148 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/ws/
--rw-rw-rw-   0 root         (0) root         (0)        0 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/ws/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1366 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/ws/ws_client.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.627148 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/
--rw-rw-rw-   0 root         (0) root         (0)      166 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     3525 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_buckets_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     3478 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_cfs_data_provider.py
--rw-rw-rw-   0 root         (0) root         (0)     1801 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_data_class.py
--rw-rw-rw-   0 root         (0) root         (0)     1357 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_data_types.py
--rw-rw-rw-   0 root         (0) root         (0)     1120 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_downloader.py
--rw-rw-rw-   0 root         (0) root         (0)     1057 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_downloader_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     1015 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_downloader_facade.py
--rw-rw-rw-   0 root         (0) root         (0)     4507 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_sets_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     2286 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_files_definition.py
--rw-rw-rw-   0 root         (0) root         (0)     4256 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_iter_object.py
--rw-rw-rw-   0 root         (0) root         (0)     3039 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_packages_definition.py
--rw-rw-rw-   0 root         (0) root         (0)      228 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_stream.py
--rw-rw-rw-   0 root         (0) root         (0)     1805 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_tools.py
--rw-rw-rw-   0 root         (0) root         (0)     1140 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_unpacker.py
--rw-rw-rw-   0 root         (0) root         (0)       71 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/buckets.py
--rw-rw-rw-   0 root         (0) root         (0)       79 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/file_downloader.py
--rw-rw-rw-   0 root         (0) root         (0)       73 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/file_sets.py
--rw-rw-rw-   0 root         (0) root         (0)       69 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/files.py
--rw-rw-rw-   0 root         (0) root         (0)       72 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/packages.py
--rw-rw-rw-   0 root         (0) root         (0)      239 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/endpoint_request.py
--rw-rw-rw-   0 root         (0) root         (0)      259 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/omm_stream.py
--rw-rw-rw-   0 root         (0) root         (0)       81 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/delivery/rdp_stream.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.627148 refinitiv-data-1.1.1/refinitiv/data/discovery/
--rw-rw-rw-   0 root         (0) root         (0)      520 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     2424 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_convert_symbols.py
--rw-rw-rw-   0 root         (0) root         (0)     2803 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.631148 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/
--rw-rw-rw-   0 root         (0) root         (0)      383 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)    11296 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/base.py
--rw-rw-rw-   0 root         (0) root         (0)     8134 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/embedded.py
--rw-rw-rw-   0 root         (0) root         (0)     4627 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/manage.py
--rw-rw-rw-   0 root         (0) root         (0)     1383 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/namespaces.py
--rw-rw-rw-   0 root         (0) root         (0)     2303 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/search.py
--rw-rw-rw-   0 root         (0) root         (0)      653 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.631148 refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/
--rw-rw-rw-   0 root         (0) root         (0)       84 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     4184 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/_chain.py
--rw-rw-rw-   0 root         (0) root         (0)     1236 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/_discovery_universe.py
--rw-rw-rw-   0 root         (0) root         (0)      393 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/_peers.py
--rw-rw-rw-   0 root         (0) root         (0)      514 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/_screener.py
--rw-rw-rw-   0 root         (0) root         (0)      369 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/_universe_expander.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.631148 refinitiv-data-1.1.1/refinitiv/data/eikon/
--rw-rw-rw-   0 root         (0) root         (0)      479 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     9653 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_data_grid.py
--rw-rw-rw-   0 root         (0) root         (0)       47 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_errors.py
--rw-rw-rw-   0 root         (0) root         (0)     9201 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_json_requests.py
--rw-rw-rw-   0 root         (0) root         (0)     7429 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_news_request.py
--rw-rw-rw-   0 root         (0) root         (0)     1099 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_streaming_prices.py
--rw-rw-rw-   0 root         (0) root         (0)     5278 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_symbology.py
--rw-rw-rw-   0 root         (0) root         (0)    13950 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_time_series.py
--rw-rw-rw-   0 root         (0) root         (0)     5121 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/eikon/_tools.py
--rw-rw-rw-   0 root         (0) root         (0)      780 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/errors.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.635148 refinitiv-data-1.1.1/refinitiv/data/session/
--rw-rw-rw-   0 root         (0) root         (0)      397 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/session/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)     1462 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/session/desktop.py
--rw-rw-rw-   0 root         (0) root         (0)     3343 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/session/platform.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.635148 refinitiv-data-1.1.1/refinitiv/data/usage_collection/
--rw-rw-rw-   0 root         (0) root         (0)      359 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/usage_collection/__init__.py
--rw-rw-rw-   0 root         (0) root         (0)      472 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/usage_collection/_abstract_logger.py
--rw-rw-rw-   0 root         (0) root         (0)      342 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/usage_collection/_filter_types.py
--rw-rw-rw-   0 root         (0) root         (0)     6534 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/usage_collection/_logger.py
--rw-rw-rw-   0 root         (0) root         (0)     1555 2023-02-23 14:40:30.000000 refinitiv-data-1.1.1/refinitiv/data/usage_collection/_utils.py
-drwxr-xr-x   0 root         (0) root         (0)        0 2023-02-23 14:40:45.635148 refinitiv-data-1.1.1/refinitiv_data.egg-info/
--rw-r--r--   0 root         (0) root         (0)     9778 2023-02-23 14:40:45.000000 refinitiv-data-1.1.1/refinitiv_data.egg-info/PKG-INFO
--rw-r--r--   0 root         (0) root         (0)    53624 2023-02-23 14:40:45.000000 refinitiv-data-1.1.1/refinitiv_data.egg-info/SOURCES.txt
--rw-r--r--   0 root         (0) root         (0)        1 2023-02-23 14:40:45.000000 refinitiv-data-1.1.1/refinitiv_data.egg-info/dependency_links.txt
--rw-r--r--   0 root         (0) root         (0)      344 2023-02-23 14:40:45.000000 refinitiv-data-1.1.1/refinitiv_data.egg-info/requires.txt
--rw-r--r--   0 root         (0) root         (0)       10 2023-02-23 14:40:45.000000 refinitiv-data-1.1.1/refinitiv_data.egg-info/top_level.txt
--rw-r--r--   0 root         (0) root         (0)       38 2023-02-23 14:40:45.635148 refinitiv-data-1.1.1/setup.cfg
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.921840 refinitiv-data-1.2.0/
+-rw-rw-rw-   0 root         (0) root         (0)     9338 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/CHANGELOG.md
+-rw-rw-rw-   0 root         (0) root         (0)    11358 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/LICENSE
+-rw-r--r--   0 root         (0) root         (0)     9778 2023-05-02 10:11:28.921840 refinitiv-data-1.2.0/PKG-INFO
+-rw-rw-rw-   0 root         (0) root         (0)     9061 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/README.md
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.797839 refinitiv-data-1.2.0/changes/
+-rw-rw-rw-   0 root         (0) root         (0)      457 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/changes/template.jinja
+-rw-rw-rw-   0 root         (0) root         (0)    21868 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/pyproject.toml
+-rw-rw-rw-   0 root         (0) root         (0)        0 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/python
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.797839 refinitiv-data-1.2.0/refinitiv/
+-rw-rw-rw-   0 root         (0) root         (0)       76 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.797839 refinitiv-data-1.2.0/refinitiv/data/
+-rw-rw-rw-   0 root         (0) root         (0)     1106 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      247 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_base_enum.py
+-rw-rw-rw-   0 root         (0) root         (0)    16562 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_config_defaults.py
+-rw-rw-rw-   0 root         (0) root         (0)     1143 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_config_functions.py
+-rw-rw-rw-   0 root         (0) root         (0)    15270 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_configure.py
+-rw-rw-rw-   0 root         (0) root         (0)     4036 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_content_type.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.797839 refinitiv-data-1.2.0/refinitiv/data/_core/
+-rw-rw-rw-   0 root         (0) root         (0)        0 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1810 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/log_reporter.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.801840 refinitiv-data-1.2.0/refinitiv/data/_core/session/
+-rw-rw-rw-   0 root         (0) root         (0)      719 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      245 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_dacs_params.py
+-rw-rw-rw-   0 root         (0) root         (0)     3967 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_default_session_manager.py
+-rw-rw-rw-   0 root         (0) root         (0)     4908 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_desktop_session.py
+-rw-rw-rw-   0 root         (0) root         (0)     9883 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_platform_session.py
+-rw-rw-rw-   0 root         (0) root         (0)    11808 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_retry_transport.py
+-rw-rw-rw-   0 root         (0) root         (0)    14681 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_session.py
+-rw-rw-rw-   0 root         (0) root         (0)     1078 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_cxn_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)      188 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_cxn_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     2472 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    10054 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)       95 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     3632 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/access_token_updater.py
+-rw-rw-rw-   0 root         (0) root         (0)    11862 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/auth_manager.py
+-rw-rw-rw-   0 root         (0) root         (0)    10563 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/connection.py
+-rw-rw-rw-   0 root         (0) root         (0)      660 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/event.py
+-rw-rw-rw-   0 root         (0) root         (0)     1411 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/event_code.py
+-rw-rw-rw-   0 root         (0) root         (0)      489 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/global_settings.py
+-rw-rw-rw-   0 root         (0) root         (0)     1475 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/grant_password.py
+-rw-rw-rw-   0 root         (0) root         (0)    12612 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/http_service.py
+-rw-rw-rw-   0 root         (0) root         (0)      620 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/null_session.py
+-rw-rw-rw-   0 root         (0) root         (0)     4281 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/refresh_token_updater.py
+-rw-rw-rw-   0 root         (0) root         (0)     2487 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/tools.py
+-rw-rw-rw-   0 root         (0) root         (0)     3990 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_core/session/updater.py
+-rw-rw-rw-   0 root         (0) root         (0)     2300 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_errors.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.801840 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/
+-rw-rw-rw-   0 root         (0) root         (0)        0 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.801840 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/
+-rw-rw-rw-   0 root         (0) root         (0)     1071 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/LICENSE
+-rw-rw-rw-   0 root         (0) root         (0)    26229 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    11923 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/configuration.py
+-rw-rw-rw-   0 root         (0) root         (0)     7238 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/configuration_set.py
+-rw-rw-rw-   0 root         (0) root         (0)     5845 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/helpers.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.805840 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/
+-rw-rw-rw-   0 root         (0) root         (0)      608 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     5334 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_containers.py
+-rw-rw-rw-   0 root         (0) root         (0)     2141 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_data_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)    12224 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     4588 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_history_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)      711 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_history_df_builder_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     1500 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_intervals_consts.py
+-rw-rw-rw-   0 root         (0) root         (0)     4195 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_mixed_streams.py
+-rw-rw-rw-   0 root         (0) root         (0)     5066 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_ohlc_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)    12099 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_pricing_recorder.py
+-rw-rw-rw-   0 root         (0) root         (0)     5290 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_recording_control.py
+-rw-rw-rw-   0 root         (0) root         (0)     3110 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_stream_update_handler.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.805840 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/
+-rw-rw-rw-   0 root         (0) root         (0)      335 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2993 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_adc_context.py
+-rw-rw-rw-   0 root         (0) root         (0)     1950 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_adc_rdp_context.py
+-rw-rw-rw-   0 root         (0) root         (0)     1979 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_adc_udf_context.py
+-rw-rw-rw-   0 root         (0) root         (0)     1359 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_context.py
+-rw-rw-rw-   0 root         (0) root         (0)     3186 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_context_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     4950 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_context.py
+-rw-rw-rw-   0 root         (0) root         (0)     2854 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_rdp_context.py
+-rw-rw-rw-   0 root         (0) root         (0)     2250 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_udf_context.py
+-rw-rw-rw-   0 root         (0) root         (0)      755 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_hp_and_cust_inst_context.py
+-rw-rw-rw-   0 root         (0) root         (0)     2678 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_hp_context.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.805840 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/
+-rw-rw-rw-   0 root         (0) root         (0)      659 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2482 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_add_periods.py
+-rw-rw-rw-   0 root         (0) root         (0)     2504 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_count_periods.py
+-rw-rw-rw-   0 root         (0) root         (0)     3621 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_date_schedule.py
+-rw-rw-rw-   0 root         (0) root         (0)     4581 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_holidays.py
+-rw-rw-rw-   0 root         (0) root         (0)     1410 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_is_working_day.py
+-rw-rw-rw-   0 root         (0) root         (0)     3836 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/get_data_func.py
+-rw-rw-rw-   0 root         (0) root         (0)     7418 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/get_history_func.py
+-rw-rw-rw-   0 root         (0) root         (0)     3417 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/get_stream.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.805840 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/news/
+-rw-rw-rw-   0 root         (0) root         (0)      280 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/news/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2306 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/news/_news.py
+-rw-rw-rw-   0 root         (0) root         (0)     2804 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/session.py
+-rw-rw-rw-   0 root         (0) root         (0)    12927 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_log.py
+-rw-rw-rw-   0 root         (0) root         (0)      338 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_open_state.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.809840 refinitiv-data-1.2.0/refinitiv/data/_qpl/
+-rw-rw-rw-   0 root         (0) root         (0)      313 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      641 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_bid_ask.py
+-rw-rw-rw-   0 root         (0) root         (0)     2781 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)      766 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_fx_spot_quote.py
+-rw-rw-rw-   0 root         (0) root         (0)     2001 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_fx_swap_points.py
+-rw-rw-rw-   0 root         (0) root         (0)     3727 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_fx_swp_to_swp.py
+-rw-rw-rw-   0 root         (0) root         (0)      888 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_serializer.py
+-rw-rw-rw-   0 root         (0) root         (0)      760 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_tenor_bid_ask.py
+-rw-rw-rw-   0 root         (0) root         (0)      220 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_qpl/_tenor_types.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.809840 refinitiv-data-1.2.0/refinitiv/data/_tools/
+-rw-rw-rw-   0 root         (0) root         (0)      213 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    13407 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_common.py
+-rw-rw-rw-   0 root         (0) root         (0)     1208 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_converter.py
+-rw-rw-rw-   0 root         (0) root         (0)     6623 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_dataframe.py
+-rw-rw-rw-   0 root         (0) root         (0)     4171 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_datetime.py
+-rw-rw-rw-   0 root         (0) root         (0)       53 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_debug.py
+-rw-rw-rw-   0 root         (0) root         (0)     1744 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_params.py
+-rw-rw-rw-   0 root         (0) root         (0)      841 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_patterns.py
+-rw-rw-rw-   0 root         (0) root         (0)     1083 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_repr.py
+-rw-rw-rw-   0 root         (0) root         (0)     1317 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_specification.py
+-rw-rw-rw-   0 root         (0) root         (0)     2196 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/_utils.py
+-rw-rw-rw-   0 root         (0) root         (0)     5257 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_tools/templates.py
+-rw-rw-rw-   0 root         (0) root         (0)      731 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/_types.py
+-rw-r--r--   0 root         (0) root         (0)      160 2023-05-02 10:11:28.000000 refinitiv-data-1.2.0/refinitiv/data/_version.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.813840 refinitiv-data-1.2.0/refinitiv/data/content/
+-rw-rw-rw-   0 root         (0) root         (0)     2082 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      510 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_content_data.py
+-rw-rw-rw-   0 root         (0) root         (0)     1195 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_content_data_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     1094 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_content_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     3533 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_content_provider_layer.py
+-rw-rw-rw-   0 root         (0) root         (0)      620 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_content_response_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)      101 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_df_build_type.py
+-rw-rw-rw-   0 root         (0) root         (0)    17783 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)     8514 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_df_builder_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)    18264 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_entire_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)      829 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_error_parser.py
+-rw-rw-rw-   0 root         (0) root         (0)     1970 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_historical_content_validator.py
+-rw-rw-rw-   0 root         (0) root         (0)     6441 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_historical_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     6937 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_historical_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)     2895 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_historical_raw_transf.py
+-rw-rw-rw-   0 root         (0) root         (0)     3209 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_historical_response_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     3064 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_intervals.py
+-rw-rw-rw-   0 root         (0) root         (0)     1593 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_universe_content_validator.py
+-rw-rw-rw-   0 root         (0) root         (0)     8436 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_universe_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)    18431 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/_universe_streams.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.813840 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/
+-rw-rw-rw-   0 root         (0) root         (0)      467 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    15389 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_custom_instruments_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     2160 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      801 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_enums.py
+-rw-rw-rw-   0 root         (0) root         (0)     2415 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_events.py
+-rw-rw-rw-   0 root         (0) root         (0)     6061 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_instrument_class.py
+-rw-rw-rw-   0 root         (0) root         (0)    12907 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_instrument_prop_classes.py
+-rw-rw-rw-   0 root         (0) root         (0)    18090 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_manage.py
+-rw-rw-rw-   0 root         (0) root         (0)     1323 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_search.py
+-rw-rw-rw-   0 root         (0) root         (0)     8665 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_stream_facade.py
+-rw-rw-rw-   0 root         (0) root         (0)     2941 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_summaries.py
+-rw-rw-rw-   0 root         (0) root         (0)       59 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/events.py
+-rw-rw-rw-   0 root         (0) root         (0)      662 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/manage.py
+-rw-rw-rw-   0 root         (0) root         (0)       59 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/search.py
+-rw-rw-rw-   0 root         (0) root         (0)       62 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/summaries.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.817840 refinitiv-data-1.2.0/refinitiv/data/content/esg/
+-rw-rw-rw-   0 root         (0) root         (0)      295 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1478 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_base_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1785 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_basic_overview_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1165 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_esg_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1881 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_full_measures_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1916 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_full_scores_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1911 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_standard_measures_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1902 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_standard_scores_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1188 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/_universe_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       78 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/basic_overview.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.817840 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/
+-rw-rw-rw-   0 root         (0) root         (0)      143 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3858 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_actions.py
+-rw-rw-rw-   0 root         (0) root         (0)     9784 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_db_manager.py
+-rw-rw-rw-   0 root         (0) root         (0)     1908 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      351 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)      309 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_errors.py
+-rw-rw-rw-   0 root         (0) root         (0)     7363 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_file_manager.py
+-rw-rw-rw-   0 root         (0) root         (0)     3264 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_package_manager.py
+-rw-rw-rw-   0 root         (0) root         (0)     1263 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_package_manager_facade.py
+-rw-rw-rw-   0 root         (0) root         (0)     2562 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_tools.py
+-rw-rw-rw-   0 root         (0) root         (0)       77 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/full_measures.py
+-rw-rw-rw-   0 root         (0) root         (0)       75 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/full_scores.py
+-rw-rw-rw-   0 root         (0) root         (0)       81 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/standard_measures.py
+-rw-rw-rw-   0 root         (0) root         (0)       79 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/standard_scores.py
+-rw-rw-rw-   0 root         (0) root         (0)       72 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/esg/universe.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.817840 refinitiv-data-1.2.0/refinitiv/data/content/estimates/
+-rw-rw-rw-   0 root         (0) root         (0)      223 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1246 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)      475 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/_enums.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.817840 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/
+-rw-rw-rw-   0 root         (0) root         (0)       63 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2235 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/_annual_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2204 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/_interim_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       70 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/annual.py
+-rw-rw-rw-   0 root         (0) root         (0)       71 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/interim.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.821840 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/
+-rw-rw-rw-   0 root         (0) root         (0)       63 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1790 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/_annual_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1794 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/_interim_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       70 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/annual.py
+-rw-rw-rw-   0 root         (0) root         (0)       71 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/interim.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.821840 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/
+-rw-rw-rw-   0 root         (0) root         (0)      581 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2214 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_annual_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2359 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_non_periodic_measures_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2369 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_annual_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2373 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_interim_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2327 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_recommendations_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2218 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_interim_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2248 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_non_periodic_measures_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2198 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_recommendations_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       70 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/annual.py
+-rw-rw-rw-   0 root         (0) root         (0)      106 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/historical_snapshots_non_periodic_measures.py
+-rw-rw-rw-   0 root         (0) root         (0)      109 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/historical_snapshots_periodic_measures_annual.py
+-rw-rw-rw-   0 root         (0) root         (0)      110 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/historical_snapshots_periodic_measures_interim.py
+-rw-rw-rw-   0 root         (0) root         (0)      100 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/historical_snapshots_recommendations.py
+-rw-rw-rw-   0 root         (0) root         (0)       71 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/interim.py
+-rw-rw-rw-   0 root         (0) root         (0)       85 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/non_periodic_measures.py
+-rw-rw-rw-   0 root         (0) root         (0)       79 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/recommendations.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.821840 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/
+-rw-rw-rw-   0 root         (0) root         (0)      117 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1780 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/_annual_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1864 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/_historical_snapshots_kpi_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1785 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/_interim_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       70 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/annual.py
+-rw-rw-rw-   0 root         (0) root         (0)       88 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/historical_snapshots_kpi.py
+-rw-rw-rw-   0 root         (0) root         (0)       71 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/interim.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.825840 refinitiv-data-1.2.0/refinitiv/data/content/filings/
+-rw-rw-rw-   0 root         (0) root         (0)      117 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)       75 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/_errors.py
+-rw-rw-rw-   0 root         (0) root         (0)      304 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/_feed_name.py
+-rw-rw-rw-   0 root         (0) root         (0)     4584 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/_filing_query.py
+-rw-rw-rw-   0 root         (0) root         (0)    20076 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/_retrieval_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     2697 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/_retrieval_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     8827 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/_search_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     4866 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/_search_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       73 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/retrieval.py
+-rw-rw-rw-   0 root         (0) root         (0)       70 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/filings/search.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.825840 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/
+-rw-rw-rw-   0 root         (0) root         (0)      595 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3970 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_content_validator.py
+-rw-rw-rw-   0 root         (0) root         (0)     2041 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_data_grid_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      821 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)    10662 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2746 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_request_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     2191 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_response_factory.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.825840 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/
+-rw-rw-rw-   0 root         (0) root         (0)      403 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3077 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_enums.py
+-rw-rw-rw-   0 root         (0) root         (0)     2949 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_events_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      762 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_historical_pricing_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     3718 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_historical_pricing_request_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     3471 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_summaries_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      264 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/events.py
+-rw-rw-rw-   0 root         (0) root         (0)      327 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/summaries.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.825840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/
+-rw-rw-rw-   0 root         (0) root         (0)      192 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     5727 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_content_provider.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.829840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/
+-rw-rw-rw-   0 root         (0) root         (0)      383 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.829840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/
+-rw-rw-rw-   0 root         (0) root         (0)       70 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.829840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/
+-rw-rw-rw-   0 root         (0) root         (0)      140 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1293 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_constituents.py
+-rw-rw-rw-   0 root         (0) root         (0)    11462 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    28196 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     2919 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_request.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.833840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)      565 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      311 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_basis_spline_smooth_model.py
+-rw-rw-rw-   0 root         (0) root         (0)     1833 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_business_sector.py
+-rw-rw-rw-   0 root         (0) root         (0)      177 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_calendar_adjustment.py
+-rw-rw-rw-   0 root         (0) root         (0)      217 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_calibration_model.py
+-rw-rw-rw-   0 root         (0) root         (0)     2943 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_curve_sub_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      654 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_economic_sector.py
+-rw-rw-rw-   0 root         (0) root         (0)     8933 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry.py
+-rw-rw-rw-   0 root         (0) root         (0)     3834 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry_group.py
+-rw-rw-rw-   0 root         (0) root         (0)     1223 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interest_calculation_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      599 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interpolation_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      270 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_issuer_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      178 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_main_constituent_asset_class.py
+-rw-rw-rw-   0 root         (0) root         (0)      146 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_price_side.py
+-rw-rw-rw-   0 root         (0) root         (0)     1283 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_rating.py
+-rw-rw-rw-   0 root         (0) root         (0)      216 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_rating_scale_source.py
+-rw-rw-rw-   0 root         (0) root         (0)      248 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_reference_entity_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      238 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_seniority.py
+-rw-rw-rw-   0 root         (0) root         (0)      382 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_types.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.833840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/
+-rw-rw-rw-   0 root         (0) root         (0)       85 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.833840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/
+-rw-rw-rw-   0 root         (0) root         (0)      514 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     4482 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_butterfly_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)     6770 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_combined_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)     9677 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_curve_definition_pricing.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.837840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)      179 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      194 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_array_main_constituent_asset_class.py
+-rw-rw-rw-   0 root         (0) root         (0)      146 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_array_risk_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      216 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_constituent_override_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      222 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_shift_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      171 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_shift_unit.py
+-rw-rw-rw-   0 root         (0) root         (0)     3924 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_flattening_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)     1356 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_constituents.py
+-rw-rw-rw-   0 root         (0) root         (0)     5452 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     5766 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     2463 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_shift_scenario.py
+-rw-rw-rw-   0 root         (0) root         (0)     3920 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_long_end_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)     3921 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_parallel_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)     3719 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_request.py
+-rw-rw-rw-   0 root         (0) root         (0)     7749 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_shift_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3921 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_short_end_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)     6772 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_time_bucket_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)     4477 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_twist_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)      483 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_types.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.837840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/
+-rw-rw-rw-   0 root         (0) root         (0)      799 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2938 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_base_definition_mixin.py
+-rw-rw-rw-   0 root         (0) root         (0)     1236 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_description.py
+-rw-rw-rw-   0 root         (0) root         (0)     1158 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_formula_description.py
+-rw-rw-rw-   0 root         (0) root         (0)     3593 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_constituents_description.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/
+-rw-rw-rw-   0 root         (0) root         (0)       69 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3716 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_curve_definition_description.py
+-rw-rw-rw-   0 root         (0) root         (0)     1492 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_request.py
+-rw-rw-rw-   0 root         (0) root         (0)     1997 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_curve_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/
+-rw-rw-rw-   0 root         (0) root         (0)       24 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      547 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/_request.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)       68 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      158 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/_quotation_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      186 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_enums/_standard_turn_period.py
+-rw-rw-rw-   0 root         (0) root         (0)     1716 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_description.py
+-rw-rw-rw-   0 root         (0) root         (0)     2176 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_formula_description.py
+-rw-rw-rw-   0 root         (0) root         (0)     2164 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_formula_parameter_description.py
+-rw-rw-rw-   0 root         (0) root         (0)     3621 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2568 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_description.py
+-rw-rw-rw-   0 root         (0) root         (0)      940 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_turn_fields.py
+-rw-rw-rw-   0 root         (0) root         (0)     1899 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2623 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_description.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/
+-rw-rw-rw-   0 root         (0) root         (0)       62 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     7044 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_curve_definition_keys.py
+-rw-rw-rw-   0 root         (0) root         (0)      915 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_request.py
+-rw-rw-rw-   0 root         (0) root         (0)     2871 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2664 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_description.py
+-rw-rw-rw-   0 root         (0) root         (0)     2027 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instruments_segment.py
+-rw-rw-rw-   0 root         (0) root         (0)     1193 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_mixin_request.py
+-rw-rw-rw-   0 root         (0) root         (0)     1689 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask.py
+-rw-rw-rw-   0 root         (0) root         (0)     1038 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask_fields.py
+-rw-rw-rw-   0 root         (0) root         (0)     2176 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_fx_forward_turn.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/
+-rw-rw-rw-   0 root         (0) root         (0)       42 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     8742 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/_curve_get_definition_item.py
+-rw-rw-rw-   0 root         (0) root         (0)      900 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_turn_adjustment.py
+-rw-rw-rw-   0 root         (0) root         (0)      657 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_types.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/
+-rw-rw-rw-   0 root         (0) root         (0)       64 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3845 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_curve_update_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1494 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_request.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)      106 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      177 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_interpolation_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      160 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_main_constituent_asset_class.py
+-rw-rw-rw-   0 root         (0) root         (0)      133 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_risk_type.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.841840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/
+-rw-rw-rw-   0 root         (0) root         (0)       50 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3135 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/_request.py
+-rw-rw-rw-   0 root         (0) root         (0)      186 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_types.py
+-rw-rw-rw-   0 root         (0) root         (0)     8503 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_curves_builder_df.py
+-rw-rw-rw-   0 root         (0) root         (0)    22286 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_curves_data_provider.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.845840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)      574 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      171 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_calendar_adjustment.py
+-rw-rw-rw-   0 root         (0) root         (0)      242 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_compounding_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      172 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_constituent_override_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      158 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_forward_curves_outputs.py
+-rw-rw-rw-   0 root         (0) root         (0)      580 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_instrument_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      170 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_main_constituent_asset_class.py
+-rw-rw-rw-   0 root         (0) root         (0)      253 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_market_data_access_denied_fallback.py
+-rw-rw-rw-   0 root         (0) root         (0)      165 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_swap_price_side.py
+-rw-rw-rw-   0 root         (0) root         (0)      201 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_zc_curves_outputs.py
+-rw-rw-rw-   0 root         (0) root         (0)      616 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_zc_interpolation_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)     2774 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_forward_curve_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3441 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_forward_curve_request_item.py
+-rw-rw-rw-   0 root         (0) root         (0)      930 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_forward_curve_types.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.845840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/
+-rw-rw-rw-   0 root         (0) root         (0)      234 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      158 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_constituents.py
+-rw-rw-rw-   0 root         (0) root         (0)     1567 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_convexity.py
+-rw-rw-rw-   0 root         (0) root         (0)     3881 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_curve.py
+-rw-rw-rw-   0 root         (0) root         (0)      261 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_curve_point.py
+-rw-rw-rw-   0 root         (0) root         (0)      746 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_instrument.py
+-rw-rw-rw-   0 root         (0) root         (0)    13718 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_interest_rate_curve_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     1737 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_par_rate_shift.py
+-rw-rw-rw-   0 root         (0) root         (0)      990 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_step.py
+-rw-rw-rw-   0 root         (0) root         (0)     1426 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_turn.py
+-rw-rw-rw-   0 root         (0) root         (0)     3014 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_valuation_time.py
+-rw-rw-rw-   0 root         (0) root         (0)     2213 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_shift_scenario.py
+-rw-rw-rw-   0 root         (0) root         (0)    10434 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_swap_zc_curve_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    19745 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_swap_zc_curve_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     8802 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     4204 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_definition_request.py
+-rw-rw-rw-   0 root         (0) root         (0)    12838 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_definitions.py
+-rw-rw-rw-   0 root         (0) root         (0)    19456 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     2988 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_request_item.py
+-rw-rw-rw-   0 root         (0) root         (0)      726 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_types.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.857840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)     3130 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      185 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_adjust_interest_to_payment_date.py
+-rw-rw-rw-   0 root         (0) root         (0)      197 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_american_monte_carlo_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      332 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_amortization_frequency.py
+-rw-rw-rw-   0 root         (0) root         (0)      208 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_amortization_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      154 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_asset_class.py
+-rw-rw-rw-   0 root         (0) root         (0)      273 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_average_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      365 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_axis.py
+-rw-rw-rw-   0 root         (0) root         (0)      281 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_barrier_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      184 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_barrier_style.py
+-rw-rw-rw-   0 root         (0) root         (0)      251 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_barrier_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      185 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_benchmark_yield_selection_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      206 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_binary_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      960 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_business_day_convention.py
+-rw-rw-rw-   0 root         (0) root         (0)      415 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_buy_sell.py
+-rw-rw-rw-   0 root         (0) root         (0)      266 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_calibration_strategy.py
+-rw-rw-rw-   0 root         (0) root         (0)      405 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_call_put.py
+-rw-rw-rw-   0 root         (0) root         (0)      390 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_cds_convention.py
+-rw-rw-rw-   0 root         (0) root         (0)      763 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_common_tools.py
+-rw-rw-rw-   0 root         (0) root         (0)      187 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_credit_spread_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     1085 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_date_moving_convention.py
+-rw-rw-rw-   0 root         (0) root         (0)      735 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_date_rolling_convention.py
+-rw-rw-rw-   0 root         (0) root         (0)      453 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_dates_calendars_frequency.py
+-rw-rw-rw-   0 root         (0) root         (0)     1225 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_day_count_basis.py
+-rw-rw-rw-   0 root         (0) root         (0)      726 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_day_of_week.py
+-rw-rw-rw-   0 root         (0) root         (0)      513 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_direction.py
+-rw-rw-rw-   0 root         (0) root         (0)      201 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_discounting_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      250 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_dividend_extrapolation.py
+-rw-rw-rw-   0 root         (0) root         (0)      342 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_dividend_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      456 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_doc_clause.py
+-rw-rw-rw-   0 root         (0) root         (0)      175 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_double_binary_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      381 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_end_of_month_convention.py
+-rw-rw-rw-   0 root         (0) root         (0)      191 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_equity_dividend_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      194 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_eti_input_volatility_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      210 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_exercise_schedule_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      645 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_exercise_style.py
+-rw-rw-rw-   0 root         (0) root         (0)      187 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_extrapolation_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      289 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_fixing_frequency.py
+-rw-rw-rw-   0 root         (0) root         (0)      282 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_format.py
+-rw-rw-rw-   0 root         (0) root         (0)      269 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_forward_compute_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      299 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_forward_extrapolation.py
+-rw-rw-rw-   0 root         (0) root         (0)      906 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_frequency.py
+-rw-rw-rw-   0 root         (0) root         (0)      393 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_funding_spread_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      271 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_fx_binary_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      299 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_fx_cross_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      274 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_fx_leg_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      644 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_fx_swap_calculation_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      225 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_fx_volatility_model.py
+-rw-rw-rw-   0 root         (0) root         (0)      823 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_holiday_outupts.py
+-rw-rw-rw-   0 root         (0) root         (0)      233 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_implied_deposit_date_convention.py
+-rw-rw-rw-   0 root         (0) root         (0)      140 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_in_or_out.py
+-rw-rw-rw-   0 root         (0) root         (0)      319 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_average_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      644 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_compounding_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      206 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_convexity_adjustment_integration_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      260 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_convexity_adjustment_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      258 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_convexity_adjustment_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      504 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_observation_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      337 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_reset_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      259 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_index_spread_compounding_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      195 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_inflation_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      248 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_input_volatility_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      211 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_interest_calculation_convention.py
+-rw-rw-rw-   0 root         (0) root         (0)      301 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_interest_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      311 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_interpolation_mode.py
+-rw-rw-rw-   0 root         (0) root         (0)      380 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_local_volatility_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      248 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      227 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_notional_exchange.py
+-rw-rw-rw-   0 root         (0) root         (0)      281 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_numeraire_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      207 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_option_volatility_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      625 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_period_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      192 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_premium_settlement_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      178 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_price_side.py
+-rw-rw-rw-   0 root         (0) root         (0)      301 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_pricing_model_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      259 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_projected_index_calculation_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      614 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_quote_fallback_logic.py
+-rw-rw-rw-   0 root         (0) root         (0)     2113 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_redemption_date_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      213 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_repo_curve_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      145 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_risk_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      324 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_rounding.py
+-rw-rw-rw-   0 root         (0) root         (0)      304 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_rounding_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      757 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_seniority.py
+-rw-rw-rw-   0 root         (0) root         (0)      208 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_settlement_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      247 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_status.py
+-rw-rw-rw-   0 root         (0) root         (0)     1545 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_stub_rule.py
+-rw-rw-rw-   0 root         (0) root         (0)      184 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_swaption_settlement_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      160 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_swaption_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      205 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_swaption_volatility_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      185 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_tenor_reference_date.py
+-rw-rw-rw-   0 root         (0) root         (0)      316 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_time_stamp.py
+-rw-rw-rw-   0 root         (0) root         (0)      189 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_underlying_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      143 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_up_or_down.py
+-rw-rw-rw-   0 root         (0) root         (0)      144 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_vol_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      343 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_volatility_adjustment_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      222 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_volatility_model.py
+-rw-rw-rw-   0 root         (0) root         (0)      184 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_volatility_term_structure_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      172 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_volatility_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      913 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_yield_type.py
+-rw-rw-rw-   0 root         (0) root         (0)    10539 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_ipa_content_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1411 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_ipa_content_validator.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.861840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/
+-rw-rw-rw-   0 root         (0) root         (0)      433 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1898 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_american_monte_carlo_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     3841 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_amortization_item.py
+-rw-rw-rw-   0 root         (0) root         (0)     2188 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_barrier_definition_element.py
+-rw-rw-rw-   0 root         (0) root         (0)      917 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_basket_item.py
+-rw-rw-rw-   0 root         (0) root         (0)     1163 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_bid_ask_mid.py
+-rw-rw-rw-   0 root         (0) root         (0)     5046 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_bond_rounding_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)      691 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_day_weight.py
+-rw-rw-rw-   0 root         (0) root         (0)     2295 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_fx_point.py
+-rw-rw-rw-   0 root         (0) root         (0)     2084 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_input_flow.py
+-rw-rw-rw-   0 root         (0) root         (0)     1846 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_interpolation_weight.py
+-rw-rw-rw-   0 root         (0) root         (0)     1531 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_numerical_method.py
+-rw-rw-rw-   0 root         (0) root         (0)      948 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_payout_scaling.py
+-rw-rw-rw-   0 root         (0) root         (0)     1359 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_pde_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     4393 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_object_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.861840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/
+-rw-rw-rw-   0 root         (0) root         (0)       24 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1842 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_cap_surface_request_item.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.861840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)      423 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      246 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_enums/_calibration_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      180 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_enums/_moneyness_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      485 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_enums/_outputs.py
+-rw-rw-rw-   0 root         (0) root         (0)      146 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_enums/_strike_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      161 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_enums/_volatility_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     3061 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_eti_surface_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     8965 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_eti_surface_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     1692 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_eti_surface_request_item.py
+-rw-rw-rw-   0 root         (0) root         (0)     1708 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_fx_statistics_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)      777 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_fx_surface_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    13475 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_fx_surface_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     2545 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_fx_surface_request_item.py
+-rw-rw-rw-   0 root         (0) root         (0)     2694 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_i_ir_vol_model_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     9532 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_i_ir_vol_model_pricing_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/
+-rw-rw-rw-   0 root         (0) root         (0)      351 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2541 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_maturity_filter.py
+-rw-rw-rw-   0 root         (0) root         (0)     2477 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_moneyness_weight.py
+-rw-rw-rw-   0 root         (0) root         (0)     1214 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_strike_filter.py
+-rw-rw-rw-   0 root         (0) root         (0)     2290 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_strike_filter_range.py
+-rw-rw-rw-   0 root         (0) root         (0)     5183 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_surface.py
+-rw-rw-rw-   0 root         (0) root         (0)     6749 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_surface_filters.py
+-rw-rw-rw-   0 root         (0) root         (0)     5848 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_surface_output.py
+-rw-rw-rw-   0 root         (0) root         (0)      294 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_surface_point.py
+-rw-rw-rw-   0 root         (0) root         (0)     1145 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_volatility_surface_point.py
+-rw-rw-rw-   0 root         (0) root         (0)     1575 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_surface_request_item.py
+-rw-rw-rw-   0 root         (0) root         (0)      468 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_surface_types.py
+-rw-rw-rw-   0 root         (0) root         (0)    11119 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_surfaces_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)    14375 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_swaption_calculation_params.py
+-rw-rw-rw-   0 root         (0) root         (0)     3498 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_swaption_surface_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1853 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_swaption_surface_request_item.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/
+-rw-rw-rw-   0 root         (0) root         (0)      284 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_bond_curves/
+-rw-rw-rw-   0 root         (0) root         (0)       44 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_bond_curves/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_bond_curves/curves/
+-rw-rw-rw-   0 root         (0) root         (0)     1181 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_bond_curves/curves/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2737 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_bond_curves/curves/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/
+-rw-rw-rw-   0 root         (0) root         (0)      164 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      472 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/_arg_enums.py
+-rw-rw-rw-   0 root         (0) root         (0)       84 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/_base_data_class.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/
+-rw-rw-rw-   0 root         (0) root         (0)     1424 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3000 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/
+-rw-rw-rw-   0 root         (0) root         (0)     2037 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     7302 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_data_classes.py
+-rw-rw-rw-   0 root         (0) root         (0)    11268 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_manage.py
+-rw-rw-rw-   0 root         (0) root         (0)     4868 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_search.py
+-rw-rw-rw-   0 root         (0) root         (0)      116 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/manage.py
+-rw-rw-rw-   0 root         (0) root         (0)       59 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/search.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/
+-rw-rw-rw-   0 root         (0) root         (0)       44 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     5917 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     2900 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_search.py
+-rw-rw-rw-   0 root         (0) root         (0)       59 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/search.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.865840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/forward_curves/
+-rw-rw-rw-   0 root         (0) root         (0)      945 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/forward_curves/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     6123 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/forward_curves/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curve_definitions/
+-rw-rw-rw-   0 root         (0) root         (0)      186 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curve_definitions/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     4997 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curve_definitions/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curves/
+-rw-rw-rw-   0 root         (0) root         (0)     1282 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curves/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     4232 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curves/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/
+-rw-rw-rw-   0 root         (0) root         (0)      196 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1829 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/_base_request_items.py
+-rw-rw-rw-   0 root         (0) root         (0)      976 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/_content_data_validator.py
+-rw-rw-rw-   0 root         (0) root         (0)      560 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/_request_factory.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/add_periods/
+-rw-rw-rw-   0 root         (0) root         (0)      260 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/add_periods/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2081 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_add_periods_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)    10019 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/count_periods/
+-rw-rw-rw-   0 root         (0) root         (0)      172 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/count_periods/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     8278 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods.py
+-rw-rw-rw-   0 root         (0) root         (0)     1860 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods_data_provider.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/
+-rw-rw-rw-   0 root         (0) root         (0)      158 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     6938 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule.py
+-rw-rw-rw-   0 root         (0) root         (0)      933 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule_data_provider.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/holidays/
+-rw-rw-rw-   0 root         (0) root         (0)      143 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/holidays/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     7269 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays.py
+-rw-rw-rw-   0 root         (0) root         (0)     3486 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays_data_provider.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/
+-rw-rw-rw-   0 root         (0) root         (0)      149 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     6928 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/_is_working_day.py
+-rw-rw-rw-   0 root         (0) root         (0)     2388 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/_is_working_day_data_provider.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.869840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/
+-rw-rw-rw-   0 root         (0) root         (0)      483 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2463 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_base_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     8331 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_contracts_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     3681 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1036 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_instrument_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3717 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_quantitative_data_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)     3615 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_stream_facade.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.873840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/
+-rw-rw-rw-   0 root         (0) root         (0)     1524 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    22906 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/_bond_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    90430 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/_bond_pricing_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)    14567 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.873840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/
+-rw-rw-rw-   0 root         (0) root         (0)     1152 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    22163 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     7581 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_pricing_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)    12450 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.873840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/
+-rw-rw-rw-   0 root         (0) root         (0)      678 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    12059 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_cds_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     7555 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_cds_pricing_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     7021 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    13621 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_premium_leg_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     9673 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_protection_leg_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.873840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/
+-rw-rw-rw-   0 root         (0) root         (0)      558 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     6350 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     8601 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     7306 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_leg_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    10144 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_pricing_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.877840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/
+-rw-rw-rw-   0 root         (0) root         (0)     2174 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.877840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_base/
+-rw-rw-rw-   0 root         (0) root         (0)      181 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_base/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      127 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_base/_barrier_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      126 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_base/_binary_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      114 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_base/_info.py
+-rw-rw-rw-   0 root         (0) root         (0)      130 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_base/_underlying_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     9594 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.877840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_enums/
+-rw-rw-rw-   0 root         (0) root         (0)      557 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_enums/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      406 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_enums/_double_binary_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      875 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_enums/_fx_binary_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      298 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_enums/_settlement_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      287 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_enums/_underlying_type.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.877840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/
+-rw-rw-rw-   0 root         (0) root         (0)      390 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1899 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_barrier_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1861 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_binary_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      989 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_cbbc_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    13901 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      875 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_double_barriers_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     4699 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_fixing_info.py
+-rw-rw-rw-   0 root         (0) root         (0)      708 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_underlying_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.881840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/
+-rw-rw-rw-   0 root         (0) root         (0)      563 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     4219 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_average_info.py
+-rw-rw-rw-   0 root         (0) root         (0)     3677 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_barrier_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2514 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_binary_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    17171 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2164 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1303 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_info.py
+-rw-rw-rw-   0 root         (0) root         (0)     3848 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_binary_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1307 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_dual_currency_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1807 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_forward_start.py
+-rw-rw-rw-   0 root         (0) root         (0)      779 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_underlying_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     4114 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_option_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3673 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_option_instrument_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    30468 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_option_pricing_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.881840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/
+-rw-rw-rw-   0 root         (0) root         (0)      531 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     5782 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     8053 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     4865 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     4384 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_pricing_parameters.py
+-rw-rw-rw-   0 root         (0) root         (0)     2984 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_contract.py
+-rw-rw-rw-   0 root         (0) root         (0)     4900 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_pricing_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.881840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/
+-rw-rw-rw-   0 root         (0) root         (0)     1533 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1779 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     5213 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_swap_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    35752 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_swap_leg_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     9941 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_swap_pricing_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.881840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/
+-rw-rw-rw-   0 root         (0) root         (0)      761 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1681 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_bermudan_swaption_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     7839 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    11982 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      612 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_market_data_rule.py
+-rw-rw-rw-   0 root         (0) root         (0)     6266 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_pricing_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.881840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/
+-rw-rw-rw-   0 root         (0) root         (0)      400 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     7211 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    12318 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3826 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_pricing_parameters.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.881840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/
+-rw-rw-rw-   0 root         (0) root         (0)      338 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      734 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.885840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/cap/
+-rw-rw-rw-   0 root         (0) root         (0)      767 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/cap/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2921 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/cap/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.885840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/eti/
+-rw-rw-rw-   0 root         (0) root         (0)      937 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/eti/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2628 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/eti/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.885840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/fx/
+-rw-rw-rw-   0 root         (0) root         (0)      904 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/fx/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2635 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/fx/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.885840 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/swaption/
+-rw-rw-rw-   0 root         (0) root         (0)      827 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/swaption/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2994 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/swaption/_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.885840 refinitiv-data-1.2.0/refinitiv/data/content/news/
+-rw-rw-rw-   0 root         (0) root         (0)      180 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      343 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/_content_validator_udf.py
+-rw-rw-rw-   0 root         (0) root         (0)      289 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)     1455 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/_news_data.py
+-rw-rw-rw-   0 root         (0) root         (0)     2464 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/_news_data_provider_layer.py
+-rw-rw-rw-   0 root         (0) root         (0)     3276 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/_tools.py
+-rw-rw-rw-   0 root         (0) root         (0)      131 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/_urgency.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.885840 refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/
+-rw-rw-rw-   0 root         (0) root         (0)      110 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3216 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/_data.py
+-rw-rw-rw-   0 root         (0) root         (0)     5481 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     4929 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2579 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/_request_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)      151 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/_sort_order.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.885840 refinitiv-data-1.2.0/refinitiv/data/content/news/images/
+-rw-rw-rw-   0 root         (0) root         (0)       63 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/images/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3478 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/images/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1391 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/images/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2821 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/images/_image.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.889840 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/
+-rw-rw-rw-   0 root         (0) root         (0)       99 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1295 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1345 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1120 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)      517 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/_image_data.py
+-rw-rw-rw-   0 root         (0) root         (0)      229 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/_news_item.py
+-rw-rw-rw-   0 root         (0) root         (0)     1267 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/_report.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.889840 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/hierarchy/
+-rw-rw-rw-   0 root         (0) root         (0)       63 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/hierarchy/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      975 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/hierarchy/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)      949 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/hierarchy/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      702 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/hierarchy/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)      355 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/hierarchy/_report.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.889840 refinitiv-data-1.2.0/refinitiv/data/content/news/story/
+-rw-rw-rw-   0 root         (0) root         (0)      127 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3375 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/_data.py
+-rw-rw-rw-   0 root         (0) root         (0)      788 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1084 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1752 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/_request_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)      377 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/_response.py
+-rw-rw-rw-   0 root         (0) root         (0)      826 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/_response_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     1383 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/story/_udf_html_parser.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.889840 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/
+-rw-rw-rw-   0 root         (0) root         (0)       99 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1316 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1242 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      823 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)      877 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_top_news_headline.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.889840 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/
+-rw-rw-rw-   0 root         (0) root         (0)      111 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1290 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1271 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      898 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)      470 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/_subcategory.py
+-rw-rw-rw-   0 root         (0) root         (0)      124 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/_top_news_id.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.893840 refinitiv-data-1.2.0/refinitiv/data/content/ownership/
+-rw-rw-rw-   0 root         (0) root         (0)      259 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      319 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/_df_builder.py
+-rw-rw-rw-   0 root         (0) root         (0)      394 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/_enums.py
+-rw-rw-rw-   0 root         (0) root         (0)     1721 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/_org_info_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3986 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/_ownership_data_provider.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.893840 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/
+-rw-rw-rw-   0 root         (0) root         (0)      353 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2432 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_breakdown_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1788 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_concentration_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2129 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_investors_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2043 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_recent_activity_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3285 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_shareholders_history_report_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2095 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_shareholders_report_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1977 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_top_n_concentration_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       73 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/breakdown.py
+-rw-rw-rw-   0 root         (0) root         (0)       77 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/concentration.py
+-rw-rw-rw-   0 root         (0) root         (0)       73 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/investors.py
+-rw-rw-rw-   0 root         (0) root         (0)       79 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/recent_activity.py
+-rw-rw-rw-   0 root         (0) root         (0)       91 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/shareholders_history_report.py
+-rw-rw-rw-   0 root         (0) root         (0)       83 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/shareholders_report.py
+-rw-rw-rw-   0 root         (0) root         (0)       83 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/top_n_concentration.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.897840 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/
+-rw-rw-rw-   0 root         (0) root         (0)      383 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2410 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_breakdown_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1771 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_concentration_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2024 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_holdings_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2156 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_investors_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2043 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_recent_activity_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3197 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_shareholders_history_report_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2084 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_shareholders_report_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1945 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_top_n_concentration_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       73 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/breakdown.py
+-rw-rw-rw-   0 root         (0) root         (0)       77 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/concentration.py
+-rw-rw-rw-   0 root         (0) root         (0)       72 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/holdings.py
+-rw-rw-rw-   0 root         (0) root         (0)       73 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/investors.py
+-rw-rw-rw-   0 root         (0) root         (0)       79 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/recent_activity.py
+-rw-rw-rw-   0 root         (0) root         (0)       91 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/shareholders_history_report.py
+-rw-rw-rw-   0 root         (0) root         (0)       83 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/shareholders_report.py
+-rw-rw-rw-   0 root         (0) root         (0)       83 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/top_n_concentration.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.897840 refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/
+-rw-rw-rw-   0 root         (0) root         (0)      111 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2140 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/_shareholders_report_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3238 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/_transaction_report_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       83 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/shareholders_report.py
+-rw-rw-rw-   0 root         (0) root         (0)       82 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/transaction_report.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.897840 refinitiv-data-1.2.0/refinitiv/data/content/ownership/investor/
+-rw-rw-rw-   0 root         (0) root         (0)       48 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/investor/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2116 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/investor/_holdings_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)       72 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/investor/holdings.py
+-rw-rw-rw-   0 root         (0) root         (0)       72 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/ownership/org_info.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.897840 refinitiv-data-1.2.0/refinitiv/data/content/pricing/
+-rw-rw-rw-   0 root         (0) root         (0)      260 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3467 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     5313 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/_pricing_content_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)    18330 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/_stream_facade.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.897840 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/
+-rw-rw-rw-   0 root         (0) root         (0)       63 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     8116 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_chain_record.py
+-rw-rw-rw-   0 root         (0) root         (0)     5139 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_chain_records.py
+-rw-rw-rw-   0 root         (0) root         (0)     1776 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_chains_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     3784 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)    55563 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_display_template.py
+-rw-rw-rw-   0 root         (0) root         (0)     8373 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)     4789 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_stream_facade.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.901840 refinitiv-data-1.2.0/refinitiv/data/content/search/
+-rw-rw-rw-   0 root         (0) root         (0)      169 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    11914 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     4249 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2029 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/_lookup_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1066 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/_metadata_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     1943 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/_views.py
+-rw-rw-rw-   0 root         (0) root         (0)       70 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/lookup.py
+-rw-rw-rw-   0 root         (0) root         (0)       72 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/search/metadata.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.901840 refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/
+-rw-rw-rw-   0 root         (0) root         (0)      294 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2824 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_asset_class.py
+-rw-rw-rw-   0 root         (0) root         (0)      552 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_asset_state.py
+-rw-rw-rw-   0 root         (0) root         (0)     4444 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_country_code.py
+-rw-rw-rw-   0 root         (0) root         (0)     7435 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      664 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_symbol_type.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.901840 refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/
+-rw-rw-rw-   0 root         (0) root         (0)      227 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3723 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     7718 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)    10810 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/_stream_facade.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.901840 refinitiv-data-1.2.0/refinitiv/data/delivery/
+-rw-rw-rw-   0 root         (0) root         (0)      178 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.905840 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/
+-rw-rw-rw-   0 root         (0) root         (0)       72 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      654 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_api_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      609 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_connection.py
+-rw-rw-rw-   0 root         (0) root         (0)     1040 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     3271 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)    27039 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_provider_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     5729 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_provider_layer.py
+-rw-rw-rw-   0 root         (0) root         (0)      199 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      961 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_endpoint_data.py
+-rw-rw-rw-   0 root         (0) root         (0)      914 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_endpoint_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     5820 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_endpoint_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2091 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_parsed_data.py
+-rw-rw-rw-   0 root         (0) root         (0)     4021 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_raw_data_parser.py
+-rw-rw-rw-   0 root         (0) root         (0)     1411 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_request.py
+-rw-rw-rw-   0 root         (0) root         (0)     5778 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_request_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     2097 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_response.py
+-rw-rw-rw-   0 root         (0) root         (0)     2506 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_response_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     3617 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_validators.py
+-rw-rw-rw-   0 root         (0) root         (0)      363 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_dictionary.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.909840 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/
+-rw-rw-rw-   0 root         (0) root         (0)      651 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    11380 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_omm_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)      137 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_protocol_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     4906 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_rdp_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)    10068 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_cxn_cache.py
+-rw-rw-rw-   0 root         (0) root         (0)     5401 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_cxn_config_data.py
+-rw-rw-rw-   0 root         (0) root         (0)    11913 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_cxn_config_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)    11475 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_factory.py
+-rw-rw-rw-   0 root         (0) root         (0)     9547 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_listener.py
+-rw-rw-rw-   0 root         (0) root         (0)      104 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     2590 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/base_stream.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.909840 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/
+-rw-rw-rw-   0 root         (0) root         (0)      191 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     5529 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_funcs.py
+-rw-rw-rw-   0 root         (0) root         (0)     2635 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_offstream.py
+-rw-rw-rw-   0 root         (0) root         (0)     2278 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_response.py
+-rw-rw-rw-   0 root         (0) root         (0)      185 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_stream_connection.py
+-rw-rw-rw-   0 root         (0) root         (0)      334 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     2351 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/event.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.909840 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/
+-rw-rw-rw-   0 root         (0) root         (0)      535 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    14002 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/_dictionary.py
+-rw-rw-rw-   0 root         (0) root         (0)      116 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/_dictionary_type.py
+-rw-rw-rw-   0 root         (0) root         (0)      575 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/_enum_type_entry.py
+-rw-rw-rw-   0 root         (0) root         (0)     1225 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/_field_description.py
+-rw-rw-rw-   0 root         (0) root         (0)      462 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)     6746 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/_types.py
+-rw-rw-rw-   0 root         (0) root         (0)     9328 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/metadata/_validator.py
+-rw-rw-rw-   0 root         (0) root         (0)    15911 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/omm_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)     6952 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/omm_stream_connection.py
+-rw-rw-rw-   0 root         (0) root         (0)     2773 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/omm_stream_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3929 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/proxy_info.py
+-rw-rw-rw-   0 root         (0) root         (0)    10724 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rdp_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)     3085 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rdp_stream_connection.py
+-rw-rw-rw-   0 root         (0) root         (0)     2116 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rdp_stream_definition.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.909840 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/
+-rw-rw-rw-   0 root         (0) root         (0)        0 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      779 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/conversion.py
+-rw-rw-rw-   0 root         (0) root         (0)     5403 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/ema.py
+-rw-rw-rw-   0 root         (0) root         (0)     9451 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/socket.py
+-rw-rw-rw-   0 root         (0) root         (0)     4741 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream.py
+-rw-rw-rw-   0 root         (0) root         (0)     4073 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_cache.py
+-rw-rw-rw-   0 root         (0) root         (0)    16405 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_connection.py
+-rw-rw-rw-   0 root         (0) root         (0)      225 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_cxn_state.py
+-rw-rw-rw-   0 root         (0) root         (0)      175 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_state.py
+-rw-rw-rw-   0 root         (0) root         (0)     3508 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_state_manager.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.909840 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/ws/
+-rw-rw-rw-   0 root         (0) root         (0)        0 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/ws/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1344 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/ws/ws_client.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.913840 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/
+-rw-rw-rw-   0 root         (0) root         (0)      166 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     3519 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_buckets_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     3480 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_cfs_data_provider.py
+-rw-rw-rw-   0 root         (0) root         (0)     1801 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_data_class.py
+-rw-rw-rw-   0 root         (0) root         (0)     1276 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_data_types.py
+-rw-rw-rw-   0 root         (0) root         (0)     1120 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_downloader.py
+-rw-rw-rw-   0 root         (0) root         (0)     1057 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_downloader_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      993 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_downloader_facade.py
+-rw-rw-rw-   0 root         (0) root         (0)     4471 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_sets_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     2280 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_files_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)     4204 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_iter_object.py
+-rw-rw-rw-   0 root         (0) root         (0)     3033 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_packages_definition.py
+-rw-rw-rw-   0 root         (0) root         (0)      228 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)     1789 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_tools.py
+-rw-rw-rw-   0 root         (0) root         (0)     1140 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_unpacker.py
+-rw-rw-rw-   0 root         (0) root         (0)       71 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/buckets.py
+-rw-rw-rw-   0 root         (0) root         (0)       79 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/file_downloader.py
+-rw-rw-rw-   0 root         (0) root         (0)       73 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/file_sets.py
+-rw-rw-rw-   0 root         (0) root         (0)       69 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/files.py
+-rw-rw-rw-   0 root         (0) root         (0)       72 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/packages.py
+-rw-rw-rw-   0 root         (0) root         (0)      239 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/endpoint_request.py
+-rw-rw-rw-   0 root         (0) root         (0)      384 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/omm_stream.py
+-rw-rw-rw-   0 root         (0) root         (0)       81 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/delivery/rdp_stream.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.913840 refinitiv-data-1.2.0/refinitiv/data/discovery/
+-rw-rw-rw-   0 root         (0) root         (0)      632 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     2471 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_convert_symbols.py
+-rw-rw-rw-   0 root         (0) root         (0)     2803 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.913840 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_explorer/
+-rw-rw-rw-   0 root         (0) root         (0)       92 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_explorer/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    11534 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_explorer/_dataclasses.py
+-rw-rw-rw-   0 root         (0) root         (0)     8350 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_explorer/_search_explorer.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.913840 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/
+-rw-rw-rw-   0 root         (0) root         (0)      383 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)    12658 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/base.py
+-rw-rw-rw-   0 root         (0) root         (0)     8134 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/embedded.py
+-rw-rw-rw-   0 root         (0) root         (0)     5282 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/manage.py
+-rw-rw-rw-   0 root         (0) root         (0)     1555 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/namespaces.py
+-rw-rw-rw-   0 root         (0) root         (0)     2235 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/search.py
+-rw-rw-rw-   0 root         (0) root         (0)      653 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/utils.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.917840 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/
+-rw-rw-rw-   0 root         (0) root         (0)       68 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      487 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/_customers.py
+-rw-rw-rw-   0 root         (0) root         (0)     3069 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/_fetch_data.py
+-rw-rw-rw-   0 root         (0) root         (0)      121 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/_relationship_type.py
+-rw-rw-rw-   0 root         (0) root         (0)     1186 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/_stakeholder_data.py
+-rw-rw-rw-   0 root         (0) root         (0)     1166 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/_stakeholders.py
+-rw-rw-rw-   0 root         (0) root         (0)      488 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_stakeholders/_suppliers.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.917840 refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/
+-rw-rw-rw-   0 root         (0) root         (0)       84 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     4156 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/_chain.py
+-rw-rw-rw-   0 root         (0) root         (0)     2562 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/_discovery_universe.py
+-rw-rw-rw-   0 root         (0) root         (0)      393 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/_peers.py
+-rw-rw-rw-   0 root         (0) root         (0)      514 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/_screener.py
+-rw-rw-rw-   0 root         (0) root         (0)      369 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/_universe_expander.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.917840 refinitiv-data-1.2.0/refinitiv/data/early_access/
+-rw-rw-rw-   0 root         (0) root         (0)     1298 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/early_access/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.917840 refinitiv-data-1.2.0/refinitiv/data/early_access/discovery/
+-rw-rw-rw-   0 root         (0) root         (0)      316 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/early_access/discovery/__init__.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.917840 refinitiv-data-1.2.0/refinitiv/data/eikon/
+-rw-rw-rw-   0 root         (0) root         (0)      479 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     9549 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_data_grid.py
+-rw-rw-rw-   0 root         (0) root         (0)       47 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_errors.py
+-rw-rw-rw-   0 root         (0) root         (0)     8917 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_json_requests.py
+-rw-rw-rw-   0 root         (0) root         (0)     7401 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_news_request.py
+-rw-rw-rw-   0 root         (0) root         (0)     1099 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_streaming_prices.py
+-rw-rw-rw-   0 root         (0) root         (0)     5234 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_symbology.py
+-rw-rw-rw-   0 root         (0) root         (0)    13670 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_time_series.py
+-rw-rw-rw-   0 root         (0) root         (0)     4829 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/eikon/_tools.py
+-rw-rw-rw-   0 root         (0) root         (0)      780 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/errors.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.917840 refinitiv-data-1.2.0/refinitiv/data/session/
+-rw-rw-rw-   0 root         (0) root         (0)     1154 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/session/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)     1742 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/session/desktop.py
+-rw-rw-rw-   0 root         (0) root         (0)     3850 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/session/platform.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.921840 refinitiv-data-1.2.0/refinitiv/data/usage_collection/
+-rw-rw-rw-   0 root         (0) root         (0)      359 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/usage_collection/__init__.py
+-rw-rw-rw-   0 root         (0) root         (0)      472 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/usage_collection/_abstract_logger.py
+-rw-rw-rw-   0 root         (0) root         (0)      342 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/usage_collection/_filter_types.py
+-rw-rw-rw-   0 root         (0) root         (0)     6287 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/usage_collection/_logger.py
+-rw-rw-rw-   0 root         (0) root         (0)     1555 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/usage_collection/_utils.py
+-rw-rw-rw-   0 root         (0) root         (0)       44 2023-05-02 10:11:11.000000 refinitiv-data-1.2.0/refinitiv/data/warnings.py
+drwxr-xr-x   0 root         (0) root         (0)        0 2023-05-02 10:11:28.921840 refinitiv-data-1.2.0/refinitiv_data.egg-info/
+-rw-r--r--   0 root         (0) root         (0)     9778 2023-05-02 10:11:28.000000 refinitiv-data-1.2.0/refinitiv_data.egg-info/PKG-INFO
+-rw-r--r--   0 root         (0) root         (0)    55166 2023-05-02 10:11:28.000000 refinitiv-data-1.2.0/refinitiv_data.egg-info/SOURCES.txt
+-rw-r--r--   0 root         (0) root         (0)        1 2023-05-02 10:11:28.000000 refinitiv-data-1.2.0/refinitiv_data.egg-info/dependency_links.txt
+-rw-r--r--   0 root         (0) root         (0)      345 2023-05-02 10:11:28.000000 refinitiv-data-1.2.0/refinitiv_data.egg-info/requires.txt
+-rw-r--r--   0 root         (0) root         (0)       10 2023-05-02 10:11:28.000000 refinitiv-data-1.2.0/refinitiv_data.egg-info/top_level.txt
+-rw-r--r--   0 root         (0) root         (0)       38 2023-05-02 10:11:28.921840 refinitiv-data-1.2.0/setup.cfg
```

### Comparing `refinitiv-data-1.1.1/LICENSE` & `refinitiv-data-1.2.0/LICENSE`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/PKG-INFO` & `refinitiv-data-1.2.0/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: refinitiv-data
-Version: 1.1.1
+Version: 1.2.0
 Summary: Client for Refinitiv Data Platform API's
 Author: REFINITIV
 License: Apache 2.0
 Project-URL: Homepage, https://developers.refinitiv.com/en/api-catalog/refinitiv-data-platform/refinitiv-data-library-for-python
 Project-URL: Documentation, https://developers.refinitiv.com/en/api-catalog/refinitiv-data-platform/refinitiv-data-library-for-python/documentation
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
```

### Comparing `refinitiv-data-1.1.1/README.md` & `refinitiv-data-1.2.0/README.md`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,12 +30,12 @@
 except ImportError:
     __version__ = "0.0.0"
 
 from ._config_functions import get_config, load_config
 from ._open_state import OpenState
 from . import delivery, session, content, errors, eikon, usage_collection, discovery
 from ._fin_coder_layer.session import open_session, close_session
-from ._fin_coder_layer.get_data import get_data
-from ._fin_coder_layer.get_history import get_history
+from ._fin_coder_layer.get_data_func import get_data
+from ._fin_coder_layer.get_history_func import get_history
 from ._fin_coder_layer.get_stream import PricingStream, open_pricing_stream
 from ._fin_coder_layer import dates_and_calendars, news
 from . import _qpl
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_config_defaults.py` & `refinitiv-data-1.2.0/refinitiv/data/_config_defaults.py`

 * *Files 2% similar despite different names*

```diff
@@ -133,14 +133,15 @@
                 "url": "/data/news/v1",
                 "underlying-platform": "udf",
                 "endpoints": {
                     "headlines": "/headlines",
                     "stories": "/stories",
                     "top-news": "/top-news",
                     "images": "/images",
+                    "online-reports": "/online-reports",
                 },
             },
             "environmental-social-governance": {
                 "url": "/data/environmental-social-governance/v1",
                 "endpoints": {
                     "universe": "/universe",
                     "basic": "/views/basic",
@@ -247,17 +248,15 @@
                     "metadata": "/metadata/views",
                 },
             }
         },
         "streaming": {
             "pricing": {
                 "url": "/streaming/pricing/v1",
-                "endpoints": {
-                    "main": {"path": "/", "protocols": ["OMM"], "locations": []}
-                },
+                "endpoints": {"main": {"path": "/", "protocols": ["OMM"], "locations": []}},
             },
             "custom-instruments": {
                 "url": "/streaming/custom-instruments/v1",
                 "endpoints": {
                     "resource": {
                         "path": "/resource",
                         "protocols": ["OMM"],
@@ -347,21 +346,22 @@
                 },
             },
         },
     },
 }
 
 current_checksum = hashlib.md5(repr(data).encode()).hexdigest()  # NOSONAR
-fixed_checksum = "08c37da0c079d1f55c88aca4e5c16287"
+fixed_checksum = "a8961eddd3d4cee9d87fcaf2d8b6b435"
 
 if current_checksum != fixed_checksum:
     warnings.warn(
         "Default library config was changed. This may cause unexpected errors. "
         "Please use user config to introduce new changes. You can reinstall "
         "refinitiv-data to revert changes back."
     )
 
 config = config_from_dict(data)
 
 # Run this module as script to update checksum after defaults change
 if __name__ == "__main__":
-    print("current checksum: ", current_checksum)
+    # Used only as a script to get new checksum
+    print("current checksum: ", current_checksum)  # pylint: disable=no-print-statement
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_config_functions.py` & `refinitiv-data-1.2.0/refinitiv/data/_config_functions.py`

 * *Files 1% similar despite different names*

```diff
@@ -35,17 +35,15 @@
     config object
     """
     return _load_config_and_set_default(path)
 
 
 def _load_config_and_set_default(path: Optional[str]) -> _RDPConfig:
     if not os.path.exists(path):
-        raise FileNotFoundError(
-            f"Can't find file: {path}. Current working folder {os.getcwd()}"
-        )
+        raise FileNotFoundError(f"Can't find file: {path}. Current working folder {os.getcwd()}")
 
     loaded_config = _read_config_file(path)
     user_config = ext_config_mod.config_from_dict(loaded_config)
 
     config = _get_config()
     config._set_config_index(0, user_config)
     return config
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_configure.py` & `refinitiv-data-1.2.0/refinitiv/data/_configure.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,7 +1,10 @@
+"""
+This module contains necessary functions for session config manipulation.
+"""
 import atexit
 import json
 import os
 import re
 from json.decoder import WHITESPACE  # noqa
 from typing import Any, Optional, Union, List
 import sys as _sys
@@ -11,14 +14,16 @@
 from watchdog.events import PatternMatchingEventHandler
 from watchdog.observers import Observer
 
 from ._errors import RDError
 from ._tools._common import get_from_path
 from . import _config_defaults
 
+__all__ = ["get", "get_str", "get_int", "get_config"]
+
 _observer: Optional[Observer] = None
 
 
 def _enable_watch():
     global _observer
     _observer = Observer()
     _event_handler = _RDPConfigChangedHandler(patterns=_config_files_paths)
@@ -277,16 +282,15 @@
             Raise exception if param name is not type 'str' .
         """
         if not isinstance(param, str):
             raise TypeError("Invalid type of parameter name, should be string")
 
         if not auto_create and not self._has_param(param):
             raise ValueError(
-                f"'{param}' is not defined in config. "
-                f"To create new parameter please use 'auto_create' property."
+                f"'{param}' is not defined in config. To create new parameter please use 'auto_create' property."
             )
 
         self[param] = value
 
     def _has_param(self, param: str):
         return param in self
 
@@ -306,17 +310,15 @@
         """
 
         try:
             return self[param]
         except KeyError:
             raise AttributeError(f"Config object doesn't has '{param}' attribute")
 
-    def _set_config_index(
-        self, index: int, config_: ext_config_mod.Configuration
-    ) -> None:
+    def _set_config_index(self, index: int, config_: ext_config_mod.Configuration) -> None:
         """
         Set config by using index.
         This method is using for overriding default config.
 
         Parameters
         ----------
             index: int
@@ -335,44 +337,137 @@
         return _RDPConfig(*configs)
 
 
 load = _load_config_from_file
 
 
 def get(key: str, default: Any = None) -> Union[dict, Any]:
+    """
+    Gets the particular value from config by key name.
+
+    Parameters
+    ----------
+    key : str
+        Config parameter name.
+    default
+        Value to return if there is no particular parameter in config.
+    Returns
+    -------
+    dict or Any
+        Value retrieved from config by name or default value.
+
+    """
     return get_config().get(key, default)
 
 
 def get_bool(item: str) -> bool:
     return get_config().get_bool(item)
 
 
 def get_str(item: str, fmt: str = "{}") -> str:
+    """
+    Converts the particular value from config as a string.
+
+    Parameters
+    ----------
+    item
+        Config parameter value to convert to string.
+    fmt
+        Placeholder for Python str.format() method to convert config value to string.
+
+    Returns
+    -------
+    str
+        Config parameter value converted to string.
+
+    """
     return get_config().get_str(item, fmt)
 
 
 def get_int(item: str) -> int:
+    """
+    Converts the particular value from config as an int.
+
+    Parameters
+    ----------
+    item
+        Config parameter value to convert to int.
+
+    Returns
+    -------
+    int
+        Config parameter value converted to int.
+
+    """
     return get_config().get_int(item)
 
 
 def get_list(item: str) -> List[Any]:
     return get_config().get_list(item)
 
 
 def get_param(param: str) -> Any:
+    """
+    Gets config parameter or raise an exception if parameter does not exist.
+
+    Parameters
+    ----------
+    param : str
+        Config parameter.
+
+    Returns
+    -------
+    Any
+        Config parameter.
+
+    Raises
+    ------
+    AttributeError
+        If the parameter does not exist.
+
+    """
     return get_config().get_param(param)
 
 
 def get_config() -> "_RDPConfig":
+    """
+    Gets the whole config or create it if it does not exist.
+
+    Returns
+    -------
+    _RDPConfig
+        RDPConfig instance.
+
+    """
     if _config is None:
         reload()
     return _config
 
 
 def set_param(param: str, value: Any, auto_create: bool = False) -> None:
+    """
+    Sets key in key-value pair inside the config instance.
+
+    Parameters
+    ----------
+    param : str
+        Parameter name.
+    value : Any
+        Parameter value.
+    auto_create : bool, default=False
+        Flag to create new key-value pair in config.
+
+    Raises
+    ------
+    TypeError
+        If the parameter name is not string.
+    ValueError
+        If config parameter does not exist and creation automatically is disabled.
+
+    """
     get_config().set_param(param, value, auto_create)
 
 
 class _defaults:  # noqa
     http_request_timeout: Optional[str] = None
     log_level: Optional[int] = None
     platform_server_mode: Optional[bool] = None
@@ -400,21 +495,17 @@
     _project_config_dir = os.environ.get(_RDPLIB_ENV_DIR) or os.getcwd()
     _config_files_paths = [
         c
         for c in [
             # CONFIG PROVIDED BY USER
             _custom_filepath,
             # PROJECT_CONFIG_FILE
-            _get_filepath(
-                rootdir=_project_config_dir, filename=_default_config_file_name
-            ),
+            _get_filepath(rootdir=_project_config_dir, filename=_default_config_file_name),
             # USER_CONFIG_FILE
-            _get_filepath(
-                rootdir=os.path.expanduser("~"), filename=_default_config_file_name
-            ),
+            _get_filepath(rootdir=os.path.expanduser("~"), filename=_default_config_file_name),
         ]
         if c
     ]
     _config = _create_rdpconfig(_config_files_paths)
     _config.load = _load_config_from_file
 
     _observer = None
@@ -426,31 +517,24 @@
 
     defaults = _defaults()
     defaults.http_request_timeout = _default_config.get(keys.http_request_timeout)
     from ._log import convert_log_level, root_logger
     import refinitiv.data as rd
 
     defaults.log_level = convert_log_level(_default_config.get(keys.log_level))
-    defaults.platform_server_mode = _default_config.get(
-        keys.platform_session_default_server_mode
-    )
+    defaults.platform_server_mode = _default_config.get(keys.platform_session_default_server_mode)
     log_debug(root_logger(), rd.__version__, _config_files_paths)
 
 
 def log_debug(logger, version: str, path_list: list):
     logger.debug(f"RD version is {version}; Python version is {_sys.version}")
 
     try:
         import pkg_resources as _pkg_resources
 
         _installed_packages = _pkg_resources.working_set
-        _installed_packages = sorted(
-            [f"{i.key}=={i.version}" for i in _installed_packages]
-        )
-        logger.debug(
-            f"Installed packages ({len(_installed_packages)}): "
-            f"{','.join(_installed_packages)}"
-        )
+        _installed_packages = sorted([f"{i.key}=={i.version}" for i in _installed_packages])
+        logger.debug(f"Installed packages ({len(_installed_packages)}): {','.join(_installed_packages)}")
     except Exception as e:
         logger.debug(f"Cannot log installed packages, {e}")
 
     logger.debug(f'Read configs: {", ".join(path_list)}')
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_content_type.py` & `refinitiv-data-1.2.0/refinitiv/data/_content_type.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from enum import Enum, auto
+from enum import auto, Enum
 
 
 class ContentType(Enum):
     CHAINS = auto()
     CONTRACTS = auto()
     CUSTOM_INSTRUMENTS_EVENTS = auto()
     CUSTOM_INSTRUMENTS_INSTRUMENTS = auto()
@@ -51,16 +51,18 @@
     HISTORICAL_PRICING_INTERDAY_SUMMARIES = auto()
     HISTORICAL_PRICING_INTRADAY_SUMMARIES = auto()
     NEWS_HEADLINES_RDP = auto()
     NEWS_HEADLINES_UDF = auto()
     NEWS_STORY_RDP = auto()
     NEWS_IMAGES = auto()
     NEWS_STORY_UDF = auto()
+    NEWS_TOP_NEWS_HIERARCHY = auto()
     NEWS_TOP_NEWS = auto()
-    NEWS_TOP_NEWS_HEADLINES = auto()
+    NEWS_ONLINE_REPORTS = auto()
+    NEWS_ONLINE_REPORTS_HIERARCHY = auto()
     NONE = auto()
     OWNERSHIP_CONSOLIDATED_BREAKDOWN = auto()
     OWNERSHIP_CONSOLIDATED_CONCENTRATION = auto()
     OWNERSHIP_CONSOLIDATED_INVESTORS = auto()
     OWNERSHIP_CONSOLIDATED_RECENT_ACTIVITY = auto()
     OWNERSHIP_CONSOLIDATED_SHAREHOLDERS_HISTORY_REPORT = auto()
     OWNERSHIP_CONSOLIDATED_SHAREHOLDERS_REPORT = auto()
@@ -76,21 +78,24 @@
     OWNERSHIP_INSIDER_SHAREHOLDERS_REPORT = auto()
     OWNERSHIP_INSIDER_TRANSACTION_REPORT = auto()
     OWNERSHIP_INVESTOR_HOLDINGS = auto()
     OWNERSHIP_ORG_INFO = auto()
     PRICING = auto()
     STREAMING_CHAINS = auto()
     STREAMING_CONTRACTS = auto()
-    STREAMING_CUSTOM = auto()
     STREAMING_CUSTOM_INSTRUMENTS = auto()
     STREAMING_PRICING = auto()
     STREAMING_TRADING = auto()
     STREAMING_CONTRIB = auto()
     STREAMING_OFF_CONTRIB = auto()
+    STREAMING_OMM = auto()
+    STREAMING_RDP = auto()
+    STREAMING_DICTIONARY = auto()
     SURFACES = auto()
+    SURFACES_SWAPTION = auto()
     ZC_CURVE_DEFINITIONS = auto()
     ZC_CURVES = auto()
     DATES_AND_CALENDARS_ADD_PERIODS = auto()
     DATES_AND_CALENDARS_HOLIDAYS = auto()
     DATES_AND_CALENDARS_COUNT_PERIODS = auto()
     DATES_AND_CALENDARS_DATE_SCHEDULE = auto()
     DATES_AND_CALENDARS_IS_WORKING_DAY = auto()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/log_reporter.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/log_reporter.py`

 * *Files 4% similar despite different names*

```diff
@@ -47,7 +47,16 @@
 
     def init_logger(self, logger):
         self._log = logger.log
         self._warning = logger.warning
         self._error = logger.error
         self._debug = logger.debug
         self._info = logger.info
+
+
+class PrvLogReporterMixin:
+    def _init_logger(self, logger):
+        self._log = logger.log
+        self._warning = logger.warning
+        self._error = logger.error
+        self._debug = logger.debug
+        self._info = logger.info
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/__init__.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_default_session_manager.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_default_session_manager.py`

 * *Files 0% similar despite different names*

```diff
@@ -94,17 +94,15 @@
             If user didn't set default session
 
         Returns
         ------
         Session object
         """
         if not self._session_wrapper.get():
-            raise AttributeError(
-                "No default session created yet. Please create a session first!"
-            )
+            raise AttributeError("No default session created yet. Please create a session first!")
         return self._session_wrapper.get()
 
     def set_default_session(self, session) -> None:
         """
         Set default session.
 
         Parameters
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_desktop_session.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_desktop_session.py`

 * *Files 4% similar despite different names*

```diff
@@ -55,20 +55,16 @@
 
         self._logger.debug(
             "".join(
                 [
                     f"DesktopSession created with following parameters:",
                     f' app_key="{app_key}", name="{name}"',
                     f' base_url="{base_url}"' if base_url is not None else "",
-                    f' platform_path_rdp="{platform_path_rdp}"'
-                    if platform_path_rdp
-                    else "",
-                    f' platform_path_udf="{platform_path_udf}"'
-                    if platform_path_udf
-                    else "",
+                    f' platform_path_rdp="{platform_path_rdp}"' if platform_path_rdp else "",
+                    f' platform_path_udf="{platform_path_udf}"' if platform_path_udf else "",
                     f' handshake_url="{handshake_url}"' if handshake_url else "",
                 ]
             )
         )
 
     def _get_session_cxn_type(self) -> SessionCxnType:
         return SessionCxnType.DESKTOP
@@ -85,21 +81,15 @@
         """
         return urljoin(self._base_url, self._handshake_url)
 
     def _get_base_url(self):
         return self._base_url
 
     def _get_rdp_url_root(self) -> str:
-        if self._platform_path_rdp is None:
-            raise ValueError(
-                f"Can't find '{self.name}.platform-paths.rdp' "
-                f"in config file. Please add this attribute."
-            )
-        url = urljoin(self._base_url, self._platform_path_rdp)
-        return url
+        return urljoin(self._base_url, self._platform_path_rdp)
 
     def set_timeout(self, timeout):
         """
         Set the timeout for requests.
         """
         self._timeout = timeout
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_platform_session.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_platform_session.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 import warnings
 from typing import TYPE_CHECKING
 
 from ._session import Session
 from ._session_cxn_type import SessionCxnType
 from ._session_type import SessionType
 from .event_code import EventCode
+from ..._errors import ScopeError
 from ..._tools import urljoin, cached_property, parse_url
 
 if TYPE_CHECKING:
     from ...delivery._data._data_provider import Request
     import httpx
     from ._session_cxn_factory import PlatformConnection
 
@@ -63,35 +64,30 @@
         self._server_mode = server_mode
         self._base_url = base_url
         self._auth_url = auth_url
         self._auth_authorize = auth_authorize
         self._auth_token = auth_token
         self._realtime_dist_system_url = realtime_distribution_system_url
 
-        self._enable_scope_verification = self.config.get(
-            f"sessions.platform.{name}.verify_scope", True
-        )
+        self._enable_scope_verification = self.config.get(f"sessions.platform.{name}.verify_scope", True)
 
         self._deployed_platform_host = deployed_platform_host
         self._deployed_platform_connection_name = self.name
 
         if self._deployed_platform_host is None and self._realtime_dist_system_url:
             parse_result = parse_url(self._realtime_dist_system_url)
             self._deployed_platform_host = parse_result.netloc
             self.debug(
                 f"Using the Refinitiv realtime distribution system : "
                 f"url at {self._realtime_dist_system_url},\n"
                 f"deployed_platform_host={self._deployed_platform_host}"
             )
 
         elif self._deployed_platform_host and not self._realtime_dist_system_url:
-            self.debug(
-                f"Using the specific "
-                f"deployed_platform_host={self._deployed_platform_host}"
-            )
+            self.debug(f"Using the specific deployed_platform_host={self._deployed_platform_host}")
 
         elif self._deployed_platform_host and self._realtime_dist_system_url:
             # what to do ?
             pass
 
     @property
     def stream_auto_reconnection(self):
@@ -103,17 +99,15 @@
 
     @property
     def signon_control(self):
         return self._take_signon_control
 
     @property
     def authentication_token_endpoint_url(self) -> str:
-        url = urljoin(
-            self._get_rdp_url_root() or "", self._auth_url or "", self._auth_token or ""
-        )
+        url = urljoin(self._get_rdp_url_root() or "", self._auth_url or "", self._auth_token or "")
         return url
 
     def _cxns_stop_auto_reconnect(self, _):
         from ...delivery._stream import get_cxn_cfg_provider
 
         cxn_cfg_provider = get_cxn_cfg_provider(self)
         cxn_cfg_provider.wait_start_connecting()
@@ -147,33 +141,25 @@
         cxn_type = self._get_session_cxn_type()
         cxn = get_session_cxn(cxn_type, self)
 
         if cxn_type is SessionCxnType.REFINITIV_DATA:
             from .event import UpdateEvent
 
             auth_mgr = cxn.auth_manager
-            auth_mgr.on(
-                UpdateEvent.AUTHENTICATION_SUCCESS, self._on_authentication_success
-            )
+            auth_mgr.on(UpdateEvent.AUTHENTICATION_SUCCESS, self._on_authentication_success)
             auth_mgr.on(
                 UpdateEvent.AUTHENTICATION_FAILED,
-                lambda message: self._call_on_event(
-                    EventCode.SessionAuthenticationFailed, message
-                ),
+                lambda message: self._call_on_event(EventCode.SessionAuthenticationFailed, message),
             )
             auth_mgr.on(
                 UpdateEvent.RECONNECTING,
-                lambda message: self._call_on_event(
-                    EventCode.SessionReconnecting, "Session is reconnecting"
-                ),
+                lambda message: self._call_on_event(EventCode.SessionReconnecting, "Session is reconnecting"),
             )
             auth_mgr.on(UpdateEvent.UPDATE_ACCESS_TOKEN, self.update_access_token)
-            auth_mgr.on(
-                UpdateEvent.REFRESH_TOKEN_EXPIRED, self._cxns_stop_auto_reconnect
-            )
+            auth_mgr.on(UpdateEvent.REFRESH_TOKEN_EXPIRED, self._cxns_stop_auto_reconnect)
             auth_mgr.on(UpdateEvent.CLOSE_AUTH_MANAGER, self._cxns_stop_auto_reconnect)
 
         self.debug(f"Created session connection {cxn_type}")
         return cxn
 
     def _get_session_cxn_type(self) -> SessionCxnType:
         if self._grant and self._grant.is_valid() and self._deployed_platform_host:
@@ -256,7 +242,28 @@
             f"\t\tauthentication_token_endpoint_url = {self.authentication_token_endpoint_url}"
         )
         return s
 
     def verify_scope(self, key: str, method: str):
         if self._enable_scope_verification:
             self._connection.auth_manager.verify_scope(key, method)
+
+    def _handle_insufficient_scope(self, path: str, method: str, message: str) -> None:
+        try:
+            required_scopes_str, missing_scopes_str = (
+                message.lstrip("access denied. Scopes required to access the resource: ")
+                .replace("[", "")
+                .replace("]", "")
+                .split(". Missing scopes: ")
+            )
+            required_scope_groups = required_scopes_str.split(" or ")
+        except (AttributeError, ValueError) as e:
+            self.warning(f"{e}. Unable to parse scope error message: {message}")
+            return
+        required_scopes = [set(i.split()) for i in required_scope_groups]
+        self._connection.auth_manager.set_scope(path, method, required_scopes)
+        raise ScopeError(
+            required_scopes,
+            self._connection.auth_manager._token_info.scope,
+            path,
+            method,
+        )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_retry_transport.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_retry_transport.py`

 * *Files 2% similar despite different names*

```diff
@@ -92,18 +92,15 @@
             reraise=True,
         )
 
     if "0.18" <= _HTTPX_VERSION < "0.20":
 
         def _handle_retry_exception(self, url, method, exc: RequestRetryException):
             stats = self._retrying.statistics
-            if (
-                self._total_timeout
-                and 0 < self._total_timeout <= stats["delay_since_first_attempt"]
-            ):
+            if self._total_timeout and 0 < self._total_timeout <= stats["delay_since_first_attempt"]:
                 raise RequestTimeout(
                     self._total_timeout,
                     stats["delay_since_first_attempt"],
                     stats["attempt_number"],
                     self._total_attempts,
                     url,
                     method,
@@ -121,27 +118,23 @@
                 raise exc
 
         def _check_response(self, method: bytes, response: tuple):
             status_code, *_ = response
             if method.decode() in self._on_methods and status_code in self._on_statuses:
                 raise RequestRetryException(
                     -1,
-                    f"Request failed and will be retried: "
-                    f"status code - {status_code}",
+                    f"Request failed and will be retried: status code - {status_code}",
                 )
             return response
 
     else:
 
         def _handle_retry_exception(self, request: Request, exc: RequestRetryException):
             stats = self._retrying.statistics
-            if (
-                self._total_timeout
-                and 0 < self._total_timeout <= stats["delay_since_first_attempt"]
-            ):
+            if self._total_timeout and 0 < self._total_timeout <= stats["delay_since_first_attempt"]:
                 raise RequestTimeout(
                     self._total_timeout,
                     stats["delay_since_first_attempt"],
                     stats["attempt_number"],
                     self._total_attempts,
                     request.url,
                     request.method,
@@ -155,22 +148,18 @@
                     request.url,
                     request.method,
                 ) from exc
             else:
                 raise exc
 
         def _check_response(self, request: Request, response: Response):
-            if (
-                request.method in self._on_methods
-                and response.status_code in self._on_statuses
-            ):
+            if request.method in self._on_methods and response.status_code in self._on_statuses:
                 raise RequestRetryException(
                     -1,
-                    f"Request failed and will be retried: "
-                    f"status code - {response.status_code}",
+                    f"Request failed and will be retried: status code - {response.status_code}",
                 )
             return response
 
 
 class RetryTransport(HTTPTransport, _RetryTransportBase):
     """
     Synchronous HTTP Transport with retry functionality
@@ -226,39 +215,33 @@
             on_methods,
             backoff_factor,
         )
 
     if "0.18" <= _HTTPX_VERSION < "0.20":
 
         def _handle_request(self, method: bytes, *args, **kwargs) -> tuple:
-            return self._check_response(
-                method, super().handle_request(method, *args, **kwargs)
-            )
+            return self._check_response(method, super().handle_request(method, *args, **kwargs))
 
         def handle_request(
             self,
             method: bytes,
             url: t.Tuple[bytes, bytes, t.Optional[int], bytes],
             *args,
             **kwargs,
         ) -> tuple:
             try:
-                return self._retrying(
-                    self._handle_request, method, url, *args, **kwargs
-                )
+                return self._retrying(self._handle_request, method, url, *args, **kwargs)
             except RequestRetryException as exc:
                 self._handle_retry_exception(url, method, exc)
 
     else:  # For version 0.20+
 
         def _handle_request(self, request: Request) -> Response:
             self._logger.debug(f"Sending request to {request.url}")
-            return self._check_response(
-                request, super().handle_request(request=request)
-            )
+            return self._check_response(request, super().handle_request(request=request))
 
         def handle_request(self, request: Request) -> Response:
             try:
                 return self._retrying(self._handle_request, request)
             except RequestRetryException as exc:
                 self._handle_retry_exception(request, exc)
 
@@ -318,37 +301,31 @@
             on_methods,
             backoff_factor,
         )
 
     if "0.18" <= _HTTPX_VERSION < "0.20":
 
         async def _handle_async_request(self, method: bytes, *args, **kwargs) -> tuple:
-            return self._check_response(
-                method, await super().handle_async_request(method, *args, **kwargs)
-            )
+            return self._check_response(method, await super().handle_async_request(method, *args, **kwargs))
 
         async def handle_async_request(
             self,
             method: bytes,
             url: t.Tuple[bytes, bytes, t.Optional[int], bytes],
             *args,
             **kwargs,
         ) -> tuple:
             try:
-                return await self._retrying(
-                    self._handle_async_request, method, url, *args, **kwargs
-                )
+                return await self._retrying(self._handle_async_request, method, url, *args, **kwargs)
             except RequestRetryException as exc:
                 self._handle_retry_exception(url, method, exc)
 
     else:
 
         async def _handle_async_request(self, request: Request) -> Response:
-            return self._check_response(
-                request, await super().handle_async_request(request=request)
-            )
+            return self._check_response(request, await super().handle_async_request(request=request))
 
         async def handle_async_request(self, request: Request) -> Response:
             try:
                 return await self._retrying(self._handle_async_request, request)
             except RequestRetryException as exc:
                 self._handle_retry_exception(request, exc)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_session.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_session.py`

 * *Files 2% similar despite different names*

```diff
@@ -59,28 +59,25 @@
         with self.__acquire_session_id_lock:
             self._session_id = next(self._id_iterator)
 
         session_type = self.type.name.lower()
         logger_name = f"sessions.{session_type}.{name}.{self.session_id}"
 
         self.class_logger().debug(
-            f'Creating session "{logger_name}" based on '
-            f'session.{session_type}.Definition("{session_type}.{name}")'
+            f'Creating session "{logger_name}" based on session.{session_type}.Definition("{session_type}.{name}")'
         )
 
         if app_key is None:
             raise ValueError("app_key value can't be None")
 
         self._state = OpenState.Closed
 
         self._app_key = app_key
         self._on_state: Callable[[OpenState, str, Session], None] = on_state
-        self._on_event: Callable[
-            [EventCode, Union[dict, str], Session], None
-        ] = on_event
+        self._on_event: Callable[[EventCode, Union[dict, str], Session], None] = on_event
         self._access_token = token
         self._dacs_params = DacsParams()
 
         if deployed_platform_username:
             self._dacs_params.deployed_platform_username = deployed_platform_username
         if dacs_position:
             self._dacs_params.dacs_position = dacs_position
@@ -165,21 +162,17 @@
     def _call_on_state(self, message: str):
         if not self._on_state:
             return
         self.debug(f"Session calls on_state({self}, {self._state}, {message})")
         try:
             self._on_state(self._state, message, self)
         except Exception as e:
-            self.error(
-                f"on_state user function on session {self.session_id} raised error {e}"
-            )
-
-    def on_event(
-        self, callback: Callable[[EventCode, Union[dict, str], "Session"], None]
-    ) -> None:
+            self.error(f"on_state user function on session {self.session_id} raised error {e}")
+
+    def on_event(self, callback: Callable[[EventCode, Union[dict, str], "Session"], None]) -> None:
         """
         Registers a defined callback function.
         Called by the library whenever the event of the session is updated.
 
         Parameters
         ----------
         callback: Callable[[EventCode, Union[dict, str], "Session"], None]
@@ -198,17 +191,15 @@
     def _call_on_event(self, event: EventCode, message: Union[dict, str]):
         if not self._on_event:
             return
         self.debug(f"Session calls on_event({self}, {event}, {message})")
         try:
             self._on_event(event, message, self)
         except Exception as e:
-            self.error(
-                f"on_event user function on session {self.session_id} raised error {e}"
-            )
+            self.error(f"on_event user function on session {self.session_id} raised error {e}")
 
     def __repr__(self):
         return create_repr(
             self,
             middle_path="session",
             content=f"{{name='{self.name}'}}",
         )
@@ -248,17 +239,15 @@
             return
         if not is_string_type(app_key):
             raise AttributeError("application key must be a string")
 
         self._app_key = app_key
 
     def update_access_token(self, access_token):
-        DEBUG and self.debug(
-            f"Session.update_access_token(access_token='{access_token}'"
-        )
+        DEBUG and self.debug(f"Session.update_access_token(access_token='{access_token}'")
         self._access_token = access_token
 
         from ...delivery._stream import stream_cxn_cache
 
         if stream_cxn_cache.has_cxns(self):
             cxns_by_session = stream_cxn_cache.get_cxns(self)
             for cxn in cxns_by_session:
@@ -421,21 +410,18 @@
 
                 time.sleep(5)
                 s = "\n\t".join([str(t) for t in threading.enumerate()])
                 self.debug(f"Threads:\n\t{s}")
 
                 if stream_cxn_cache.has_cxns(self):
                     raise AssertionError(
-                        f"Not all cxns are closed (session={self},\n"
-                        f"cxns={stream_cxn_cache.get_cxns(self)})"
+                        f"Not all cxns are closed (session={self},\ncxns={stream_cxn_cache.get_cxns(self)})"
                     )
 
-            self._config.remove_listener(
-                configure.ConfigEvent.UPDATE, self._on_config_updated
-            )
+            self._config.remove_listener(configure.ConfigEvent.UPDATE, self._on_config_updated)
             self._call_on_state(SESSION_IS_CLOSED)
             self.debug(f"Closed session")
 
         return self._state
 
     async def close_async(self) -> OpenState:
         """
@@ -462,27 +448,29 @@
 
                 await asyncio.sleep(5)
                 s = "\n\t".join([str(t) for t in threading.enumerate()])
                 self.debug(f"Threads:\n\t{s}")
 
                 if stream_cxn_cache.has_cxns(self):
                     raise AssertionError(
-                        f"Not all cxns are closed (session={self},\n"
-                        f"cxns={stream_cxn_cache.get_cxns(self)})"
+                        f"Not all cxns are closed (session={self},\ncxns={stream_cxn_cache.get_cxns(self)})"
                     )
 
-            self._config.remove_listener(
-                configure.ConfigEvent.UPDATE, self._on_config_updated
-            )
+            self._config.remove_listener(configure.ConfigEvent.UPDATE, self._on_config_updated)
             self._call_on_state(SESSION_IS_CLOSED)
             self.debug(f"Closed async session")
 
         return self._state
 
     async def http_request_async(self, request: "Request") -> "httpx.Response":
         return await self._http_service.request_async(request)
 
     def http_request(self, request: "Request") -> "httpx.Response":
         return self._http_service.request(request)
 
     def verify_scope(self, key: str, method: str):
+        # for override
+        pass
+
+    def _handle_insufficient_scope(self, path: str, method: str, message: str) -> None:
+        # for override
         pass
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_cxn_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_cxn_factory.py`

 * *Files 13% similar despite different names*

```diff
@@ -19,24 +19,18 @@
 SessionConnection = Union[
     RefinitivDataConnection,
     RefinitivDataAndDeployedConnection,
     DesktopConnection,
     DeployedConnection,
 ]
 
-PlatformConnection = Union[
-    RefinitivDataConnection, RefinitivDataAndDeployedConnection, DeployedConnection
-]
+PlatformConnection = Union[RefinitivDataConnection, RefinitivDataAndDeployedConnection, DeployedConnection]
 
 
-def get_session_cxn(
-    session_cxn_type: SessionCxnType, session: "Session"
-) -> SessionConnection:
+def get_session_cxn(session_cxn_type: SessionCxnType, session: "Session") -> SessionConnection:
     cxn_class = cxn_class_by_type.get(session_cxn_type)
 
     if not cxn_class:
-        raise ValueError(
-            f"Can't find cxn_class by session_cxn_type: {session_cxn_type}"
-        )
+        raise ValueError(f"Can't find cxn_class by session_cxn_type: {session_cxn_type}")
 
     cxn = cxn_class(session)
     return cxn
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_definition.py`

 * *Files 14% similar despite different names*

```diff
@@ -2,28 +2,22 @@
 
 from ._session_type import SessionType
 from ... import _configure as configure
 
 
 def _retrieve_config_and_set_type(session_name, session_type):
     if session_type == SessionType.PLATFORM:
-        session_config = configure.get(
-            configure.keys.platform_session(session_name), {}
-        )
+        session_config = configure.get(configure.keys.platform_session(session_name), {})
     elif session_type == SessionType.DESKTOP:
         session_config = configure.get(configure.keys.desktop_session(session_name), {})
     else:
-        raise TypeError(
-            f"Invalid session type: {session_type}, please set 'desktop' or 'platform'."
-        )
+        raise TypeError(f"Invalid session type: {session_type}, please set 'desktop' or 'platform'.")
 
     if not session_config:
-        raise ValueError(
-            f"Can't get config by name: {session_name}. Please check config name"
-        )
+        raise ValueError(f"Can't get config by name: {session_name}. Please check config name")
 
     return session_config
 
 
 def _get_session_type_and_name(config_path: str):
     from ._session_provider import get_session_type
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/_session_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/_session_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,17 +9,15 @@
 from .grant_password import GrantPassword
 
 CANNOT_FIND_APP_KEY = "Can't find 'app-key' in config object."
 
 DEPLOYED_PLATFORM_URL_CONFIG_KEY = "realtime-distribution-system.url"
 DEPLOYED_PLATFORM_USER_CONFIG_KEY = "realtime-distribution-system.dacs.username"
 DEPLOYED_PLATFORM_DACS_POS_CONFIG_KEY = "realtime-distribution-system.dacs.position"
-DEPLOYED_PLATFORM_DACS_ID_CONFIG_KEY = (
-    "realtime-distribution-system.dacs.application-id"
-)
+DEPLOYED_PLATFORM_DACS_ID_CONFIG_KEY = "realtime-distribution-system.dacs.application-id"
 
 PLATFORM_ARGUMENT_KEY_TO_CONFIG_KEY = {
     "app_key": "app-key",
     "signon_control": "signon_control",
     "deployed_platform_host": DEPLOYED_PLATFORM_URL_CONFIG_KEY,
     "deployed_platform_username": DEPLOYED_PLATFORM_USER_CONFIG_KEY,
     "dacs_position": DEPLOYED_PLATFORM_DACS_POS_CONFIG_KEY,
@@ -76,15 +74,14 @@
         username = config.get("username")
         password = config.get("password")
         arguments["grant"] = grant or GrantPassword(username, password)
 
         for argument, value in PLATFORM_ARGUMENT_KEY_TO_CONFIG_KEY.items():
             arguments[argument] = config.get(value)
     else:
-
         for argument, value in DESKTOP_ARGUMENT_KEY_TO_CONFIG_KEY.items():
             arguments[argument] = config.get(value)
 
     return arguments
 
 
 def _validate_platform_session_arguments(
@@ -98,17 +95,15 @@
 
     if app_key is None:
         raise AttributeError(
             "Please, set app-key in [session.platform.default] section of "
             "the config file or provide 'app-key' attribute to the definition."
         )
 
-    if not grant.is_valid() and (
-        not deployed_platform_host or not deployed_platform_username
-    ):
+    if not grant.is_valid() and (not deployed_platform_host or not deployed_platform_username):
         raise AttributeError(
             "To create platform session, please provide 'grant' attribute to the "
             "definition or set 'username' and 'password' in the config file. "
             "To create deployed session, please provide 'deployed_platform_host' "
             "and 'deployed_platform_username' to the definition or the config file."
         )
 
@@ -120,34 +115,28 @@
     deployed_platform_host=None,
     deployed_platform_username=None,
     dacs_position=None,
     dacs_application_id=None,
     grant=None,
 ):
     session_config = configure.get(configure.keys.platform_session(session_name), {})
-    default_session_config = configure.get(
-        configure.keys.platform_session("default"), {}
-    )
+    default_session_config = configure.get(configure.keys.platform_session("default"), {})
 
     if isinstance(session_config, dict) and len(session_config) == 0:
-        raise ValueError(
-            f"Session name: {session_name} is invalid or session_name object is empty"
-        )
+        raise ValueError(f"Session name: {session_name} is invalid or session_name object is empty")
 
     arguments = {
         "app-key": app_key,
         "signon_control": signon_control,
         DEPLOYED_PLATFORM_URL_CONFIG_KEY: deployed_platform_host,
         DEPLOYED_PLATFORM_USER_CONFIG_KEY: deployed_platform_username,
         DEPLOYED_PLATFORM_DACS_POS_CONFIG_KEY: dacs_position,
         DEPLOYED_PLATFORM_DACS_ID_CONFIG_KEY: dacs_application_id,
     }
-    filtered_arguments = {
-        key: value for key, value in arguments.items() if value is not None
-    }
+    filtered_arguments = {key: value for key, value in arguments.items() if value is not None}
 
     merged_config = ext_config_mod.ConfigurationSet(
         ext_config_mod.config_from_dict(filtered_arguments),
         session_config,
         default_session_config,
     )
 
@@ -161,68 +150,51 @@
     _validate_platform_session_arguments(
         app_key=app_key,
         grant=grant,
         deployed_platform_host=deployed_platform_host,
         deployed_platform_username=deployed_platform_username,
     )
 
-    return make_session_provider(
-        SessionType.PLATFORM, merged_config, grant, session_name
-    )
+    return make_session_provider(SessionType.PLATFORM, merged_config, grant, session_name)
 
 
-def _validate_desktop_session_app_key(
-    session_config=None, app_key=None, session_name=None
-):
+def _validate_desktop_session_app_key(session_config=None, app_key=None, session_name=None):
     if not session_config and not app_key:
-        raise ValueError(
-            f"Can't get config by name: {session_name}. "
-            f"Please check config name or provide app_key"
-        )
+        raise ValueError(f"Can't get config by name: {session_name}. Please check config name or provide app_key")
 
     if not app_key:
         raise AttributeError(CANNOT_FIND_APP_KEY)
 
 
 def _make_desktop_session_provider_by_arguments(session_name, app_key=None):
     session_config = configure.get(configure.keys.desktop_session(session_name), {})
-    default_session_config = configure.get(
-        configure.keys.desktop_session("workspace"), {}
-    )
+    default_session_config = configure.get(configure.keys.desktop_session("workspace"), {})
 
     if isinstance(session_config, dict) and len(session_config) == 0:
-        raise ValueError(
-            f"Session name: {session_name} is invalid or session_name object is empty"
-        )
+        raise ValueError(f"Session name: {session_name} is invalid or session_name object is empty")
 
     arguments = {"app-key": app_key}
-    filtered_arguments = {
-        key: value for key, value in arguments.items() if value is not None
-    }
+    filtered_arguments = {key: value for key, value in arguments.items() if value is not None}
 
     merged_config = ext_config_mod.ConfigurationSet(
         ext_config_mod.config_from_dict(filtered_arguments),
         session_config,
         default_session_config,
     )
 
     _validate_desktop_session_app_key(
         session_config=session_config,
         app_key=merged_config.get("app-key"),
         session_name=session_name,
     )
 
-    return make_session_provider(
-        SessionType.DESKTOP, merged_config, session_name=session_name
-    )
+    return make_session_provider(SessionType.DESKTOP, merged_config, session_name=session_name)
 
 
-def _validate_session_arguments(
-    app_key, session_type, grant, deployed_platform_host, deployed_platform_username
-):
+def _validate_session_arguments(app_key, session_type, grant, deployed_platform_host, deployed_platform_username):
     if app_key is None:
         raise AttributeError(CANNOT_FIND_APP_KEY)
 
     if (
         session_type == SessionType.PLATFORM
         and not grant.is_valid()
         and (not deployed_platform_host or not deployed_platform_username)
@@ -242,49 +214,39 @@
     )
 
     config_path = session_name  # can't rename argument, because public API, make it right at least in function body
     session_name, session_type = _get_session_type_and_name(config_path)
     session_config = _retrieve_config_and_set_type(session_name, session_type)
 
     if session_type == SessionType.DESKTOP:
-        default_session_config = configure.get(
-            configure.keys.desktop_session("workspace"), {}
-        )
+        default_session_config = configure.get(configure.keys.desktop_session("workspace"), {})
     else:
-        default_session_config = configure.get(
-            configure.keys.platform_session("default"), {}
-        )
+        default_session_config = configure.get(configure.keys.platform_session("default"), {})
 
-    merged_config = ext_config_mod.ConfigurationSet(
-        session_config, default_session_config
-    )
+    merged_config = ext_config_mod.ConfigurationSet(session_config, default_session_config)
     app_key = merged_config.get("app-key")
     deployed_platform_host = merged_config.get(DEPLOYED_PLATFORM_URL_CONFIG_KEY)
     deployed_platform_username = merged_config.get(DEPLOYED_PLATFORM_USER_CONFIG_KEY)
 
     grant = None
     if session_type == SessionType.PLATFORM:
-        grant = GrantPassword(
-            merged_config.get("username"), merged_config.get("password")
-        )
+        grant = GrantPassword(merged_config.get("username"), merged_config.get("password"))
 
     _validate_session_arguments(
         app_key=app_key,
         session_type=session_type,
         grant=grant,
         deployed_platform_host=deployed_platform_host,
         deployed_platform_username=deployed_platform_username,
     )
 
     return make_session_provider(session_type, merged_config, grant, session_name)
 
 
-def make_session_provider(
-    session_type, config=None, grant=None, session_name="default"
-):
+def make_session_provider(session_type, config=None, grant=None, session_name="default"):
     config = config or {}
     session_class = session_class_by_session_type.get(session_type)
     if not session_class:
         raise ValueError(f"Cannot find session class by session type {session_type}")
     session_class = get_session_class(session_type)
 
     def session_provider():
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/access_token_updater.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/access_token_updater.py`

 * *Files 10% similar despite different names*

```diff
@@ -40,30 +40,26 @@
     @Updater.delay.setter
     def delay(self, value: int):
         if value <= 0:
             raise ValueError("Delay must be greater than 0")
         Updater.delay.fset(self, value)
 
     def _do_update(self):
-        response = self._request_token(
-            self._grant, self._app_key, self._url, self._signon_control
-        )
+        response = self._request_token(self._grant, self._app_key, self._url, self._signon_control)
         status_code = response.status_code
         json_content = response.json()
 
         if status_code == codes.ok:
             event = UpdateEvent.ACCESS_TOKEN_SUCCESS
             message = "All is well"
 
         elif status_code in UNAUTHORIZED_CODES:
             event = UpdateEvent.ACCESS_TOKEN_UNAUTHORIZED
             error = json_content.get("error", "empty error")
-            error_description = json_content.get(
-                "error_description", "empty error description"
-            )
+            error_description = json_content.get("error_description", "empty error description")
             message = error_description
             self.error(f"[Error {status_code} - {error}] {error_description}")
 
         else:
             event = UpdateEvent.ACCESS_TOKEN_FAILED
             error = json_content.get("error")
             error_description = json_content.get(
@@ -71,17 +67,15 @@
                 getattr(response, "text", "empty error description"),
             )
             message = error_description
             self.error(f"[Error {status_code} - {error}] {error_description}")
 
         self._callback(event, message, json_content)
 
-    def _request_token(
-        self, grant: "GrantPassword", app_key: str, url: str, take_signon_control: bool
-    ):
+    def _request_token(self, grant: "GrantPassword", app_key: str, url: str, take_signon_control: bool):
         username = grant.get_username()
         data = {
             "scope": grant.get_token_scope(),
             "grant_type": "password",
             "username": username,
             "password": grant.get_password(),
             "takeExclusiveSignOnControl": "true" if take_signon_control else "false",
@@ -89,24 +83,19 @@
 
         if app_key is not None:
             data["client_id"] = app_key
 
         headers = {"Accept": "application/json"}
         try:
             start = time.time()
-            request = Request(
-                url=url, method=RequestMethod.POST, headers=headers, data=data
-            )
+            request = Request(url=url, method=RequestMethod.POST, headers=headers, data=data)
             response = self._session.http_request(request)
             end = time.time()
             self.latency_secs = end - start
-            self.debug(
-                f"Latency: {self.latency_secs} sec.\n"
-                f"Access token response: {response.text}"
-            )
+            self.debug(f"Latency: {self.latency_secs} sec.\nAccess token response: {response.text}")
         except Exception as e:
             response = NullResponse()
             response.text = str(e)
 
         return response
 
     def _do_dispose(self):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/auth_manager.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/auth_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,11 @@
 import threading
 import time
 from dataclasses import dataclass, field
-from typing import TYPE_CHECKING, Callable, Set, Union
-from ._scope_map import SCOPE_BY_KEY
+from typing import TYPE_CHECKING, Callable, Set, Union, List
 from .access_token_updater import AccessTokenUpdater
 from .event import UpdateEvent
 from .refresh_token_updater import RefreshTokenUpdater
 from .tools import MINUTES_10, get_delays
 from ..log_reporter import LogReporter
 from ..._errors import ScopeError
 from ..._tools import cached_property, CallbackHandler
@@ -101,14 +100,16 @@
         self._ee = CallbackHandler()
         self._token_info = TokenInfo()
 
         self._closed: bool = False
         self._authorized: bool = False
         self._disposed: bool = False
 
+        self._scope_map = {}
+
         self._result_evt: threading.Event = threading.Event()
         self._start_evt: threading.Event = threading.Event()
         self._thread: threading.Thread = threading.Thread(
             target=self._do_authorize,
             name="AuthManager-Thread",
             daemon=True,
         )
@@ -186,33 +187,28 @@
 
         is_authorized = self.is_authorized()
         self.debug(f"AuthManager: end authorize, result {is_authorized}")
         return is_authorized
 
     def _do_authorize(self):
         while not self._disposed:
-
             self._start_evt.wait()
 
             if self._disposed:
                 break
 
-            self.debug(
-                f"AuthManager: Access token will be requested "
-                f"in {self._access_token_updater.delay} seconds"
-            )
+            self.debug(f"AuthManager: Access token will be requested in {self._access_token_updater.delay} seconds")
             self._access_token_updater.start()
 
             if self.is_authorized():
                 latency_secs = self._access_token_updater.latency_secs
                 delay = self._token_info.calc_token_update_time(latency_secs)
                 self._refresh_token_updater.delay = delay
                 self.debug(
-                    f"AuthManager: Refresh token will be requested "
-                    f"in {self._refresh_token_updater.delay} seconds"
+                    f"AuthManager: Refresh token will be requested in {self._refresh_token_updater.delay} seconds"
                 )
                 self._refresh_token_updater.start()
 
     def close(self):
         """
         The method stops refresh token updater and access token updater
 
@@ -230,31 +226,24 @@
         self._ee.emit(UpdateEvent.CLOSE_AUTH_MANAGER, self)
         self._start_evt.clear()
         self._access_token_updater.stop()
         self._refresh_token_updater.stop()
         self._closed = True
         self._authorized = False
 
-    def _access_token_update_handler(
-        self, event: str, message: str, json_content: dict
-    ) -> None:
-        self.debug(
-            f"AuthManager: Access token handler, event: {event}, message: {message}"
-        )
+    def _access_token_update_handler(self, event: str, message: str, json_content: dict) -> None:
+        self.debug(f"AuthManager: Access token handler, event: {event}, message: {message}")
 
         if event is UpdateEvent.ACCESS_TOKEN_SUCCESS:
             self._authorized = True
             delays.reset()
             token_info = TokenInfo.from_dict(json_content)
             self._token_info.update(token_info)
             access_token = self._token_info.access_token
-            self.debug(
-                f"Access token {access_token}. "
-                f"Expire in {self._token_info.expires_in} seconds"
-            )
+            self.debug(f"Access token {access_token}. Expire in {self._token_info.expires_in} seconds")
             self._ee.emit(UpdateEvent.UPDATE_ACCESS_TOKEN, access_token)
             self._access_token_updater.stop()
             self._ee.emit(event, message)
             self._ee.emit(UpdateEvent.AUTHENTICATION_SUCCESS, message)
             self._result_evt.set()
 
         elif event is UpdateEvent.ACCESS_TOKEN_UNAUTHORIZED:
@@ -279,29 +268,22 @@
                 self.debug(f"AuthManager: reconnecting in {delay} secs")
                 self._access_token_updater.delay = delay
                 self._ee.emit(UpdateEvent.RECONNECTING, message)
 
             else:
                 self.close()
 
-    def _refresh_token_update_handler(
-        self, event: str, message: str, json_content: dict
-    ) -> None:
-        self.debug(
-            f"AuthManager: Refresh token handler, event: {event}, message: {message}"
-        )
+    def _refresh_token_update_handler(self, event: str, message: str, json_content: dict) -> None:
+        self.debug(f"AuthManager: Refresh token handler, event: {event}, message: {message}")
 
         if event is UpdateEvent.REFRESH_TOKEN_SUCCESS:
             token_info = TokenInfo.from_dict(json_content)
             self._token_info.update(token_info)
             access_token = token_info.access_token
-            self.debug(
-                f"Received access token {token_info.refresh_token}. "
-                f"Expire in {token_info.expires_in} seconds"
-            )
+            self.debug(f"Received access token {token_info.refresh_token}. Expire in {token_info.expires_in} seconds")
             self._ee.emit(UpdateEvent.UPDATE_ACCESS_TOKEN, access_token)
             latency_secs = self._access_token_updater.latency_secs
             delay = token_info.calc_token_update_time(latency_secs)
             self._refresh_token_updater.delay = delay
             self.debug(f"Set refresh token delay to {delay} seconds")
             self._ee.emit(event, message)
 
@@ -313,17 +295,15 @@
 
         elif event is UpdateEvent.REFRESH_TOKEN_FAILED:
             self._ee.emit(event, message)
             self._ee.emit(UpdateEvent.AUTHENTICATION_FAILED, message)
 
             if self._auto_reconnect:
                 delay = delays.next()
-                self.debug(
-                    f"AuthManager: Trying to get Refresh token again in {delay} secs"
-                )
+                self.debug(f"AuthManager: Trying to get Refresh token again in {delay} secs")
                 self._refresh_token_updater.delay = delay
                 self._ee.emit(UpdateEvent.RECONNECTING, message)
 
             else:
                 self._authorized = False
                 self.close()
 
@@ -361,19 +341,21 @@
         self._ee.remove_all_listeners()
         self._ee = None
         self._token_info = None
         self._start_evt = None
         self._thread = None
         self._session = None
 
+    def set_scope(self, key: str, method: str, required_scopes: List[Set[str]]):
+        self._scope_map[key.rstrip("/"), method.lower()] = required_scopes
+
     def verify_scope(self, key: str, method: str):
-        required_scope_sets = SCOPE_BY_KEY.get((key.rstrip("/"), method.lower()))
+        required_scope_sets = self._scope_map.get((key.rstrip("/"), method.lower()))
 
         if required_scope_sets is None:
             self.debug(f"Scope for key={key}, method={method} not found.")
 
         if required_scope_sets and all(
-            not required_scope.issubset(self._token_info.scope)
-            for required_scope in required_scope_sets
+            not required_scope.issubset(self._token_info.scope) for required_scope in required_scope_sets
         ):
             self.warning(f"No user scope for key={key}, method={method}.")
             raise ScopeError(required_scope_sets, self._token_info.scope, key, method)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/connection.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/connection.py`

 * *Files 3% similar despite different names*

```diff
@@ -130,37 +130,27 @@
                     saved_port = first_line.strip()
                     test_proxy_url = update_port_in_url(self._base_url, saved_port)
                     test_proxy_result = self.check_proxy(test_proxy_url)
                     if test_proxy_result:
                         port = saved_port
                         self.debug(f"Port {port} was retrieved from .portInUse file")
                     else:
-                        self.debug(
-                            f"Retrieved port {saved_port} value "
-                            f"from .portIntUse isn't valid."
-                        )
+                        self.debug(f"Retrieved port {saved_port} value from .portIntUse isn't valid.")
 
         if port is None:
-            self.debug(
-                "Warning: file .portInUse was not found. "
-                "Try to fallback to default port number."
-            )
+            self.debug("Warning: file .portInUse was not found. Try to fallback to default port number.")
             port = self.get_port_number_from_range(("9000", "9060"), self._base_url)
 
         if port is None:
-            self.error(
-                "Error: no proxy address identified.\nCheck if Desktop is running."
-            )
+            self.error("Error: no proxy address identified.\nCheck if Desktop is running.")
             return None
 
         return port
 
-    def get_port_number_from_range(
-        self, ports: Iterable[str], url: str
-    ) -> Optional[str]:
+    def get_port_number_from_range(self, ports: Iterable[str], url: str) -> Optional[str]:
         for port_number in ports:
             self.debug(f"Try defaulting to port {port_number}...")
             test_proxy_url = update_port_in_url(url, port_number)
             test_proxy_result = self.check_proxy(test_proxy_url)
             if test_proxy_result:
                 self.debug(f"Default proxy port {port_number} was successfully checked")
                 return port_number
@@ -173,18 +163,15 @@
         timeout = timeout if timeout is not None else self.get_timeout()
         url = urljoin(url, "/api/status")
 
         try:
             request = Request(url=url, method=RequestMethod.GET, timeout=timeout)
             response = self._session.http_request(request)
 
-            self.debug(
-                f"Checking proxy url {url} response : "
-                f"{response.status_code} - {response.text}"
-            )
+            self.debug(f"Checking proxy url {url} response : {response.status_code} - {response.text}")
             return True
         except (socket.timeout, httpx.ConnectTimeout):
             self.debug(f"Timeout on checking proxy url {url}")
         except ConnectionError as e:
             self.debug(f"Connexion Error on checking proxy {url} : {e!r}")
         except Exception as e:
             self.debug(f"Error on checking proxy url {url} : {e!r}")
@@ -229,30 +216,24 @@
             if response:
                 if response.status_code == httpx.codes.OK:
                     result = response.json()
                     self._session._access_token = result.get("access_token", None)
 
                 elif response.status_code == httpx.codes.BAD_REQUEST:
                     self.error(
-                        f"Status code {response.status_code}: "
-                        f"Bad request on handshake url {url} : {response.text}"
-                    )
-                    key_is_incorrect_msg = (
-                        f"Status code {response.status_code}: App key is incorrect"
+                        f"Status code {response.status_code}: Bad request on handshake url {url} : {response.text}"
                     )
+                    key_is_incorrect_msg = f"Status code {response.status_code}: App key is incorrect"
                     self._session._call_on_event(
                         EventCode.SessionAuthenticationFailed,
                         key_is_incorrect_msg,
                     )
                     raise DesktopSessionError(1, key_is_incorrect_msg)
                 else:
-                    self.debug(
-                        f"Response {response.status_code} on handshake url {url} : "
-                        f"{response.text}"
-                    )
+                    self.debug(f"Response {response.status_code} on handshake url {url} : {response.text}")
 
         except (socket.timeout, httpx.ConnectTimeout):
             raise DesktopSessionError(1, f"Timeout on handshake url {url}")
         except Exception as e:
             raise DesktopSessionError(1, f"Error on handshake url {url} : {e!r}")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/event.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/event.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/event_code.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/event_code.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,30 +1,23 @@
 from enum import unique, Enum
 
 
 @unique
 class EventCode(Enum):
     """
-    Each session can report different status events during it's lifecycle.
-        StreamConnecting : the connection to the stream service within the session
-         is pending.
-        StreamConnected : the connection to the stream service has been
-        successfully established.
+    Each session can report different status events during its lifecycle.
+        StreamConnecting : the connection to the stream service within the session is pending.
+        StreamConnected : the connection to the stream service has been successfully established.
         StreamDisconnected : the connection to the stream service is not established.
-        SessionAuthenticationSuccess : the session has successfully authenticated
-        this client.
-        SessionAuthenticationFailed : the session has failed to authenticate
-        this client.
-        StreamAuthenticationSuccess: the stream has successfully
-        authenticated this client.
+        SessionAuthenticationSuccess : the session has successfully authenticated this client.
+        SessionAuthenticationFailed : the session has failed to authenticate this client.
+        StreamAuthenticationSuccess: the stream has successfully authenticated this client.
         StreamAuthenticationFailed: the stream has failed to authenticate this client.
-        DataRequestOk : the request for content from the session data services has
-        completed successfully.
-        DataRequestFailed : the request for content from the session data services
-        has failed.
+        DataRequestOk : the request for content from the session data services has completed successfully.
+        DataRequestFailed : the request for content from the session data services has failed.
     """
 
     StreamConnecting = 1
     StreamConnected = 2
     StreamDisconnected = 3
     StreamAuthenticationSuccess = 4
     StreamAuthenticationFailed = 5
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/grant_password.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/grant_password.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/http_service.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/http_service.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,21 +12,20 @@
 from ..._tools import cached_property
 
 if TYPE_CHECKING:
     from ...delivery._data._data_provider import Request
     from ._session import Session
 
 APPLICATION_JSON = "application/json"
+HTTPX_VERSION = "0.20.0"
 
 
 def get_http_limits(config):
     max_connections = config.get(configure.keys.http_max_connections)
-    max_keepalive_connections = config.get(
-        configure.keys.http_max_keepalive_connections
-    )
+    max_keepalive_connections = config.get(configure.keys.http_max_keepalive_connections)
     limits = httpx.Limits(
         max_connections=max_connections,
         max_keepalive_connections=max_keepalive_connections,
     )
     return limits
 
 
@@ -74,17 +73,15 @@
         super().__init__(session)
         self._session: "Session" = session
 
     def is_closed(self) -> bool:
         client = self._client
         return client is None or client.is_closed
 
-    def build_request(
-        self, client: Union[httpx.AsyncClient, httpx.Client], request: "Request"
-    ) -> httpx.Request:
+    def build_request(self, client: Union[httpx.AsyncClient, httpx.Client], request: "Request") -> httpx.Request:
         headers = request.headers
         timeout = request.timeout or get_http_request_timeout_secs(self._session)
 
         access_token = self._session._access_token
         if access_token is not None:
             headers["Authorization"] = "Bearer {}".format(access_token)
 
@@ -115,15 +112,15 @@
             f"\theaders = {headers}\n"
             f"\tparams = {request.params}\n"
             f"\tcookies = {cookies}\n"
             f"\tdata = {request.data}\n"
             f"\tjson = {request.json}"
         )
 
-        if httpx.__version__ < "0.20.0":
+        if httpx.__version__ < HTTPX_VERSION:
             request = client.build_request(
                 method=method,
                 url=request.url,
                 data=request.data,
                 json=request.json,
                 params=request.params,
                 headers=headers,
@@ -196,19 +193,19 @@
             except RuntimeError as e:
                 self.assert_err_still_in_flight(e)
 
     def send(self, request: "Request") -> httpx.Response:
         client = self._auto_retry_client if request.auto_retry else self._client
         request = self.build_request(client, request)
 
-        if httpx.__version__ < "0.20.0":
-            response = client.send(request, timeout=request.timeout)
+        if httpx.__version__ < HTTPX_VERSION:
+            response = client.send(request, timeout=request.timeout, follow_redirects=True)
 
         else:
-            response = client.send(request)
+            response = client.send(request, follow_redirects=True)
 
         return response
 
 
 class AsyncHTTPClient(BaseHTTPClient):
     @property
     def _client(self) -> httpx.AsyncClient:
@@ -231,17 +228,15 @@
         retry_client = self._auto_retry_client
 
         config = self._session.config
 
         if client is None or client.is_closed:
             # httpx has its default Accept header and
             # server wants application/json or nothing
-            self._client = httpx.AsyncClient(
-                headers={"Accept": APPLICATION_JSON}, limits=get_http_limits(config)
-            )
+            self._client = httpx.AsyncClient(headers={"Accept": APPLICATION_JSON}, limits=get_http_limits(config))
 
         retry_config = config.get(configure.keys.http_auto_retry_config, None)
 
         if retry_config and (retry_client is None or retry_client.is_closed):
             retry_transport = RetryAsyncTransport(
                 total_attempts=retry_config.get("number-of-retries", 3),
                 on_statuses=retry_config.get("on-errors", []),
@@ -273,19 +268,19 @@
 
         if client is None or client.is_closed:
             await self.open_async()
             client = self._auto_retry_client if request.auto_retry else self._client
 
         request = self.build_request(client, request)
 
-        if httpx.__version__ < "0.20.0":
-            response = await client.send(request, timeout=request.timeout)
+        if httpx.__version__ < HTTPX_VERSION:
+            response = await client.send(request, timeout=request.timeout, follow_redirects=True)
 
         else:
-            response = await client.send(request)
+            response = await client.send(request, follow_redirects=True)
 
         return response
 
 
 class HTTPService(LogReporter):
     def __init__(self, session: "Session") -> None:
         super().__init__(session)
@@ -331,25 +326,18 @@
             httpx.RequestError,
             httpx.ReadTimeout,
             httpx.RemoteProtocolError,
             httpx.TooManyRedirects,
             httpx.TransportError,
             httpx.TimeoutException,
         ) as error:
-            self.error(
-                f"An error occurred while requesting {error.request.url!r}.\n"
-                f"\t{error!r}"
-            )
+            self.error(f"An error occurred while requesting {error.request.url!r}.\n\t{error!r}")
             raise error
 
-        self.debug(
-            f"HTTP Response id {request.id}\n"
-            f"\tstatus_code = {response.status_code}\n"
-            f"\ttext = {response.text}"
-        )
+        self.debug(f"HTTP Response id {request.id}\n\tstatus_code = {response.status_code}\n\ttext = {response.text}")
         return response
 
     async def open_async(self):
         await self._client_async.open_async()
 
     async def close_async(self):
         await self._client_async.close_async()
@@ -376,23 +364,16 @@
             httpx.RequestError,
             httpx.ReadTimeout,
             httpx.RemoteProtocolError,
             httpx.TooManyRedirects,
             httpx.TransportError,
             httpx.TimeoutException,
         ) as error:
-            self.error(
-                f"An error occurred while requesting {error.request.url!r}.\n"
-                f"\t{error!r}"
-            )
+            self.error(f"An error occurred while requesting {error.request.url!r}.\n\t{error!r}")
             raise error
 
-        self.debug(
-            f"HTTP Response id {request.id}\n"
-            f"\tstatus_code = {response.status_code}\n"
-            f"\ttext = {response.text}"
-        )
+        self.debug(f"HTTP Response id {request.id}\n\tstatus_code = {response.status_code}\n\ttext = {response.text}")
         return response
 
 
 def get_service(session: "Session") -> HTTPService:
     return HTTPService(session)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/null_session.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/null_session.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/refresh_token_updater.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/refresh_token_updater.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,41 +49,36 @@
         if self._token_info.expires_at <= cur_time:
             event = UpdateEvent.REFRESH_TOKEN_EXPIRED
             message = "Time expired for the refresh token update"
             self.debug(message)
             self._callback(event, message, {})
             return
 
-        response = self._request_token(
-            self._grant, self._token_info, self._app_key, self._url
-        )
+        response = self._request_token(self._grant, self._token_info, self._app_key, self._url)
 
         try:
             json_content = response.json()
         except json.decoder.JSONDecodeError:
             message = (
-                f"Malformed JSON received during token refresh: '{response.text}'. "
-                f"Status code: {response.status_code}"
+                f"Malformed JSON received during token refresh: '{response.text}'. Status code: {response.status_code}"
             )
             self.error(message)
             self._callback(UpdateEvent.REFRESH_TOKEN_FAILED, message, {})
             return
 
         status_code = response.status_code
 
         if status_code == codes.ok:
             event = UpdateEvent.REFRESH_TOKEN_SUCCESS
             message = "All is well"
 
         elif status_code in UNAUTHORIZED_CODES:
             event = UpdateEvent.REFRESH_TOKEN_BAD
             error = json_content.get("error")
-            error_description = json_content.get(
-                "error_description", "empty error description"
-            )
+            error_description = json_content.get("error_description", "empty error description")
             message = error_description
             self.error(f"[Error {status_code} - {error}] {error_description}")
 
         else:
             event = UpdateEvent.REFRESH_TOKEN_FAILED
             error = json_content.get("error")
             error_description = json_content.get(
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/tools.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/tools.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_core/session/updater.py` & `refinitiv-data-1.2.0/refinitiv/data/_core/session/updater.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_errors.py` & `refinitiv-data-1.2.0/refinitiv/data/_errors.py`

 * *Files 4% similar despite different names*

```diff
@@ -64,17 +64,15 @@
 class StreamingPricesError(RDError):
     pass
 
 
 class ItemWasNotRequested(RDError):
     def __init__(self, item_type, not_requested, requested):
         item_type = f"{item_type}{'s' if len(not_requested) > 1 else ''}"
-        super().__init__(
-            -1, f"{item_type} {not_requested} was not requested : {requested}"
-        )
+        super().__init__(-1, f"{item_type} {not_requested} was not requested : {requested}")
 
 
 class EnvError(RDError):
     pass
 
 
 class BondPricingError(RDError):
@@ -97,20 +95,20 @@
     def __init__(
         self,
         required_scope: List[Set[str]],
         available_scope: Set[str],
         key: str,
         method: str,
     ):
-        missing_scopes = (scope - available_scope for scope in required_scope)
+        missing_scopes = [scope - available_scope for scope in required_scope]
         self.required_scope = required_scope
         self.available_scope = available_scope
         self.key = key
         self.method = method
         self.missing_scopes = missing_scopes
         super().__init__(
             -1,
-            message=f"Insufficient scope for key={key}, method={method} failed.\n"
-            f"Required scope: {' OR '.join(map(str, required_scope))}\n"
-            f"Token scope: {available_scope}\n"
+            message=f"Insufficient scope for key={key}, method={method}.\n"
+            f"Required scopes: {' OR '.join(map(str, required_scope))}\n"
+            f"Available scopes: {available_scope or '{}'}\n"
             f"Missing scopes: {' OR '.join(map(str, missing_scopes))}",
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/LICENSE` & `refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/LICENSE`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/__init__.py`

 * *Files 1% similar despite different names*

```diff
@@ -87,16 +87,15 @@
             elif all(s and s.isidentifier() for s in config_.split(".")):
                 config_ = ("python", config_, *default_args)
             else:
                 raise ValueError(f'Cannot determine config type from "{config_}"')
 
         if not isinstance(config_, (tuple, list)) or len(config_) == 0:
             raise ValueError(
-                "configuration parameters must be a list of dictionaries,"
-                " strings, or non-empty tuples/lists"
+                "configuration parameters must be a list of dictionaries, strings, or non-empty tuples/lists"
             )
         type_ = config_[0]
         if type_ == "dict":
             instances.append(config_from_dict(*config_[1:], **default_kwargs))
         elif type_ in ("env", "environment"):
             params = list(config_[1:]) + default_args[(len(config_) - 1) :]
             instances.append(config_from_env(*params, **default_kwargs))
@@ -144,17 +143,15 @@
                 instances.append(config_from_path(*config_[1:], **default_kwargs))
             except FileNotFoundError:
                 if not ignore_missing_paths:
                     raise
         else:
             raise ValueError(f'Unknown configuration type "{type_}"')
 
-    return ConfigurationSet(
-        *instances, interpolate=interpolate, interpolate_type=interpolate_type
-    )
+    return ConfigurationSet(*instances, interpolate=interpolate, interpolate_type=interpolate_type)
 
 
 class EnvConfiguration(Configuration):
     """Configuration from Environment variables."""
 
     def __init__(
         self,
@@ -184,17 +181,15 @@
 
     def reload(self) -> None:
         """Reload the environment values."""
         result = {}
         for key, value in os.environ.items():
             if not key.startswith(self._prefix + self._separator):
                 continue
-            result[
-                key[len(self._prefix) :].replace(self._separator, ".").strip(".")
-            ] = value
+            result[key[len(self._prefix) :].replace(self._separator, ".").strip(".")] = value
         super().__init__(
             result,
             lowercase_keys=self._lowercase,
             interpolate=self._interpolate,
             interpolate_type=self._interpolate_type,
         )
 
@@ -260,17 +255,15 @@
         if not os.path.exists(path) or not os.path.isdir(path):
             raise FileNotFoundError()
 
         dotted_path_levels = len(path.split("/"))
         files_keys = (
             (
                 os.path.join(x[0], y),
-                ".".join(
-                    (x[0].split("/") + [y])[(dotted_path_levels + self._remove_level) :]
-                ),
+                ".".join((x[0].split("/") + [y])[(dotted_path_levels + self._remove_level) :]),
             )
             for x in os.walk(path)
             for y in x[2]
             if not x[0].split("/")[-1].startswith("..")
         )
 
         result = {}
@@ -336,17 +329,15 @@
             lowercase_keys=lowercase_keys,
             interpolate=interpolate,
             interpolate_type=interpolate_type,
         )
         self._reload(data, read_from_file)
         self._data = data if read_from_file and isinstance(data, str) else None
 
-    def _reload(
-        self, data: Union[str, TextIO], read_from_file: bool = False
-    ) -> None:  # pragma: no cover
+    def _reload(self, data: Union[str, TextIO], read_from_file: bool = False) -> None:  # pragma: no cover
         raise NotImplementedError()
 
     def reload(self) -> None:
         """Reload the configuration."""
         if self._data:  # pragma: no branch
             self._reload(self._data, True)
 
@@ -479,17 +470,15 @@
         if read_from_file:
             if isinstance(data, str):
                 data = open(data, "rt").read()
             else:
                 data = data.read()
         data = cast(str, data)
         result: Dict[str, Any] = dict(
-            (y.strip() for y in x.split("=", 1))  # type: ignore
-            for x in data.splitlines()
-            if x
+            (y.strip() for y in x.split("=", 1)) for x in data.splitlines() if x  # type: ignore
         )
         self._config = self._flatten_dict(result)
 
 
 def config_from_dotenv(
     data: Union[str, TextIO],
     read_from_file: bool = False,
@@ -563,24 +552,17 @@
             interpolate=interpolate,
             interpolate_type=interpolate_type,
         )
         self.reload()
 
     def reload(self) -> None:
         """Reload the path."""
-        variables = [
-            x
-            for x in dir(self._module)
-            if not x.startswith("__") and x.startswith(self._prefix)
-        ]
+        variables = [x for x in dir(self._module) if not x.startswith("__") and x.startswith(self._prefix)]
         result = {
-            k[len(self._prefix) :]
-            .replace(self._separator, ".")
-            .strip("."): getattr(self._module, k)
-            for k in variables
+            k[len(self._prefix) :].replace(self._separator, ".").strip("."): getattr(self._module, k) for k in variables
         }
         super().__init__(
             result,
             lowercase_keys=self._lowercase,
             interpolate=self._interpolate,
             interpolate_type=self._interpolate_type,
         )
@@ -634,17 +616,15 @@
         data,
         lowercase_keys=lowercase_keys,
         interpolate=interpolate,
         interpolate_type=interpolate_type,
     )
 
 
-def create_path_from_config(
-    path: str, cfg: Configuration, remove_level: int = 1
-) -> Configuration:
+def create_path_from_config(path: str, cfg: Configuration, remove_level: int = 1) -> Configuration:
     """
     Output a path configuration from a :class:`Configuration` instance.
 
     :param path: path to create the config files in
     :param cfg: :class:`Configuration` instance
     :param remove_level: how many levels to remove
     """
@@ -662,17 +642,15 @@
 
 
 if yaml is not None:  # pragma: no branch
 
     class YAMLConfiguration(FileConfiguration):
         """Configuration from a YAML input."""
 
-        def _reload(
-            self, data: Union[str, TextIO], read_from_file: bool = False
-        ) -> None:
+        def _reload(self, data: Union[str, TextIO], read_from_file: bool = False) -> None:
             """Reload the YAML data."""
             if read_from_file and isinstance(data, str):
                 loaded = yaml.load(open(data, "rt"), Loader=yaml.FullLoader)
             else:
                 loaded = yaml.load(data, Loader=yaml.FullLoader)
             if not isinstance(loaded, dict):
                 raise ValueError("Data should be a dictionary")
@@ -724,17 +702,15 @@
                 data=data,
                 read_from_file=read_from_file,
                 lowercase_keys=lowercase_keys,
                 interpolate=interpolate,
                 interpolate_type=interpolate_type,
             )
 
-        def _reload(
-            self, data: Union[str, TextIO], read_from_file: bool = False
-        ) -> None:
+        def _reload(self, data: Union[str, TextIO], read_from_file: bool = False) -> None:
             """Reload the TOML data."""
             if read_from_file:
                 if isinstance(data, str):
                     loaded = toml.load(open(data, "rt"))
                 else:
                     loaded = toml.load(data)
             else:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/configuration.py` & `refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/configuration.py`

 * *Files 4% similar despite different names*

```diff
@@ -82,62 +82,40 @@
             return {
                 k[(len(prefix) + 1) :].lower(): v
                 for k, v in d.items()
                 for k, v in d.items()
                 if k.startswith(prefix + ".")
             }
         else:
-            return {
-                k[(len(prefix) + 1) :]: v
-                for k, v in d.items()
-                if k.startswith(prefix + ".")
-            }
+            return {k[(len(prefix) + 1) :]: v for k, v in d.items() if k.startswith(prefix + ".")}
 
     def _flatten_dict(self, d: Mapping[str, Any]) -> Dict[str, Any]:
         """
         Flatten one level of a dictionary.
 
         :param d: dict
         :return: a flattened dict
         """
         nested = {k for k, v in d.items() if isinstance(v, (dict, Configuration))}
         if self._lowercase:
-            result = {
-                k.lower() + "." + ki: vi
-                for k in nested
-                for ki, vi in self._flatten_dict(d[k]).items()
-            }
-            result.update(
-                (k.lower(), v)
-                for k, v in d.items()
-                if not isinstance(v, (dict, Configuration))
-            )
+            result = {k.lower() + "." + ki: vi for k in nested for ki, vi in self._flatten_dict(d[k]).items()}
+            result.update((k.lower(), v) for k, v in d.items() if not isinstance(v, (dict, Configuration)))
         else:
-            result = {
-                k + "." + ki: vi
-                for k in nested
-                for ki, vi in self._flatten_dict(d[k]).items()
-            }
-            result.update(
-                (k, v) for k, v in d.items() if not isinstance(v, (dict, Configuration))
-            )
+            result = {k + "." + ki: vi for k in nested for ki, vi in self._flatten_dict(d[k]).items()}
+            result.update((k, v) for k, v in d.items() if not isinstance(v, (dict, Configuration)))
         return result
 
     def _get_subset(self, prefix: str) -> Union[Dict[str, Any], Any]:
         """
         Return the subset of the config dictionary whose keys start with :attr:`prefix`.
 
         :param prefix: string
         :return: dict
         """
-        d = {
-            k[(len(prefix) + 1) :]: v
-            for k, v in self._config.items()
-            if k.startswith(prefix + ".")
-        }
+        d = {k[(len(prefix) + 1) :]: v for k, v in self._config.items() if k.startswith(prefix + ".")}
         if not d:
             prefixes = prefix.split(".")
             if len(prefixes) == 1:
                 return deepcopy(self._config.get(prefix, {}))
             d = self._config
             while prefixes:  # pragma: no branches
                 p = prefixes[0]
@@ -181,18 +159,15 @@
     def as_dict(self) -> dict:
         """Return the representation as a dictionary."""
         return self._config
 
     def as_attrdict(self) -> AttributeDict:
         """Return the representation as an attribute dictionary."""
         return AttributeDict(
-            {
-                x: Configuration(v).as_attrdict() if isinstance(v, dict) else v
-                for x, v in self.items(levels=1)
-            }
+            {x: Configuration(v).as_attrdict() if isinstance(v, dict) else v for x, v in self.items(levels=1)}
         )
 
     def get_bool(self, item: str) -> bool:
         """
         Get the item value as a bool.
 
         :param item: key
@@ -256,64 +231,51 @@
 
         :param item: key
         """
         b = self[item]
         b = b if isinstance(b, bytes) else b.encode()
         return base64.b64decode(b, validate=True)
 
-    def keys(
-        self, levels: Optional[int] = None
-    ) -> Union["Configuration", Any, KeysView[str]]:
+    def keys(self, levels: Optional[int] = None) -> Union["Configuration", Any, KeysView[str]]:
         """Return a set-like object providing a view on the configuration keys."""
         assert levels is None or levels > 0
         levels = self._default_levels if levels is None else levels
         try:
             return self["keys"]  # don't filter levels, existing attribute
         except KeyError:
             return cast(
                 KeysView[str],
-                list(
-                    {
-                        ".".join(x.split(".")[:levels])
-                        for x in set(self.as_dict().keys())
-                    }
-                ),
+                list({".".join(x.split(".")[:levels]) for x in set(self.as_dict().keys())}),
             )
 
-    def values(
-        self, levels: Optional[int] = None
-    ) -> Union["Configuration", Any, ValuesView[Any]]:
+    def values(self, levels: Optional[int] = None) -> Union["Configuration", Any, ValuesView[Any]]:
         """Return a set-like object providing a view on the configuration values."""
         assert levels is None or levels > 0
         levels = self._default_levels if levels is None else levels
         try:
             return self["values"]
         except KeyError:
             return dict(self.items(levels=levels)).values()
 
-    def items(
-        self, levels: Optional[int] = None
-    ) -> Union["Configuration", Any, ItemsView[str, Any]]:
+    def items(self, levels: Optional[int] = None) -> Union["Configuration", Any, ItemsView[str, Any]]:
         """Return a set-like object providing a view on the configuration items."""
         assert levels is None or levels > 0
         levels = self._default_levels if levels is None else levels
         try:
             return self["items"]
         except KeyError:
             keys = cast(KeysView[str], self.keys(levels=levels))
             return {k: self._get_subset(k) for k in keys}.items()
 
     def __iter__(self) -> Iterator[Tuple[str, Any]]:  # noqa: D105
         return iter(dict(self.items()))  # type: ignore
 
     def __reversed__(self) -> Iterator[Tuple[str, Any]]:  # noqa: D105
         if version_info < (3, 8):
-            return OrderedDict(  # type: ignore
-                reversed(list(self.items()))
-            )  # pragma: no cover
+            return OrderedDict(reversed(list(self.items())))  # type: ignore  # pragma: no cover
         else:
             return reversed(dict(self.items()))  # type: ignore
 
     def __len__(self) -> int:  # noqa: D105
         return len(self.keys())
 
     def __setitem__(self, key: str, value: Any) -> None:  # noqa: D105
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/configuration_set.py` & `refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/configuration_set.py`

 * *Files 2% similar despite different names*

```diff
@@ -33,27 +33,19 @@
         interpolate_type: InterpolateEnumType = InterpolateEnumType.STANDARD,
     ):  # noqa: D107
         self._interpolate = {} if interpolate is True else interpolate
         self._interpolate_type = interpolate_type
         try:
             self._configs: List[Configuration] = list(configs)
         except Exception:  # pragma: no cover
-            raise ValueError(
-                "configs should be a non-empty iterable of Configuration objects"
-            )
+            raise ValueError("configs should be a non-empty iterable of Configuration objects")
         if not self._configs:  # pragma: no cover
-            raise ValueError(
-                "configs should be a non-empty iterable of Configuration objects"
-            )
-        if not all(
-            isinstance(x, Configuration) for x in self._configs
-        ):  # pragma: no cover
-            raise ValueError(
-                "configs should be a non-empty iterable of Configuration objects"
-            )
+            raise ValueError("configs should be a non-empty iterable of Configuration objects")
+        if not all(isinstance(x, Configuration) for x in self._configs):  # pragma: no cover
+            raise ValueError("configs should be a non-empty iterable of Configuration objects")
         self._writable = False
         self._default_levels = 1
 
     def _from_configs(self, attr: str, *args: Any, **kwargs: dict) -> Any:
         last_err = Exception()
         values = []
         for config_ in self._configs:
@@ -136,35 +128,29 @@
         """
         Get the item values as a dictionary.
 
         :param item: key
         """
         return Configuration({k: v for k, v in dict(self[item]).items()}).as_dict()
 
-    def keys(
-        self, levels: Optional[int] = None
-    ) -> Union["Configuration", Any, KeysView[str]]:
+    def keys(self, levels: Optional[int] = None) -> Union["Configuration", Any, KeysView[str]]:
         """Return a set-like object providing a view on the configuration keys."""
         if self._default_levels:
             return Configuration(self.as_dict()).keys(levels or self._default_levels)
         with Configuration(self.as_dict()).dotted_iter() as cfg:
             return cfg.keys(levels)
 
-    def values(
-        self, levels: Optional[int] = None
-    ) -> Union["Configuration", Any, ValuesView[Any]]:
+    def values(self, levels: Optional[int] = None) -> Union["Configuration", Any, ValuesView[Any]]:
         """Return a set-like object providing a view on the configuration values."""
         if self._default_levels:
             return Configuration(self.as_dict()).values(levels or self._default_levels)
         with Configuration(self.as_dict()).dotted_iter() as cfg:
             return cfg.values(levels)
 
-    def items(
-        self, levels: Optional[int] = None
-    ) -> Union["Configuration", Any, ItemsView[str, Any]]:
+    def items(self, levels: Optional[int] = None) -> Union["Configuration", Any, ItemsView[str, Any]]:
         """Return a set-like object providing a view on the configuration items."""
         if self._default_levels:
             return Configuration(self.as_dict()).items(levels or self._default_levels)
         with Configuration(self.as_dict()).dotted_iter() as cfg:
             return cfg.items(levels)
 
     def __setitem__(self, key: str, value: Any) -> None:  # noqa: D105
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_external_libraries/python_configuration/helpers.py` & `refinitiv-data-1.2.0/refinitiv/data/_external_libraries/python_configuration/helpers.py`

 * *Files 2% similar despite different names*

```diff
@@ -75,34 +75,30 @@
     if isinstance(value, str) and "://" in value:
         from urllib.parse import urlparse
 
         url = urlparse(value)
         if url.password is None:
             return value
         else:
-            return url._replace(
-                netloc="{}:{}@{}".format(url.username, mask, url.hostname)
-            ).geturl()
+            return url._replace(netloc="{}:{}@{}".format(url.username, mask, url.hostname)).geturl()
     return value
 
 
 def interpolate_standard(text: str, d: dict, found: Set[Tuple[str, ...]]) -> str:
     """
     Return the string interpolated as many times as needed.
 
     :param text: string possibly containing an interpolation pattern
     :param d: dictionary
     :param found: variables found so far
     """
     if not isinstance(text, str):
         return text
 
-    variables = tuple(
-        sorted(x[1] for x in string.Formatter().parse(text) if x[1] is not None)
-    )
+    variables = tuple(sorted(x[1] for x in string.Formatter().parse(text) if x[1] is not None))
 
     if not variables:
         return text
 
     if variables in found:
         raise ValueError("Cycle detected while interpolating keys")
     else:
@@ -149,22 +145,16 @@
             if variable in dict_:
                 level = level + i
                 break
         else:
             raise KeyError(variable)
         levels[variable] = level + 1
 
-        new_d = (
-            ([{}] * level) + d[level:]
-            if method == InterpolateEnumType.DEEP_NO_BACKTRACK
-            else d
-        )
-        resolved[variable] = interpolate_deep(
-            attr, d[level][variable], new_d, resolved, levels, method
-        )
+        new_d = ([{}] * level) + d[level:] if method == InterpolateEnumType.DEEP_NO_BACKTRACK else d
+        resolved[variable] = interpolate_deep(attr, d[level][variable], new_d, resolved, levels, method)
 
     return text.format(**resolved)
 
 
 def flatten(d: List[dict]) -> dict:
     """
     Flatten a list of dictionaries.
@@ -172,17 +162,15 @@
     :param d: dictionary list
     """
     result = {}
     [result.update(dict_) for dict_ in d[::-1]]
     return result
 
 
-def interpolate_object(
-    attr: str, obj: Any, d: List[dict], method: InterpolateEnumType
-) -> Any:
+def interpolate_object(attr: str, obj: Any, d: List[dict], method: InterpolateEnumType) -> Any:
     """
     Return the interpolated object.
 
     :param attr: attribute name
     :param obj: object to interpolate
     :param d: dictionary
     :param method: interpolation method
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_containers.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_containers.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,21 +1,22 @@
-from typing import TYPE_CHECKING, Any, Union, Dict
+import warnings
+from typing import Any, Dict, TYPE_CHECKING, Union
 from urllib.parse import quote_plus
 
-from pandas import DataFrame
 
 from .._tools import (
     ADC_FUNC_PATTERN_IN_FIELDS,
     ADC_TR_PATTERN,
-    fields_arg_parser,
     cached_property,
+    fields_arg_parser,
     iterator_str_arg_parser,
 )
 
 if TYPE_CHECKING:
+    from pandas import DataFrame
     from .._types import OptStrStrs, StrStrings
 
 
 class Container:
     def __init__(self, raw: Any = None) -> None:
         self._raw = raw
 
@@ -33,15 +34,19 @@
 class UniverseContainer(Container):
     def __init__(self, raw: "OptStrStrs" = None) -> None:
         super().__init__(raw)
         self._hp = []
 
     @cached_property
     def _universe(self) -> "StrStrings":
-        return iterator_str_arg_parser.get_list(self._raw)
+        raw = iterator_str_arg_parser.get_list(self.raw or [])
+        unique_rics = list(dict.fromkeys(raw).keys())
+        if len(unique_rics) < len(raw):
+            warnings.warn("You have duplicated instruments in your input. Output will contain unique instruments only.")
+        return unique_rics
 
     @cached_property
     def hp_and_cust_inst(self):
         return self.hp + self.cust_inst
 
     @cached_property
     def adc(self) -> "StrStrings":
@@ -55,32 +60,32 @@
     def hp(self) -> "StrStrings":
         return self._hp
 
     @cached_property
     def is_universe_expander(self):
         from ..discovery._universe_expanders._universe_expander import UniverseExpander
 
-        return isinstance(self._raw, UniverseExpander)
+        return isinstance(self.raw, UniverseExpander)
 
     def calc_hp(self, adc_raw: Union[Dict, None]) -> None:
-        if adc_raw is not None:
-            adc_rics = [
-                quote_plus(item[0]) if "/" in item[0] else item[0]
-                for item in adc_raw.get("data", {})
-            ]
+        if not adc_raw:
+            self._hp = self.adc
         else:
-            adc_rics = None
-        self._hp = adc_rics or self.adc
+            rics_from_server = list(map(lambda i: quote_plus(i[0]) if "/" in i[0] else i[0], adc_raw.get("data", {})))
+            self._hp = list(dict.fromkeys(rics_from_server).keys())
 
     def __iter__(self):
         return iter(self._universe)
 
     def __len__(self):
         return len(self._universe)
 
+    def __repr__(self):
+        return f"UniverseContainer({self._universe})"
+
 
 class FieldsContainer(Container):
     def __init__(self, raw: "OptStrStrs" = None) -> None:
         super().__init__(raw)
         self._adc_fields: "OptStrStrs" = None
         self._hp_fields: "OptStrStrs" = None
 
@@ -92,15 +97,19 @@
             if ADC_TR_PATTERN.match(field) or ADC_FUNC_PATTERN_IN_FIELDS.match(field):
                 self._adc_fields.append(field)
             else:
                 self._hp_fields.append(field)
 
     @cached_property
     def _fields(self) -> "StrStrings":
-        return fields_arg_parser.get_list(self._raw or [])
+        raw = fields_arg_parser.get_list(self.raw or [])
+        unique_fields = list(dict.fromkeys(raw).keys())
+        if len(unique_fields) < len(raw):
+            warnings.warn("You have duplicated fields in your input. Output will contain unique fields only.")
+        return unique_fields
 
     @property
     def adc(self) -> "StrStrings":
         if self._adc_fields is None:
             self._parse()
         return self._adc_fields
 
@@ -118,50 +127,54 @@
     def is_disjoint_adc(self) -> bool:
         return set(self.adc).isdisjoint(set(self._fields))
 
     @cached_property
     def is_one_adc_no_hp(self) -> bool:
         return len(self.adc) == 1 and not self.hp
 
+    def insert(self, index: int, value: str) -> "StrStrings":
+        copy = list(self._fields)
+        copy.insert(index, value)
+        return copy
+
     def __getattr__(self, attr: str) -> Any:
         try:
             return getattr(self._fields, attr)
         except KeyError:
             raise AttributeError(attr)
 
     def __iter__(self):
         return iter(self._fields)
 
+    def __repr__(self):
+        return f"FieldsContainer({self._fields})"
+
 
 class DataContainer(Container):
-    def __init__(
-        self, raw: Union[dict, list, None], df: Union[DataFrame, None]
-    ) -> None:
+    def __init__(self, raw: Union[dict, list, None], df: Union["DataFrame", None]) -> None:
         super().__init__(raw)
         self._df = df
 
     @property
-    def df(self) -> DataFrame:
+    def df(self) -> "DataFrame":
         return self._df
 
 
 class ADCDataContainer(DataContainer):
     def __init__(
         self,
         raw: Union[dict, list, None],
-        df: Union[DataFrame, None],
+        df: Union["DataFrame", None],
         fields: "FieldsContainer",
     ):
         super().__init__(raw, df)
         self._fields = fields
 
     def __bool__(self):
-        is_none = self._raw in [{}, None] or (
-            self._raw and self._fields.is_disjoint_adc
-        )
+        is_none = self.raw in [{}, None] or (self.raw and self._fields.is_disjoint_adc)
         return not is_none
 
 
 class HPDataContainer(DataContainer):
     pass
 
 
@@ -170,15 +183,15 @@
 
 
 class HPAndCustInstDataContainer(HPDataContainer):
     def __init__(
         self,
         columns: Union[list, None],
         raw: Union[dict, list, None],
-        df: Union[DataFrame, None],
+        df: Union["DataFrame", None],
     ):
         super().__init__(raw, df)
         self._columns = columns
 
     @property
     def columns(self):
         return self._columns
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_history_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_historical_df_builder.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,240 +1,209 @@
-import abc
-from itertools import groupby, zip_longest
-from typing import TYPE_CHECKING, List
+from itertools import product
+from typing import List, Dict
 
-from ._intervals_consts import NON_INTRA_DAY_INTERVALS
-from .._tools._dataframe import convert_dtypes
-from .._tools import ohlc
+import pandas as pd
 
-if TYPE_CHECKING:
-    from .context_collection import ADCContext, CustInstContext, HPContext
-    from ._containers import FieldsContainer, UniverseContainer
-    from ..content._df_builder import DFBuilder
-
-
-class HistoryProvider(abc.ABC):
-    @property
-    @abc.abstractmethod
-    def dfbuilder(self) -> "DFBuilder":
-        #  for override
-        pass
-
-    @property
-    @abc.abstractmethod
-    def date_name(self) -> str:
-        #  for override
-        pass
-
-    def get_df(
-        self,
-        adc: "ADCContext",
-        hp: "HPContext",
-        cust_inst: "CustInstContext",
-        universe: "UniverseContainer",
-        fields: "FieldsContainer",
-        interval: str,
-        use_field_names_in_headers: bool,
-    ):
-        if adc.can_build_df:
-            return adc.build_df()
-
-        if hp.can_build_df:
-            return hp.build_df()
+from ._historical_raw_transf import transform_for_df_by_fields, transform_for_df_by_headers_names
+from .._tools._dataframe import convert_dtypes
+from .._types import Strings
 
-        if cust_inst.can_join_hp_multiindex_df:
-            return cust_inst.join_hp_multiindex_df(hp.df, use_field_names_in_headers)
 
-        if cust_inst.can_build_df:
-            return cust_inst.build_df(use_field_names_in_headers)
+def process_bad_raws(bad_raws, listofcolumns, last_raw_columns, fields, num_fields):
+    listofcolumns_insert = listofcolumns.insert
 
-        return self.build_common_df(
-            adc, hp, cust_inst, universe, fields, interval, use_field_names_in_headers
-        )
+    for idx, bad_raw in bad_raws:
+        raw_columns = fields or last_raw_columns or "Field"
+        inst_name = bad_raw["universe"]["ric"]
+        if num_fields == 1:
+            processed_columns = [inst_name]
 
-    def build_common_df(
-        self,
-        adc: "ADCContext",
-        hp: "HPContext",
-        cust_inst: "CustInstContext",
-        universe: "UniverseContainer",
-        fields: "FieldsContainer",
-        interval: str,
-        use_field_names_in_headers: bool,
-    ):
-        comm_universe = universe.hp
-        headers = None
-        if hp.raw:
-            hp.prepare_to_build(interval)
-            headers = hp.headers
-            fields = hp.fields
-
-        if adc.raw:
-            adc.prepare_to_build(interval)
-            comm_universe = adc.universe
-            headers = adc.headers
-
-        data = prepare_data(adc.data, hp.data, comm_universe, fields, self.date_name)
-        df = self.dfbuilder.build_date_as_index(
-            {"data": data, "headers": headers},
-            use_field_names_in_headers,
-            use_multiindex=bool(cust_inst.raw),
-        )
+        else:
+            processed_columns = list(product([inst_name], raw_columns))
 
-        if cust_inst.raw:
-            cust_inst.prepare_to_build(use_field_names_in_headers, df, headers)
-            df = cust_inst.df
+        listofcolumns_insert(idx, processed_columns)
 
-        df = convert_dtypes(df)
 
-        if len(comm_universe) > 1:
-            df.rename(columns={k: v for k, v in enumerate(comm_universe)}, inplace=True)
+def process_data(data_append, index_append, date, items, num_allcolumns, num_raws, left_num_columns):
+    prev_idx = None
+    counter = 0
+
+    template = [pd.NA] * num_allcolumns
+    for instidx, raw_data, raw_columns in items:
+        if (counter != 0 and counter % num_raws == 0) or prev_idx == instidx:
+            index_append(date)
+            data_append(template)
+            template = [pd.NA] * num_allcolumns
+            prev_idx = instidx
+
+        if prev_idx is None:
+            prev_idx = instidx
+
+        counter += 1
+
+        left_idx = left_num_columns[instidx]
+        right_idx = left_idx + len(raw_columns)
+        for item, i in zip(raw_data, range(left_idx, right_idx)):
+            template[i] = item
+
+    index_append(date)
+    data_append(template)
+
+
+class HistoricalBuilder:
+    def _prepare_columns(self, raws, listofcolumns, bad_raws, fields, universe, items_by_date, num_fields):
+        columns = None
+        listofcolumns_append = listofcolumns.append
+        bad_raws_append = bad_raws.append
+        for instidx, raw in enumerate(raws):
+            # it means error in response for custom instruments
+            if not raw:
+                raw = {"universe": {"ric": universe[instidx]}}
+                bad_raws_append((instidx, raw))
+                continue
+
+            # it means error in response for historical pricing
+            if isinstance(raw, list):
+                raw = raw[0]
+                bad_raws_append((instidx, raw))
+                continue
+
+            # it means in response for historical pricing events
+            if isinstance(raw, dict) and not raw.get("headers"):
+                raw = {"universe": {"ric": universe[instidx]}}
+                bad_raws_append((instidx, raw))
+                continue
 
-        if interval is not None and interval not in NON_INTRA_DAY_INTERVALS:
-            df.index.names = ["Timestamp"]
+            else:
+                if fields:
+                    transformed = transform_for_df_by_fields(raw, fields)
 
-        df.sort_index(ascending=True, inplace=True)
-        df.ohlc = ohlc.__get__(df, None)
+                else:
+                    transformed = transform_for_df_by_headers_names(raw)
 
-        return df
+            columns = transformed.fields
 
+            for date, raw_data in zip(transformed.dates, transformed.data):
+                items = items_by_date.setdefault(date, [])
+                items.append((instidx, raw_data, columns))
 
-def get_column_to_idx(columns):
-    columns = [
-        column.upper() if column.startswith("TR") else column for column in columns
-    ]
-    column_to_idx = {"Date": 1}
-    column_to_idx.update({column: columns.index(column) + 2 for column in columns})
-    return column_to_idx
-
-
-def merge_data(adc_data, hp_data, index):
-    adc = adc_data[index] if index < len(adc_data) else []
-    hp = hp_data[index] if index < len(hp_data) else []
-    return [*adc, *hp]
-
-
-def add_one_data_item_to_data(
-    data_item, data, column_to_idx, columns, instrument, is_one_inst, index
-):
-    data_item_values = next(iter(data_item.values()))
-    for data_item_value in data_item_values:
-        data_item = [instrument] if is_one_inst else [index]
-        data_item_value_keys = {key.casefold(): key for key in data_item_value.keys()}
-
-        for column in columns:
-            column_casefold = column.casefold()
-            if column_casefold in data_item_value_keys:
-                data_item.insert(
-                    column_to_idx[column],
-                    data_item_value.get(data_item_value_keys[column_casefold]),
-                )
+            inst_name = raw["universe"]["ric"]
+            if num_fields == 1:
+                processed_columns = [inst_name]
 
             else:
-                data_item.insert(column_to_idx[column], None)
+                processed_columns = list(product([inst_name], columns))
 
-        data.append(data_item)
+            listofcolumns_append(processed_columns)
 
+        return columns
 
-def add_many_data_item_to_data(
-    data_item, data, column_to_idx, columns, instrument, is_one_inst, index, date_name
-):
-    for one, two in zip_longest(
-        *[sorted(item, key=lambda d: d[date_name]) for item in data_item.values()],
-        fillvalue=None,
-    ):
-        data_item = [instrument] if is_one_inst else [index]
+    def build_one(self, raw: dict, fields: Strings, axis_name: str, **__) -> pd.DataFrame:
+        if not raw["data"]:
+            return pd.DataFrame()
 
-        if all([one, two]):
-            item = {**one, **two}
+        if fields:
+            transformed = transform_for_df_by_fields(raw, fields)
 
         else:
-            item = one or two
+            transformed = transform_for_df_by_headers_names(raw)
 
-        key_casefold_to_key_map = {key.casefold(): key for key in item.keys()}
-        for column in columns:
-            column_casefold = column.casefold()
-            if column_casefold in key_casefold_to_key_map.keys():
-                data_item.insert(
-                    column_to_idx[column],
-                    item.get(key_casefold_to_key_map[column_casefold]),
-                )
+        data = transformed.data
+        columns = transformed.fields
+        index = transformed.dates
+
+        inst_name = raw["universe"]["ric"]
+        columns = pd.Index(data=columns, name=inst_name)
+        index = pd.Index(data=index, name=axis_name)
+        df = pd.DataFrame(data=data, columns=columns, index=index)
+        df = convert_dtypes(df)
+        df.sort_index(inplace=True)
+        return df
 
-            else:
-                data_item.insert(column_to_idx[column], None)
+    def build(self, raws: List[dict], universe: Strings, fields: Strings, axis_name: str, **__) -> pd.DataFrame:
+        items_by_date: Dict[str, list] = {}
+        listofcolumns = []
+        num_raws = len(raws)
+        bad_raws = []
+        num_fields = len(fields)
+
+        last_raw_columns = self._prepare_columns(
+            raws, listofcolumns, bad_raws, fields, universe, items_by_date, num_fields
+        )
 
-        data.append(data_item)
+        if not items_by_date:
+            return pd.DataFrame()
 
+        if bad_raws:
+            process_bad_raws(bad_raws, listofcolumns, last_raw_columns, fields, num_fields)
 
-def prepare_data(
-    adc_data: List[list],
-    hp_data: List[list],
-    instruments: "UniverseContainer",
-    columns: List[str],
-    date_name: str,
-) -> List[list]:
-
-    is_one_inst = len(instruments) == 1
-    data = []
-    column_to_idx = get_column_to_idx(columns)
-
-    for index, instrument in enumerate(instruments):
-        merged_data = merge_data(adc_data, hp_data, index)
-        unique_dates = list(dict.fromkeys([item[date_name] for item in merged_data]))
-        columns = column_to_idx.keys()
-
-        for date_item in unique_dates:
-            group_items = groupby(
-                [item for item in merged_data if item[date_name] == date_item],
-                lambda item: item["Type"],
-            )
-
-            data_item = {}
-            for item_type, items in group_items:
-                data_item[item_type] = list(items)
-
-            if len(data_item) == 1:
-                add_one_data_item_to_data(
-                    data_item,
-                    data,
-                    column_to_idx,
-                    columns,
-                    instrument,
-                    is_one_inst,
-                    index,
+        left_num_columns = {
+            split_idx: sum([len(subcols) for subcols in listofcolumns[:split_idx]]) for split_idx in range(num_raws)
+        }
+
+        allcolumns = [col for subcolumns in listofcolumns for col in subcolumns]
+
+        num_allcolumns = len(allcolumns)
+        data = []
+        index = []
+        data_append = data.append
+        index_append = index.append
+        for date, items in items_by_date.items():
+            num_items = len(items)
+
+            if num_items > 1:
+                process_data(
+                    data_append,
+                    index_append,
+                    date,
+                    items,
+                    num_allcolumns,
+                    num_raws,
+                    left_num_columns,
                 )
 
             else:
-                add_many_data_item_to_data(
-                    data_item,
-                    data,
-                    column_to_idx,
-                    columns,
-                    instrument,
-                    is_one_inst,
-                    index,
-                    date_name,
-                )
+                index_append(date)
+                instidx, raw_data, raw_columns = items[0]
+                left = [pd.NA] * left_num_columns[instidx]
+                right = [pd.NA] * (num_allcolumns - len(raw_columns) - len(left))
+                data_append(left + raw_data + right)
 
-    return data
+        if num_fields == 1:
+            columns = pd.Index(data=allcolumns, name=fields[0])
 
+        else:
+            columns = pd.MultiIndex.from_tuples(allcolumns)
 
-def chunks(generator, chunk_size):
-    """Yield successive chunks from a generator"""
-    chunk = []
+        index = pd.Index(data=index, name=axis_name)
+        df = pd.DataFrame(data=data, columns=columns, index=index)
+        df = convert_dtypes(df)
+        df.sort_index(inplace=True)
+        return df
+
+
+class CustomInstsBuilder(HistoricalBuilder):
+    def build_one(self, raw: dict, fields: Strings, axis_name: str, **__) -> pd.DataFrame:
+        if fields:
+            transformed = transform_for_df_by_fields(raw, fields)
 
-    for item in generator:
-        if len(chunk) >= chunk_size:
-            yield chunk
-            chunk = [item]
         else:
-            chunk.append(item)
+            transformed = transform_for_df_by_headers_names(raw)
 
-    if chunk:
-        yield chunk
+        data = transformed.data
+        columns = transformed.fields
+        index = transformed.dates
+
+        if all(i is pd.NA for j in data for i in j):
+            return pd.DataFrame()
+
+        inst_name = raw["universe"]["ric"]
+        columns = pd.Index(data=columns, name=inst_name)
+        index = pd.Index(data=index, name=axis_name)
+        df = pd.DataFrame(data=data, columns=columns, index=index)
+        df = convert_dtypes(df)
+        df.sort_index(inplace=True)
+        return df
 
 
-def group_universe(universe: List[str]) -> List[tuple]:
-    valid_universe = (
-        i for i in universe if not (i.startswith("0#.") or i.startswith("Peers("))
-    )
-    return [(name, len(list(items))) for name, items in groupby(valid_universe)]
+historical_builder = HistoricalBuilder()
+custom_insts_builder = CustomInstsBuilder()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_intervals_consts.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_intervals_consts.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_mixed_streams.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_mixed_streams.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,17 +23,15 @@
 
     def _get_symbol(self, universe):
         if is_instrument_id.match(universe):
             data_provider_layer = DataProviderLayer(
                 data_type=ContentType.CUSTOM_INSTRUMENTS_INSTRUMENTS,
                 universe=self.universe,
             )
-            instrument_response = data_provider_layer.get_data(
-                self._session, method=RequestMethod.GET
-            )
+            instrument_response = data_provider_layer.get_data(self._session, method=RequestMethod.GET)
             symbol = instrument_response.data.raw.get("symbol")
             if not self._uuid:
                 self._uuid = uuid_from_symbol(symbol)
         else:
             if not symbol_with_user_id.match(universe):
                 if not self._uuid:
                     self._uuid = get_user_id(self._session)
@@ -89,23 +87,19 @@
         for name in self.universe:
             stream = self._create_stream_by_name(name)
             stream.on(StreamStateEvent.CLOSED, self._on_stream_close)
             retval[name] = stream
         return retval
 
     def add_instruments(self, *instruments):
-        instruments = [
-            self._get_symbol(name) if name.startswith("S)") else name for name in instruments
-        ]
+        instruments = [self._get_symbol(name) if name.startswith("S)") else name for name in instruments]
         super().add_instruments(*instruments)
 
     def remove_instruments(self, *args):
-        args = [
-            self._get_symbol(name) if name.startswith("S)") else name for name in args
-        ]
+        args = [self._get_symbol(name) if name.startswith("S)") else name for name in args]
         super().remove_instruments(*args)
 
 
 class Stream(_Stream):
     @cached_property
     def _stream(self) -> MixedStreams:
         return MixedStreams(
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_ohlc_builder.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_ohlc_builder.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,17 +13,15 @@
     df = df.convert_dtypes()
     df.index.name = "Timestamp"
 
     df.ohlc = ohlc.__get__(df, None)
     return df
 
 
-def create_df(
-    data: list, timestamps: list, fields: list, stream_name: str
-) -> DataFrame:
+def create_df(data: list, timestamps: list, fields: list, stream_name: str) -> DataFrame:
     numpy_array = np.array(data)
     timestamp_array = np.array(timestamps)
 
     if np.size(numpy_array):
         dataframe = DataFrame(numpy_array, columns=fields, index=timestamp_array)
     else:
         dataframe = DataFrame()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_pricing_recorder.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_pricing_recorder.py`

 * *Files 2% similar despite different names*

```diff
@@ -61,16 +61,15 @@
         self._recording_control: Optional[RecordingControl] = None
 
     @staticmethod
     def _parse_input_frequency_and_duration(value: str) -> int:
         last_char = value[-1]
         if last_char not in ["s", "h", "d", "m"] and not value.endswith("min"):
             raise ValueError(
-                "Please provide 'duration' or 'frequency' value in "
-                "valid format. For example: '10s', '2min', '1h'"
+                "Please provide 'duration' or 'frequency' value in valid format. For example: '10s', '2min', '1h'"
             )
 
         result = None
         try:
             if "s" == last_char:
                 result = int(value[:-1])
             elif value.endswith("min"):
@@ -82,61 +81,50 @@
             elif "d" == last_char:
                 seconds = int(value[:-1])
                 result = seconds * 3600 * 24
             elif "m" == last_char:
                 seconds = int(value[:-1])
                 result = seconds * 3600 * 24 * 30
         except ValueError:
-            raise ValueError(
-                "Please provide 'duration' value"
-                " in valid format. For example: '10s', '2min', '1h'"
-            )
+            raise ValueError("Please provide 'duration' value in valid format. For example: '10s', '2min', '1h'")
 
         return result
 
     @staticmethod
     def _validate_count_argument(ticks_per_bar):
         try:
             ticks_per_bar = int(ticks_per_bar)
         except ValueError:
             raise ValueError(
-                "Invalid argument. Please provide 'ticks_per_bar'"
-                " in the following format: '10', '100', '500'"
+                "Invalid argument. Please provide 'ticks_per_bar' in the following format: '10', '100', '500'"
             )
 
         if ticks_per_bar <= 0:
             raise ValueError("Invalid argument. 'ticks_per_bar' should be more then 0")
 
     def _validate_arguments(self, frequency: str, duration: str, ticks_per_bar: str):
         if ticks_per_bar != "1" and frequency != "tick":
-            raise ValueError(
-                "Please provide 'tick' value as frequency when you are using 'ticks_per_bar' argument."
-            )
+            raise ValueError("Please provide 'tick' value as frequency when you are using 'ticks_per_bar' argument.")
 
         self._validate_count_argument(ticks_per_bar)
 
         if duration and frequency != "tick":
             frequency = self._parse_input_frequency_and_duration(frequency)
             duration = self._parse_input_frequency_and_duration(duration)
 
             self._expected_count_of_callbacks = duration / frequency
 
-            float_part, self._expected_count_of_callbacks = math.modf(
-                self._expected_count_of_callbacks
-            )
+            float_part, self._expected_count_of_callbacks = math.modf(self._expected_count_of_callbacks)
 
             self._callback_called_count = 0
             if duration % frequency:
                 self._expected_count_of_callbacks += 1
 
             if frequency > duration:
-                raise ValueError(
-                    "Please check your arguments, 'duration'"
-                    " should be higher that 'frequency'."
-                )
+                raise ValueError("Please check your arguments, 'duration' should be higher that 'frequency'.")
 
     @staticmethod
     def _check_df(df: pd.DataFrame) -> pd.DataFrame:
         if isinstance(df, pd.DataFrame):
             df.fillna(pd.NA, inplace=True)
 
         else:
@@ -214,17 +202,15 @@
         no_duration = not self._duration
         ticks_1 = self._ticks_per_bar == 1
         ticks_not_1 = self._ticks_per_bar != 1
 
         # stream.recorder.record(frequency='tick')
         if all([frequency_tick, no_duration, ticks_1]):
             if not isinstance(self._update_handler, CollectUpdates_StreamUpdateHandler):
-                self._update_handler = CollectUpdates_StreamUpdateHandler(
-                    self._stream, recorder=self
-                )
+                self._update_handler = CollectUpdates_StreamUpdateHandler(self._stream, recorder=self)
             self._recording_control = NoBlocking_RecordingControl(self._update_handler)
             self._recording_control.start_recording()
 
         # stream.recorder.record(frequency='tick', ticks_per_bar=10)
         elif all([frequency_tick, no_duration, ticks_not_1]):
             self._create_ohlc_builder(Ticks_OHLCBuilder)
             self._update_handler = BuildDF_StreamUpdateHandler(
@@ -235,19 +221,16 @@
                 on_record_callback=on_data,
             )
             self._recording_control = NoBlocking_RecordingControl(self._update_handler)
             self._recording_control.start_recording()
 
         # stream.recorder.record(frequency='tick', duration="60s")
         elif all([frequency_tick, duration, ticks_1]):
-
             if not isinstance(self._update_handler, CollectUpdates_StreamUpdateHandler):
-                self._update_handler = CollectUpdates_StreamUpdateHandler(
-                    self._stream, recorder=self
-                )
+                self._update_handler = CollectUpdates_StreamUpdateHandler(self._stream, recorder=self)
 
             self._recording_control = Blocking_RecordingControl(self._update_handler)
             interval = self._parse_input_frequency_and_duration(self._duration)
             self._recording_control.start_recording(interval)
             self.stop()
 
         # stream.recorder.record(frequency='tick', duration="60s", ticks_per_bar=10)
@@ -264,33 +247,29 @@
             interval = self._parse_input_frequency_and_duration(self._duration)
             self._recording_control.start_recording(interval)
             self.stop()
 
         # stream.recorder.record(frequency='5s')
         elif all([frequency, no_duration, ticks_1]):
             self._create_ohlc_builder(OHLCBuilder)
-            self._update_handler = CollectUpdates_StreamUpdateHandler(
-                self._stream, recorder=self
-            )
+            self._update_handler = CollectUpdates_StreamUpdateHandler(self._stream, recorder=self)
             self._recording_control = RepeatNonBlocking_RecordingControl(
                 self._on_data,
                 self._update_handler,
                 self._ohlc_builder,
                 self._logger,
                 self,
             )
             interval = self._parse_input_frequency_and_duration(self._frequency)
             self._recording_control.start_recording(interval)
 
         # stream.recorder.record(frequency='5s', duration="17s")
         elif all([frequency, duration, ticks_1]):
             self._create_ohlc_builder(OHLCBuilder)
-            self._update_handler = CollectUpdates_StreamUpdateHandler(
-                self._stream, recorder=self
-            )
+            self._update_handler = CollectUpdates_StreamUpdateHandler(self._stream, recorder=self)
             duration = self._parse_input_frequency_and_duration(self._duration)
             self._recording_control = RepeatBlocking_RecordingControl(
                 duration,
                 self._on_data,
                 self._update_handler,
                 self._ohlc_builder,
                 self._logger,
@@ -306,23 +285,20 @@
                 f"frequency={self._frequency}, "
                 f"duration={self._duration}, "
                 f"ticks_per_bar={self._ticks_per_bar}"
             )
 
     def _create_ohlc_builder(self, klass):
         if not isinstance(self._ohlc_builder, klass):
-            self._ohlc_builder = klass(
-                self._frequency, self._stream.universe, self._stream.fields
-            )
+            self._ohlc_builder = klass(self._frequency, self._stream.universe, self._stream.fields)
 
     def get_history(self) -> "pd.DataFrame":
         dfs = []
 
         if self._frequency == "tick" and self._ticks_per_bar == 1:
-
             updates_by_stream_name = self._update_handler.updates_by_stream_name
             for universe, stream_data in updates_by_stream_name.items():
                 timestamps, data, fields = retrieve_data_for_df(stream_data)
                 dataframe = create_df(data, timestamps, fields, universe)
                 dfs.append(dataframe)
 
             if not dfs:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_recording_control.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_recording_control.py`

 * *Files 4% similar despite different names*

```diff
@@ -37,17 +37,15 @@
         ohlc_df = self.ohlc_builder._last_recorded_ohlc_updates
 
         if self.on_record_callback:
             try:
                 if not ohlc_df.empty:
                     ohlc_df.fillna(NA, inplace=True)
 
-                t = threading.Thread(
-                    target=self.on_record_callback, args=(ohlc_df, self._recorder)
-                )
+                t = threading.Thread(target=self.on_record_callback, args=(ohlc_df, self._recorder))
                 t.start()
 
             except Exception as error:
                 self.logger.exception("Error occurred in user's callback function.")
                 self.logger.exception(error)
 
 
@@ -64,17 +62,15 @@
     def delete_recording(self):
         pass
 
 
 class NoBlocking_RecordingControl(RecordingControl):
     def __init__(
         self,
-        update_handler: Union[
-            "CollectUpdates_StreamUpdateHandler", "BuildDF_StreamUpdateHandler"
-        ],
+        update_handler: Union["CollectUpdates_StreamUpdateHandler", "BuildDF_StreamUpdateHandler"],
     ) -> None:
         self.update_handler = update_handler
 
     def start_recording(self):
         self.update_handler.start_handling()
 
     def stop_recording(self):
@@ -86,17 +82,15 @@
     def delete_recording(self):
         self.update_handler.dispose()
 
 
 class Blocking_RecordingControl(RecordingControl):
     def __init__(
         self,
-        update_handler: Union[
-            "CollectUpdates_StreamUpdateHandler", "BuildDF_StreamUpdateHandler"
-        ],
+        update_handler: Union["CollectUpdates_StreamUpdateHandler", "BuildDF_StreamUpdateHandler"],
     ) -> None:
         self.update_handler = update_handler
         self.blocking = threading.Event()
 
     def start_recording(self, interval: int = 0):
         self.update_handler.start_handling()
         self.blocking.wait(interval)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/_stream_update_handler.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/_stream_update_handler.py`

 * *Files 1% similar despite different names*

```diff
@@ -54,19 +54,15 @@
 
     def _on_update_handler(self, stream: "_UniverseStream", message: dict):
         message["Timestamp"] = datetime.now()
 
         updates = self.updates_by_stream_name.setdefault(stream.name, [])
         updates.append(message)
 
-        if (
-            self._frequency == "tick"
-            and self._ticks_per_bar == 1
-            and self._on_data_callback
-        ):
+        if self._frequency == "tick" and self._ticks_per_bar == 1 and self._on_data_callback:
             self._on_data_callback(message, self._recorder)
 
         self._do_on_update_handler(stream, message)
 
     def _do_on_update_handler(self, stream: "_UniverseStream", message: dict):
         # for override
         pass
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_adc_context.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_context.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,153 +1,168 @@
 import abc
-from collections import Counter
-from itertools import groupby
-from typing import Optional, Dict, List
+from typing import Dict, Optional, List, TYPE_CHECKING, Union
 
+import pandas as pd
 from pandas import DataFrame
 
 from ._context import Context
-from .._containers import UniverseContainer
-from .._history_provider import group_universe, chunks
-from .._intervals_consts import NON_INTRA_DAY_INTERVALS
-from ..._tools import ohlc
 from ..._tools._dataframe import convert_dtypes
 
+if TYPE_CHECKING:
+    from ...content._df_builder import DFBuilder
 
-class ADCContext(Context, abc.ABC):
-    @property
-    def can_build_df(self) -> bool:
-        return bool(self._adc_data and not (self._hp_data or self._cust_inst_data))
 
+class CustInstContext(Context, abc.ABC):
     @property
     def can_get_data(self) -> bool:
-        if not self.fields.adc:
-            return False
+        return bool(self.universe.cust_inst)
 
-        return bool(self.universe.adc) and not (
-            self.universe.is_universe_expander
-            and self.fields.hp
-            and self.fields.is_disjoint_adc
-        )
+    @property
+    def can_build_df(self) -> bool:
+        return bool(self._cust_inst_data and not (self._adc_data or self._hp_data))
+
+    @property
+    def can_join_hp_multiindex_df(self) -> bool:
+        return bool(not self._adc_data and (self._hp_data and self._cust_inst_data and len(self.universe.hp) > 1))
 
     @property
     def raw(self) -> Optional[Dict]:
-        return self._adc_data and self._adc_data.raw
+        return self._cust_inst_data and self._cust_inst_data.raw
 
     @property
     def df(self) -> Optional[DataFrame]:
-        if type(self._adc_data.df) is DataFrame and not self._adc_data.df.empty:
-            return convert_dtypes(self._adc_data.df)
-        return self._adc_data.df
+        return self._cust_inst_data and self._cust_inst_data.df
 
+    @property
     @abc.abstractmethod
-    def is_raw_headers_in_fields(self, raw, fields) -> bool:
+    def dfbuilder(self) -> "DFBuilder":
         # for override
         pass
 
-    def build_df(self, *args) -> DataFrame:
-        df = self.df
-        if not self.is_raw_headers_in_fields(self.raw, self.fields):
-            df = DataFrame()
+    @property
+    @abc.abstractmethod
+    def date_name(self) -> str:
+        #  for override
+        pass
 
-        df.sort_index(ascending=True, inplace=True)
-        df = convert_dtypes(df)
-        df.ohlc = ohlc.__get__(df, None)
+    def build_df(self) -> DataFrame:
+        """
+        Builds custom instrument dataframe.
+
+        Returns
+        -------
+        pd.DataFrame
+            Custom instrument dataframe.
+        """
+        fields = self.fields
+        df = self.df
+        if fields:
+            data = self.prepare_data(self.raw, fields)
+            headers = self.prepare_headers(fields.raw)
+            use_multiindex = len(fields.raw) > 1 and len(self.universe.cust_inst) > 1
+            df = self.dfbuilder.build_date_as_index(
+                {"data": data, "headers": headers},
+                self.use_field_names_in_headers,
+                use_multiindex=use_multiindex,
+            )
         return df
 
     @abc.abstractmethod
-    def get_cols(self, headers, fields, *args) -> List[str]:
+    def prepare_data(self, raw, fields) -> list:
         # for override
         pass
 
-    def prepare_data(self, interval, *args):
-        """
-        Transform adc data to further merging.
+    @staticmethod
+    @abc.abstractmethod
+    def prepare_headers(raw: Union[list, dict]) -> List:
+        # for override
+        pass
 
-        Transformed data looks like:
-        {
-            "RIC": [{"Date": "value", "field": "value", "another_field": "value"}],
-            "NEXT_RIC": [{"Date": "value", "field": "value", "next_field": "value"}]
-        }
-        """
-        headers = self.dfbuilder.get_headers(self.raw)
-        cols = ["Type", *self.get_cols(headers, self.fields.adc)]
-        data = []
-        universe = []
-        index_universe = 0
-        universe_count = group_universe(self.universe.adc)
-        len_universe_count = len(universe_count)
-        counter_items = Counter()
+    def join_hp_multiindex_df(self, hp_df: DataFrame):
+        """
+        Joins historical pricing multiindex dataframe with custom instruments dataframe.
 
-        if interval in NON_INTRA_DAY_INTERVALS:
+        Parameters
+        ----------
+        hp_df : pd.DataFrame
+            Historical pricing multiindex dataframe.
+
+        Returns
+        -------
+        pd.DataFrame
+            Historical pricing multiindex dataframe, joined with custom instruments
+            dataframe.
+        """
+        fields = self._get_fields_from_raw()
+        data = self.prepare_data(self.raw, fields)
+        headers = self.prepare_headers(fields)
+        cust_df = self.dfbuilder.build_date_as_index(
+            {"data": data, "headers": headers},
+            self.use_field_names_in_headers,
+            use_multiindex=True,
+        )
+        joined_df = hp_df.join(cust_df, how="outer")
+        df = convert_dtypes(joined_df)
+        return df
 
-            def process_item_date(s: str) -> str:
-                return s.split("T")[0]
+    def _get_fields_from_raw(self):
+        fields = []
+        if isinstance(self.raw, list):
+            headers = self.raw[0]["headers"]
 
         else:
+            headers = self.raw["headers"]
 
-            def process_item_date(s: str) -> str:
-                return s.replace("T", " ").replace("Z", "")
-
-        for ric, items in groupby(self.raw["data"], lambda i: i[0]):
-            items = list(items)
-            len_items = len(items)
-            counter_items.update([len_items])
-            result_items = []
-
-            if index_universe < len_universe_count:
-                name_universe, count = universe_count[index_universe]
-
-            else:
-                count = 0
-                name_universe = ""
-
-            for item in items:
-                item[0] = "adc"
-                item = {k: v for k, v in zip(cols, item)}
-                item_date = item.get(self.date_name)
-
-                if item_date:
-                    item[self.date_name] = process_item_date(item_date)
-
-                result_items.append(item)
+        for header in headers:
+            name = header.get("name")
+            if name and name.lower() not in {"date", "instrument"}:
+                fields.append(name)
+
+        return fields
+
+    def _get_fields_from_headers(self, headers):
+        name = "name" if self.use_field_names_in_headers else "title"
+        return [
+            header[name]
+            for header in self.dfbuilder.get_headers({"headers": headers})
+            if header[name].lower() not in {"date", "instrument"}
+        ]
 
-            repeat_universe = 1
-            if ric == name_universe:
-
-                if count == len_items:
-                    repeat_universe = count
-
-                    for i in result_items:
-                        data.append([i])
-
-                elif (
-                    count < len_items
-                    and counter_items.most_common(1)[0][0] * (count + 1) == len_items
-                ):
-                    repeat_universe = count + 1
-                    for chunk in chunks(result_items, len_items // repeat_universe):
-                        data.append(chunk)
+    def join_common_df(self, df: DataFrame, headers):
+        """
+        Creates dataframe with ADC, historical pricing and custom instruments data.
 
-                elif count < len_items:
-                    repeat_universe = count
-                    for chunk in chunks(result_items, len_items // count):
-                        data.append(chunk)
+        Join or merge previously created ADC and historical pricing dataframe with
+        custom instruments dataframe.
 
-                elif count > len_items:
-                    for chunk in chunks(result_items, len_items):
-                        data.append(chunk)
+        Parameters
+        ----------
+        df : pd.DataFrame
+            Previously created ADC and historical pricing dataframe.
+        headers : List
+            Common headers for building dataframe
+
+        Returns
+        -------
+        pd.Dataframe that includes ADC, hp and custom instruments data.
+        """
+        if not self.fields:
+            fields = self._get_fields_from_raw()
 
-                index_universe += 1
+        else:
+            fields = self._get_fields_from_headers(headers)
 
-            else:
-                data.append(result_items)
+        data = self.prepare_data(self.raw, fields)
+        headers = self.prepare_headers(fields)
+        cust_df = self.dfbuilder.build_date_as_index(
+            {"data": data, "headers": headers},
+            self.use_field_names_in_headers,
+            use_multiindex=True,
+        )
 
-            universe.extend([ric] * repeat_universe)
+        if not self._adc_data and self._hp_data:
+            df = df.join(cust_df, how="outer")
 
-        return data, universe
+        else:
+            df = pd.merge(df, cust_df, on=["Date"])
 
-    def prepare_to_build(self, interval, *args):
-        data, universe = self.prepare_data(interval)
-        self.universe = UniverseContainer(universe)
-        self._data = data
-        self._headers = self.prepare_headers(self.raw)
+        return df
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_adc_rdp_context.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_adc_rdp_context.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,16 +1,17 @@
-from typing import List
+from typing import List, Dict
 
 from ._adc_context import ADCContext
 from ._context import RDPMixin
+from ..._tools import cached_property
 
 
 class ADCRDPContext(RDPMixin, ADCContext):
-    def prepare_headers(self, raw: dict, *args) -> List[List[str]]:
-        fields = self.fields.raw
+    @staticmethod
+    def prepare_headers(raw: dict, fields: list) -> List[Dict[str, str]]:
         field_to_idx = {field.casefold(): fields.index(field) + 2 for field in fields}
 
         headers = [
             {"name": "instrument", "title": "Instrument"},
             {"name": "date", "title": "Date"},
         ]
 
@@ -27,52 +28,32 @@
                 field_to_idx.pop(field)
 
         for field, index in field_to_idx.items():
             headers.insert(index, {"name": field.upper(), "title": field.upper()})
 
         return headers
 
-    def get_cols(self, headers, fields, *args) -> List[str]:
-        cols = []
-        fields = [field.casefold() for field in fields]
+    def get_fields(self, headers, user_fields) -> List[str]:
+        fields = []
+        user_fields = [field.casefold() for field in user_fields]
 
         for header in headers:
             header_name = header["name"]
             header_title = header["title"]
             field = f"{header_name}.{header_title}".casefold()
 
             if header_name.casefold() == "instrument":
                 continue
 
-            elif field in fields:
-                cols.append(f"{header_name}.{header_title}")
+            elif field in user_fields:
+                fields.append(f"{header_name}.{header_title}")
 
             else:
-                cols.append(header_name)
+                fields.append(header_name)
 
-        return cols
+        return fields
 
-    def is_raw_headers_in_fields(self, raw, fields) -> bool:
-        fields = {field.casefold() for field in fields}
-        headers = set()
-
-        for header in raw["headers"]:
-            header_name = header["name"]
-            if header_name not in {"instrument", "date"} and ")" not in header_name:
-                header_title = header["title"]
-
-                if " " in header_title:
-                    headers.add(header_name.casefold())
-
-                elif header_name == "RIC" and header_title == "RIC":
-                    headers.add("tr.ric")
-
-                else:
-                    if header_name.split(".")[-1] == header_title:
-                        headers.add(header_name.casefold())
-
-                    else:
-                        headers.add(
-                            f"{header_name.casefold()}.{header_title.casefold()}"
-                        )
-
-        return headers.issubset(fields)
+    @cached_property
+    def headers_names(self) -> List[str]:
+        return [
+            header.get("name" if self.use_field_names_in_headers else "title") for header in self.raw.get("headers", [])
+        ]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_adc_udf_context.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_rdp_context.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,50 +1,87 @@
-from typing import List
+from typing import List, Dict
 
-from ._adc_context import ADCContext
-from ._context import UDFMixin
+from dateutil.parser import parse
 
+from ._context import RDPMixin
+from ._cust_inst_context import CustInstContext
 
-class ADCUDFContext(UDFMixin, ADCContext):
-    def prepare_headers(self, raw: dict, *args) -> List[List[str]]:
-        """
-        Prepare headers to pass them to df_builder.
 
-        Args:
-            raw (dict): adc raw data from adc response.
+class CustInstRDPContext(RDPMixin, CustInstContext):
+    @staticmethod
+    def get_headers(headers: dict) -> List[str]:
+        return [item["name"].lower() if item["name"] == "DATE" else item["name"] for item in headers]
 
-        Returns:
-            List with inserted list of dicts that describe each header in understandable
-            by df builder format.
-        """
-        fields = self.fields.raw
+    def _parse_list_to_data(self, raw: list, field_to_idx: dict) -> List[List[dict]]:
+        data = []
 
-        field_to_idx = {field.upper(): fields.index(field) + 2 for field in fields}
-        headers = [{"displayName": "Instrument"}, {"displayName": "Date"}]
+        for raw_item in raw:
+            if not raw_item:
+                continue
 
-        for header in raw["headers"][0]:
-            if len(header) > 1:
-                field_upper = header["field"].upper()
+            ric = raw_item["universe"]["ric"]
+            headers = self.get_headers(raw_item["headers"])
 
-                if field_upper not in field_to_idx:
-                    continue
+            for raw_item_data in raw_item["data"]:
+                template = {"instrument": ric}
 
-                headers.insert(field_to_idx[field_upper], header)
-                field_to_idx.pop(field_upper)
+                for header, raw_data in zip(headers, raw_item_data):
+                    try:
+                        parse(raw_data, fuzzy=False)
+                        raw_data = f"{raw_data} 00:00:00"
+                        template.update({header: raw_data})
+                    except (ValueError, TypeError):
+                        template.update({header: raw_data})
 
-        for field, index in field_to_idx.items():
-            headers.insert(index, {"displayName": field, "field": field})
+                item = []
+                for field in field_to_idx:
+                    item.insert(field_to_idx[field], template.get(field))
 
-        return [headers]
+                data.append(item)
 
-    def get_cols(self, headers, *args) -> List[str]:
-        return [col["name"] for col in headers if col["name"] != "Instrument"]
+        return data
 
-    def is_raw_headers_in_fields(self, raw, fields) -> bool:
-        fields = {field.casefold() for field in fields}
-        headers = set()
+    def _parse_dict_to_data(self, raw: dict, field_to_idx: dict) -> List[List[dict]]:
+        data = []
+        ric = raw["universe"]["ric"]
+        headers = self.get_headers(raw["headers"])
 
-        for header in raw["headers"][0]:
-            if header.get("field"):
-                headers.add(header.get("field").casefold())
+        for raw_data_item in raw["data"]:
+            template = {"instrument": ric}
 
-        return headers.issubset(fields)
+            for header, raw_data in zip(headers, raw_data_item):
+                try:
+                    parse(raw_data, fuzzy=False)
+                    raw_data = f"{raw_data} 00:00:00"
+                    template.update({header: raw_data})
+                except (ValueError, TypeError):
+                    template.update({header: raw_data})
+
+            item = []
+            for field in field_to_idx:
+                item.insert(field_to_idx[field], template.get(field))
+
+            data.append(item)
+
+        return data
+
+    def prepare_data(self, raw, fields) -> list:
+        field_to_idx = {"instrument": 0, "date": 1}
+        field_to_idx.update({field: fields.index(field) + 2 for field in fields})
+
+        if isinstance(raw, dict):
+            return self._parse_dict_to_data(raw, field_to_idx)
+
+        elif isinstance(raw, list):
+            return self._parse_list_to_data(raw, field_to_idx)
+
+    @staticmethod
+    def prepare_headers(raw) -> List[Dict[str, str]]:
+        headers = [
+            {"name": "instrument", "title": "Instrument"},
+            {"name": "date", "title": "Date"},
+        ]
+
+        for item in raw:
+            headers.append({"name": item, "title": item})
+
+        return headers
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_context_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_context_factory.py`

 * *Files 21% similar despite different names*

```diff
@@ -4,24 +4,19 @@
 from ._adc_context import ADCContext
 from ._adc_rdp_context import ADCRDPContext
 from ._adc_udf_context import ADCUDFContext
 from ._cust_inst_context import CustInstContext
 from ._cust_inst_rdp_context import CustInstRDPContext
 from ._cust_inst_udf_context import CustInstUDFContext
 from ._hp_and_cust_inst_context import HPAndCustInstContext
-from ._hp_context import HPContext
-from ._hp_rdp_context import HPRDPContext
-from ._hp_udf_context import HPUDFContext
+from ._hp_context import HPContext, HPUDFContext, HPRDPContext
 from ...content.fundamental_and_reference._data_grid_type import DataGridType
 
 if TYPE_CHECKING:
-    from .._containers import (
-        FieldsContainer,
-        UniverseContainer,
-    )
+    from .._containers import FieldsContainer, UniverseContainer
 
 
 class ContextType(Enum):
     ADC = auto()
     HP = auto()
     HPAndCustInst = auto()
     CustInst = auto()
@@ -48,53 +43,56 @@
 
 
 def get_context(
     context_type: ContextType,
     data_grid_type: "DataGridType",
     universe: "UniverseContainer",
     fields: "FieldsContainer",
-) -> Union[CustInstContext, ADCContext, HPContext]:
-    data_grid_type_by_context_class = (
-        data_grid_type_by_context_class_by_context_type.get(context_type)
-    )
+    use_field_names_in_headers: bool,
+) -> Union[CustInstContext, ADCContext, HPContext, HPAndCustInstContext]:
+    data_grid_type_by_context_class = data_grid_type_by_context_class_by_context_type.get(context_type)
 
     if not data_grid_type_by_context_class:
         raise TypeError(f"Unexpected context_type. Type: {context_type}")
 
     context_class = data_grid_type_by_context_class.get(data_grid_type)
 
     if not context_class:
         raise TypeError(f"Unexpected type. Type: {data_grid_type}")
 
-    return context_class(universe, fields)
+    return context_class(universe, fields, use_field_names_in_headers)
 
 
 def get_cust_inst_context(
     data_grid_type: "DataGridType",
     universe: "UniverseContainer",
     fields: "FieldsContainer",
+    use_field_names_in_headers: bool,
 ) -> CustInstContext:
-    return get_context(ContextType.CustInst, data_grid_type, universe, fields)
+    return get_context(ContextType.CustInst, data_grid_type, universe, fields, use_field_names_in_headers)
 
 
 def get_adc_context(
     data_grid_type: "DataGridType",
     universe: "UniverseContainer",
     fields: "FieldsContainer",
+    use_field_names_in_headers: bool,
 ) -> ADCContext:
-    return get_context(ContextType.ADC, data_grid_type, universe, fields)
+    return get_context(ContextType.ADC, data_grid_type, universe, fields, use_field_names_in_headers)
 
 
 def get_hp_context(
     data_grid_type: "DataGridType",
     universe: "UniverseContainer",
     fields: "FieldsContainer",
+    use_field_names_in_headers: bool,
 ) -> HPContext:
-    return get_context(ContextType.HP, data_grid_type, universe, fields)
+    return get_context(ContextType.HP, data_grid_type, universe, fields, use_field_names_in_headers)
 
 
 def get_hp_and_custinst_context(
     data_grid_type: "DataGridType",
     universe: "UniverseContainer",
     fields: "FieldsContainer",
-) -> HPContext:
-    return get_context(ContextType.HPAndCustInst, data_grid_type, universe, fields)
+    use_field_names_in_headers: bool,
+) -> HPAndCustInstContext:
+    return get_context(ContextType.HPAndCustInst, data_grid_type, universe, fields, use_field_names_in_headers)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_udf_context.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_udf_context.py`

 * *Files 8% similar despite different names*

```diff
@@ -2,34 +2,29 @@
 
 from ._context import UDFMixin
 from ._cust_inst_context import CustInstContext
 
 
 class CustInstUDFContext(UDFMixin, CustInstContext):
     def get_headers(self, headers: dict) -> List[str]:
-        return [
-            item["name"].capitalize() if item["name"] == "DATE" else item["name"]
-            for item in headers
-        ]
+        return [item["name"].capitalize() if item["name"] == "DATE" else item["name"] for item in headers]
 
     def _parse_list_to_data(self, raw: list, field_to_idx: dict) -> List[List[dict]]:
         data = []
 
         for raw_item in raw:
             if not raw_item:
                 continue
 
             ric = raw_item["universe"]["ric"]
             headers = self.get_headers(raw_item["headers"])
 
             for raw_item_data in raw_item["data"]:
                 template = {"Instrument": ric}
-                template.update(
-                    {header: data for header, data in zip(headers, raw_item_data)}
-                )
+                template.update({header: data for header, data in zip(headers, raw_item_data)})
                 item = []
                 for field in field_to_idx:
                     item.insert(field_to_idx[field], template.get(field))
 
                 data.append(item)
 
         return data
@@ -38,35 +33,34 @@
         data = []
 
         ric = raw["universe"]["ric"]
         headers = self.get_headers(raw["headers"])
 
         for raw_data_item in raw["data"]:
             template = {"Instrument": ric}
-            template.update(
-                {header: data for header, data in zip(headers, raw_data_item)}
-            )
+            template.update({header: data for header, data in zip(headers, raw_data_item)})
             item = []
             for field in field_to_idx:
                 item.insert(field_to_idx[field], template.get(field))
 
             data.append(item)
 
         return data
 
-    def prepare_data(self, raw, fields, *args) -> List[List[dict]]:
+    def prepare_data(self, raw, fields) -> List[List[dict]]:
         field_to_idx = {"Instrument": 0, "Date": 1}
         field_to_idx.update({item: fields.index(item) + 2 for item in fields})
 
         if isinstance(raw, dict):
             return self._parse_dict_to_data(raw, field_to_idx)
 
         elif isinstance(raw, list):
             return self._parse_list_to_data(raw, field_to_idx)
 
-    def prepare_headers(self, raw: Union[list, dict], *args) -> List[List[Dict]]:
+    @staticmethod
+    def prepare_headers(raw: Union[list, dict]) -> List[List[Dict]]:
         headers = [{"displayName": "Instrument"}, {"displayName": "Date"}]
 
         for item in raw:
             headers.append({"displayName": item, "field": item})
 
         return [headers]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+"""Dates and calendars functions."""
+
 __all__ = (
     "add_periods",
     "count_periods",
     "date_schedule",
     "is_working_day",
     "holidays",
     "PeriodType",
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_add_periods.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_is_working_day.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,68 +1,46 @@
-import numpy as np
-from typing import List, Optional, Union
+from typing import List, Optional
 
 from ..._types import OptDateTime
-from ...content.ipa._enums import DateMovingConvention, EndOfMonthConvention
-from ...content.ipa.dates_and_calendars.add_periods import Definition
+from ...content.ipa.dates_and_calendars.is_working_day import Definition
 
 
-def add_periods(
-    start_date: "OptDateTime" = None,
-    period: str = None,
-    calendars: Optional[List[str]] = None,
+def is_working_day(
+    date: "OptDateTime" = None,
     currencies: Optional[List[str]] = None,
-    date_moving_convention: Optional[Union[DateMovingConvention, str]] = None,
-    end_of_month_convention: Optional[Union[EndOfMonthConvention, str]] = None,
-) -> np.datetime64:
+    calendars: Optional[List[str]] = None,
+) -> bool:
     """
-    With this function you can add a time period to a given date using a given calendar.
+    Checks if the date is a working day or not for any of the calendars or currencies provided.
 
     Parameters
     ----------
-        start_date: str or datetime or timedelta, optional
-            Start date of calculation.
-        period: str, optional
-            String representing the tenor.
+        date: str or datetime or timedelta, optional
+            Particular date to check.
         calendars: list of str, optional
-            Calendars to use the date for working day or weekend.
+            Calendars to use the date for the working day or weekend.
             Optional if currencies is provided.
         currencies: list of str, optional
-            Currencies to use the date for working day or weekend.
+            Currencies to use for calculation of the date for the working day or weekend.
             Optional if calendars is provided.
-        date_moving_convention : DateMovingConvention or str, optional
-            The method to adjust dates.
-        end_of_month_convention : EndOfMonthConvention or str, optional
-            End of month convention.
 
     Returns
     -------
-    np.datetime64
-        Added period date
+    bool
+        If requested day is working day returns True, otherwise returns False
 
     Examples
     --------
-     >>> import datetime
-     >>> import refinitiv.data as rd
-     >>> from refinitiv.data import dates_and_calendars
-     >>>
-     >>> rd.open_session("platform.default")
-     >>>
-     >>> added_period = rd.dates_and_calendars.add_periods(
-     ...    start_date=datetime.date(2014, 1, 1),
-     ...    period="1Y",
-     ...    calendars=["BAR", "KOR"],
-     ...    date_moving_convention=dates_and_calendars.DateMovingConvention.NEXT_BUSINESS_DAY,
-     ...    end_of_month_convention=dates_and_calendars.EndOfMonthConvention.LAST28
-     ... )
+    >>> import datetime
+    >>> import refinitiv.data as rd
+    >>>
+    >>> rd.open_session("platform.default")
+    >>>
+    >>> is_working = rd.dates_and_calendars.is_working_day(
+    ...    date=datetime.datetime(2020, 7, 10),
+    ...    currencies=["EUR"]
+    >>>)
     """
 
-    response = Definition(
-        start_date=start_date,
-        period=period,
-        calendars=calendars,
-        currencies=currencies,
-        date_moving_convention=date_moving_convention,
-        end_of_month_convention=end_of_month_convention,
-    ).get_data()
+    response = Definition(date=date, calendars=calendars, currencies=currencies).get_data()
 
-    return np.datetime64(response.data.added_period.date)
+    return response.data.day.is_working_day
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_count_periods.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_count_periods.py`

 * *Files 8% similar despite different names*

```diff
@@ -17,62 +17,59 @@
     end_date: "OptDateTime" = None,
     period_type: Optional[Union[PeriodType, str]] = None,
     calendars: Optional[List[str]] = None,
     currencies: Optional[List[str]] = None,
     day_count_basis: Optional[Union[DayCountBasis, str]] = None,
 ) -> CountedPeriods:
     """
-    With this function you can get the period of time
-    between a start date and an end date using one or many calendars.
+    Gets the quantity of time periods based on the provided start date, end date and period type (such as working day, non-working day etc).
 
     Parameters
     ----------
         start_date: str or datetime or timedelta, optional
-            Start date of calculation.
+            Calculation start date.
         end_date: str or datetime or timedelta, optional
-            End date of calculation.
+            Calculation end date.
         period_type : PeriodType or str, optional
-            The method we chose to count the period of time based on value from PeriodType enumeration.
+            Date periods counting method.
         calendars: list of str, optional
-            Calendars to use the date for working day or weekend.
+            Calendars to determine the working days and national holidays for particular countries.
             Optional if currencies is provided.
         currencies: list of str, optional
-            Currencies to use the date for working day or weekend.
+            Currencies to use for calculation of the date for the working day or weekend.
             Optional if calendars is provided.
         day_count_basis: DayCountBasis or str, optional
-            Day count basis value from DayCountBasis enumeration.
+            Predefined values for day count basis.
 
     Returns
     -------
     CountedPeriods
         Counted periods object with count and tenor values.
 
     Examples
     --------
-     >>> import datetime
-     >>> import refinitiv.data as rd
-     >>> from refinitiv.data import dates_and_calendars
-     >>>
-     >>> rd.open_session("platform.default")
-     >>>
-     >>> counted_period = rd.dates_and_calendars.count_periods(
-     ...    start_date=datetime.timedelta(-11),
-     ...    end_date=datetime.timedelta(-3),
-     ...    period_type=dates_and_calendars.PeriodType.WORKING_DAY,
-     ...    currencies=["EUR"],
-     >>>)
+    >>> import datetime
+    >>> import refinitiv.data as rd
+    >>> from refinitiv.data import dates_and_calendars
+    >>>
+    >>> rd.open_session("platform.default")
+    >>>
+    >>> counted_period = rd.dates_and_calendars.count_periods(
+    ...    start_date=datetime.timedelta(-11),
+    ...    end_date=datetime.timedelta(-3),
+    ...    period_type=dates_and_calendars.PeriodType.WORKING_DAY,
+    ...    currencies=["EUR"],
+    >>>)
     """
 
     response = Definition(
         start_date=start_date,
         end_date=end_date,
         period_type=period_type,
         calendars=calendars,
         currencies=currencies,
         day_count_basis=day_count_basis,
     ).get_data()
 
-    response = CountedPeriods(
-        response.data.counted_period.count, response.data.counted_period.tenor
-    )
+    response = CountedPeriods(response.data.counted_period.count, response.data.counted_period.tenor)
 
     return response
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_date_schedule.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_date_schedule.py`

 * *Files 6% similar despite different names*

```diff
@@ -13,15 +13,15 @@
     calendar_day_of_month: Optional[int] = None,
     calendars: Optional[List[str]] = None,
     currencies: Optional[List[str]] = None,
     day_of_week: Optional[Union[DayOfWeek, str]] = None,
     count: Optional[int] = None,
 ) -> List[np.datetime64]:
     """
-    With this function you can get date schedules.
+    Gets a list of dates based on the provided values, which can then be used as input for other functions.
 
     Parameters
     ----------
         frequency: DateScheduleFrequency or str, optional
             The frequency of dates in the predefined period.
         start_date: str or datetime or timedelta, optional
             The start date of the predetermined list of dates.
@@ -31,55 +31,54 @@
             The end date of the predetermined list of dates.
             If start_date is not set end_date is used to
             define a list of dates from today to the end date;
             end_date and count should not be set at a time;
             end_date must be later or equal to start_date.
             Mandatory if count is not specified.
         calendar_day_of_month : int, optional
-            The number of the day of the month to which dates are adjusted.
+            The number of the days of the month to which the dates are adjusted.
             The first date in the list is defined as the corresponding
             day of the month to which the start date belongs.
             Mandatory if frequency is set to 'Monthly'.
         calendars: list of str, optional
-            Calendars to use the date for working day or weekend.
+            Calendars to determine the working days and national holidays for particular countries.
             Optional if currencies is provided.
         currencies: list of str, optional
-            Currencies to use the date for working day or weekend.
+            Currencies to use for calculation of the date for the working day or weekend.
             Optional if calendars is provided.
         day_of_week : DayOfWeek or str, optional
             The day of week to which dates are adjusted.
             The first date in the list is defined as
             corresponding day of week following the start date.
             The last date in the list is defined as
             corresponding day of week preceding the end date.
         count : int, optional
-            A number is used to define a list of dates from the
-            start date (or today if the start day is not set) to the end date.
+            The number of dates from the start date to retrieve.
             Mandatory if end_date is not specified.
 
     Returns
     -------
     List[np.datetime64]
         List of np.datetime64 dates.
 
     Examples
     --------
-     >>> import datetime
-     >>> import refinitiv.data as rd
-     >>> from refinitiv.data import dates_and_calendars
-     >>>
-     >>> rd.open_session("platform.default")
-     >>>
-     >>> dates = rd.dates_and_calendars.date_schedule(
-     ...    start_date=datetime.date(2019, 4, 30),
-     ...    count=10,
-     ...    frequency=dates_and_calendars.DateScheduleFrequency.WEEKLY,
-     ...    calendars=["EMU", "GER"],
-     ...    day_of_week="Tuesday",
-     >>>)
+    >>> import datetime
+    >>> import refinitiv.data as rd
+    >>> from refinitiv.data import dates_and_calendars
+    >>>
+    >>> rd.open_session("platform.default")
+    >>>
+    >>> dates = rd.dates_and_calendars.date_schedule(
+    ...    start_date=datetime.date(2019, 4, 30),
+    ...    count=10,
+    ...    frequency=dates_and_calendars.DateScheduleFrequency.WEEKLY,
+    ...    calendars=["EMU", "GER"],
+    ...    day_of_week="Tuesday",
+    >>>)
     """
 
     response = Definition(
         frequency=frequency,
         start_date=start_date,
         end_date=end_date,
         calendar_day_of_month=calendar_day_of_month,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/dates_and_calendars/_holidays.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/dates_and_calendars/_holidays.py`

 * *Files 5% similar despite different names*

```diff
@@ -18,25 +18,20 @@
     def __init__(self, custom_holidays: list):
         super().__init__()
         self._custom_holidays = custom_holidays
 
     @property
     def rules(self) -> List[Holiday]:
         # rules are compatible with pandas
-        return [
-            self._create_rule(hol["date"], hol["name"]) for hol in self._custom_holidays
-        ]
+        return [self._create_rule(hol["date"], hol["name"]) for hol in self._custom_holidays]
 
     @property
     def rules_list(self) -> List[dict]:
         # for easy access to all holidays the list of holidays is exposed
-        return [
-            self._create_holiday(hol["date"], hol["name"])
-            for hol in self._custom_holidays
-        ]
+        return [self._create_holiday(hol["date"], hol["name"]) for hol in self._custom_holidays]
 
     @property
     def offset(self):
         return pd.offsets.CustomBusinessDay(calendar=self)
 
     @staticmethod
     def _create_holiday(date, name):
@@ -77,62 +72,59 @@
 def holidays(
     start_date: "OptDateTime" = None,
     end_date: "OptDateTime" = None,
     calendars: Optional[List[str]] = None,
     currencies: Optional[List[str]] = None,
 ) -> HolidaysResponse:
     """
-    With this function you can get the holidays between
-    a start date and an end date using one or many calendars.
+    Gets the holidays between the start date and the end date using one or many calendars.
 
     Parameters
     ----------
     start_date: str or datetime or timedelta, optional
-        Start date of calculation.
+        Calculation start date.
     end_date: str or datetime or timedelta, optional
-        End date of calculation.
+        Calculation end date.
     calendars: list of str, optional
-        Calendars to use the date for working day or weekend.
+        Calendars to determine the working days and national holidays for particular countries.
         Optional if currencies is provided.
     currencies: list of str, optional
-        Currencies to use the date for working day or weekend.
+        Currencies to use for calculation of the date for the working day or weekend.
         Optional if calendars is provided.
 
     Returns
     -------
     HolidaysResponse
         HolidaysResponse object with dataframe, requested calendars and offset
 
     Examples
     --------
-     >>> import datetime
-     >>> import refinitiv.data as rd
-     >>>
-     >>> rd.open_session("platform.default")
-     >>>
-     >>> holidays_response = rd.dates_and_calendars.holidays(
-     >>>    start_date=datetime.datetime(2019, 11, 20),
-     >>>    end_date=datetime.datetime(2020, 2, 20),
-     >>>    calendars=["EMU", "GER"],
-     >>>)
+    >>> import datetime
+    >>> import refinitiv.data as rd
+    >>>
+    >>> rd.open_session("platform.default")
+    >>>
+    >>> holidays_response = rd.dates_and_calendars.holidays(
+    >>>    start_date=datetime.datetime(2019, 11, 20),
+    >>>    end_date=datetime.datetime(2020, 2, 20),
+    >>>    calendars=["EMU", "GER"],
+    >>>)
     """
 
     holiday_outputs = ["Date", "Names", "Calendars", "Countries"]
 
     response = Definition(
         start_date=start_date,
         end_date=end_date,
         calendars=calendars,
         currencies=currencies,
         holiday_outputs=holiday_outputs,
     ).get_data()
 
-    _holidays_per_calendar = holidays_per_calendar(
-        response.data.raw[0].get("holidays", [])
-    )
+    _holidays_per_calendar = holidays_per_calendar(response.data.raw[0].get("holidays", []))
 
     holiday_calendars = {}
     for calendar, holidays_ in _holidays_per_calendar.items():
         holiday_calendars[calendar] = HolidayCalendar(holidays_)
 
     all_calendar_holidays = []
     for items in _holidays_per_calendar.values():
@@ -144,12 +136,10 @@
                     break
 
             if flag:
                 all_calendar_holidays.append(item)
 
     global_holiday_calendar = HolidayCalendar(all_calendar_holidays)
 
-    response = HolidaysResponse(
-        response.data.df, holiday_calendars, global_holiday_calendar.offset
-    )
+    response = HolidaysResponse(response.data.df, holiday_calendars, global_holiday_calendar.offset)
 
     return response
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/get_data.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/get_history_func.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,235 +1,211 @@
-from typing import TYPE_CHECKING, Union, Iterable, Tuple
+from datetime import date, datetime, timedelta
+from typing import Optional, Union, Iterable
 
 from pandas import DataFrame
 
-from ._containers import (
-    FieldsContainer,
-    UniverseContainer,
-    ADCDataContainer,
-    HPAndCustInstDataContainer,
-)
-from ._data_provider import DataProvider, convert_types
-from .context_collection import get_adc_context
-from .context_collection._context_factory import get_hp_and_custinst_context
+from refinitiv.data.errors import RDError
+from ._containers import UniverseContainer, FieldsContainer, ADCDataContainer, HPDataContainer, CustInstDataContainer
+from ._data_provider import get_hp_data, get_custominsts_data, get_adc_data
+from ._history_df_builder_factory import get_history_df_builder
+from ._intervals_consts import INTERVALS
+from .context_collection import get_hp_context, get_adc_context, get_cust_inst_context
 from .._core.session import get_default, raise_if_closed
-from .._errors import ScopeError
-from .._tools import (
-    DEBUG,
-)
-from .._tools._dataframe import (
-    convert_dtypes,
-)
-from ..content import fundamental_and_reference
-from ..content.fundamental_and_reference._data_grid_type import (
-    get_data_grid_type_by_session,
-)
-from ..errors import RDError
+from .._tools import fr_datetime_adapter
+from .._types import OptDateTime
+from ..content.fundamental_and_reference._data_grid_type import get_data_grid_type_by_session
 from ..usage_collection._filter_types import FilterType
 from ..usage_collection._logger import get_usage_logger
 from ..usage_collection._utils import ModuleName
 
-if TYPE_CHECKING:
-    from logging import Logger
 
-
-def get_log_string(fields, universe):
-    return f"Fields: {fields} for {universe}"
-
-
-def show_requests_in_log(logger: "Logger", response: "Response", universe, fields):
-    request_messages = response.request_message
-    statuses = response.http_status
-    if not isinstance(response.request_message, list):
-        request_messages = [response.request_message]
-        statuses = [response.http_status]
-
-    for request, status in zip(request_messages, statuses):
-        path = request.url.path
-        cur_universe = path.rsplit("/", 1)[-1]
-        if cur_universe not in universe:
-            cur_universe = universe
-        logger.info(
-            f"Request to {path} with {get_log_string(fields, cur_universe)}\n"
-            f"status: {status}\n"
-        )
-
-
-def get_adc_data(
-    params: dict, logger: "Logger", raise_if_error=False
-) -> Tuple[dict, "DataFrame"]:
-    fields = params.get("fields", "")
-    universe = params["universe"]
-    logger.info(f"Requesting {get_log_string(fields, universe)} \n")
-    definition = fundamental_and_reference.Definition(**params)
-    raw = {}
-    df = DataFrame()
-    try:
-        response = definition.get_data()
-        raw = response.data.raw
-        df = response.data.df
-    except ScopeError as e:
-        if raise_if_error:
-            raise e
-        logger.error(str(e))
-    except RDError as e:
-        if raise_if_error:
-            raise e
-        logger.debug(f"Failure response into content layer: {str(e)}")
-    except Exception as e:
-        if DEBUG:
-            raise e
-        if raise_if_error:
-            raise e
-        logger.exception(f"Failure sending request with {definition}")
-    else:
-        show_requests_in_log(logger, response, universe, fields)
-    return raw, df
-
-
-def get_data_from_stream(universe, fields, logger):
-    from . import Stream
-
-    logger.info(f"Requesting pricing info for fields={fields} via websocket\n")
-    stream = Stream(universe=universe, fields=fields)
-    columns, data, df = None, None, DataFrame()
-
-    try:
-        stream.open(with_updates=False)
-        columns, data = get_columns_and_data_from_stream(stream, fields)
-        df = stream.get_snapshot(fields=fields)
-        if len(df.columns) == 1 or not any([_stream.fields for _stream in stream]):
-            df = DataFrame()
-        else:
-            df = convert_dtypes(df)
-        stream.close()
-
-    except Exception as e:
-        logger.debug(f"Failure retrieving data for {stream._stream.universe}")
-
-    return columns, data, df
-
-
-def get_columns_from_stream(stream):
-    columns = set()
-    for _stream in stream:
-        fields = _stream.fields or []
-        columns.update(fields)
-    return list(columns)
-
-
-def get_columns_and_data_from_stream(stream, fields):
-    stream_columns = get_columns_from_stream(stream)
-    if fields:
-        columns = [i for i in fields if i in stream_columns]
-    else:
-        columns = stream_columns
-    data = {
-        _stream.name: convert_types([_stream[column] for column in columns], columns)
-        for _stream in stream
-    }
-    return columns, data
-
-
-def update_universe(raw, _universe):
-    index = 0  # instrument
-    data = raw.get("data")
-    if data and all(isinstance(i[index], str) for i in data):
-        universe = [i[index] for i in data]
-    else:
-        universe = _universe
-    return universe
-
-
-def get_data(
+def get_history(
     universe: Union[str, Iterable[str]],
     fields: Union[str, Iterable[str], None] = None,
-    parameters: Union[str, dict, None] = None,
+    interval: Optional[str] = None,
+    start: "OptDateTime" = None,
+    end: "OptDateTime" = None,
+    adjustments: Optional[str] = None,
+    count: Optional[int] = None,
     use_field_names_in_headers: bool = False,
+    parameters: Union[str, dict, None] = None,
 ) -> DataFrame:
     """
-    Retrieves pricing snapshots, as well as Fundamental and Reference data.
+    Retrieves the pricing history, as well as Fundamental and Reference data history.
 
     Parameters
     ----------
     universe: str | list
         Instruments to request
     fields: str | list, optional
         Fields to request
-    parameters: str | dict, optional
-        Single key=value global parameter or dictionary of global parameters to request
-    use_field_names_in_headers: bool, default False
+    interval: str, optional
+        Date interval. Supported intervals are:
+        tick, tas, taq, minute, 1min, 5min, 10min, 30min, 60min, hourly, 1h, daily,
+        1d, 1D, 7D, 7d, weekly, 1W, monthly, 1M, quarterly, 3M, 6M, yearly, 1Y
+    start: str or date or datetime or timedelta, optional
+        The start date and timestamp of the requested history
+    end: str or date or datetime or timedelta, optional
+        The end date and timestamp of the requested history
+    adjustments : str, optional
+        Tells the system whether to apply or not apply CORAX (Corporate Actions)
+        events or exchange/manual corrections or price and volume adjustment
+        according to trade/quote qualifier summarization actions to historical time
+        series data. Possible values are:
+        exchangeCorrection, manualCorrection, CCH, CRE, RTS, RPO, unadjusted,
+        qualifiers
+    count : int, optional
+        The maximum number of data points returned. Values range: 1 - 10000.
+        Applies only to pricing fields.
+    use_field_names_in_headers : bool, default False
         If True - returns field name as column headers for data instead of title
+    parameters: str | dict, optional
+        Single global parameter key=value or dictionary
+        of global parameters to request.
+        Applies only to TR fields.
 
     Returns
     -------
     pandas.DataFrame
 
     Examples
     --------
-    >>> get_data(universe=['IBM.N', 'VOD.L'], fields=['BID', 'ASK'])
-    >>> get_data(
-    ...     universe=['GOOG.O', 'AAPL.O'],
-    ...     fields=['TR.EV','TR.EVToSales'],
-    ...     parameters = {'SDate': '0CY', 'Curn': 'CAD'}
-    ...)
+    >>> get_history(universe="GOOG.O")
+    >>> get_history(universe="GOOG.O", fields="tr.Revenue", interval="1Y")
+    >>> get_history(
+    ...     universe="GOOG.O",
+    ...     fields=["BID", "ASK", "tr.Revenue"],
+    ...     interval="1Y",
+    ...     start="2015-01-01",
+    ...     end="2020-10-01",
+    ... )
     """
     session = get_default()
     raise_if_closed(session)
 
     logger = session.logger()
 
+    if interval is not None and interval not in INTERVALS:
+        raise ValueError(f"Not supported interval value.\nSupported intervals are: {list(INTERVALS.keys())}")
+
     # Library usage logging
     get_usage_logger().log_func(
-        name=f"{ModuleName.ACCESS}.get_data",
-        func_path=f"{__name__}.get_data",
+        name=f"{ModuleName.ACCESS}.get_history",
+        func_path=f"{__name__}.get_history",
         kwargs=dict(
             universe=universe,
             fields=fields,
+            interval=interval,
+            start=start,
+            end=end,
+            count=count,
+            adjustments=adjustments,
             parameters=parameters,
             use_field_names_in_headers=use_field_names_in_headers,
         ),
         desc={FilterType.SYNC, FilterType.LAYER_ACCESS},
     )
 
     universe = UniverseContainer(universe)
     fields = FieldsContainer(fields)
     data_grid_type = get_data_grid_type_by_session(session)
-    adc = get_adc_context(data_grid_type, universe, fields)
-    hp_and_cust_inst = get_hp_and_custinst_context(data_grid_type, universe, fields)
+    hp = get_hp_context(data_grid_type, universe, fields, use_field_names_in_headers)
+    adc = get_adc_context(data_grid_type, universe, fields, use_field_names_in_headers)
+    cust_inst = get_cust_inst_context(data_grid_type, universe, fields, use_field_names_in_headers)
+    exceptions = list()
 
-    adc_raw, adc_df = None, None
+    adc_raw = None
+    adc_df = None
     if adc.can_get_data:
-        adc_raw, adc_df = get_adc_data(
-            params={
-                "universe": universe.adc,
-                "fields": fields.adc,
-                "parameters": parameters,
-                "use_field_names_in_headers": use_field_names_in_headers,
-            },
+        adc_params = get_adc_params(start, end, interval)
+        adc_params.update(parameters or {})
+        adc_raw, adc_df, adc_exception_msg = get_adc_data(
+            universe=universe.adc,
+            fields=fields.adc,
+            parameters=adc_params,
+            use_field_names_in_headers=use_field_names_in_headers,
             logger=logger,
         )
+        exceptions.append(adc_exception_msg)
 
     adc_data = ADCDataContainer(adc_raw, adc_df, fields)
-    universe.calc_hp(adc_raw)
+    universe.calc_hp(adc_data.raw)
 
-    stream_columns, stream_data, stream_df = None, None, None
-    if hp_and_cust_inst.can_get_data:
-        stream_columns, stream_data, stream_df = get_data_from_stream(
-            universe.hp_and_cust_inst, fields.hp, logger
+    hp_raw = None
+    hp_df = None
+    if hp.can_get_data:
+        hp_raw, hp_df, hp_exception_msg = get_hp_data(
+            universe=universe.hp,
+            interval=interval,
+            start=start,
+            end=end,
+            adjustments=adjustments,
+            count=count,
+            fields=fields.hp,
+            logger=logger,
         )
+        exceptions.append(hp_exception_msg)
 
-    hp_and_cust_inst_data = HPAndCustInstDataContainer(
-        stream_columns, stream_data, stream_df
-    )
+    hp_data = HPDataContainer(hp_raw, hp_df)
+    cust_inst_raw = None
+    cust_inst_df = None
+    if cust_inst.can_get_data:
+        cust_inst_raw, cust_inst_df, cust_inst_exception_msg = get_custominsts_data(
+            universe=universe.cust_inst,
+            interval=interval,
+            start=start,
+            end=end,
+            count=count,
+            logger=logger,
+        )
+        exceptions.append(cust_inst_exception_msg)
 
-    adc.set_data(adc_data, hp_and_cust_inst_data)
-    hp_and_cust_inst.set_data(adc_data, hp_and_cust_inst_data)
+    cust_inst_data = CustInstDataContainer(cust_inst_raw, cust_inst_df)
 
-    provider = DataProvider()
-    return provider.get_df(
-        adc,
-        hp_and_cust_inst,
-        adc_data,
-        hp_and_cust_inst_data,
-        use_field_names_in_headers,
-    )
+    if exceptions and all(exceptions):
+        except_msg = "\n\n".join(exceptions)
+        raise RDError(-1, except_msg)
+
+    if not any({adc_data, hp_data, cust_inst_data}):
+        return DataFrame()
+
+    adc.set_data(adc_data, hp_data, cust_inst_data)
+    hp.set_data(adc_data, hp_data, cust_inst_data)
+    cust_inst.set_data(adc_data, hp_data, cust_inst_data)
+
+    history_provider = get_history_df_builder(data_grid_type)
+    return history_provider.build_df_date_as_index(adc, hp, cust_inst, universe.hp, interval)
+
+
+def get_adc_params(
+    start: Union[str, date, datetime, timedelta],
+    end: Union[str, date, datetime, timedelta],
+    interval: Optional[str],
+) -> dict:
+    """
+    Gets parameters for ADC request.
+
+    Parameters
+    ----------
+    start : str or date or datetime or timedelta
+        Parameters start date.
+    end : str or date or datetime or timedelta
+        Parameters end date.
+    interval : str, optional
+        Interval using to calculate parameters.
+
+    Returns
+    -------
+    parameters : dict
+        Parameters for ADC requests.
+    """
+    parameters = {}
+    if start is not None:
+        parameters["SDate"] = fr_datetime_adapter.get_str(start)
+
+    if end is not None:
+        parameters["EDate"] = fr_datetime_adapter.get_str(end)
+
+    if interval is not None:
+        parameters["FRQ"] = INTERVALS[interval]["adc"]
+
+    return parameters
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/get_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/get_stream.py`

 * *Files 2% similar despite different names*

```diff
@@ -32,22 +32,22 @@
     on_data: Optional[Callable] = None,
 ) -> "PricingStream":
     """
     Creates and opens a pricing stream.
 
     Parameters
     ----------
-    universe : str | Iterable[str]
-        Instruments to request
+    universe : str | List[str]
+        Instruments to request.
     fields : str | list, optional
-        Fields to request
+        Fields to request.
     service : str, optional
-        Name of the streaming service publishing the instruments
+        Name of the streaming service publishing the instruments.
     on_data : function, optional
-        Callback function
+        Callback function.
 
     Returns
     ----------
     PricingStream
 
     Examples
     -------
@@ -92,17 +92,15 @@
 
     def get_snapshot(
         self,
         universe: Union[str, List[str], None] = None,
         fields: Optional[List[str]] = None,
         convert: bool = True,
     ) -> "pd.DataFrame":
-        return self._stream.get_snapshot(
-            universe=universe, fields=fields, convert=convert
-        )
+        return self._stream.get_snapshot(universe=universe, fields=fields, convert=convert)
 
     def _get_fields(self, universe: str, fields: Optional[list] = None) -> dict:
         return self._stream._get_fields(universe=universe, fields=fields)
 
     def add_instruments(self, *args):
         self._stream.add_instruments(*args)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/news/_news.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/news/_news.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,107 +1,92 @@
 from typing import Optional, Union
 
 import pandas as pd
 
 from ..._types import OptDateTime
 from ...content.news import story as _story
 from ...content.news import headlines as _headlines
-from ...content.news._sort_order import SortOrder
+from ...content.news.headlines._sort_order import SortOrder
 
-
-from enum import Enum, unique
+from enum import unique
+from ..._base_enum import StrEnum
 
 
 @unique
-class Format(Enum):
+class Format(StrEnum):
     TEXT = "Text"
     HTML = "Html"
 
 
 def get_story(
     story_id: str,
     format: Optional[Union[Format, str]] = Format.HTML,
 ) -> str:
     """
-    This function describes parameters to retrieve data for news story.
+    Retrieves the news story items.
 
     Parameters
     ----------
     story_id : str
         News Story ID.
-    format : Format, optional
-        Specifies the type of response. If parameter Format.TEXT
-        return text string, otherwise returns HTML response
+    format : str, Format, optional
+        Response format.
 
     Returns
     -------
     str
         Story html or text response
 
     Examples
     --------
     >>> import refinitiv.data as rd
-    >>> response = rd.news.get_story("urn:newsml:reuters.com:20220713:nL1N2YU10J")
+    >>> response = rd.news.get_story("urn:newsml:reuters.com:20220713:nL1N2YU10J", format=rd.news.Format.TEXT)
     """
 
-    response = _story.Definition(story_id).get_data()
-
-    if format == Format.HTML or format == "Html":
-        response = (
-            response.data.raw.get("story", {}).get("storyHtml")
-            or response.data.story.content.html
-        )
-    else:
-        response = response.data.story.content.text
-
-    return response
+    content = _story.Definition(story_id).get_data().data.story.content
+    return content.html if format == Format.HTML else content.text
 
 
 def get_headlines(
     query: str,
     count: int = 10,
     start: "OptDateTime" = None,
     end: "OptDateTime" = None,
-    order_by: SortOrder = SortOrder.new_to_old,
+    order_by: Union[str, SortOrder] = SortOrder.new_to_old,
 ) -> pd.DataFrame:
     """
-    This function describes parameters to retrieve data for news headlines.
+    Retrieves news headlines.
 
     Parameters
     ----------
     query: str
-        The user search query.
+        The user search query for news headlines.
     count: int, optional
-        Count to limit number of headlines. Min value is 0. Default: 10
+        Count to limit number of headlines.
     start: str or timedelta, optional
         Beginning of date range.
         String format is: '%Y-%m-%dT%H:%M:%S'. e.g. '2016-01-20T15:04:05'.
     end: str or timedelta, optional
         End of date range.
         String format is: '%Y-%m-%dT%H:%M:%S'. e.g. '2016-01-20T15:04:05'.
-    order_by: SortOrder
-        Value from SortOrder enum. Default: SortOrder.new_to_old
+    order_by: str or SortOrder
+        Sort order for headline items.
 
     Returns
     -------
     pd.DataFrame
         Headlines dataframe
 
     Examples
     --------
     >>> from datetime import timedelta
     >>> import refinitiv.data as rd
-    >>> definition = rd.news.get_headlines(
+    >>> response = rd.news.get_headlines(
     ...     "Refinitiv",
     ...     start="20.03.2021",
     ...     end=timedelta(days=-4),
     ...     count=3
     ... )
     """
 
-    response = _headlines.Definition(
-        query=query, count=count, date_from=start, date_to=end, sort_order=order_by
-    ).get_data()
-
-    response = response.data.df
-
-    return response
+    definition = _headlines.Definition(query=query, count=count, date_from=start, date_to=end, sort_order=order_by)
+    return definition.get_data().data.df
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_fin_coder_layer/session.py` & `refinitiv-data-1.2.0/refinitiv/data/_fin_coder_layer/session.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,7 +1,9 @@
+"""Session functions."""
+
 from typing import Optional, TYPE_CHECKING
 
 from .._config_functions import load_config, get_config
 from .._core.session import get_default, set_default, is_open, is_closed
 from .._core.session._session_definition import Definition
 from .._log import root_logger
 
@@ -12,25 +14,24 @@
 
 def open_session(
     name: Optional[str] = None,
     app_key: Optional[str] = None,
     config_name: Optional[str] = None,
 ) -> "Session":
     """
-    Opens a session.
-    Without params opens default session from default config session
+    Opens and returns a session.
 
     Parameters
     ----------
     name: str, optional
-        Session name from the config file
+        Session name from the config file.
     app_key: str, optional
-        The application access key
+        The application key.
     config_name: str, optional
-        The config name. If provided, overrides default config
+        The config name. If provided, overrides default config.
 
     Returns
     -------
     Session
     """
     # Because it's a path in configuration profile, not a session name,
     # but we can't change argument name, because it's a public API
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_log.py` & `refinitiv-data-1.2.0/refinitiv/data/_log.py`

 * *Files 2% similar despite different names*

```diff
@@ -16,18 +16,15 @@
 
 TRACE = 5
 
 py_grade = logging._nameToLevel.copy()
 py_grade["TRACE"] = TRACE
 # add an additional level
 logging.addLevelName(TRACE, "TRACE")
-py_grade = {
-    f"py_{lname}": {"name": lname, "level": llevel}
-    for lname, llevel in py_grade.items()
-}
+py_grade = {f"py_{lname}": {"name": lname, "level": llevel} for lname, llevel in py_grade.items()}
 
 ts_grade = {
     "ts_trace": {"name": "trace", "level": 0},
     "ts_debug": {"name": "debug", "level": 1},
     "ts_info": {"name": "info", "level": 2},
     "ts_warn": {"name": "warn", "level": 3},
     "ts_error": {"name": "error", "level": 4},
@@ -56,17 +53,15 @@
 | warn       | WARNING  |
 +------------+----------+
 | error      | ERROR    |
 +------------+----------+
 | silent     | CRITICAL |
 +------------+----------+
 """
-py_by_ts_nameToName = {
-    ts_grade[ts_]["name"]: py_grade[py_]["name"] for ts_, py_ in conversion_schema
-}
+py_by_ts_nameToName = {ts_grade[ts_]["name"]: py_grade[py_]["name"] for ts_, py_ in conversion_schema}
 
 """
 +------------+--------+
 | TypeScript | Python |
 +------------+--------+
 | 0          | 5      |
 +------------+--------+
@@ -77,17 +72,15 @@
 | 3          | 30     |
 +------------+--------+
 | 4          | 40     |
 +------------+--------+
 | 5          | 50     |
 +------------+--------+
 """
-py_by_ts_levelToLevel = {
-    ts_grade[ts_]["level"]: py_grade[py_]["level"] for ts_, py_ in conversion_schema
-}
+py_by_ts_levelToLevel = {ts_grade[ts_]["level"]: py_grade[py_]["level"] for ts_, py_ in conversion_schema}
 
 # ---------------------------------------------------------------------------
 #   File handler
 # ---------------------------------------------------------------------------
 
 
 bytes_by_suffix = {
@@ -134,15 +127,14 @@
         when="h",
         interval=1,
         utc=False,
         at_time=None,
         *args,
         **kwargs,
     ):
-
         if file_mode.startswith("w"):
             try:
                 os.remove(filename)
             except Exception:
                 pass
 
         self.filename = filename
@@ -276,22 +268,15 @@
         "%(thread)d-%(threadName)s|"
         "%(name)s] \n"
         "%(module)s."
         "%(funcName)s "
         "%(message)s"
     )
 else:
-    fmt = (
-        "[%(asctime)s] - "
-        "[%(levelname)s] - "
-        "[%(name)s] - "
-        "[%(thread)d] | "
-        "%(threadName)s\n"
-        "%(message)s"
-    )
+    fmt = "[%(asctime)s] - " "[%(levelname)s] - " "[%(name)s] - " "[%(thread)d] | " "%(threadName)s\n" "%(message)s"
 
 _stdout_formatter = RDFormatter(fmt)
 
 
 def _create_log_stdout_handler():
     handler_ = logging.StreamHandler(sys.stdout)
     handler_.setFormatter(_stdout_formatter)
@@ -434,17 +419,15 @@
 
     log_file_enabled_ = configure.get(configure.keys.log_file_enabled, True)
     if log_file_enabled_:
         name_ = configure.get_str(configure.keys.log_filename)
         file_size_ = configure.get_str(configure.keys.log_file_size)
         max_files_ = configure.get_int(configure.keys.log_max_files)
         interval_ = configure.get_str(configure.keys.log_interval)
-        _log_file_handler = _create_log_file_handler(
-            name_, file_size_, max_files_, interval_
-        )
+        _log_file_handler = _create_log_file_handler(name_, file_size_, max_files_, interval_)
         logger_.addHandler(_log_file_handler)
 
     log_console_enabled_ = configure.get(configure.keys.log_console_enabled, True)
     if log_console_enabled_:
         logger_.addHandler(_log_stream_handler)
     else:
         logger_.propagate = False
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_qpl/_bid_ask.py` & `refinitiv-data-1.2.0/refinitiv/data/_qpl/_bid_ask.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_qpl/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/_qpl/_data_provider.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_qpl/_fx_spot_quote.py` & `refinitiv-data-1.2.0/refinitiv/data/_qpl/_fx_spot_quote.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_qpl/_fx_swap_points.py` & `refinitiv-data-1.2.0/refinitiv/data/_qpl/_fx_swap_points.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,21 +23,17 @@
         For the Refinitiv composite source, use Composite, or ignore the property
     overrides : dict, TenorBidAsk, list of TenorBidAsk, optional
         An array of tenors and corresponding swap bid, ask prices
     """
 
     def __init__(
         self,
-        additional_tenor_types: Optional[
-            Union["StrStrings", TenorTypes, List[TenorTypes]]
-        ] = None,
+        additional_tenor_types: Optional[Union["StrStrings", TenorTypes, List[TenorTypes]]] = None,
         source: "OptStr" = None,
-        overrides: Optional[
-            Union[dict, List[dict], TenorBidAsk, List[TenorBidAsk]]
-        ] = None,
+        overrides: Optional[Union[dict, List[dict], TenorBidAsk, List[TenorBidAsk]]] = None,
     ) -> None:
         super().__init__()
         self.additional_tenor_types = additional_tenor_types
         self.source = source
         self.overrides = overrides
 
     @property
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_qpl/_fx_swp_to_swp.py` & `refinitiv-data-1.2.0/refinitiv/data/_qpl/_fx_swp_to_swp.py`

 * *Files 0% similar despite different names*

```diff
@@ -10,14 +10,15 @@
     from ._fx_swap_points import FxSwapPoints
     from .._types import OptDateTime, OptStrStrs
     from ..delivery._data._response import Response
 
 
 def fx_swp_to_swp(
     fx_cross_code: str,
+    *,
     market_data_date_time: "OptDateTime" = None,
     tenors: "OptStrStrs" = None,
     fields: "OptStrStrs" = None,
     spot_ccy_1: Optional[Union[dict, "FxSpotQuote"]] = None,
     spot_ccy_2: Optional[Union[dict, "FxSpotQuote"]] = None,
     swap_points_ccy_1: Optional[Union[dict, "FxSwapPoints"]] = None,
     swap_points_ccy_2: Optional[Union[dict, "FxSwapPoints"]] = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_qpl/_serializer.py` & `refinitiv-data-1.2.0/refinitiv/data/_qpl/_serializer.py`

 * *Files 14% similar despite different names*

```diff
@@ -15,24 +15,20 @@
     def _get_list_param(self, name):
         return self._dict.get(name)
 
     def _set_list_param(self, name, value):
         if value is None:
             return
 
-        self._dict[name] = [
-            item.get_dict() if hasattr(item, "get_dict") else item for item in value
-        ]
+        self._dict[name] = [item.get_dict() if hasattr(item, "get_dict") else item for item in value]
 
     def _get_list_of_enums(self, name):
         return self._dict.get(name)
 
     def _set_list_of_enums(self, enum_type, name, value):
         if value is None:
             return
 
-        self._dict[name] = [
-            item.value if isinstance(item, enum_type) else item for item in value
-        ]
+        self._dict[name] = [item.value if isinstance(item, enum_type) else item for item in value]
 
     def get_dict(self):
         return self._dict
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_qpl/_tenor_bid_ask.py` & `refinitiv-data-1.2.0/refinitiv/data/_qpl/_tenor_bid_ask.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_common.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_common.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,14 +1,15 @@
 import threading
 from collections import defaultdict
 from enum import Enum
 from typing import Any, Callable, List, Optional, TYPE_CHECKING, Union, Iterable
 
 import numpy as np
 import pandas as pd
+import simplejson as json
 
 forbidden_symbols = ':/|\\?*<>"'
 
 if TYPE_CHECKING:
     from pandas import DataFrame
 
 
@@ -58,23 +59,20 @@
         while self.not_finished():
             self.finished.wait(self.interval)
             if not self.finished.is_set():
                 self.function(*self.args, **self.kwargs)
 
 
 class CallbackHandler:
-    def __init__(self, max_listeners: int = 1):
+    def __init__(self):
         self._lock = threading.Lock()
-        self._max_listeners = max_listeners
         self._callbacks = defaultdict(list)
 
     def on(self, event: str, callback: Callable) -> None:
         with self._lock:
-            if len(self._callbacks[event]) >= self._max_listeners:
-                self._callbacks[event].pop(0)
             self._callbacks[event].append(callback)
 
     def remove_listener(self, event: str, callback: Callable) -> None:
         with self._lock:
             self._callbacks[event].remove(callback)
 
     def emit(self, event: str, *args, **kwargs) -> None:
@@ -217,18 +215,15 @@
     return all([isinstance(i, str) for i in input_list])
 
 
 def validate_bool_value(value: bool) -> bool:
     if isinstance(value, bool):
         return value
     else:
-        raise ValueError(
-            f"Please provide boolean value ('True' or 'False'), "
-            f"current value is '{value}'"
-        )
+        raise ValueError(f"Please provide boolean value ('True' or 'False'), current value is '{value}'")
 
 
 def args_parser(param: Any, expect_none: bool = False) -> Optional[list]:
     if isinstance(param, str):
         return [param]
 
     if isinstance(param, list):
@@ -295,34 +290,30 @@
 
     if isinstance(param, list):
         if is_all_same_type(str, param):
             return param
         else:
             raise ValueError(f"Not all elements are strings in {param}")
 
-    raise TypeError(
-        f"Invalid type, expected str or list, {type(param).__name__} is given"
-    )
+    raise TypeError(f"Invalid type, expected str or list, {type(param).__name__} is given")
 
 
 def parse_iterable_of_str(param: Union[str, Iterable[str]]) -> list:
     if isinstance(param, str):
         return [param]
 
     if isinstance(param, Iterable):
         if not isinstance(param, list):
             param = list(param)
         if is_all_same_type(str, param):
             return param
         else:
             raise ValueError(f"Not all elements are strings in {param}")
 
-    raise TypeError(
-        f"Invalid type, expected str or list, {type(param).__name__} is given"
-    )
+    raise TypeError(f"Invalid type, expected str or list, {type(param).__name__} is given")
 
 
 def make_parse_enum(enum_class: iterable, can_be_lower: bool = True) -> callable:
     enum_values = [k.value for k in enum_class]
 
     def parse_enum(param: Union[str, list, Enum]) -> Union[str, list]:
         if isinstance(param, list):
@@ -345,17 +336,15 @@
 
     return parse_enum
 
 
 def make_convert_to_enum(enum_class: iterable) -> callable:
     lower_values = {item.value.lower(): item for item in enum_class}
 
-    def convert_to_enum(
-        param: Union[str, List[str], Enum, List[Enum]]
-    ) -> Union[Enum, List[Enum]]:
+    def convert_to_enum(param: Union[str, List[str], Enum, List[Enum]]) -> Union[Enum, List[Enum]]:
         if isinstance(param, str) and param.lower() in lower_values:
             return lower_values[param.lower()]
 
         if isinstance(param, list):
             return [convert_to_enum(p) for p in param]
 
         if isinstance(param, enum_class):
@@ -405,16 +394,15 @@
     t_names = [tp.__name__ for tp in types if tp if tp.__name__ != "NoneType"]
 
     if None in types:
         raise TypeError("Use 'type(None)' instead 'None', in 'types'")
 
     if type(val) not in types:
         raise TypeError(
-            f"Parameter '{val_name}' of invalid type provided: '{type(val).__name__}', "
-            f"expected types: {t_names}"
+            f"Parameter '{val_name}' of invalid type provided: '{type(val).__name__}', expected types: {t_names}"
         )
 
 
 fields_arg_parser = ArgsParser(parse_list_of_str)
 universe_arg_parser = fields_arg_parser
 hp_universe_parser = ArgsParser(parse_hp_universe)
 custom_insts_historical_universe_parser = ArgsParser(parse_hp_universe)
@@ -430,33 +418,27 @@
     return ArgsParser(make_convert_to_enum(*args, **kwargs))
 
 
 def make_enum_arg_parser_by_members(enum_class, can_be_lower=True):
     parser = make_parse_enum(enum_class, can_be_lower)
 
     def _parser(value):
-        lower_values = {
-            name.lower(): item for name, item in enum_class.__members__.items()
-        }
+        lower_values = {name.lower(): item for name, item in enum_class.__members__.items()}
         if isinstance(value, str) and value.lower() in lower_values:
             value = lower_values[value.lower()]
         return parser(value)
 
     return ArgsParser(_parser)
 
 
 def merge_dict_to_dict(dest: dict, source: dict) -> dict:
     for source_key, source_value in source.items():
         dest_value = dest.get(source_key)
 
-        if (
-            source_key in dest
-            and isinstance(source_value, dict)
-            and isinstance(dest_value, dict)
-        ):
+        if source_key in dest and isinstance(source_value, dict) and isinstance(dest_value, dict):
             merge_dict_to_dict(dest_value, source_value)
 
         else:
             dest[source_key] = source_value
 
     return dest
 
@@ -474,7 +456,27 @@
 
 
 def get_correct_filename(filename, replace_with=""):
     result = filename
     for symbol in forbidden_symbols:
         result = result.replace(symbol, replace_with)
     return result
+
+
+class lazy_dump:
+    def __init__(self, value):
+        self.value = value
+
+    def __str__(self):
+        return json.dumps(self.value)
+
+
+class lazy_formatting:
+    def __init__(self, text, *args):
+        self.text = text
+        self.args = args
+
+    def __str__(self):
+        return self.text % self.args
+
+    def __repr__(self):
+        return self.text % self.args
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_converter.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_converter.py`

 * *Files 1% similar despite different names*

```diff
@@ -28,17 +28,15 @@
             df.fillna(pd.NA, inplace=True)
             df = df.convert_dtypes()
     else:
         df = pd.DataFrame([], columns=columns)
     return df
 
 
-def convert_content_data_to_df(
-    content_data: dict, use_field_names_in_headers: bool = True
-):
+def convert_content_data_to_df(content_data: dict, use_field_names_in_headers: bool = True):
     if "headers" not in content_data or "data" not in content_data:
         return pd.DataFrame()
 
     if use_field_names_in_headers:
         key = "name"
     else:
         key = "title"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_dataframe.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_dataframe.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,28 +1,58 @@
 import re
 from typing import List, Union, TYPE_CHECKING
 
 import numpy as np
 import pandas as pd
-from pandas import DataFrame, MultiIndex
+from pandas.core.dtypes.base import ExtensionDtype
 from pandas.core.dtypes.cast import convert_dtypes as pandas_covert_dtypes
-from pandas.core.dtypes.common import (
-    is_object_dtype,
-    is_datetime64_any_dtype,
-    pandas_dtype,
-)
-from pandas.core.tools.datetimes import DatetimeScalarOrArrayConvertible
-
-
-DtypeObj = Union[np.dtype, "ExtensionDtype"]
+from pandas.core.dtypes.common import is_object_dtype, is_datetime64_any_dtype, pandas_dtype
 
+from .._types import TimestampOrNaT
 
 if TYPE_CHECKING:
     from pandas.core.series import Series
 
+DtypeObj = Union[np.dtype, ExtensionDtype]
+
+
+def set_df_column_value(df: pd.DataFrame, loc: int, value) -> pd.DataFrame:
+    """
+    Set the given value in the column with position 'loc'.
+
+    Library pandas changed `iloc` property and recommends using `isetitem`
+    from 1.5.0 version.
+
+    More details:
+    https://pandas.pydata.org/docs/whatsnew/v1.5.0.html
+
+    Link for isetitem method:
+    https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isetitem.html
+
+    Parameters
+    ----------
+    df : pd.DataFrame
+        Pandas dataframe to convert.
+    loc : int
+        Index for current column, when we need update value.
+    value : scalar or arraylike
+        New value for column.
+
+    Returns
+    -------
+    pd.DataFrame
+        Modified dataframe
+    """
+    if hasattr(df, "isetitem"):
+        df.isetitem(loc, value)
+    else:
+        df.iloc[:, loc] = value
+
+    return df
+
 
 def convert_df_columns_to_datetime(
     df: pd.DataFrame, entry: str, utc: bool = None, delete_tz: bool = False
 ) -> pd.DataFrame:
     """Converts particular dataframe columns to datetime according the pattern.
 
     Converts particular dataframe column or columns if one of more columns
@@ -40,19 +70,15 @@
         Convert to timezone-unaware if True.
 
     Returns
     -------
     pd.DataFrame
         Converted dataframe
     """
-    columns_indexes = [
-        index
-        for index, name in enumerate(df.columns.values)
-        if entry.lower() in name.lower()
-    ]
+    columns_indexes = [index for index, name in enumerate(df.columns.values) if entry.lower() in name.lower()]
 
     return convert_df_columns_to_datetime_by_idx(df, columns_indexes, utc, delete_tz)
 
 
 def convert_df_columns_to_datetime_by_idx(
     df: pd.DataFrame,
     columns_indexes: List[int],
@@ -74,64 +100,60 @@
 
     Returns
     -------
     df
         Converted dataframe.
     """
     for idx in columns_indexes:
-        df.iloc[:, idx] = pd.to_datetime(df.iloc[:, idx], utc=utc, errors="coerce")
+        date_value = pd.to_datetime(df.iloc[:, idx], utc=utc, errors="coerce")
+        set_df_column_value(df, idx, date_value)
         if delete_tz:
-            df.iloc[:, idx] = df.iloc[:, idx].dt.tz_localize(None)
+            date_value = df.iloc[:, idx].dt.tz_localize(None)
+            set_df_column_value(df, idx, date_value)
 
     return df
 
 
-def convert_df_columns_to_datetime_re(
-    df: pd.DataFrame, pattern: re.compile
-) -> pd.DataFrame:
+def convert_df_columns_to_datetime_re(df: pd.DataFrame, pattern: re.compile) -> pd.DataFrame:
     """Convert dataframe columns to datetime using regular expression pattern.
 
     Parameters
     ----------
     df : pd.Dataframe
         Pandas dataframe to convert.
     pattern : re.compile
         Regular expression pattern to check columns.
 
     Returns
     -------
     df
         Converted dataframe
     """
-    column_indexes = [
-        index for index, name in enumerate(df.columns.values) if pattern.search(name)
-    ]
+    column_indexes = [index for index, name in enumerate(df.columns.values) if pattern.search(name)]
 
-    return convert_df_columns_to_datetime_by_idx(
-        df, column_indexes, utc=True, delete_tz=True
-    )
+    return convert_df_columns_to_datetime_by_idx(df, column_indexes, utc=True, delete_tz=True)
 
 
-def convert_str_to_datetime(s: str) -> DatetimeScalarOrArrayConvertible:
-    date = pd.to_datetime(s, utc=True, errors="coerce")
-    date = date.tz_localize(None)
-    return date
+def convert_str_to_timestamp(s: str) -> TimestampOrNaT:
+    timestamp = pd.to_datetime(s, utc=True, errors="coerce")
+    localized = timestamp.tz_localize(None)
+    return localized
 
 
-def sort_df_by_universe(df: DataFrame, universe: List[str]) -> DataFrame:
+def sort_df_by_universe(df: pd.DataFrame, universe: List[str]) -> pd.DataFrame:
     length = len(universe)
 
     if length == 1:
         return df
 
     columns = df.columns
 
     def make_getidx():
         get_index = universe.index
-        if isinstance(columns, MultiIndex):
+        if isinstance(columns, pd.MultiIndex):
 
             def geti(i):
                 return i[0]
 
         else:
 
             def geti(i):
@@ -181,15 +203,15 @@
     Returns
     -------
     pd.DataFrame
     """
     if df.empty:
         return df
 
-    df.fillna(np.nan, inplace=True)
+    df = df.fillna(np.nan)
     columns_indexes = [index for index, _ in enumerate(df.columns.values)]
 
     for index in columns_indexes:
         series = df.iloc[:, index]
         if is_datetime64_any_dtype(series.dtype):
             continue
 
@@ -203,18 +225,15 @@
             new_series.replace("", np.nan, inplace=True)
 
             inferred_dtype = _get_inferred_dtype(new_series)
             if str(inferred_dtype).lower() != "object":
                 series = new_series
 
         result = series.astype(inferred_dtype)
-        if hasattr(df, "isetitem"):
-            df.isetitem(index, result)
-        else:
-            df.iloc[:, index] = result
+        set_df_column_value(df, index, result)
 
     df.fillna(pd.NA, inplace=True)
     return df
 
 
 def _get_inferred_dtype(series: "Series") -> DtypeObj:
     inferred_dtype = pandas_covert_dtypes(series.values)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_datetime.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_datetime.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,17 +4,15 @@
 
 import dateutil.parser
 from dateutil import tz
 
 ownership_expr = re.compile("[-]?[0-9]+[MDQ]{1}[A]?")
 
 
-def to_datetime(
-    date_value: Union[str, timedelta, Tuple[datetime, date]]
-) -> Union[tuple, datetime, None]:
+def to_datetime(date_value: Union[str, timedelta, Tuple[datetime, date]]) -> Union[tuple, datetime, None]:
     if date_value is None:
         return None
 
     if isinstance(date_value, timedelta):
         return datetime.now(tz.tzlocal()) + date_value
 
     if isinstance(date_value, (datetime, date)):
@@ -26,19 +24,15 @@
         raise e
     except Exception as e:
         raise ValueError(e)
 
 
 def get_date_from_today(days_count):
     if type(days_count) != int:
-        raise ValueError(
-            "The parameter {} should be an integer, found {}".format(
-                days_count, type(days_count)
-            )
-        )
+        raise ValueError("The parameter {} should be an integer, found {}".format(days_count, type(days_count)))
     return datetime.now(tz.tzlocal()) + timedelta(days=-days_count)
 
 
 def convert_to_datetime(value):
     if value is None:
         return None
 
@@ -106,16 +100,17 @@
     def convert(self, value):
         return self.converter.convert(value)
 
     def get_str(self, value):
         return self.formatter.to_str(self.convert(value))
 
     def get_localize(self, value):
-        dt = self.converter.convert(value)
-        return dt.replace(tzinfo=tz.gettz("UTC"))
+        if value is not None:
+            dt = self.converter.convert(value)
+            return dt.replace(tzinfo=tz.gettz("UTC"))
 
 
 class OwnerShipDateTimeAdapter(DateTimeAdapter):
     def get_str(self, value):
         if isinstance(value, str) and ownership_expr.match(value):
             return value
         return self.formatter.to_str(self.convert(value))
@@ -125,14 +120,19 @@
 _date_formatter = FundamentalAndReferenceFormatter()
 _converter = Converter()
 
 _z_ends_date_time_adapter = DateTimeAdapter(_converter, _nanoseconds_formatter)
 _t_ends_date_time_adapter = DateTimeAdapter(_converter, _date_formatter)
 hp_datetime_adapter = _z_ends_date_time_adapter
 news_datetime_adapter = _z_ends_date_time_adapter
+filling_search_datetime_adapter = _z_ends_date_time_adapter
 fr_datetime_adapter = _t_ends_date_time_adapter
 add_periods_datetime_adapter = _t_ends_date_time_adapter
 qpl_datetime_adapter = _t_ends_date_time_adapter
 tds_datetime_adapter = _z_ends_date_time_adapter
 custom_inst_datetime_adapter = _z_ends_date_time_adapter
 cfs_datetime_adapter = DateTimeAdapter(_converter, CFSFormatter())
 ownership_datetime_adapter = OwnerShipDateTimeAdapter(_converter, OwnershipFormatter())
+
+
+def to_iso_format(value):
+    return to_datetime(value).isoformat()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_params.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_params.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,17 +36,15 @@
         self,
         arg_name: str,
         query_param_name: Optional[str] = None,
         function: Optional[Callable] = None,
         is_true: Callable = None,
     ):
         super().__init__(arg_name, query_param_name, function, is_true)
-        self.function = (
-            lambda v, *args, **kwargs: v if not hasattr(v, "get_dict") else v.get_dict()
-        )
+        self.function = lambda v, *args, **kwargs: v if not hasattr(v, "get_dict") else v.get_dict()
 
 
 def get_params(params_config: List[ParamItem], *args, **kwargs):
     retval = []
     for item in params_config:
         param = kwargs.get(item.arg_name)
         if not item.is_true(param):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_patterns.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_patterns.py`

 * *Files 10% similar despite different names*

```diff
@@ -10,15 +10,15 @@
 
 # re for finding expressions inside ADC fields like
 # "TR.F.TotRevPerShr(SDate=0,EDate=-2,Period=FY0,Frq=FY).date"
 ADC_PARAM_IN_FIELDS = re.compile(r".*\(.+\).*")
 
 # re for ADC functions in fields like AVAIL(, AVG(
 # AVAIL(TR.GrossProfit(Period=LTM,Methodology=InterimSum))
-ADC_FUNC_PATTERN_IN_FIELDS = re.compile(r"^[A-Z_\d-]*\(")
+ADC_FUNC_PATTERN_IN_FIELDS = re.compile(r"^[A-Z_\d-]*\(", re.I)
 
 # re for finding column name include sub-string
 # ..._DATE...int
 # ...DATE...int
 # ..._DT...int
 # ...DT...int
 # ...DAT...int
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_repr.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_repr.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_specification.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_specification.py`

 * *Files 15% similar despite different names*

```diff
@@ -20,28 +20,24 @@
 
 @dataclass(frozen=True)
 class AndSpecification(BaseSpecification):
     first: BaseSpecification
     second: BaseSpecification
 
     def is_satisfied_by(self, candidate: Any) -> bool:
-        return self.first.is_satisfied_by(candidate) and self.second.is_satisfied_by(
-            candidate
-        )
+        return self.first.is_satisfied_by(candidate) and self.second.is_satisfied_by(candidate)
 
 
 @dataclass(frozen=True)
 class OrSpecification(BaseSpecification):
     first: BaseSpecification
     second: BaseSpecification
 
     def is_satisfied_by(self, candidate: Any) -> bool:
-        return self.first.is_satisfied_by(candidate) or self.second.is_satisfied_by(
-            candidate
-        )
+        return self.first.is_satisfied_by(candidate) or self.second.is_satisfied_by(candidate)
 
 
 @dataclass(frozen=True)
 class NotSpecification(BaseSpecification):
     subject: BaseSpecification
 
     def is_satisfied_by(self, candidate: Any) -> bool:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/_utils.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 import inspect
 import re
+from typing import Tuple
 from urllib.parse import ParseResult, urlparse, ParseResultBytes
 
 pattern_1 = re.compile("(.)([A-Z][a-z]+)")
 pattern_2 = re.compile("([a-z0-9])([A-Z])")
 
 
 def camel_to_snake(s):
@@ -21,18 +22,15 @@
     return s.lower()
 
 
 def parse_url(url: str) -> ParseResult:
     import sys
 
     py_ver = sys.version_info
-    if py_ver.major == 3 and (
-        py_ver.minor <= 11 or (py_ver.minor == 11 and py_ver.micro < 1)
-    ):
-
+    if py_ver.major == 3 and (py_ver.minor <= 11 or (py_ver.minor == 11 and py_ver.micro < 1)):
         result_urlparse = urlparse(url)
 
         if isinstance(result_urlparse, ParseResultBytes):
             return result_urlparse
 
         scheme = result_urlparse.scheme
         netloc = result_urlparse.netloc
@@ -53,27 +51,28 @@
             netloc=netloc,
             path=path,
             params=result_urlparse.params,
             query=query,
             fragment=fragment,
         )
     else:
-
         result = urlparse(url)
 
     return result
 
 
 def validate_endpoint_request_url_parameters(url, path_parameters):
     if url == "":
         raise ValueError("Requested URL is missing, please provide valid URL")
 
     if url.endswith("{universe}") and not path_parameters:
-        raise ValueError(
-            "Path parameter 'universe' is missing, please provide path parameter"
-        )
+        raise ValueError("Path parameter 'universe' is missing, please provide path parameter")
 
 
 def inspect_parameters_without_self(class_: object):
     cls_init_attributes = dict(inspect.signature(class_.__init__).parameters)
     cls_init_attributes.pop("self", None)
     return cls_init_attributes.keys()
+
+
+def version_to_tuple(version: str) -> Tuple[int, ...]:
+    return tuple(map(int, version.split(".")))
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_tools/templates.py` & `refinitiv-data-1.2.0/refinitiv/data/_tools/templates.py`

 * *Files 2% similar despite different names*

```diff
@@ -103,17 +103,15 @@
         self.index = index
         self.template_text = template_text
         self.prefix = prefix
 
     def __str__(self):
         line_index, col_index = index_to_line_and_col(self.index, self.template_text)
         target_line = self.template_text.splitlines()[line_index]
-        target_line, shift = shorten_string_to_include_position(
-            target_line, self.limit, col_index, self.padding
-        )
+        target_line, shift = shorten_string_to_include_position(target_line, self.limit, col_index, self.padding)
         return "\n".join(
             [
                 f"{self.prefix}Invalid placeholder in the template string: "
                 f"line {line_index + 1}, col {col_index + 1}:",
                 target_line,
                 "-" * (col_index - shift) + "^",
             ]
@@ -140,17 +138,15 @@
         return 0, 0
 
     col_index = index - len("".join(lines[:-1]))
     line_index = len(lines) - 1
     return line_index, col_index
 
 
-def shorten_string_to_include_position(
-    line: str, limit: int, pos: int, padding: int = 0
-) -> Tuple[str, int]:
+def shorten_string_to_include_position(line: str, limit: int, pos: int, padding: int = 0) -> Tuple[str, int]:
     """Shorten string to given limit to include given position
 
     Can be used when we need to display position in a string when screen width is
     limited.
 
     Parameters
     ----------
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/_types.py` & `refinitiv-data-1.2.0/refinitiv/data/_types.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,13 @@
 from datetime import date, datetime, timedelta
 from typing import Optional, Callable, List, Union
 
+import pandas as pd
+from pandas._libs import NaTType
+
 OptStr = Optional[str]
 Strings = List[str]
 Dicts = List[dict]
 OptStrings = Optional[Strings]
 OptDicts = Optional[Dicts]
 
 OptInt = Optional[int]
@@ -18,7 +21,9 @@
 
 ExtendedParams = OptDict
 StrStrings = Union[str, Strings]
 DictDicts = Union[dict, Dicts]
 OptStrStrs = Optional[StrStrings]
 DateTime = Union[str, date, datetime, timedelta]
 OptDateTime = Optional[DateTime]
+
+TimestampOrNaT = Union[pd.Timestamp, NaTType]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_content_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_content_data_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 from typing import Union, TYPE_CHECKING
 
 from ._content_response_factory import ContentResponseFactory
 from ..delivery._data._data_provider import DataProvider
 
 if TYPE_CHECKING:
-    from ..delivery._data._response_factory import ABCResponseFactory
+    from ..delivery._data._response_factory import BaseResponseFactory
     from ..delivery._data._request_factory import RequestFactory
     from ..delivery._data._validators import ValidatorContainer, BaseValidator
     from ..delivery._data._connection import HttpSessionConnection
     from ..delivery._data._raw_data_parser import Parser
 
 
 class ContentDataProvider(DataProvider):
     def __init__(
         self,
         connection: "HttpSessionConnection" = None,
         request: "RequestFactory" = None,
-        response: "ABCResponseFactory" = None,
+        response: "BaseResponseFactory" = None,
         parser: "Parser" = None,
         validator: Union["BaseValidator", "ValidatorContainer"] = None,
     ):
         response = response or ContentResponseFactory()
         super().__init__(
             connection=connection,
             request=request,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_content_response_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_content_data_factory.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,46 +1,28 @@
-from typing import Type, TYPE_CHECKING
+from typing import Any, TYPE_CHECKING, Type
 
 from ._content_data import Data
-from ..delivery._data._response_factory import ResponseFactory
+from ._df_builder import build_empty_df
+from ..delivery._data._data_factory import BaseDataFactory
 
 if TYPE_CHECKING:
-    from ..delivery._data._endpoint_data import ABCData
-    from ..delivery._data._response import Response
-    from ..delivery._data._parsed_data import ParsedData
-
-
-class ContentResponseFactory(ResponseFactory):
-    def __init__(
-        self,
-        response_class: Type["Response"] = None,
-        data_class: Type["ABCData"] = None,
-    ):
-        data_class = data_class or Data
-        super().__init__(response_class, data_class)
+    from ..delivery._data._endpoint_data import EndpointData
 
-    @staticmethod
-    def get_dfbuilder(content_type=None, dfbuild_type=None, **kwargs):
+
+class ContentDataFactory(BaseDataFactory[Data]):
+    def __init__(self, data_class: Type["EndpointData"] = None):
+        self.data_class = data_class or Data
+
+    def get_dfbuilder(self, content_type=None, dfbuild_type=None, **kwargs):
         from ._df_builder_factory import get_dfbuilder, DFBuildType
         from .._content_type import ContentType
 
-        content_type = content_type or kwargs.get(
-            "__content_type__", ContentType.DEFAULT
-        )
-        dfbuild_type = dfbuild_type or kwargs.get(
-            "__dfbuild_type__", DFBuildType.DATE_AS_INDEX
-        )
+        content_type = content_type or kwargs.get("__content_type__", ContentType.DEFAULT)
+        dfbuild_type = dfbuild_type or kwargs.get("__dfbuild_type__", DFBuildType.DATE_AS_INDEX)
         return get_dfbuilder(content_type, dfbuild_type)
 
-    def create_data_success(self, parsed_data: "ParsedData", **kwargs) -> Data:
-        return self.data_class(
-            self.get_raw(parsed_data),
-            self.get_dfbuilder(**kwargs),
-            **kwargs,
-        )
-
-    def create_data_fail(self, parsed_data: "ParsedData", **kwargs) -> Data:
-        return self.data_class(
-            parsed_data.content_data or {},
-            self.get_dfbuilder(**kwargs),
-            **kwargs,
-        )
+    def create_data_success(self, raw: Any, **kwargs) -> Data:
+        return self.data_class(raw=raw, _dfbuilder=self.get_dfbuilder(**kwargs), _kwargs=kwargs)
+
+    def create_data_fail(self, raw: Any, **kwargs) -> Data:
+        raw = raw if raw else {}
+        return self.data_class(raw=raw, _dfbuilder=build_empty_df, _kwargs=kwargs)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_df_builder.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_df_builder.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,28 +1,26 @@
 import abc
 import dataclasses
 from copy import deepcopy
 from functools import partial
 from itertools import product
-from typing import List, Any, Dict, Tuple
+from typing import List, Any, Dict, Tuple, Union
 
 import pandas as pd
-from pandas.core.tools.datetimes import DatetimeScalarOrArrayConvertible
 
 from .._tools import ADC_FUNC_PATTERN_IN_FIELDS
-from .._tools._dataframe import convert_str_to_datetime, convert_dtypes
+from .._tools._dataframe import convert_dtypes, convert_str_to_timestamp
+from .._types import TimestampOrNaT
 
 
 @dataclasses.dataclass
 class CacheItem:
     """Data cache item."""
 
-    fields_by_inst: Dict[str, List[Any]] = dataclasses.field(
-        init=False, default_factory=dict
-    )
+    fields_by_inst: Dict[str, List[Any]] = dataclasses.field(init=False, default_factory=dict)
 
     def add(self, inst: str, fields: List[Any]):
         """
         Add cache item as an instance:fields key-value pair.
 
         Parameters
         ----------
@@ -126,23 +124,23 @@
                     'inst_A': [1, 2, 3]
                 }
             }
         ]
     }
     """
 
-    _cache: Dict[str, CacheItems] = dataclasses.field(init=False, default_factory=dict)
+    _cache: Dict[TimestampOrNaT, CacheItems] = dataclasses.field(init=False, default_factory=dict)
 
-    def can_update_fields(self, date: str, inst: str) -> bool:
+    def can_update_fields(self, date: TimestampOrNaT, inst: str) -> bool:
         """
         Boolean that indicates can item fields be updated or not.
 
         Parameters
         ----------
-        date : str
+        date : TimestampOrNaT
             Date string to retrieve a bunch of items that belong to this date.
         inst : str
             Instance name to check if CacheItems has this instance or not.
 
         Returns
         -------
         bool
@@ -150,42 +148,42 @@
         """
         for item in self._cache.get(date, CacheItems()):
             if not item.has(inst):
                 return True
 
         return False
 
-    def add(self, date: str, inst: str, fields: List[Any]):
+    def add(self, date: TimestampOrNaT, inst: str, fields: List[Any]):
         """Add items to CacheItems container in data cache during initialization.
 
         Parameters
         ----------
-        date : str
+        date : TimestampOrNaT
             Date string.
         inst : str
             Instance name.
         fields : List[Any]
             Instance fields, filled according template.
         """
         items = self._cache.setdefault(date, CacheItems())
         items.add(inst, fields)
 
     def update_fields(
         self,
-        date: str,
+        date: TimestampOrNaT,
         inst: str,
         fields: List[Any],
         num_columns: int,
         unique_insts: List[str],
     ) -> List[Any]:
         """Updates fields in data cache and returns updated ones.
 
         Parameters
         ----------
-        date : str
+        date : TimestampOrNaT
             Date string.
         inst : str
             Instance name.
         fields : List[Any]
             Fields list.
         num_columns : int
             Data columns quantity.
@@ -214,57 +212,59 @@
         return cache_fields
 
 
 def partial_process_index(
     num_unique_insts: int,
     num_columns: int,
     date_cache: DateCache,
-    index: List[DatetimeScalarOrArrayConvertible],
+    index: List[TimestampOrNaT],
     unique_insts: List[str],
     inst: str,
-    date: str,
+    date: TimestampOrNaT,
     fields: List[Any],
 ):
     is_add = True
 
     if num_unique_insts > 1:
         total = num_unique_insts * num_columns
         template = [pd.NA] * total
-
         idx = unique_insts.index(inst)
         right_idx = idx * num_columns + num_columns
         left_idx = idx * num_columns
         for item, idx in zip(fields, range(left_idx, right_idx)):
             template[idx] = item
+
         fields = template
 
         if date_cache.can_update_fields(date, inst):
             is_add = False
-            fields = date_cache.update_fields(
-                date, inst, fields, num_columns, unique_insts
-            )
+            fields = date_cache.update_fields(date, inst, fields, num_columns, unique_insts)
 
         else:
             date_cache.add(date, inst, fields)
-            index.append(convert_str_to_datetime(date))
+            index.append(date)
 
     else:
-        index.append(convert_str_to_datetime(date))
+        index.append(date)
 
     return fields, is_add
 
 
 class DFBuilder(abc.ABC):
+    DATE_PATTERN = "date"
+    DATETIME_PATTERN = "datetime"
+
     @staticmethod
     def get_header_key(use_field_names_in_headers: bool) -> str:
         return "name" if use_field_names_in_headers else "title"
 
     @classmethod
-    def is_date_column(cls, _: dict, column: str) -> bool:
-        return "date" in column.lower()
+    def is_date_column(cls, header: dict, column: str) -> bool:
+        # for override
+        pass
 
     @abc.abstractmethod
     def get_instrument_column_name(self, header_key: str) -> str:
         # for override
         pass
 
     @abc.abstractmethod
@@ -287,49 +287,61 @@
             col = header[header_key]
             columns.append(col)
             if self.is_date_column(header, col):
                 date_idxs.append(idx)
 
         return columns, date_idxs
 
-    def build_index(
-        self, content_data: dict, use_field_names_in_headers: bool = False, **kwargs
-    ) -> pd.DataFrame:
+    def get_idx_to_header_name_wid_date_dict(
+        self, headers: List[dict], use_field_names_in_headers: bool
+    ) -> Dict[int, str]:
+        header_key = self.get_header_key(use_field_names_in_headers)
+        idx_to_adc_header_name_wid_date = {}
+        for idx, header in enumerate(headers):
+            col = header[header_key]
+            if self.is_date_column(header, col):
+                idx_to_adc_header_name_wid_date[idx] = col
+
+        return idx_to_adc_header_name_wid_date
+
+    def build_index(self, content_data: dict, use_field_names_in_headers: bool = False, **kwargs) -> pd.DataFrame:
         data = []
-        columns, date_idxs = self.get_date_idxs_and_columns(
-            self.get_headers(content_data), use_field_names_in_headers
-        )
+        columns, date_idxs = self.get_date_idxs_and_columns(self.get_headers(content_data), use_field_names_in_headers)
         for fields in content_data.get("data", []):
             fields = list(fields)
 
             for idx, item in enumerate(fields):
                 if item is None:
                     fields[idx] = pd.NA
 
             for idx in date_idxs:
-                fields[idx] = convert_str_to_datetime(fields[idx])
+                fields[idx] = convert_str_to_timestamp(fields[idx])
 
             data.append(fields)
 
         df = pd.DataFrame(data=data, columns=columns)
         df = convert_dtypes(df)
         return df
 
     def build_date_as_index(
         self,
         content_data: dict,
         use_field_names_in_headers: bool = False,
         use_multiindex: bool = False,
+        sort_ascending: bool = False,
         **kwargs,
     ) -> pd.DataFrame:
+        if not content_data["data"]:
+            return pd.DataFrame()
+
         header_key = self.get_header_key(use_field_names_in_headers)
         instrument_column_name = self.get_instrument_column_name(header_key)
         date_column_name = self.get_date_column_name(header_key)
         columns = []
-        date_idxs = []
+        date_idxs: List[int] = []
         inst_idx = None
         date_idx = None
         headers = self.get_headers(content_data)
         skip_num = 0
         for idx, header in enumerate(headers):
             col = header[header_key]
             if inst_idx is None and col == instrument_column_name:
@@ -358,30 +370,30 @@
             num_unique_insts,
             num_columns,
             date_cache,
             index,
             unique_insts,
         )
         for fields in fields_list:
-            fields = list(fields)
+            fields: List[Union[str, float, int]] = list(fields)
             date_str = fields[date_idx]
 
             if not date_str:
                 continue
 
             inst = fields[inst_idx]
             fields.pop(date_idx)
             fields.pop(inst_idx)
 
             fields = [pd.NA if i is None else i for i in fields]
 
             for idx in date_idxs:
-                fields[idx] = convert_str_to_datetime(fields[idx])
+                fields[idx] = convert_str_to_timestamp(fields[idx])
 
-            fields, is_add = process_index(inst, date_str, fields)
+            fields, is_add = process_index(inst, convert_str_to_timestamp(date_str), fields)
 
             is_add and data.append(fields)
 
         if num_columns > 1 and num_unique_insts > 1 or use_multiindex:
             columns = pd.MultiIndex.from_tuples(product(unique_insts, columns))
 
         elif num_unique_insts == 1:
@@ -389,15 +401,15 @@
 
         elif num_columns == 1:
             columns = pd.Index(data=unique_insts, name=columns.pop())
 
         index = pd.Index(data=index, name=date_column_name)
         df = pd.DataFrame(data=data, columns=columns, index=index)
         df = convert_dtypes(df)
-        df.sort_index(ascending=False, inplace=True)
+        df.sort_index(ascending=sort_ascending, inplace=True)
         return df
 
 
 class DFBuilderRDP(DFBuilder):
     """
     {
         "links": {"count": 2},
@@ -465,16 +477,18 @@
     def get_headers(self, content_data) -> List[dict]:
         return content_data.get("headers", [])
 
     @classmethod
     def is_date_column(cls, header: dict, column: str) -> bool:
         header_type = header.get("type")
         if header_type:
-            return header_type in {"datetime", "date"}
-        return super().is_date_column(header, column)
+            is_date_column = header_type in {cls.DATETIME_PATTERN, cls.DATE_PATTERN}
+        else:
+            is_date_column = cls.DATE_PATTERN in column.lower()
+        return is_date_column
 
 
 class DFBuilderUDF(DFBuilder):
     """
     {
         "columnHeadersCount": 1,
         "data": [
@@ -511,33 +525,30 @@
                 "title": header.get("displayName"),
             }
             for header in headers
         ]
 
     @classmethod
     def is_date_column(cls, header: dict, column: str) -> bool:
-        title = header.get("title")
-        if title:
-            return "date" == title.lower()
-
-        return "date" in column.lower() and not ADC_FUNC_PATTERN_IN_FIELDS.match(column)
+        header_title = header.get("title", column)
+        return cls.DATE_PATTERN in header_title.lower() and not ADC_FUNC_PATTERN_IN_FIELDS.match(header_title)
 
 
 def build_dates_calendars_df(raw: Any, **kwargs):
     raw = deepcopy(raw)
     add_periods_data = []
 
     clean_request_items = []
     for item in raw:
         if not item.get("error"):
             clean_request_items.append(item)
 
     for request_item in clean_request_items:
         if request_item.get("date"):
-            request_item["date"] = convert_str_to_datetime(request_item["date"])
+            request_item["date"] = convert_str_to_timestamp(request_item["date"])
 
         request_item.pop("holidays", None)
         add_periods_data.append(request_item)
 
     _df = pd.DataFrame(add_periods_data)
 
     return _df
@@ -554,15 +565,15 @@
     holidays_data = []
 
     for request_item_holiday in raw:
         for holiday in request_item_holiday.get("holidays", []):
             if holiday.get("names"):
                 for holiday_name in holiday["names"]:
                     holiday_name["tag"] = request_item_holiday.get("tag")
-                    holiday_name["date"] = convert_str_to_datetime(holiday.get("date"))
+                    holiday_name["date"] = convert_str_to_timestamp(holiday.get("date"))
                     holidays_data.append(holiday_name)
             else:
                 holiday["tag"] = request_item_holiday.get("tag")
                 holidays_data.append(holiday)
     return holidays_data
 
 
@@ -573,15 +584,15 @@
 
 
 def build_dates_calendars_date_schedule_df(raw: Any, **kwargs) -> pd.DataFrame:
     raw = deepcopy(raw)
 
     _dates = []
     for date in raw.get("dates"):
-        _dates.append(convert_str_to_datetime(date))
+        _dates.append(convert_str_to_timestamp(date))
 
     raw["dates"] = _dates
     df = pd.DataFrame(raw)
     return df
 
 
 def build_empty_df(*args, **kwargs) -> pd.DataFrame:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_df_builder_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_df_builder_factory.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,60 +1,43 @@
 from typing import Callable, Any, Dict, Union
 
 import pandas as pd
 
+from .news.online_reports._df_builder import news_online_reports_build_df
+from .news.online_reports.hierarchy._df_builder import news_online_reports_hierarchy_build_df
 from .._content_type import ContentType
 from ._df_build_type import DFBuildType
 from ._df_builder import (
     dfbuilder_udf,
     dfbuilder_rdp,
     default_build_df,
     build_empty_df,
     build_dates_calendars_holidays_df,
     build_dates_calendars_df,
     build_dates_calendars_date_schedule_df,
 )
-from ._historical_df_builder import (
-    custom_insts_builder,
-    historical_builder,
-)
-from .custom_instruments._custom_instruments_data_provider import (
-    custom_instruments_build_df,
-)
-from .ipa._curves._curves_builder_df import (
-    bond_curve_build_df,
-    cross_currency_curves_curve_build_df,
-)
+from ._historical_df_builder import custom_insts_builder, historical_builder
+from .custom_instruments._custom_instruments_data_provider import custom_instruments_build_df
+from .ipa._curves._curves_builder_df import bond_curve_build_df, cross_currency_curves_curve_build_df
 from .ipa._curves._curves_data_provider import (
     forward_curve_build_df,
     zc_curve_definitions_build_df,
     zc_curves_build_df,
     cross_currency_curves_definitions_search_build_df,
 )
-from .ipa.financial_contracts._contracts_data_provider import (
-    financial_contracts_build_df,
-)
-from .news._tools import (
-    news_build_df_rdp,
-    news_build_df_udf,
-)
+from .ipa.financial_contracts._contracts_data_provider import financial_contracts_build_df
+from .news._tools import news_build_df_rdp, news_build_df_udf
 from .news.top_news.hierarchy._df_builder import news_top_hierarchy_build_df
 from .news.top_news._df_builder import news_top_build_df
 from .pricing._pricing_content_provider import pricing_build_df
 from .pricing.chain._chains_data_provider import chains_build_df
-from .search._data_provider import (
-    discovery_lookup_build_df,
-    discovery_metadata_build_df,
-    discovery_search_build_df,
-)
+from .search._data_provider import discovery_lookup_build_df, discovery_metadata_build_df, discovery_search_build_df
 from ..delivery._data._data_type import DataType
 
-content_type_by_build_type: Dict[
-    Union[ContentType, DataType], Callable[[Any, Dict[str, Any]], pd.DataFrame]
-] = {
+content_type_by_build_type: Dict[Union[ContentType, DataType], Callable[[Any, Dict[str, Any]], pd.DataFrame]] = {
     ContentType.CHAINS: chains_build_df,
     ContentType.CONTRACTS: financial_contracts_build_df,
     ContentType.CUSTOM_INSTRUMENTS_INSTRUMENTS: custom_instruments_build_df,
     ContentType.CUSTOM_INSTRUMENTS_SEARCH: custom_instruments_build_df,
     ContentType.DATA_GRID_RDP: {
         DFBuildType.INDEX: dfbuilder_rdp.build_index,
         DFBuildType.DATE_AS_INDEX: dfbuilder_rdp.build_date_as_index,
@@ -95,29 +78,26 @@
     ContentType.CROSS_CURRENCY_CURVES_CURVES: cross_currency_curves_curve_build_df,
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_CREATE: build_empty_df,
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_DELETE: build_empty_df,
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_GET: build_empty_df,
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_UPDATE: build_empty_df,
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_SEARCH: cross_currency_curves_definitions_search_build_df,
     ContentType.CROSS_CURRENCY_CURVES_TRIANGULATE_DEFINITIONS_SEARCH: default_build_df,
-    # df will be built while merge responses from endpoint ###########
     ContentType.CUSTOM_INSTRUMENTS_INTERDAY_SUMMARIES: custom_insts_builder,
     ContentType.CUSTOM_INSTRUMENTS_EVENTS: custom_insts_builder,
     ContentType.CUSTOM_INSTRUMENTS_INTRADAY_SUMMARIES: custom_insts_builder,
     ContentType.HISTORICAL_PRICING_EVENTS: historical_builder,
     ContentType.HISTORICAL_PRICING_INTERDAY_SUMMARIES: historical_builder,
     ContentType.HISTORICAL_PRICING_INTRADAY_SUMMARIES: historical_builder,
-    ##################################################################
     ContentType.NEWS_HEADLINES_RDP: news_build_df_rdp,
     ContentType.NEWS_HEADLINES_UDF: news_build_df_udf,
-    ContentType.NEWS_TOP_NEWS: news_top_hierarchy_build_df,
-    ContentType.NEWS_TOP_NEWS_HEADLINES: news_top_build_df,
-    ContentType.NEWS_STORY_RDP: build_empty_df,
-    ContentType.NEWS_STORY_UDF: build_empty_df,
-    ContentType.NEWS_IMAGES: build_empty_df,
+    ContentType.NEWS_TOP_NEWS_HIERARCHY: news_top_hierarchy_build_df,
+    ContentType.NEWS_TOP_NEWS: news_top_build_df,
+    ContentType.NEWS_ONLINE_REPORTS: news_online_reports_build_df,
+    ContentType.NEWS_ONLINE_REPORTS_HIERARCHY: news_online_reports_hierarchy_build_df,
     ContentType.OWNERSHIP_CONSOLIDATED_BREAKDOWN: dfbuilder_rdp.build_index,
     ContentType.OWNERSHIP_CONSOLIDATED_CONCENTRATION: dfbuilder_rdp.build_index,
     ContentType.OWNERSHIP_CONSOLIDATED_INVESTORS: dfbuilder_rdp.build_index,
     ContentType.OWNERSHIP_CONSOLIDATED_RECENT_ACTIVITY: dfbuilder_rdp.build_index,
     ContentType.OWNERSHIP_CONSOLIDATED_SHAREHOLDERS_HISTORY_REPORT: dfbuilder_rdp.build_index,
     ContentType.OWNERSHIP_CONSOLIDATED_SHAREHOLDERS_REPORT: dfbuilder_rdp.build_index,
     ContentType.OWNERSHIP_CONSOLIDATED_TOP_N_CONCENTRATION: dfbuilder_rdp.build_index,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_entire_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_entire_data_provider.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,33 +1,33 @@
 import abc
 from datetime import timedelta
-from typing import Callable, List, Tuple, Any, Optional, TYPE_CHECKING
+from typing import Callable, List, Any, Optional, TYPE_CHECKING
 
-from ._historical_join_responses import historical_join_responses
+from ._content_data_factory import ContentDataFactory
+from ._df_builder import build_empty_df
 from .._content_type import ContentType
 from .._tools import hp_datetime_adapter
 from ..content._intervals import (
     interval_arg_parser,
     get_day_interval_type,
     DayIntervalType,
     Intervals,
 )
-
+from ..delivery._data._data_provider import Response
 
 if TYPE_CHECKING:
-    from ..delivery._data._data_provider import Response
-
+    from ..delivery._data._response import BaseResponse
 
 INTERVALS_BY_SECONDS = {
-    Intervals.ONE_MINUTE.value: 59,
-    Intervals.FIVE_MINUTES.value: 299,
-    Intervals.TEN_MINUTES.value: 599,
-    Intervals.THIRTY_MINUTES.value: 1799,
-    Intervals.SIXTY_MINUTES.value: 3599,
-    Intervals.HOURLY.value: 3599,
+    Intervals.ONE_MINUTE: 59,
+    Intervals.FIVE_MINUTES: 299,
+    Intervals.TEN_MINUTES: 599,
+    Intervals.THIRTY_MINUTES: 1799,
+    Intervals.SIXTY_MINUTES: 3599,
+    Intervals.HOURLY: 3599,
 }
 
 EVENTS_MAX_LIMIT = 10000
 
 
 def remove_last_date_elements(data: List[List[Any]]) -> List[List[Any]]:
     end_date = data[-1][0]
@@ -35,87 +35,70 @@
         if item[0] != end_date:
             data = data[:-index]
             return data
 
     return data
 
 
-def create_raw(responses: List["Response"], entire_data: List[List[Any]]) -> dict:
-    for response in responses:
-        if response.is_success:
-            raw = response.data.raw
-            raw["data"] = entire_data
-            return response.data.raw
-
-    return {}
-
-
 class EntireDataProvider(abc.ABC):
     @abc.abstractmethod
-    def request_with_dates(self, *args) -> Tuple[List["Response"], List[List[Any]]]:
+    def request_with_dates(self, *args) -> Response:
         pass
 
     @abc.abstractmethod
-    def request_with_count(self, *args) -> Tuple[List["Response"], List[List[Any]]]:
+    def request_with_count(self, *args) -> Response:
         pass
 
     @abc.abstractmethod
     def get_request_function(self, **kwargs) -> Optional[Callable]:
         pass
 
     @abc.abstractmethod
     def get_request_function_async(self, **kwargs) -> Optional[Callable]:
         pass
 
-    def get_data(self, provide_data: Callable, **kwargs) -> "Response":
+    def get_data(self, provide_data: Callable, **kwargs) -> Response:
         request_function = self.get_request_function(**kwargs)
 
         if request_function:
-            responses, entire_data = request_function(provide_data, **kwargs)
-            raw = create_raw(responses, entire_data)
+            response = request_function(provide_data, **kwargs)
 
         else:
             response = provide_data(**kwargs)
-            responses = [response]
-            raw = response.data.raw
 
-        response = historical_join_responses(responses, raw)
         return response
 
-    async def get_data_async(self, provide_data: Callable, **kwargs) -> "Response":
+    async def get_data_async(self, provide_data: Callable, **kwargs) -> Response:
         request_function = self.get_request_function_async(**kwargs)
 
         if request_function:
-            responses, entire_data = await request_function(provide_data, **kwargs)
-            raw = create_raw(responses, entire_data)
+            response = await request_function(provide_data, **kwargs)
 
         else:
             response = await provide_data(**kwargs)
-            responses = [response]
-            raw = response.data.raw
 
-        response = historical_join_responses(responses, raw)
         return response
 
 
 class SummariesEntireDataProvider(EntireDataProvider):
     def request_with_dates(
         self,
         provide_data: Callable,
         interval,
         start: str,
         end: str,
         count: Optional[int] = None,
         **kwargs,
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    ) -> Response:
         interval_sec = INTERVALS_BY_SECONDS[interval_arg_parser.get_str(interval)]
 
         entire_data = []
         responses = []
         unique_data_count = set()
+        last_raw = {}
 
         finished_date = hp_datetime_adapter.get_localize(start)
         # need do ... while
         end_date = finished_date + timedelta(microseconds=1)
 
         while end_date > finished_date and len(unique_data_count) <= 1:
             response = provide_data(
@@ -127,18 +110,19 @@
             )
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
             entire_data.extend(data)
 
             if count is not None and len(entire_data) >= count:
                 entire_data = entire_data[:count]
                 break
 
             unique_data_count.add(len(data))
@@ -146,30 +130,36 @@
             end_date = data[-1][0]
             end = end_date
             end_date = hp_datetime_adapter.get_localize(end_date)
 
             if (end_date - finished_date).seconds < interval_sec:
                 break
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
     async def request_with_dates_async(
         self,
         provide_data: Callable,
         interval,
         start: str,
         end: str,
         count: Optional[int] = None,
         **kwargs,
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    ) -> Response:
         interval_sec = INTERVALS_BY_SECONDS[interval_arg_parser.get_str(interval)]
 
         entire_data = []
         responses = []
         unique_data_count = set()
+        last_raw = {}
 
         finished_date = hp_datetime_adapter.get_localize(start)
         # need do ... while
         end_date = finished_date + timedelta(microseconds=1)
 
         while end_date > finished_date and len(unique_data_count) <= 1:
             response = await provide_data(
@@ -181,18 +171,19 @@
             )
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
             entire_data.extend(data)
 
             if count is not None and len(entire_data) >= count:
                 entire_data = entire_data[:count]
                 break
 
             unique_data_count.add(len(data))
@@ -200,31 +191,37 @@
             end_date = data[-1][0]
             end = end_date
             end_date = hp_datetime_adapter.get_localize(end_date)
 
             if (end_date - finished_date).seconds < interval_sec:
                 break
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
     def request_with_count(
         self,
         provide_data: Callable,
         interval,
         count: int,
         end: str,
         start: Optional[str] = None,
         **kwargs,
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    ) -> Response:
         interval_sec = INTERVALS_BY_SECONDS[interval_arg_parser.get_str(interval)]
 
         c = count
         entire_data = []
         responses = []
         unique_data_count = set()
+        last_raw = {}
 
         finished_date = None
         if start:
             finished_date = hp_datetime_adapter.get_localize(start)
 
         while c > 0 and len(unique_data_count) <= 1:
             response = provide_data(
@@ -236,18 +233,19 @@
             )
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
             entire_data.extend(data)
 
             unique_data_count.add(len(data))
 
             c -= len(data)
             count = c
             end_date = data[-1][0]
@@ -255,31 +253,37 @@
 
             if finished_date:
                 end_date = hp_datetime_adapter.get_localize(end_date)
 
                 if (end_date - finished_date).seconds < interval_sec:
                     break
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
     async def request_with_count_async(
         self,
         provide_data: Callable,
         interval,
         count: int,
         end: str,
         start: Optional[str] = None,
         **kwargs,
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    ) -> Response:
         interval_sec = INTERVALS_BY_SECONDS[interval_arg_parser.get_str(interval)]
 
         c = count
         entire_data = []
         responses = []
         unique_data_count = set()
+        last_raw = {}
 
         finished_date = None
         if start:
             finished_date = hp_datetime_adapter.get_localize(start)
 
         while c > 0 and len(unique_data_count) <= 1:
             response = await provide_data(
@@ -291,18 +295,19 @@
             )
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
             entire_data.extend(data)
 
             unique_data_count.add(len(data))
 
             c -= len(data)
             count = c
             end_date = data[-1][0]
@@ -310,30 +315,32 @@
 
             if finished_date:
                 end_date = hp_datetime_adapter.get_localize(end_date)
 
                 if (end_date - finished_date).seconds < interval_sec:
                     break
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
     def get_request_function(
         self,
         interval,
         count: Optional[int] = None,
         start: Optional[str] = None,
         end: Optional[str] = None,
         **kwargs,
     ) -> Optional[Callable]:
         request_function = None
 
-        if (
-            interval is None
-            or get_day_interval_type(interval) is not DayIntervalType.INTRA
-        ):
+        if interval is None or get_day_interval_type(interval) is not DayIntervalType.INTRA:
             return request_function
 
         if start is not None and end is not None:
             request_function = self.request_with_dates
 
         elif count is not None and count > 0:
             request_function = self.request_with_count
@@ -346,18 +353,15 @@
         count: Optional[int] = None,
         start: Optional[str] = None,
         end: Optional[str] = None,
         **kwargs,
     ) -> Optional[Callable]:
         request_function = None
 
-        if (
-            interval is None
-            or get_day_interval_type(interval) is not DayIntervalType.INTRA
-        ):
+        if interval is None or get_day_interval_type(interval) is not DayIntervalType.INTRA:
             return request_function
 
         if start is not None and end is not None:
             request_function = self.request_with_dates_async
 
         elif count is not None and count > 0:
             request_function = self.request_with_count_async
@@ -369,35 +373,37 @@
     def request_with_dates(
         self,
         provide_data: Callable,
         start: str,
         end: str,
         count: Optional[int] = None,
         **kwargs,
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    ) -> Response:
         entire_data = []
         responses = []
+        last_raw = {}
 
         finished_date = hp_datetime_adapter.get_localize(start)
         # need do ... while
         end_date = finished_date + timedelta(microseconds=1)
         response_count = EVENTS_MAX_LIMIT
 
         while end_date > finished_date and response_count >= EVENTS_MAX_LIMIT:
             response = provide_data(count=count, start=start, end=end, **kwargs)
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
 
             response_count = len(data)
             if response_count >= EVENTS_MAX_LIMIT:
                 data = remove_last_date_elements(data)
 
             entire_data.extend(data)
 
@@ -405,44 +411,51 @@
             end = end_date
             end_date = hp_datetime_adapter.get_localize(end_date)
 
             if count is not None and len(entire_data) >= count:
                 entire_data = entire_data[:count]
                 break
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
     async def request_with_dates_async(
         self,
         provide_data: Callable,
         start: str,
         end: str,
         count: Optional[int] = None,
         **kwargs,
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    ) -> Response:
         entire_data = []
         responses = []
+        last_raw = {}
 
         finished_date = hp_datetime_adapter.get_localize(start)
         # need do ... while
         end_date = finished_date + timedelta(microseconds=1)
         response_count = EVENTS_MAX_LIMIT
 
         while end_date > finished_date and response_count >= EVENTS_MAX_LIMIT:
             response = await provide_data(count=count, start=start, end=end, **kwargs)
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
 
             response_count = len(data)
             if response_count >= EVENTS_MAX_LIMIT:
                 data = remove_last_date_elements(data)
 
             entire_data.extend(data)
 
@@ -450,83 +463,100 @@
             end = end_date
             end_date = hp_datetime_adapter.get_localize(end_date)
 
             if count is not None and len(entire_data) >= count:
                 entire_data = entire_data[:count]
                 break
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
-    def request_with_count(
-        self, provide_data: Callable, count: int, start: str, end: str, **kwargs
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    def request_with_count(self, provide_data: Callable, count: int, start: str, end: str, **kwargs) -> Response:
         entire_data = []
         responses = []
         c = count
         response_count = EVENTS_MAX_LIMIT
+        last_raw = {}
 
         while c > 0 and response_count >= EVENTS_MAX_LIMIT:
             response = provide_data(count=count, start=start, end=end, **kwargs)
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
 
             response_count = len(data)
             if response_count >= EVENTS_MAX_LIMIT:
                 data = remove_last_date_elements(data)
 
             entire_data.extend(data)
 
             c -= len(data)
             count = c
             end_date = data[-1][0]
             end = end_date
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
     async def request_with_count_async(
         self, provide_data: Callable, count: int, start: str, end: str, **kwargs
-    ) -> Tuple[List["Response"], List[List[Any]]]:
+    ) -> Response:
         entire_data = []
         responses = []
         c = count
         response_count = EVENTS_MAX_LIMIT
+        last_raw = {}
 
         while c > 0 and response_count >= EVENTS_MAX_LIMIT:
             response = await provide_data(count=count, start=start, end=end, **kwargs)
             responses.append(response)
 
             if not response.is_success:
                 break
 
             raw = response.data.raw
+            last_raw = raw
             if len(raw) == 0 or not raw.get("data"):
                 break
 
-            data = raw["data"]
+            data = list(raw["data"])
 
             response_count = len(data)
             if response_count >= EVENTS_MAX_LIMIT:
                 data = remove_last_date_elements(data)
 
             entire_data.extend(data)
 
             c -= len(data)
             count = c
             end_date = data[-1][0]
             end = end_date
 
-        return responses, entire_data
+        return entire_create_response(
+            responses,
+            last_raw,
+            entire_data,
+            kwargs,
+        )
 
     def get_request_function(
         self,
         count: Optional[int] = None,
         start: Optional[str] = None,
         end: Optional[str] = None,
         **kwargs,
@@ -565,14 +595,57 @@
     ContentType.HISTORICAL_PRICING_INTERDAY_SUMMARIES: SummariesEntireDataProvider(),
     ContentType.HISTORICAL_PRICING_INTRADAY_SUMMARIES: SummariesEntireDataProvider(),
     ContentType.CUSTOM_INSTRUMENTS_INTERDAY_SUMMARIES: SummariesEntireDataProvider(),
     ContentType.CUSTOM_INSTRUMENTS_INTRADAY_SUMMARIES: SummariesEntireDataProvider(),
 }
 
 
+class EntireDataFactory(ContentDataFactory):
+    def get_dfbuilder(self, **__):
+        return build_empty_df
+
+
+multi_entire_data_factory = EntireDataFactory()
+
+
 def get_entire_data_provider(content_type: ContentType) -> EntireDataProvider:
     entire_data_provider = entire_data_provider_by_content_type.get(content_type)
 
     if not entire_data_provider:
         raise ValueError(f"Cannot find entire data provider for {content_type}")
 
     return entire_data_provider
+
+
+def entire_create_response(responses: List["BaseResponse"], last_raw: dict, entire_data: list, kwargs) -> Response:
+    raw = dict(last_raw)
+    raw["data"] = entire_data
+
+    request_messages = []
+    http_responses = []
+    http_statuses = []
+    http_headers = []
+    errors = []
+    is_success = False
+
+    for response in responses:
+        is_success = is_success or response.is_success
+        if response.errors:
+            errors += response.errors
+        request_messages.extend(response.request_message)
+        http_responses.extend(response.http_response)
+        http_statuses.extend(response.http_status)
+        http_headers.extend(response.http_headers)
+
+    return Response(
+        is_success,
+        request_messages,
+        http_responses,
+        http_headers,
+        http_statuses,
+        errors,
+        closure=None,
+        requests_count=len(responses),
+        _data_factory=multi_entire_data_factory,
+        _kwargs=kwargs,
+        _raw=raw,
+    )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_error_parser.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_error_parser.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_historical_content_validator.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_historical_content_validator.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,17 +3,15 @@
 
 from .._tools import cached_property
 from ..delivery._data._data_provider import ContentValidator
 
 if TYPE_CHECKING:
     from ..delivery._data._data_provider import ParsedData
 
-user_has_no_permissions_expr = re.compile(
-    r"TS[A-Z]*\.((Interday)|(Intraday)|(QS))\.UserNotPermission\.[0-9]{5}"
-)
+user_has_no_permissions_expr = re.compile(r"TS[A-Z]*\.((Interday)|(Intraday)|(QS))\.UserNotPermission\.[0-9]{5}")
 
 
 class HistoricalContentValidator(ContentValidator):
     @classmethod
     def user_has_permissions(cls, data: "ParsedData") -> bool:
         content_data = data.content_data
         if isinstance(content_data, list) and len(content_data):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_historical_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_historical_data_provider.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,49 +1,46 @@
 import asyncio
 from abc import abstractmethod
 from concurrent.futures import ThreadPoolExecutor, wait
 from functools import partial
-from typing import List, TYPE_CHECKING
+from typing import List, TYPE_CHECKING, Any, Callable, Dict
 
-from ._content_data import Data
+from ._content_data_factory import ContentDataFactory
 from ._content_data_provider import ContentDataProvider
 from ._entire_data_provider import get_entire_data_provider
-from ._historical_join_responses import historical_join_responses
 from ._intervals import DayIntervalType, get_day_interval_type
 from .._errors import RDError
 from .._tools import fields_arg_parser
-from ..delivery._data._data_provider import DataProvider
-
+from ..delivery._data._data_provider import Response
+from ..delivery._data._response import create_response
 
 if TYPE_CHECKING:
-    from .._content_type import ContentType
-    from ._df_build_type import DFBuildType
-    from .._types import Strings
-    from ..delivery._data._data_provider import Response
+    import pandas as pd
+    from ._content_data import Data
     from ._historical_df_builder import HistoricalBuilder
 
 
 def copy_fields(fields: List[str]) -> List[str]:
     if fields is None:
         return []
 
     if not isinstance(fields, (list, str)):
         raise AttributeError(f"fields not support type {type(fields)}")
     fields = fields_arg_parser.get_list(fields)
 
     return fields[:]
 
 
-def get_first_success_response(responses: List["Response"]) -> "Response":
+def get_first_success_response(responses: List[Response]) -> Response:
     successful = (response for response in responses if response.is_success)
     first_successful = next(successful, None)
     return first_successful
 
 
-def validate_responses(responses: List["Response"]):
+def validate_responses(responses: List[Response]):
     response = get_first_success_response(responses)
 
     if response is None:
         error_message = "ERROR: No successful response.\n"
 
         error_codes = set()
 
@@ -62,21 +59,47 @@
 
         error_message = error_message[:-2]
         error = RDError(1, f"No data to return, please check errors: {error_message}")
         error.response = responses
         raise error
 
 
+class HistoricalDataFactoryMultiResponse(ContentDataFactory):
+    def get_dfbuilder(
+        self, responses: List[Response] = None, **kwargs
+    ) -> Callable[[Any, Dict[str, Any]], "pd.DataFrame"]:
+        df_builder: "HistoricalBuilder" = super().get_dfbuilder(**kwargs)
+
+        if len(responses) == 1:
+            return df_builder.build_one
+
+        else:
+            return df_builder.build
+
+    def create_data_success(self, raw: Any, **kwargs) -> "Data":
+        responses: List[Response] = kwargs.get("responses")
+
+        if responses is None:
+            raise ValueError("Cannot get df_builder, responses in None.")
+
+        if len(responses) == 1:
+            raw = raw[0]
+
+        return self.data_class(raw=raw, _dfbuilder=self.get_dfbuilder(**kwargs), _kwargs=kwargs)
+
+
 class HistoricalDataProvider(ContentDataProvider):
+    data_factory_multi_response = HistoricalDataFactoryMultiResponse()
+
     @abstractmethod
     def _get_axis_name(self, interval, **kwargs) -> str:
         # for override
         pass
 
-    def get_data(self, *args, **kwargs) -> "Response":
+    def get_data(self, *args, **kwargs) -> Response:
         universe: List[str] = kwargs.pop("universe", [])
         entire_data_provider = get_entire_data_provider(kwargs.get("__content_type__"))
 
         with ThreadPoolExecutor(thread_name_prefix="HistoricalRequestThread") as ex:
             futures = []
             for inst_name in universe:
                 fut = ex.submit(
@@ -92,89 +115,44 @@
             responses = []
             for fut in futures:
                 exception = fut.exception()
 
                 if exception:
                     raise exception
 
-                responses.append(fut.result())
+                response = fut.result()
+                responses.append(response)
 
         validate_responses(responses)
 
-        return self._process_responses(
-            responses,
-            universe,
-            copy_fields(kwargs.get("fields")),
-            kwargs.get("interval"),
-            kwargs.get("__content_type__"),
-            kwargs.get("__dfbuild_type__"),
-        )
+        kwargs["responses"] = responses
+        kwargs["universe"] = universe
+        kwargs["fields"] = copy_fields(kwargs.get("fields"))
+        kwargs["axis_name"] = self._get_axis_name(kwargs.get("interval"))
+        return create_response(responses, self.data_factory_multi_response, kwargs)
 
-    async def get_data_async(self, *args, **kwargs) -> "Response":
+    async def get_data_async(self, *args, **kwargs) -> Response:
         universe: List[str] = kwargs.pop("universe", [])
         entire_data_provider = get_entire_data_provider(kwargs.get("__content_type__"))
 
         tasks = []
         for inst_name in universe:
             tasks.append(
                 entire_data_provider.get_data_async(
                     partial(super().get_data_async, *args), universe=inst_name, **kwargs
                 )
             )
 
         responses = await asyncio.gather(*tasks)
-        if len(responses) == 1 and get_first_success_response(responses) is None:
-            return responses.pop()
-
-        return self._process_responses(
-            responses,
-            universe,
-            copy_fields(kwargs.get("fields")),
-            kwargs.get("interval"),
-            kwargs.get("__content_type__"),
-            kwargs.get("__dfbuild_type__"),
-        )
-
-    def _process_responses(
-        self,
-        responses: List["Response"],
-        universe: "Strings",
-        fields: "Strings",
-        interval,
-        content_type: "ContentType",
-        dfbuild_type: "DFBuildType",
-    ) -> "Response":
-        df_builder: "HistoricalBuilder" = self.response.get_dfbuilder(
-            content_type, dfbuild_type
-        )
-
-        if len(responses) == 1:
-            raw = responses[0].data.raw
-            data = Data(
-                raw,
-                dfbuilder=partial(
-                    df_builder.build_one,
-                    fields=fields,
-                    axis_name=self._get_axis_name(interval),
-                ),
-            )
-        else:
-            raws = [response.data.raw for response in responses]
-            data = Data(
-                raws,
-                dfbuilder=partial(
-                    df_builder.build,
-                    universe=universe,
-                    fields=fields,
-                    axis_name=self._get_axis_name(interval),
-                ),
-            )
 
-        response = historical_join_responses(responses, data)
-        return response
+        kwargs["responses"] = responses
+        kwargs["universe"] = universe
+        kwargs["fields"] = copy_fields(kwargs.get("fields"))
+        kwargs["axis_name"] = self._get_axis_name(kwargs.get("interval"))
+        return create_response(responses, self.data_factory_multi_response, kwargs)
 
 
 field_timestamp_by_day_interval_type = {
     DayIntervalType.INTER: "DATE",
     DayIntervalType.INTRA: "DATE_TIME",
 }
 
@@ -194,26 +172,22 @@
     return ",".join(result)
 
 
 def get_fields_summaries(fields, **kwargs):
     fields = fields_arg_parser.get_list(fields)
     result = copy_fields(fields)
     interval = kwargs.get("interval")
-    field_timestamp = field_timestamp_by_day_interval_type.get(
-        get_day_interval_type(interval or DayIntervalType.INTER)
-    )
+    field_timestamp = field_timestamp_by_day_interval_type.get(get_day_interval_type(interval or DayIntervalType.INTER))
     if field_timestamp not in result:
         result.append(field_timestamp)
     return ",".join(result)
 
 
 class SummariesDataProvider(HistoricalDataProvider):
     def _get_axis_name(self, interval, **kwargs):
-        axis_name = axis_by_day_interval_type.get(
-            get_day_interval_type(interval or DayIntervalType.INTER)
-        )
+        axis_name = axis_by_day_interval_type.get(get_day_interval_type(interval or DayIntervalType.INTER))
         return axis_name
 
 
 class EventsDataProvider(HistoricalDataProvider):
     def _get_axis_name(self, interval, **kwargs):
         return "Timestamp"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_historical_response_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_response_factory.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,51 +1,55 @@
 from typing import TYPE_CHECKING
 
-from ._content_response_factory import ContentResponseFactory
+from .._content_response_factory import ContentResponseFactory
 
 if TYPE_CHECKING:
-    from ..delivery._data._data_provider import ParsedData
+    from ...delivery._data._data_provider import ParsedData
 
 error_message_by_code = {
-    "default": "{error_message}. Requested ric: {rics}. Requested fields: {fields}",
-    "TS.Intraday.UserRequestError.90001": "{rics} - The universe is not found",
-    "TS.Intraday.Warning.95004": "{rics} - Trades interleaving with corrections is currently not supported. Corrections will not be returned.",
-    "TS.Intraday.UserRequestError.90006": "{error_message} Requested ric: {rics}",
+    "default": "{error_message} Requested universes: {universes}. Requested fields: {fields}",
+    412: "Unable to resolve all requested identifiers in {universes}.",
+    218: "Unable to resolve all requested fields in {fields}. The formula must "
+    "contain at least one field or function.",
 }
 
 
-class HistoricalResponseFactory(ContentResponseFactory):
-    def get_raw(self, data: "ParsedData") -> dict:
-        return data.content_data[0] if data.content_data else {}
+class DataGridResponseFactory(ContentResponseFactory):
+    def create_fail(self, parsed_data: "ParsedData", universe=None, fields=None, **kwargs):
+        error_code = parsed_data.first_error_code
+        if error_code not in error_message_by_code.keys():
+            parsed_data.error_messages = error_message_by_code["default"].format(
+                error_message=parsed_data.first_error_message,
+                fields=fields,
+                universes=universe,
+            )
+        else:
+            parsed_data.error_messages = error_message_by_code[error_code].format(fields=fields, universes=universe)
+
+        return super().create_fail(parsed_data, **kwargs)
+
 
+class DataGridRDPResponseFactory(DataGridResponseFactory):
     def create_success(self, parsed_data: "ParsedData", **kwargs):
-        self._try_write_error(parsed_data, **kwargs)
-        return super().create_success(parsed_data, **kwargs)
+        inst = super().create_success(parsed_data, **kwargs)
+        descriptions = self.get_raw(parsed_data).get("messages", {}).get("descriptions", [])
+        for descr in descriptions:
+            code = descr.get("code")
+            if code in {416, 413}:
+                inst.errors.append((code, descr.get("description")))
+
+        return inst
 
-    def create_fail(self, parsed_data: "ParsedData", **kwargs):
-        self._try_write_error(parsed_data, **kwargs)
-        return super().create_fail(parsed_data, **kwargs)
 
-    def _try_write_error(
-        self, parsed_data: "ParsedData", universe=None, fields=None, **kwargs
-    ):
-        raw = self.get_raw(parsed_data)
-        error_code = parsed_data.first_error_code or raw.get("status", {}).get("code")
-
-        if not error_code:
-            return
-
-        error_message = parsed_data.first_error_message or raw.get("status", {}).get(
-            "message"
-        )
-        rics = raw.get("universe", {}).get("ric", universe)
-        parsed_data.error_codes = error_code
+class DataGridUDFResponseFactory(DataGridResponseFactory):
+    def get_raw(self, parsed_data: "ParsedData"):
+        return parsed_data.content_data.get("responses", [{}])[0]
 
-        if error_code not in error_message_by_code.keys():
-            parsed_data.error_messages = error_message_by_code["default"].format(
-                error_message=error_message, rics=rics, fields=fields
-            )
+    def create_success(self, parsed_data: "ParsedData", **kwargs):
+        inst = super().create_success(parsed_data, **kwargs)
+        error = self.get_raw(parsed_data).get("error", [])
+        for err in error:
+            code = err.get("code")
+            if code == 416:
+                inst.errors.append((code, err.get("message")))
 
-        else:
-            parsed_data.error_messages = error_message_by_code[error_code].format(
-                rics=rics, error_message=error_message
-            )
+        return inst
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_intervals.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_intervals.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,19 +1,20 @@
 from enum import Enum
 from typing import Union
 
 from .._tools import make_enum_arg_parser
+from .._base_enum import StrEnum
 
 
 class DayIntervalType(Enum):
     INTRA = 0
     INTER = 1
 
 
-class Intervals(Enum):
+class Intervals(StrEnum):
     """
     The list of interval types of the boundary is described below.
 
     The supported values of intervals :
 
         Time:
 
@@ -66,26 +67,24 @@
     THREE_MONTHS = "P3M"
     QUARTERLY = "P3M"
     TWELVE_MONTHS = "P12M"
     YEARLY = "P1Y"
     ONE_YEAR = "P1Y"
 
 
-_ISO8601_INTERVALS = [k.value for k in Intervals]
+_ISO8601_INTERVALS = [k for k in Intervals]
 """['PT1M', 'PT5M', 'PT10M', 'PT30M', 'PT60M', 'PT1H']"""
 _INTRADAY = _ISO8601_INTERVALS[:6]
 """['P1D', 'P7D', 'P1W', 'P1M', 'P3M', 'P12M', 'P1Y']"""
 _INTERDAY = _ISO8601_INTERVALS[6:]
 
 interval_arg_parser = make_enum_arg_parser(Intervals, can_be_lower=True)
 
 
-def get_day_interval_type(
-    interval: Union[str, Intervals, DayIntervalType]
-) -> DayIntervalType:
+def get_day_interval_type(interval: Union[str, Intervals, DayIntervalType]) -> DayIntervalType:
     if isinstance(interval, DayIntervalType):
         return interval
 
     interval = interval_arg_parser.get_str(interval)
 
     if interval in _INTRADAY:
         day_interval_type = DayIntervalType.INTRA
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_join_responses.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_response.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,74 +1,75 @@
-__all__ = "ContentUsageLoggerMixin"
+from dataclasses import dataclass
+from typing import TYPE_CHECKING, List, TypeVar, Generic, Any, Union
 
-from types import SimpleNamespace
-from typing import List, Callable
+from ._endpoint_data import Error
+from ..._tools import cached_property
 
-import pandas as pd
+if TYPE_CHECKING:
+    from ._data_factory import BaseDataFactory
+    import httpx
 
-from ._content_data import Data
-from ..delivery._data._parsed_data import ParsedData
-from ..delivery._data._response import Response
-
-
-def join_responses(
-    responses: List[Response],
-    join_dataframes: Callable = pd.concat,
-    response_class=Response,
-    data_class=Data,
-    reset_index=False,
-    limit: int = None,
-) -> Response:
-    def build_df(*args, **kwargs):
-        dfs = []
-        df = None
-
-        for response in responses:
-            dfs.append(response.data.df)
-
-        all_dfs_is_none = all(a is None for a in dfs)
-        if not all_dfs_is_none:
-            df = join_dataframes(dfs)
-
-        if reset_index and df is not None:
-            df = df.reset_index(drop=True)
-        if limit:
-            df = df[:limit]
-        return df
+TypeData = TypeVar("TypeData")
 
-    if len(responses) == 1:
-        return responses[0]
+
+@dataclass
+class BaseResponse(Generic[TypeData]):
+    is_success: bool
+    request_message: Union[List["httpx.Request"], "httpx.Request"]
+    http_response: Union[List["httpx.Response"], "httpx.Response"]
+    http_headers: Union[List["httpx.Headers"], "httpx.Headers"]
+    http_status: Union[List[dict], dict]
+    errors: List[Error]
+    closure: Union[str, None]
+    requests_count: int
+    _data_factory: "BaseDataFactory"
+    _kwargs: dict
+    _raw: Any
+
+    @cached_property
+    def data(self) -> TypeData:
+        return self._data_factory.create_data(self._raw, owner_=self, **self._kwargs)
+
+
+class Response(BaseResponse[TypeData]):
+    pass
+
+
+def create_response(responses: List[BaseResponse], data_factory: "BaseDataFactory", kwargs: dict) -> Response:
+    from ._response_factory import get_closure
 
     raws = []
-    http_statuses = []
-    http_headers = []
     request_messages = []
     http_responses = []
+    http_statuses = []
+    http_headers = []
     errors = []
-    is_successes = []
+    is_success = False
+    closure = None
+    once = False
 
     for response in responses:
+        is_success = is_success or response.is_success
         raws.append(response.data.raw)
-        http_statuses.append(response.http_status)
-        http_headers.append(response.http_headers)
-        request_messages.append(response.request_message)
-        http_responses.append(response.http_response)
-        is_successes.append(response.is_success)
-
         if response.errors:
             errors += response.errors
-
-    raw_response = SimpleNamespace()
-    raw_response.headers = http_headers
-    raw_response.request = request_messages
-    is_success = any(is_successes)
-    data_factory = SimpleNamespace()
-    response = response_class(
-        is_success, ParsedData(http_statuses, raw_response), data_factory
-    )
-    data_factory.create_data = lambda *args, **kwargs: data_class(
-        raws, build_df, owner_=response
+        request_messages.append(response.request_message)
+        http_responses.append(response.http_response)
+        http_statuses.append(response.http_status)
+        http_headers.append(response.http_headers)
+        if not once:
+            closure = get_closure(response.http_response)
+            once = True
+
+    return Response(
+        is_success,
+        request_messages,
+        http_responses,
+        http_headers,
+        http_statuses,
+        errors,
+        closure=closure,
+        requests_count=len(responses),
+        _data_factory=data_factory,
+        _kwargs=kwargs,
+        _raw=raws,
     )
-    response.errors += errors
-    response.http_response = http_responses
-
-    return response
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_universe_content_validator.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_universe_content_validator.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_universe_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_universe_stream.py`

 * *Files 6% similar despite different names*

```diff
@@ -17,16 +17,16 @@
 if TYPE_CHECKING:
     from ..delivery.omm_stream import ContribType, ContribResponse
     from .._core.session import Session
 
 _id_iterator = itertools.count()
 
 # regular expression pattern for intra-field position sequence
-_partial_update_intra_field_positioning_sequence_regular_expression_pattern = (
-    re.compile(r"[\x1b\x5b|\x9b]([0-9]+)\x60([^\x1b^\x5b|\x9b]+)")
+_partial_update_intra_field_positioning_sequence_regular_expression_pattern = re.compile(
+    r"[\x1b\x5b|\x9b]([0-9]+)\x60([^\x1b^\x5b|\x9b]+)"
 )
 
 
 def _decode_intra_field_position_sequence(cached_value: str, new_value: str):
     # find all partial update in the value
     tokens = _partial_update_intra_field_positioning_sequence_regular_expression_pattern.findall(
         new_value,
@@ -35,31 +35,27 @@
     # check this value contains a partial update or not?
     if len(tokens) == 0:
         # no partial update required, so done
         return new_value
 
     # do a partial update
     updated_value = cached_value
-    for (offset, replace) in tokens:
+    for offset, replace in tokens:
         # convert offset from str to int
         offset = int(offset)
         assert offset < len(updated_value)
 
         # replace the value in the string
-        updated_value = (
-            updated_value[:offset] + replace + updated_value[offset + len(replace) :]
-        )
+        updated_value = updated_value[:offset] + replace + updated_value[offset + len(replace) :]
 
     # done, return
     return updated_value
 
 
-class _UniverseStream(
-    StreamCache, StreamStateManager, OMMStreamListener["_UniverseStream"]
-):
+class _UniverseStream(StreamCache, StreamStateManager, OMMStreamListener["_UniverseStream"]):
     def __init__(
         self,
         content_type,
         name,
         session=None,
         fields=None,
         service=None,
@@ -230,33 +226,27 @@
             fields = self._filter_fields(fields)
             message["Fields"] = fields
         self._record = message
 
         if DEBUG:
             fields = self._record.get("Fields", [])
             num_fields = len(fields)
-            self._debug(
-                f"|>|>|>|>|>|>{self._classname} "
-                f"has fields in record {num_fields} after refresh"
-            )
+            self._debug(f"|>|>|>|>|>|>{self._classname} has fields in record {num_fields} after refresh")
 
         return message.get("Fields")
 
     def _do_on_stream_status(self, stream: "_OMMStream", message: dict, *_) -> Any:
         self._status = message
         return message
 
     def _do_on_stream_update(self, stream: "_OMMStream", message: dict, *args) -> Any:
         if DEBUG:
             fields = self._record.get("Fields", [])
             num_fields = len(fields)
-            self._debug(
-                f"|>|>|>|>|>|> {self._classname} "
-                f"has fields in record {num_fields} after update"
-            )
+            self._debug(f"|>|>|>|>|>|> {self._classname} has fields in record {num_fields} after update")
 
         self._write_to_record(message)
         return message.get("Fields")
 
     def send_open_message(self):
         self._stream.send_open_message()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/_universe_streams.py` & `refinitiv-data-1.2.0/refinitiv/data/content/_universe_streams.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,24 +30,23 @@
     StreamStateManager,
     OMMStreamListener,
     StreamStateEvent,
     StreamEvent,
 )
 from ..delivery._stream.base_stream import StreamOpenWithUpdatesMixin
 from ..delivery.omm_stream import ContribResponse
+from ..delivery.omm_stream import ErrorContribResponse
 
 if TYPE_CHECKING:
     from ..delivery.omm_stream import ContribType
 
 _id_iterator = itertools.count()
 
 
-def build_df(
-    universe: Strings, fields: Strings, values_by_field: dict, convert: bool
-) -> pd.DataFrame:
+def build_df(universe: Strings, fields: Strings, values_by_field: dict, convert: bool) -> pd.DataFrame:
     data = []
     for inst_name in universe:
         items = []
 
         for values in values_by_field.values():
             item = values.pop(0)
 
@@ -193,17 +192,15 @@
 
         self._universe: Strings = universe_arg_parser.get_list(universe)
         self.fields: Strings = fields_arg_parser.get_list(fields or [])
         self._service = service
         self._api = api
         self._extended_params = extended_params
         self._id = next(_id_iterator)
-        self._classname: str = (
-            f"[{self.__class__.__name__} id={self._id} universe={universe}]"
-        )
+        self._classname: str = f"[{self.__class__.__name__} id={self._id} universe={universe}]"
         self._completed = set()
         self._content_type = content_type
         self._item_facade_class = item_facade_class
         self._lock_insts_fields = threading.Lock()
 
     @property
     def session(self) -> "Session":
@@ -275,17 +272,15 @@
             stream._wrapper = wrapper
 
         return wrapper
 
     def __len__(self):
         return len(self._stream_by_name)
 
-    def _get_value_by_fields_by_inst_names(
-        self, universe: "Strings", fields: "Strings"
-    ) -> dict:
+    def _get_value_by_fields_by_inst_names(self, universe: "Strings", fields: "Strings") -> dict:
         retval = {}
         for name in universe:
             stream = self._stream_by_name.get(name)
 
             values_by_fields = stream.get_fields(fields)
             if values_by_fields:
                 retval[name] = values_by_fields
@@ -337,54 +332,48 @@
                 validate("Instrument", universe, self.universe)
             except ItemWasNotRequested as e:
                 self._error(e)
                 return pd.DataFrame()
         else:
             universe = self.universe
 
-        value_by_fields_by_inst_names: dict = self._get_value_by_fields_by_inst_names(
-            universe, fields
-        )
+        value_by_fields_by_inst_names: dict = self._get_value_by_fields_by_inst_names(universe, fields)
 
         if fields:
             fields = try_copy_to_list(fields)
             fields = fields_arg_parser.get_unique(fields or [])
 
             try:
                 self.fields and validate("Field", fields, self.fields)
             except ItemWasNotRequested as e:
                 self._error(e)
                 return pd.DataFrame()
         else:
             fields = get_available_fields(value_by_fields_by_inst_names)
 
         values_by_field = {
-            field: [
-                value_by_fields_by_inst_names.get(name, {}).get(field)
-                for name in universe
-            ]
-            for field in fields
+            field: [value_by_fields_by_inst_names.get(name, {}).get(field) for name in universe] for field in fields
         }
 
         return build_df(universe, fields, values_by_field, convert)
 
     def contribute(
         self,
         name: str,
         fields: dict,
         contrib_type: Union[str, "ContribType", None] = None,
         post_user_info: Optional[dict] = None,
     ) -> "ContribResponse":
         self._debug(f"{self._classname} contribute on {name}")
         stream = self._stream_by_name.get(name)
-        if stream:
-            return stream.contribute(fields, contrib_type, post_user_info)
-        else:
+        if stream is None:
             self._error(f"Can't contribute to unsubscribed item {name}")
-            return ContribResponse({})
+            return ErrorContribResponse({"Text": f"Can't contribute to unsubscribed item {name}"})
+        else:
+            return stream.contribute(fields, contrib_type, post_user_info)
 
     async def contribute_async(
         self,
         name: str,
         fields: dict,
         contrib_type: Union[str, "ContribType", None] = None,
         post_user_info: Optional[dict] = None,
@@ -403,35 +392,28 @@
         self._debug(f"{self._classname} open streaming on {self.universe}")
 
         if not self.values():
             raise ValueError("No instrument to subscribe")
 
         self._completed.clear()
 
-        with ThreadPoolExecutor(
-            thread_name_prefix="OpenUniverseStreams-Thread"
-        ) as executor:
-            futures = [
-                executor.submit(stream.open, with_updates=with_updates)
-                for stream in self.values()
-            ]
+        with ThreadPoolExecutor(thread_name_prefix="OpenUniverseStreams-Thread") as executor:
+            futures = [executor.submit(stream.open, with_updates=with_updates) for stream in self.values()]
             wait(futures)
             for fut in futures:
                 exception = fut.exception()
                 if exception:
                     raise exception
 
         self._debug(f"{self._classname} streaming on {self.universe} is open")
 
     def _do_close(self, *args, **kwargs) -> None:
         self._debug(f"{self._classname} close streaming on {str(self.universe)}")
 
-        with ThreadPoolExecutor(
-            thread_name_prefix="CloseUniverseStreams-Thread"
-        ) as executor:
+        with ThreadPoolExecutor(thread_name_prefix="CloseUniverseStreams-Thread") as executor:
             futures = [executor.submit(stream.close) for stream in self.values()]
             wait(futures)
             for fut in futures:
                 exception = fut.exception()
                 if exception:
                     raise exception
 
@@ -491,34 +473,30 @@
 
     def _remove_fields_from_record(self, fields):
         for name in self._stream_by_name:
             self._stream_by_name[name].remove_fields_from_record(fields)
 
     def add_fields(self, fields) -> None:
         with self._lock_insts_fields:
-
             fields = fields_arg_parser.get_list(fields)
 
             exists_fields = set(fields) & set(self.fields)
             if exists_fields:
                 self._error(f"{exists_fields} already in fields list")
 
-            fields = [
-                i for i in fields if i not in self.fields
-            ]  # universe should be unique
+            fields = [i for i in fields if i not in self.fields]  # universe should be unique
             self.fields.extend(fields)
             if self.is_open:
                 if isinstance(self._session, DesktopSession):
                     self._update_fields_desktop_session()
                 else:
                     self._update_fields_platform_session()
 
     def remove_fields(self, fields) -> None:
         with self._lock_insts_fields:
-
             fields = fields_arg_parser.get_list(fields)
 
             not_exists_fields = set(fields) - set(self.fields)
             if not_exists_fields:
                 self._error(f"{not_exists_fields} not in fields list")
 
             fields = [i for i in fields if i in self.fields]
@@ -563,28 +541,26 @@
             stream = self._stream_by_name[instrument]
             stream.off(StreamStateEvent.CLOSED, self._on_stream_close)
             stream.close()
         del self._stream_by_name[instrument]
 
     def add_instruments(self, instruments) -> None:
         with self._lock_insts_fields:
-
             instruments = universe_arg_parser.get_list(instruments)
 
             if self.is_unopened:
                 # universe update is enough, cache will create when stream open
                 self._universe.extend(instruments)
                 return
 
             for instrument in instruments:
                 self._add_instrument(instrument)
 
     def remove_instruments(self, instruments) -> None:
         with self._lock_insts_fields:
-
             instruments = universe_arg_parser.get_list(instruments)
 
             if not self._universe:
                 self._error("nothing to delete")
                 return
 
             for instrument in instruments:
@@ -594,13 +570,11 @@
 class StreamIterator:
     def __init__(self, stream: _UniverseStreams):
         self._streaming_prices = stream
         self._index = 0
 
     def __next__(self):
         if self._index < len(self._streaming_prices._universe):
-            result = self._streaming_prices[
-                self._streaming_prices._universe[self._index]
-            ]
+            result = self._streaming_prices[self._streaming_prices._universe[self._index]]
             self._index += 1
             return result
         raise StopIteration()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_custom_instruments_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_custom_instruments_data_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -58,17 +58,15 @@
 
 # a20140be-3648-4892-9d1b-ce78ee8617fd
 is_instrument_id = re.compile(r"[a-z0-9]{8}(-[a-z0-9]{4}){3}-[a-z0-9]{12}")
 
 # S)INST.GESG1-0000
 symbol_with_user_id = re.compile(r".*\.[A-Z0-9]+-[A-Z0-9]+")
 
-wrong_uuid_regexp = re.compile(
-    r"(Validation Error: .UUID suffix ).*( not matched with userID)"
-)
+wrong_uuid_regexp = re.compile(r"(Validation Error: .UUID suffix ).*( not matched with userID)")
 wrong_symbol = "S)Instrument.UUID-0000"
 
 
 def provide_session(func):
     def _func(value, session, *args, **kwargs):
         return func(value, session)
 
@@ -107,26 +105,20 @@
 #   Request factory
 # --------------------------------------------------------------------------------------
 def has_all_error_user_id(response):
     return all(wrong_uuid_regexp.match(error.message) for error in response.errors)
 
 
 def check_response(response, config):
-    return (
-        None
-        if has_all_error_user_id(response)
-        else default_check_response(response, config)
-    )
+    return None if has_all_error_user_id(response) else default_check_response(response, config)
 
 
 def get_user_id(session=None) -> str:
     session = get_valid_session(session)
-    response = get_data_by_data_type(
-        ContentType.CUSTOM_INSTRUMENTS_INSTRUMENTS, session, universe=wrong_symbol
-    )
+    response = get_data_by_data_type(ContentType.CUSTOM_INSTRUMENTS_INSTRUMENTS, session, universe=wrong_symbol)
     check_response(response, session.config)
     errors = response.errors
     messages = [error.message for error in errors]
     user_id = ""
     for message in messages:
         if wrong_uuid_regexp.match(message):
             _, user_id = message.rsplit(" ", 1)
@@ -237,25 +229,21 @@
 
 # --------------------------------------------------------------------------------------
 #   Raw data parser
 # --------------------------------------------------------------------------------------
 
 
 class CustomInstsParser(Parser):
-    def parse_raw_response(
-        self, raw_response: "httpx.Response"
-    ) -> Tuple[bool, ParsedData]:
+    def parse_raw_response(self, raw_response: "httpx.Response") -> Tuple[bool, ParsedData]:
         is_success = False
 
         if raw_response is None:
             return is_success, ParsedData({}, {})
 
-        is_success = raw_response.status_code in success_http_codes + [
-            requests.codes.no_content
-        ]
+        is_success = raw_response.status_code in success_http_codes + [requests.codes.no_content]
 
         if is_success:
             parsed_data = self.process_successful_response(raw_response)
 
         else:
             parsed_data = self.process_failed_response(raw_response)
 
@@ -300,17 +288,15 @@
             error_message = raw_response.text
 
         if error_code == 403:
             if not error_message.endswith("."):
                 error_message += ". "
             error_message += "Contact Refinitiv to check your permissions."
 
-        return ParsedData(
-            status, raw_response, error_codes=error_code, error_messages=error_message
-        )
+        return ParsedData(status, raw_response, error_codes=error_code, error_messages=error_message)
 
 
 # --------------------------------------------------------------------------------------
 #   Content data validator
 # --------------------------------------------------------------------------------------
 
 
@@ -346,30 +332,22 @@
         return extend_params(query_parameters, extended_params)
 
     def extend_body_parameters(self, body_parameters, **kwargs):
         return body_parameters
 
 
 custom_insts_events_query_params = [
-    ValueParamItem(
-        "start", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true
-    ),
-    ValueParamItem(
-        "end", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true
-    ),
+    ValueParamItem("start", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true),
+    ValueParamItem("end", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true),
     ValueParamItem("count", function=check_count),
 ]
 custom_insts_summaries_query_params = [
     ValueParamItem("interval", function=interval_arg_parser.get_str),
-    ValueParamItem(
-        "start", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true
-    ),
-    ValueParamItem(
-        "end", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true
-    ),
+    ValueParamItem("start", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true),
+    ValueParamItem("end", function=custom_inst_datetime_adapter.get_str, is_true=is_date_true),
     ValueParamItem("count", function=check_count),
 ]
 
 custom_insts_body_params = [
     ParamItem("exchange_name", "exchangeName"),
     ParamItem("instrument_name", "instrumentName"),
     ParamItem("time_zone", "timeZone"),
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_enums.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_enums.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,28 +1,28 @@
-from enum import Enum
+from ..._base_enum import StrEnum
 
 
-class CustomInstrumentTypes(Enum):
+class CustomInstrumentTypes(StrEnum):
     Formula = "formula"
     UDC = "udc"
     Basket = "basket"
 
 
-class SpreadAdjustmentMethod(Enum):
+class SpreadAdjustmentMethod(StrEnum):
     CLOSE_TO_CLOSE = "close-to-close"
     OPEN_TO_OPEN = "open-to-open"
     CLOSE_TO_OPEN = "close-to-open"
     CLOSE_TO_OPEN_OLD_GAP = "close-to-open-old-gap"
     CLOSE_TO_OPEN_NEW_GAP = "close-to-open-new-gap"
 
 
-class VolumeBasedRolloverMethod(Enum):
+class VolumeBasedRolloverMethod(StrEnum):
     VOLUME = "volume"
     OPEN_INTEREST = "openInterest"
     VOLUME_AND_OPEN_INTEREST = "volumeAndOpenInterest"
     VOLUME_OR_OPEN_INTEREST = "volumeOrOpenInterest"
 
 
-class DayBasedRolloverMethod(Enum):
+class DayBasedRolloverMethod(StrEnum):
     DAYS_BEFORE_EXPIRY = "daysBeforeExpiry"
     DAYS_BEFORE_END_OF_MONTH = "daysBeforeEndOfMonth"
     DAYS_AFTER_BEGINNING_OF_MONTH = "daysAfterBeginningOfMonth"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_events.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_events.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,22 +1,20 @@
 from typing import TYPE_CHECKING
 
 from .._content_data import Data
 from .._content_provider_layer import ContentUsageLoggerMixin
 from ..._content_type import ContentType
-from ..._tools import custom_insts_historical_universe_parser, try_copy_to_list
+from ..._tools import custom_insts_historical_universe_parser, try_copy_to_list, custom_inst_datetime_adapter
 from ...delivery._data._data_provider import DataProviderLayer, BaseResponse
 
 if TYPE_CHECKING:
     from ..._types import OptDateTime, StrStrings, OptInt, ExtendedParams, OptStrStrs
 
 
-class Definition(
-    ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]
-):
+class Definition(ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]):
     """
     Summary line of this class that defines parameters for requesting events from custom instruments
 
     Parameters
     ----------
     universe : str or list
         The Id or Symbol of custom instrument to operate on
@@ -45,14 +43,16 @@
         universe: "StrStrings",
         start: "OptDateTime" = None,
         end: "OptDateTime" = None,
         count: "OptInt" = None,
         fields: "OptStrStrs" = None,
         extended_params: "ExtendedParams" = None,
     ):
+        start = custom_inst_datetime_adapter.get_localize(start)
+        end = custom_inst_datetime_adapter.get_localize(end)
         fields = try_copy_to_list(fields)
         universe = try_copy_to_list(universe)
         universe = custom_insts_historical_universe_parser.get_list(universe)
         super().__init__(
             data_type=ContentType.CUSTOM_INSTRUMENTS_EVENTS,
             universe=universe,
             start=start,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_instrument_class.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_instrument_class.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_instrument_prop_classes.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_instrument_prop_classes.py`

 * *Files 1% similar despite different names*

```diff
@@ -100,26 +100,21 @@
     """
 
     constituents: List[Union[Constituent, dict]]
     normalize_by_weight: bool = False
 
     def _to_dict(self) -> dict:
         return {
-            "constituents": [
-                item._to_dict() if isinstance(item, Constituent) else item
-                for item in self.constituents
-            ],
+            "constituents": [item._to_dict() if isinstance(item, Constituent) else item for item in self.constituents],
             "normalizeByWeight": self.normalize_by_weight,
         }
 
     @classmethod
     def _from_dict(cls, data: dict) -> "Basket":
-        constituents = [
-            Constituent._from_dict(constituent) for constituent in data["constituents"]
-        ]
+        constituents = [Constituent._from_dict(constituent) for constituent in data["constituents"]]
         normalize_by_weight = data["normalizeByWeight"]
         return cls(constituents=constituents, normalize_by_weight=normalize_by_weight)
 
 
 @dataclass
 class Months(Serializable["Months"]):
     """
@@ -185,30 +180,26 @@
     roll_on_expiry: bool = True
     number_of_days: int = 1
     join_at_day: int = 1
 
     def _to_dict(self) -> dict:
         return {
             "volumeBased": {
-                "method": volume_based_rollover_method_enum_arg_parser.get_str(
-                    self.method
-                ),
+                "method": volume_based_rollover_method_enum_arg_parser.get_str(self.method),
                 "numberOfDays": self.number_of_days,
                 "joinAtDay": self.join_at_day,
                 "rollOccursWithinMonths": self.roll_occurs_within_months,
                 "rollOnExpiry": self.roll_on_expiry,
             }
         }
 
     @classmethod
     def _from_dict(cls, data: dict) -> "VolumeBasedRollover":
         data = convert_camel_to_snake(data)
-        data["method"] = volume_based_rollover_method_enum_arg_parser.get_enum(
-            data.get("method")
-        )
+        data["method"] = volume_based_rollover_method_enum_arg_parser.get_enum(data.get("method"))
         return cls(**data)
 
 
 @dataclass
 class DayBasedRollover(Serializable["DayBasedRollover"]):
     """
     This method supplies an entirely different approach to creating a futures continuation contract.
@@ -226,31 +217,27 @@
     method: Union[DayBasedRolloverMethod, str]
     number_of_days: int
     months_prior: Optional[int] = None
 
     def _to_dict(self) -> dict:
         retval = {
             "dayBased": {
-                "method": day_based_rollover_method_enum_arg_parser.get_str(
-                    self.method
-                ),
+                "method": day_based_rollover_method_enum_arg_parser.get_str(self.method),
                 "numberOfDays": self.number_of_days,
             }
         }
         if self.months_prior:
             retval["dayBased"]["monthsPrior"] = self.months_prior
 
         return retval
 
     @classmethod
     def _from_dict(cls, data: dict) -> "DayBasedRollover":
         data = convert_camel_to_snake(data)
-        data["method"] = day_based_rollover_method_enum_arg_parser.get_enum(
-            data.get("method")
-        )
+        data["method"] = day_based_rollover_method_enum_arg_parser.get_enum(data.get("method"))
         return cls(**data)
 
 
 @dataclass
 class ManualItem:
     """
     month: int
@@ -284,20 +271,15 @@
         manual_items: args: ManualItem
     """
 
     def __init__(self, *args: ManualItem):
         self.manual_items = args
 
     def _to_dict(self) -> dict:
-        return {
-            "manual": [
-                item._to_dict() if isinstance(item, ManualItem) else item
-                for item in self.manual_items
-            ]
-        }
+        return {"manual": [item._to_dict() if isinstance(item, ManualItem) else item for item in self.manual_items]}
 
     @classmethod
     def _from_dict(cls, data: List) -> "ManualRollover":
         data = [ManualItem._from_dict(item) for item in data]
         return cls(*data)
 
 
@@ -332,17 +314,15 @@
         if self.adjustment:
             retval["adjustment"] = self.adjustment
         return retval
 
     @classmethod
     def _from_dict(cls, data: dict) -> "SpreadAdjustment":
         data = data.get("spreadAdjustment")
-        data["method"] = spread_adjustment_method_enum_arg_parser.get_enum(
-            data.get("method")
-        )
+        data["method"] = spread_adjustment_method_enum_arg_parser.get_enum(data.get("method"))
         return cls(**data)
 
 
 _rollover_key_name_by_class = {
     "dayBased": DayBasedRollover,
     "volumeBased": VolumeBasedRollover,
     "manual": ManualRollover,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_manage.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_manage.py`

 * *Files 0% similar despite different names*

```diff
@@ -147,16 +147,15 @@
     ...         *calendar_holiday.data.holidays,
     ...         ci.manage.Holiday(date="1991-08-24", name="Independence Day of Ukraine"),
     ...         {"date": "2022-12-18", "reason": "Hanukkah"},
     ...     ],
     >>> )
     """
     warnings.warn(
-        "'create()' is legacy interface. "
-        "Will be changed to 'create_formula()', 'create_basket()', 'create_udc()'",
+        "'create()' is legacy interface. Will be changed to 'create_formula()', 'create_basket()', 'create_udc()'",
     )
     data = _create(
         symbol,
         formula,
         basket,
         udc,
         instrument_name,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_search.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,17 +5,15 @@
 from ..._content_type import ContentType
 from ...delivery._data._data_provider import DataProviderLayer, BaseResponse
 
 if TYPE_CHECKING:
     from ..._types import ExtendedParams
 
 
-class Definition(
-    ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]
-):
+class Definition(ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]):
     """
     This class describe parameters to retrieve data for search custom instrument
 
     Parameters
     ----------
     access : str
         The search based on relationship to the custom instrument, for now only "owner" is supported. Can be omitted, default value is "owner"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_stream_facade.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_stream_facade.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,17 +38,15 @@
         universe=universe,
     )
 
     result = []
     for item in universe:
         # a20140be-3648-4892-9d1b-ce78ee8617fd
         if is_instrument_id.match(item):
-            instrument_response = data_provider_layer.get_data(
-                session, method=RequestMethod.GET
-            )
+            instrument_response = data_provider_layer.get_data(session, method=RequestMethod.GET)
             # S)MyNewInstrument5.GE-1525-0
             symbol = instrument_response.data.raw.get("symbol")
 
         else:
             # S)MyNewInstrument5
             if not symbol_with_user_id.match(item) and not uuid:
                 # S)MyNewInstrument5.GE-1525-0
@@ -113,53 +111,42 @@
 
         if not uuid:
             uuid = get_user_id(self._session)
         self._uuid = uuid
 
     @cached_property
     def _stream(self) -> _UniverseStreams:
-        if self._api:
-            content_type = ContentType.STREAMING_CUSTOM
-        else:
-            content_type = ContentType.STREAMING_CUSTOM_INSTRUMENTS
-
         return CustomInstsUniverseStreams(
-            content_type=content_type,
+            content_type=ContentType.STREAMING_CUSTOM_INSTRUMENTS,
             item_facade_class=CustomInstrumentsStream,
             universe=self._universe,
             session=self._session,
             fields=self._fields,
             service=self._service,
             api=self._api,
             extended_params=self._extended_params,
         )
 
     def _get_fields(self, universe: str, fields: Optional[list] = None) -> dict:
         _fields = {
-            universe: {
-                key: value
-                for key, value in self._stream[universe].items()
-                if fields is None or key in fields
-            }
+            universe: {key: value for key, value in self._stream[universe].items() if fields is None or key in fields}
         }
         return _fields
 
     def get_snapshot(
         self,
         universe: Union[str, List[str], None] = None,
         fields: Optional[List[str]] = None,
         convert: bool = True,
     ) -> "pandas.DataFrame":
         if isinstance(universe, str):
             universe = [universe]
         if universe is not None:
             universe = [get_valid_symbol(i, self._uuid) for i in universe]
-        df = self._stream.get_snapshot(
-            universe=universe, fields=fields, convert=convert
-        )
+        df = self._stream.get_snapshot(universe=universe, fields=fields, convert=convert)
         convert_df_columns_to_datetime_re(df, PRICING_DATETIME_PATTERN)
         return df
 
     def on_refresh(self, func: Callable[[Any, str, "Stream"], Any]) -> "Stream":
         self._stream.on_refresh(make_callback(func))
         return self
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/_summaries.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/_summaries.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,24 +4,23 @@
 from .._content_data import Data
 from .._content_provider_layer import ContentUsageLoggerMixin
 from .._intervals import DayIntervalType, get_day_interval_type, Intervals
 from ..._tools import (
     validate_types,
     custom_insts_historical_universe_parser,
     try_copy_to_list,
+    custom_inst_datetime_adapter,
 )
 from ...delivery._data._data_provider import DataProviderLayer, BaseResponse
 
 if TYPE_CHECKING:
     from ..._types import StrStrings, OptDateTime, OptInt, ExtendedParams
 
 
-class Definition(
-    ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]
-):
+class Definition(ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]):
     """
     Summary line of this class that defines parameters for requesting summaries from custom instruments
 
     Parameters
     ----------
     universe : str or list
         The Id or Symbol of custom instrument to operate on
@@ -53,14 +52,16 @@
         interval: Union[str, Intervals] = None,
         start: "OptDateTime" = None,
         end: "OptDateTime" = None,
         count: "OptInt" = None,
         fields: "StrStrings" = None,
         extended_params: "ExtendedParams" = None,
     ) -> None:
+        start = custom_inst_datetime_adapter.get_localize(start)
+        end = custom_inst_datetime_adapter.get_localize(end)
         day_interval_type = get_day_interval_type(interval or DayIntervalType.INTER)
         content_type = get_content_type_by_interval(day_interval_type)
         validate_types(count, [int, type(None)], "count")
 
         fields = try_copy_to_list(fields)
         universe = try_copy_to_list(universe)
         universe = custom_insts_historical_universe_parser.get_list(universe)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/custom_instruments/manage.py` & `refinitiv-data-1.2.0/refinitiv/data/content/custom_instruments/manage.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_base_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_base_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_basic_overview_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_basic_overview_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_esg_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_esg_data_provider.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,49 +1,34 @@
+from functools import partial
+
 from .._content_data_provider import ContentDataProvider
 from .._error_parser import ErrorParser
 from .._universe_content_validator import UniverseContentValidator
-from ..._tools import universe_arg_parser
+from ..._tools import universe_arg_parser, ValueParamItem, ParamItem
 from ...delivery._data._data_provider import (
     RequestFactory,
     ValidatorContainer,
 )
 
 
 # ---------------------------------------------------------------------------
 #   Request
 # ---------------------------------------------------------------------------
 
+esg_query_params = [
+    ValueParamItem("universe", function=partial(universe_arg_parser.get_str, delim=",")),
+    ParamItem("start"),
+    ParamItem("end"),
+]
 
-class ESGRequestFactory(RequestFactory):
-    def get_query_parameters(self, *args, **kwargs):
-        query_parameters = []
-
-        #
-        # universe
-        #
-        universe = kwargs.get("universe")
-        if universe:
-            universe = universe_arg_parser.get_str(universe, delim=",")
-            query_parameters.append(("universe", universe))
-
-        #
-        # start
-        #
-        start = kwargs.get("start")
-        if start is not None:
-            query_parameters.append(("start", start))
-
-        #
-        # end
-        #
-        end = kwargs.get("end")
-        if end is not None:
-            query_parameters.append(("end", end))
 
-        return query_parameters
+class ESGRequestFactory(RequestFactory):
+    @property
+    def query_params_config(self):
+        return esg_query_params
 
 
 # ---------------------------------------------------------------------------
 #   Provider
 # ---------------------------------------------------------------------------
 
 esg_data_provider = ContentDataProvider(
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_full_measures_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_full_measures_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_full_scores_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_full_scores_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_standard_measures_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_standard_measures_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_standard_scores_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_standard_scores_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/_universe_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/_universe_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_actions.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_actions.py`

 * *Files 2% similar despite different names*

```diff
@@ -113,17 +113,15 @@
         not_updated = downloaded - updated
         return not_updated, self._get("EXTRACT")
 
     def cleanup(self):
         try:
             os.remove(self.logpath)
         except PermissionError:
-            log.root_logger().warning(
-                f"Cannot remove {self.logpath}, file will be cleared."
-            )
+            log.root_logger().warning(f"Cannot remove {self.logpath}, file will be cleared.")
             with open(self.logpath, "w") as f:
                 f.write("")
             log.root_logger().warning(f"{self.logpath} cleared.")
         except FileNotFoundError:
             # do nothing
             pass
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_db_manager.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_db_manager.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 import os
 import sqlite3
 from logging import Logger
 from typing import Optional, Iterable
 
-import mysql.connector
+import pymysql.cursors
 
 from ._actions import Actions
 from ._df_builder import bulk_build_df
 from ._errors import (
     KeyNotFoundException,
     TableNotFoundException,
     UpdateDBException,
@@ -83,17 +83,15 @@
             keys[name] = "null" if value is None else value
         else:
             raise KeyNotFoundException()
 
     return tpl.substitute(keys)
 
 
-def prepare_search_query(
-    query: str, universe: Iterable[str], columns: Iterable[str] = None
-) -> str:
+def prepare_search_query(query: str, universe: Iterable[str], columns: Iterable[str] = None) -> str:
     """
     Parameters
     ----------
     query: str
         query is like:
          "SELECT * FROM TABLE WHERE INSTRUMENT IN #universe#"
          "SELECT COLUMN1, COLUMN2 FROM TABLE WHERE INSTRUMENT IN #universe#"
@@ -108,17 +106,15 @@
         Returns query is like
         "SELECT COLUMN1, COLUMN2
          FROM TABLE WHERE INSTRUMENT IN ('4295865175', '4295906529')"
     """
     if "*" in query and columns:
         query = query.replace("*", ", ".join(columns))
 
-    return StringTemplateWithDottedNames(query).safe_substitute(
-        universe=f"({', '.join(map(repr, universe))})"
-    )
+    return StringTemplateWithDottedNames(query).safe_substitute(universe=f"({', '.join(map(repr, universe))})")
 
 
 class DBManager(_LogReporter):
     def __init__(self, connection, config, actions: Actions, logger) -> None:
         super().__init__(logger=logger)
         self._connection = connection
         self._cursor = self._connection.cursor()
@@ -224,17 +220,15 @@
 
     def get_data(self, universe: Iterable[str], fields: Iterable[str] = None) -> Data:
         fields = fields or []
         query = self._config.get("search-query")
 
         column_by_field = self._config["output-fields-mapping"].as_dict()
         field_by_column = {v: k for k, v in column_by_field.items()}
-        columns = [
-            field_by_column[field] for field in fields if field in field_by_column
-        ]
+        columns = [field_by_column[field] for field in fields if field in field_by_column]
 
         try:
             query = prepare_search_query(query, universe, columns)
             raw = self.exec(query)
             error = None
         except Exception as e:
             raw = {}
@@ -242,34 +236,34 @@
                 "message": str(e),
             }
             self._actions.add("ERROR", **error)
             error["query"] = query
             self._info(f"error: {error}")
 
         if error:
-            return Data(raw, build_empty_df)
+            return Data(raw=raw, _dfbuilder=build_empty_df)
 
         return Data(
-            raw,
-            bulk_build_df,
-            column_by_field=column_by_field,
-            columns=self._cursor.description,
+            raw=raw,
+            _dfbuilder=bulk_build_df,
+            _kwargs={
+                "column_by_field": column_by_field,
+                "columns": self._cursor.description,
+            },
         )
 
 
 def create_sqlite_connector(config):
-    connection = sqlite3.connect(
-        database=config.get("connection.parameters.database"), uri=True
-    )
+    connection = sqlite3.connect(database=config.get("connection.parameters.database"), uri=True)
     return connection
 
 
 def create_mysql_connector(config):
     conn_params = config["connection"]["parameters"]
-    connection = mysql.connector.connect(
+    connection = pymysql.connect(
         user=conn_params.get("user"),
         password=conn_params.get("password"),
         host=conn_params.get("host"),
         port=conn_params.get("port"),
         database=conn_params.get("database"),
     )
     return connection
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_file_manager.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_file_manager.py`

 * *Files 3% similar despite different names*

```diff
@@ -48,27 +48,21 @@
             bucket_name=self._bucket,
         )
         _packages = _packages_def.get_data(session=self.session)
         session = get_valid_session(self.session)
         self.init_logger(session.logger())
 
         if not _packages.data.packages:
-            message = (
-                f"package (name={self._package_name}, bucket={self._bucket}) not found"
-            )
+            message = f"package (name={self._package_name}, bucket={self._bucket}) not found"
             self._actions.add(
                 "PACKAGE NOT FOUND",
                 bucket=self._bucket,
                 package=self._package_name,
             )
-            self._info(
-                f"package "
-                f"bucket='{self._bucket}', package_name='{self._package_name}' "
-                f"not found"
-            )
+            self._info(f"package bucket='{self._bucket}', package_name='{self._package_name}' not found")
             raise PackageNotFoundException(message)
 
         _package_id = _packages.data.packages[0].package_id
 
         self._actions.update()
         downloaded = self._actions.get_downloaded()
 
@@ -85,17 +79,15 @@
         )
         _file_sets = _file_sets_def.get_data(session=self.session)
         _file_sets = [file_set for file_set in _file_sets.data.file_sets]
 
         if not init_file:
             # if no init files are loaded,
             # get the latest init and delta files with the same or newer date
-            _file_sets = get_filesets_with_the_newest_init_file_and_delta_files(
-                _file_sets
-            )
+            _file_sets = get_filesets_with_the_newest_init_file_and_delta_files(_file_sets)
 
         else:
             # if init file already downloaded, get deltas with same or newer date
             _file_sets = get_filesets_with_delta_files(_file_sets)
 
         # download files
         has_files_for_download = False
@@ -110,26 +102,24 @@
 
     def __retry_download_file(self, file_id: str, filename_ext: str):
         count_of_try = self._auto_retry_count + 1
 
         while count_of_try:
             try:
                 self._info(f"download file {filename_ext} to {self._path}")
-                fd = cfs.file_downloader.Definition(
-                    {"id": file_id, "filename": filename_ext}
-                ).retrieve(session=self.session)
+                fd = cfs.file_downloader.Definition({"id": file_id, "filename": filename_ext}).retrieve(
+                    session=self.session
+                )
                 fd.download(self._path)
                 self._actions.downloaded(path=self._path, filename=filename_ext)
 
                 if self._auto_extract:
                     fd.extract(self._path)
                     extracted_filename = remove_one_ext(filename_ext)
-                    self._actions.extracted(
-                        path=self._path, filename=extracted_filename
-                    )
+                    self._actions.extracted(path=self._path, filename=extracted_filename)
                     self._info(f"extract file {filename_ext} to {self._path}")
 
                 break
             except Exception as e:
                 details = self._actions.add(
                     "DOWNLOAD CRASHED",
                     path=self._path,
@@ -153,17 +143,15 @@
             self._error(f"error: {message}")
             raise FileNotFoundError(message)
         with open(filepath) as f:
             for line_n, line in enumerate(f):
                 try:
                     data.append(json.loads(line))
                 except Exception as e:
-                    error_details = self._actions.add(
-                        "ERROR", message=str(e), line_number=line_n, filename=filename
-                    )
+                    error_details = self._actions.add("ERROR", message=str(e), line_number=line_n, filename=filename)
                     self._info(f"error: {error_details}")
                     continue
 
         return data
 
     def read_files_for_update(self) -> list:
         self._actions.update()
@@ -203,11 +191,9 @@
             try:
                 os.remove(path)
             except PermissionError:
                 if filename == "log.txt":
                     with open(path, "w"):
                         # only create a new file
                         pass
-                    self._info(
-                        f"WARNING: Cannot remove {path}. The log has been cleared."
-                    )
+                    self._info(f"WARNING: Cannot remove {path}. The log has been cleared.")
                 self._info(f"WARNING: Cannot remove {path}")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_package_manager.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_package_manager.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_package_manager_facade.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_package_manager_facade.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/esg/bulk/_tools.py` & `refinitiv-data-1.2.0/refinitiv/data/content/esg/bulk/_tools.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,13 @@
 import re
 
 from ...._tools._datetime import Converter
 
 datetime_converter = Converter()
-regexp_filename_obj = re.compile(
-    r".*([Dd]elta|[Ii]nit).*(\d{4}-\d{2}-\d{2}).*(\.jsonl.gz|\.jsonl)$"
-)
+regexp_filename_obj = re.compile(r".*([Dd]elta|[Ii]nit).*(\d{4}-\d{2}-\d{2}).*(\.jsonl.gz|\.jsonl)$")
 regexp_init_archives = re.compile(r".*(-[Ii]nit-).*\d{4}-\d{2}-\d{2}.*(.gz)$")
 regexp_delta_files = re.compile(r".*(Jsonl-[Dd]elta-).*\d{4}-\d{2}-\d{2}.*")
 regexp_init_file = re.compile(r".*(Jsonl-[Ii]nit-).*\d{4}-\d{2}-\d{2}.*")
 regexp_jsonl_fileset = re.compile(r".*(-Jsonl-).*\d{4}-\d{2}-\d{2}.*")
 regexp_filename_date = re.compile(r".*(\d{4}-\d{2}-\d{2}).*")
 
 
@@ -43,35 +41,28 @@
 def get_init_archives(filenames):
     # return init archives with correct name
     return filter(lambda a: regexp_init_archives.match(a), filenames)
 
 
 def get_filesets_with_delta_files(file_sets):
     # return init archives with correct name
-    result = [
-        file_set for file_set in file_sets if regexp_delta_files.match(file_set["name"])
-    ]
+    result = [file_set for file_set in file_sets if regexp_delta_files.match(file_set["name"])]
     return result
 
 
 def get_filesets_with_the_newest_init_file_and_delta_files(file_sets):
-    file_sets = [
-        file_set
-        for file_set in file_sets
-        if regexp_jsonl_fileset.match(file_set["name"])
-    ]
+    file_sets = [file_set for file_set in file_sets if regexp_jsonl_fileset.match(file_set["name"])]
     init_filesets = filter(lambda a: "init" in a["name"].lower(), file_sets)
     init_dates = [get_date_from_filename(i["name"]) for i in init_filesets]
     init_dates.sort()
     if not init_dates:
         return []
     init_date = datetime_converter.convert(init_dates[-1])
     result = filter(
-        lambda a: init_date
-        <= datetime_converter.convert(get_date_from_filename(a["name"])),
+        lambda a: init_date <= datetime_converter.convert(get_date_from_filename(a["name"])),
         file_sets,
     )
     return result
 
 
 def sorted_files(files):
     if not files:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/story/_request_factory.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,39 +1,53 @@
-from ._enums import Package
-from .._content_data_provider import ContentDataProvider
-from .._error_parser import ErrorParser
-from .._universe_content_validator import UniverseContentValidator
-from ..._tools import (
-    universe_arg_parser,
-    make_enum_arg_parser,
-    extend_params,
-)
-from ...delivery._data._data_provider import (
-    RequestFactory,
-    ValidatorContainer,
-)
-
-
-class EstimatesRequestFactory(RequestFactory):
-    def get_query_parameters(self, *_, **kwargs) -> list:
-        query_parameters = []
-        universe = universe_arg_parser.get_str(kwargs.get("universe"), delim=",")
-        query_parameters.append(("universe", universe))
-
-        package = kwargs.get("package")
-        if package is not None:
-            package = package_estimates_arg_parser.get_str(package)
-            query_parameters.append(("package", package))
+from ...._core.session import DesktopSession
+from ...._tools import extend_params
+from ....delivery._data import RequestMethod
+from ....delivery._data._request_factory import RequestFactory
 
-        return query_parameters
 
+class NewsStoryUDFRequestFactory(RequestFactory):
+    def extend_body_parameters(self, body_parameters, extended_params=None, **kwargs):
+        if extended_params:
+            body_parameters["Entity"]["W"].update(extended_params)
+        return body_parameters
+
+    def get_body_parameters(self, session, *args, **kwargs):
+        entity = {
+            "E": "News_Story",
+        }
+        w = dict()
+
+        story_id = kwargs.get("story_id")
+        w["storyId"] = story_id
+
+        app_key = session.app_key
+        w["productName"] = app_key
+
+        entity["W"] = w
+        body_parameters = {"Entity": entity}
+        return body_parameters
+
+    def get_url(self, session, *args, **kwargs):
+        url = session._get_rdp_url_root()
+        if isinstance(session, DesktopSession):
+            url = session._get_udf_url()
+        return url
+
+    def update_url(self, url_root, url, path_parameters, query_parameters):
+        return url
+
+    def get_request_method(self, **kwargs) -> RequestMethod:
+        return RequestMethod.POST
+
+
+class StoryRDPRequestFactory(RequestFactory):
     def extend_query_parameters(self, query_parameters, extended_params=None):
         return extend_params(query_parameters, extended_params)
 
+    def get_path_parameters(self, session=None, *, story_id=None, **kwargs):
+        return {"storyId": story_id}
 
-package_estimates_arg_parser = make_enum_arg_parser(Package)
+    def get_header_parameters(self, session=None, **kwargs):
+        return {"accept": "application/json"}
 
-estimates_data_provider = ContentDataProvider(
-    request=EstimatesRequestFactory(),
-    validator=ValidatorContainer(content_validator=UniverseContentValidator()),
-    parser=ErrorParser(),
-)
+    def get_url(self, *args, **kwargs):
+        return super().get_url(*args, **kwargs) + "/{storyId}"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/_annual_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/_annual_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals/_interim_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals/_interim_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/_annual_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/_annual_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_actuals_kpi/_interim_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_actuals_kpi/_interim_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/__init__.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_annual_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_annual_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_non_periodic_measures_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_non_periodic_measures_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -36,17 +36,15 @@
     Examples
     --------
     >>> from refinitiv.data.content import estimates
     >>> definition = estimates.view_summary.historical_snapshots_non_periodic_measures.Definition(universe="IBM.N", package=estimates.Package.BASIC)
     >>> response = definition.get_data()
     """
 
-    _USAGE_CLS_NAME = (
-        "Estimates.Summary.HistoricalSnapshotsNonPeriodicMeasuresDefinition"
-    )
+    _USAGE_CLS_NAME = "Estimates.Summary.HistoricalSnapshotsNonPeriodicMeasuresDefinition"
 
     def __init__(
         self,
         universe: "StrStrings",
         package: Union[str, Package],
         use_field_names_in_headers: bool = False,
         extended_params: "ExtendedParams" = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_annual_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_annual_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -36,17 +36,15 @@
     Examples
     --------
     >>> from refinitiv.data.content import estimates
     >>> definition = estimates.view_summary.historical_snapshots_periodic_measures_annual.Definition(universe="IBM.N", package=estimates.Package.BASIC)
     >>> response = definition.get_data()
     """
 
-    _USAGE_CLS_NAME = (
-        "Estimates.Summary.HistoricalSnapshotsPeriodicMeasuresAnnualDefinition"
-    )
+    _USAGE_CLS_NAME = "Estimates.Summary.HistoricalSnapshotsPeriodicMeasuresAnnualDefinition"
 
     def __init__(
         self,
         universe: "StrStrings",
         package: Union[str, Package],
         use_field_names_in_headers: bool = False,
         extended_params: "ExtendedParams" = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_interim_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_periodic_measures_interim_definition.py`

 * *Files 4% similar despite different names*

```diff
@@ -36,17 +36,15 @@
     Examples
     --------
     >>> from refinitiv.data.content import estimates
     >>> definition = estimates.view_summary.historical_snapshots_periodic_measures_interim.Definition(universe="IBM.N", package=estimates.Package.BASIC)
     >>> response = definition.get_data()
     """
 
-    _USAGE_CLS_NAME = (
-        "Estimates.Summary.HistoricalSnapshotsPeriodicMeasuresInterimDefinition"
-    )
+    _USAGE_CLS_NAME = "Estimates.Summary.HistoricalSnapshotsPeriodicMeasuresInterimDefinition"
 
     def __init__(
         self,
         universe: "StrStrings",
         package: Union[str, Package],
         use_field_names_in_headers: bool = False,
         extended_params: "ExtendedParams" = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_historical_snapshots_recommendations_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_historical_snapshots_recommendations_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_interim_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_interim_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_non_periodic_measures_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_non_periodic_measures_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary/_recommendations_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary/_recommendations_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/_annual_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/_annual_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/_historical_snapshots_kpi_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/_historical_snapshots_kpi_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/estimates/view_summary_kpi/_interim_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/estimates/view_summary_kpi/_interim_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/filings/_retrieval_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/filings/_retrieval_data_provider.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,29 +1,26 @@
 import asyncio
 import mimetypes
 import os
 import urllib
-from typing import Callable
+from dataclasses import dataclass
+from typing import Callable, List
 from uuid import uuid4
 
 import httpx
 
 from ._errors import DownloadFileError
 from .._content_data import Data
 from .._content_data_provider import ContentDataProvider
 from .._content_response_factory import ContentResponseFactory
 from .._error_parser import ErrorParser
 from .._universe_content_validator import UniverseContentValidator
 from ..._content_type import ContentType
 from ..._tools._common import get_response_reason
-from ...delivery._data._data_provider import (
-    RequestFactory,
-    EndpointData,
-    Error,
-)
+from ...delivery._data._data_provider import EndpointData, Error, RequestFactory
 from ...delivery._data._endpoint_data import RequestMethod
 
 
 # ---------------------------------------------------------------------------
 #   Request factory
 # ---------------------------------------------------------------------------
 
@@ -35,17 +32,15 @@
         if kwargs.get("filename"):
             return url + "/{filename}"
         elif kwargs.get("dcn") or kwargs.get("doc_id") or kwargs.get("filing_id"):
             return url + "/search/{identifier}/{value}"
         else:
             return url
 
-    def get_path_parameters(
-        self, session=None, *, path_parameters=None, filename=None, **kwargs
-    ) -> dict:
+    def get_path_parameters(self, session=None, *, path_parameters=None, filename=None, **kwargs) -> dict:
         path_parameters = path_parameters or {}
         # only one of filename, dcn, doc_id and filing_id will be included in the parameter
         if filename:
             path_parameters["filename"] = filename
         else:
             identifiers = {"dcn", "doc_id", "filing_id"}
             for key, value in kwargs.items():
@@ -63,24 +58,23 @@
 
 
 # ---------------------------------------------------------------------------
 #   Data
 # ---------------------------------------------------------------------------
 
 
+@dataclass
 class DownloadFileData(EndpointData):
-    def __init__(self, file_location: str = None, raw=None, is_success: bool = False):
-        super(DownloadFileData, self).__init__(raw)
-        self.file_location = file_location
-        self.is_success = is_success
+    file_location: str = None
+    is_success: bool = False
 
 
+@dataclass
 class DownloadFilesData:
-    def __init__(self, files: list = None):
-        self.files = files
+    files: List = None
 
 
 # ---------------------------------------------------------------------------
 #   Download response
 # ---------------------------------------------------------------------------
 
 
@@ -110,20 +104,22 @@
         self.data = DownloadFilesData(files)
         self.errors = errors
 
 
 # ---------------------------------------------------------------------------
 #   File
 # ---------------------------------------------------------------------------
+def get_file_location_with_extension(file_location: str, extension: str):
+    if file_location.endswith(extension):
+        return file_location
+    return file_location + extension
 
 
 class FilingsFile:
-    def __init__(
-        self, filename: str = None, signed_url: str = None, mimetype: str = None
-    ):
+    def __init__(self, filename: str = None, signed_url: str = None, mimetype: str = None):
         """
         Parameters
         ----------
         filename : str
             Name of the file
         signed_url : str
             Signed URL to download the file
@@ -149,35 +145,31 @@
                     "http_status_code": http_response.status_code,
                     "http_reason": get_response_reason(http_response),
                 },
                 "raw_response": http_response,
             }
 
             if http_response.is_success:
-                extension = mimetypes.guess_extension(
-                    http_response.headers.get("content-type")
-                )
+                extension = mimetypes.guess_extension(http_response.headers.get("content-type"))
                 data["content_data"] = DownloadFileData(
-                    file_location=file_location + extension,
                     raw=http_response.__dict__,
+                    file_location=get_file_location_with_extension(file_location, extension),
                     is_success=http_response.is_success,
                 )
             else:
                 data["error_code"] = http_response.status_code
-                data["error_message"] = error.message or get_response_reason(
-                    http_response
-                )
+                data["error_message"] = error.message or get_response_reason(http_response)
                 data["content_data"] = DownloadFileData(
-                    file_location=None,
                     raw={
                         "error": {
                             "code": data["error_code"],
                             "description": data["error_message"],
                         }
                     },
+                    file_location=None,
                     is_success=False,
                 )
         else:
             # Only error before http request will come here
             data = {
                 "is_success": False,
                 "status": {
@@ -185,35 +177,31 @@
                     "http_reason": get_response_reason(http_response),
                 },
                 "raw_response": http_response,
                 "error_code": error.code,
                 "error_message": error.message,
             }
             data["content_data"] = DownloadFileData(
-                file_location=None,
                 raw={
                     "error": {
                         "code": data["error_code"],
                         "description": data["error_message"],
                     }
                 },
+                file_location=None,
                 is_success=False,
             )
 
         return (
             DownloadFileResponse(**data),
             extension,
         )
 
-    def _process_exception(
-        self, http_response: httpx.Response = None, error: Exception = None
-    ) -> DownloadFileResponse:
-        exception_response, _ = self._process_response(
-            http_response=http_response, error=error
-        )
+    def _process_exception(self, http_response: httpx.Response = None, error: Exception = None) -> DownloadFileResponse:
+        exception_response, _ = self._process_response(http_response=http_response, error=error)
         return exception_response
 
     def _prepare_and_validate_file_path(self, path):
         file_location = os.path.join(f"{path or os.getcwd()}", f"{self.filename}")
         temp_file = file_location + str(uuid4())
         if path is not None and not os.path.exists(path):
             raise DownloadFileError(None, f"No such directory exists: '{path}'")
@@ -237,30 +225,24 @@
         extension = ""
         file_location = ""
         temp_file = ""
 
         try:
             file_location, temp_file = self._prepare_and_validate_file_path(path)
             with open(temp_file, "wb") as f:
-                with httpx.stream(
-                    method=RequestMethod.GET, url=self.signed_url
-                ) as http_response:
+                with httpx.stream(method=RequestMethod.GET, url=self.signed_url) as http_response:
                     if http_response.is_error:
                         # Raise Error
-                        raise DownloadFileError(
-                            http_response.status_code, http_response.read().decode()
-                        )
+                        raise DownloadFileError(http_response.status_code, http_response.read().decode())
                     # Process streaming data
                     for chunk in http_response.iter_bytes():
                         f.write(chunk)
                     # Check if streaming data is completely consumed
                     if http_response.is_stream_consumed:
-                        file_response, extension = self._process_response(
-                            file_location, http_response
-                        )
+                        file_response, extension = self._process_response(file_location, http_response)
                     else:
                         raise DownloadFileError(400, "File download is not completed.")
         except DownloadFileError as err:
             file_response = self._process_exception(http_response, err)
         except Exception as err:
             error = DownloadFileError(None, err.__str__())
             file_response = self._process_exception(http_response, error)
@@ -272,24 +254,23 @@
             if os.path.exists(temp_file):
                 os.remove(temp_file)
             raise DownloadFileError(
                 code=file_response.errors[0].code,
                 message=file_response.errors[0].message,
             )
         else:
-            file_location = file_location + extension
+            file_location = get_file_location_with_extension(file_location, extension)
+
             if os.path.exists(file_location):
                 os.remove(file_location)
             os.rename(temp_file, file_location)
 
         return file_response
 
-    async def download_async(
-        self, path: str = None, callback: Callable = None
-    ) -> DownloadFileResponse:
+    async def download_async(self, path: str = None, callback: Callable = None) -> DownloadFileResponse:
         """
 
         Parameters
         ----------
         path : str
             Destination of the download file. Default is current working directory.
         callback : Callable
@@ -306,31 +287,25 @@
         file_location = ""
         temp_file = ""
         client = httpx.AsyncClient()
 
         try:
             file_location, temp_file = self._prepare_and_validate_file_path(path)
             with open(temp_file, "wb") as f:
-                async with client.stream(
-                    method=RequestMethod.GET, url=self.signed_url
-                ) as http_response:
+                async with client.stream(method=RequestMethod.GET, url=self.signed_url) as http_response:
                     if http_response.is_error:
                         # Raise Error
                         error_message = await http_response.aread()
-                        raise DownloadFileError(
-                            http_response.status_code, error_message.decode()
-                        )
+                        raise DownloadFileError(http_response.status_code, error_message.decode())
                     # Process streaming data
                     async for chunk in http_response.aiter_bytes():
                         f.write(chunk)
                     # Check if streaming data is completely consumed
                     if http_response.is_stream_consumed:
-                        file_response, extension = self._process_response(
-                            file_location, http_response
-                        )
+                        file_response, extension = self._process_response(file_location, http_response)
                     else:
                         raise DownloadFileError(400, "File download is not completed.")
         except DownloadFileError as err:
             file_response = self._process_exception(http_response, err)
         except Exception as err:
             error = DownloadFileError(None, err.__str__())
             file_response = self._process_exception(http_response, error)
@@ -339,15 +314,16 @@
         if file_response is None:
             return DownloadFileResponse(error=ValueError("File response is None!"))
 
         if len(file_response.errors) > 0:
             if os.path.exists(temp_file):
                 os.remove(temp_file)
         else:
-            file_location = file_location + extension
+            file_location = get_file_location_with_extension(file_location, extension)
+
             if os.path.exists(file_location):
                 os.remove(file_location)
             os.rename(temp_file, file_location)
 
         if callback is not None and callable(callback):
             callback(file_response)
 
@@ -367,17 +343,15 @@
         Returns
         -------
         DownloadAllFileResponse
 
         """
         # Raise an exception if there are errors
         if len(self) == 0:
-            raise DownloadFileError(
-                code=None, message="Cannot download any file. Files are empty."
-            )
+            raise DownloadFileError(code=None, message="Cannot download any file. Files are empty.")
 
         files = []
         errors = []
 
         for each_file in self:
             # might raise an error in case there are problems while downloading files
             try:
@@ -390,17 +364,15 @@
 
         return download_all_response
 
     def _chunks(self, path, n):
         for i in range(0, len(self), n):
             yield [each_file.download_async(path=path) for each_file in self[i : i + n]]
 
-    async def download_async(
-        self, path: str = None, callback: Callable = None
-    ) -> DownloadAllFileResponse:
+    async def download_async(self, path: str = None, callback: Callable = None) -> DownloadAllFileResponse:
         """
         Parameters
         ----------
         path : str
             Destination of the download file. Default is current working directory.
         callback: Callable
             Callback function will be called after the process is completed
@@ -410,19 +382,15 @@
         DownloadAllFileResponse
 
         """
         files = []
         errors = []
 
         if len(self) == 0:
-            errors.append(
-                DownloadFileError(
-                    code=None, message="Cannot download any file. Files are empty."
-                )
-            )
+            errors.append(DownloadFileError(code=None, message="Cannot download any file. Files are empty."))
             return DownloadAllFileResponse(files=files, errors=errors)
 
         responses = []
         # Split into chunks because there will be errors when calling too many tasks at the same time
         for task in self._chunks(path, 10):
             responses.extend(await asyncio.gather(*task))
 
@@ -439,20 +407,21 @@
 
 
 # ---------------------------------------------------------------------------
 #   Response data
 # ---------------------------------------------------------------------------
 
 
+@dataclass
 class FilingsData(Data):
-    _files = None
+    _files: ListOfFile = None
 
     @property
     def df(self):
-        if self._dataframe is None and self._raw:
+        if self._dataframe is None and self.raw:
             # generate headers for df
             headers = [
                 {"title": "Filename", "type": "string"},
                 {"title": "SignedURL", "type": "string"},
                 {"title": "MimeType", "type": "string"},
             ]
 
@@ -475,31 +444,31 @@
                 },
                 'ecpfilings_97654291060_dissemination_txt': {
                     'signedUrl': 'https://cdn-filings.filings.refinitiv.com/retrieval/filings/ecpfilings_97654291060_dissemination_txt?ClientID=API_Playground&Expires=1648731287&Signature=e-FxtjqWnh~Q20uL4xaNXxlZhlP7Gvqqa1uVHsbIOFgpVqZd39oKUnyNn1do8MdpUYveP~YRvHzJ8uAYncjY1NaMHixYmfoZxqcfAHasmdMwdtaAzihPxmZobLHu7kK0iIfpCglF9RH2e7yT-WhglXbdUDvlY~eVnAFPMf4Q-tkGPQGAtmR1pZvYX53GCo-XIHO3-bX-YcY4MbsAQdzqKNdaHVKySZ0RoyfJsjdOKmJFmEygfoKQvg3zL-HV2FsD9uCZXPlV9elN3OnGyXoOSOmX4unroh7vwI5NBV3pO5x37JIl~WE4a9KU6~sacGpXiz8Sg~RkaiyQMbSUR1tsVA__&Key-Pair-Id=APKAIDW27KNAZ6YUBN7A',
                     'mimeType': 'text/plain'
                 }
             }
             """
-            if "signedUrl" in self._raw:
+            if "signedUrl" in self.raw:
                 # Need to extract filename from signedURL
-                url_parse = urllib.parse.urlparse(self._raw["signedUrl"])
+                url_parse = urllib.parse.urlparse(self.raw["signedUrl"])
 
                 # url_parse.path can be '/retrieval/filings/ecpfilings_34359955599_pdf'
                 # [[Filename, SignedURL, MimeType]]
                 # [["ecpfilings_34359955599_pdf", self._raw["signedUrl"], ""]]
-                data = [[url_parse.path.split("/")[-1], self._raw["signedUrl"], ""]]
+                data = [[url_parse.path.split("/")[-1], self.raw["signedUrl"], ""]]
             else:
                 # we can get filename from key in self._raw in this case
                 data = [
                     [filename, attributes["signedUrl"], attributes["mimeType"]]
-                    for filename, attributes in self._raw.items()
+                    for filename, attributes in self.raw.items()
                 ]
 
-            self._raw.update({"headers": headers, "data": data})
-            self._dataframe = self._dfbuilder(self._raw, **self._kwargs)
+            self.raw.update({"headers": headers, "data": data})
+            self._dataframe = self._dfbuilder(self.raw, **self._kwargs)
         return self._dataframe
 
     @property
     def files(self):
         """
         Returns
         -------
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/filings/_retrieval_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/filings/_retrieval_definition.py`

 * *Files 18% similar despite different names*

```diff
@@ -31,35 +31,30 @@
     --------
     >>> from refinitiv.data.content import filings
     >>> definition = filings.retrieval.Definition(filename="ecpfilings_34359955599_pdf")
     >>> response = definition.get_data()
     >>> response.data.files[0].download(path="C:\\Downloads\\download_test")
 
     Download all files at once
+
     >>> response.data.files.download(path="C:\\Downloads\\download_test")
     """
 
     def __init__(
         self,
         filename: "OptStr" = None,
         dcn: "OptStr" = None,
         doc_id: "OptStr" = None,
         filing_id: "OptStr" = None,
     ):
-        not_none_count = sum(
-            param is not None for param in [filename, dcn, doc_id, filing_id]
-        )
+        not_none_count = sum(param is not None for param in [filename, dcn, doc_id, filing_id])
         if not_none_count == 0:
-            raise ValueError(
-                "One of filename, dcn, doc_id or filing_id, is required in a Definition."
-            )
+            raise ValueError("One of filename, dcn, doc_id or filing_id, is required in a Definition.")
         elif not_none_count > 1:
-            raise ValueError(
-                "Only one of filename, dcn, doc_id or filing_id, can be used in a Definition"
-            )
+            raise ValueError("Only one of filename, dcn, doc_id or filing_id, can be used in a Definition")
 
         self.filename = filename
         self.dcn = dcn
         self.doc_id = doc_id
         self.filing_id = filing_id
 
         super().__init__(
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/filings/_search_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/filings/_search_data_provider.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,35 +1,85 @@
+from dataclasses import dataclass
 from typing import Callable, TYPE_CHECKING
 
 import pandas as pd
 
-from ._retrieval_data_provider import (
-    ListOfFile,
-    DownloadFileError,
-    DownloadAllFileResponse,
-)
+from ._feed_name import Feed
+from ._filing_query import get_query
+from ._retrieval_data_provider import DownloadAllFileResponse, DownloadFileError, ListOfFile
 from .retrieval import Definition as RetrievalDefinition
 from .._content_data import Data
 from .._content_data_provider import ContentDataProvider
 from .._content_response_factory import ContentResponseFactory
 from .._error_parser import ErrorParser
 from .._universe_content_validator import UniverseContentValidator
+from ..._tools import ParamItem, make_enum_arg_parser
 from ...delivery._data._data_provider import (
     RequestFactory,
 )
 
 if TYPE_CHECKING:
     from ...delivery._data._data_provider import ParsedData
 
+filings_search_body_params_config = [
+    ParamItem("query"),
+    ParamItem("variables"),
+]
+
+feed_arg_parser = make_enum_arg_parser(Feed)
+
 
 class FilingsSearchRequestFactory(RequestFactory):
-    def get_body_parameters(self, *_, **kwargs) -> dict:
-        query = kwargs.get("query", "")
-        variables = kwargs.get("variables", {})
-        return {"query": query, "variables": variables}
+    def get_body_parameters(self, *args, body_params_config=None, **kwargs) -> dict:
+        body_params = {}
+        if kwargs.get("query"):
+            body_params["query"] = kwargs.get("query")
+        if kwargs.get("variables"):
+            body_params["variables"] = kwargs.get("variables")
+
+        form_type = kwargs.get("form_type")
+        feed = kwargs.get("feed")
+        if feed:
+            feed = feed_arg_parser.get_str(feed)
+        org_id = kwargs.get("org_id")
+        start_date = kwargs.get("start_date")
+        end_date = kwargs.get("end_date")
+        text = kwargs.get("text")
+        sections = kwargs.get("sections")
+        limit = kwargs.get("limit")
+        sort_order = kwargs.get("sort_order")
+        if any(
+            (
+                form_type,
+                feed,
+                org_id,
+                start_date,
+                end_date,
+                text,
+                sections,
+                sort_order,
+                limit,
+            )
+        ):
+            body_params["query"] = get_query(
+                form_type=form_type,
+                feed=feed,
+                org_id=org_id,
+                start_date=start_date,
+                end_date=end_date,
+                text=text,
+                sections=sections,
+                limit=limit,
+                sort_order=sort_order,
+            )
+        return body_params
+
+    @property
+    def body_params_config(self):
+        return filings_search_body_params_config
 
 
 class FilingsSearchFile:
     def __init__(
         self,
         title: str = None,
         filename: str = None,
@@ -92,17 +142,15 @@
         if err:
             raise DownloadFileError(
                 code=None,
                 message=f"Cannot download file. Missing one of Filename, DCN, DocID and Filing ID",
             )
         return response.data.files.download(path=path)
 
-    async def download_async(
-        self, path: str = None, callback: Callable = None
-    ) -> DownloadAllFileResponse:
+    async def download_async(self, path: str = None, callback: Callable = None) -> DownloadAllFileResponse:
         """
 
         Parameters
         ----------
         path : str
             Destination of the download file. Default is current working directory.
         callback: Callable
@@ -120,30 +168,29 @@
                     message=f"Cannot download file. Missing one of Filename, DCN, DocID and Filing ID",
                 )
             ]
             return DownloadAllFileResponse(files=[self], errors=errors)
         return await response.data.files.download_async(path=path, callback=callback)
 
 
+@dataclass
 class FilingsSearchData(Data):
-    _files = None
+    _files: ListOfFile = None
 
     def _get_data_df(self):
         data_df = []
-        for row in self._raw["data"].get("FinancialFiling"):
+        for row in self.raw["data"].get("FinancialFiling"):
             filing_document = row.get("FilingDocument", {})
             identifiers = filing_document.get("Identifiers", [])
             dcn = ""
             if len(identifiers) > 0:
                 dcn = identifiers[0].get("Dcn", "")
             doc_id = filing_document.get("DocId", "")
             financial_filing_id = filing_document.get("FinancialFilingId", "")
-            document_title = filing_document.get("DocumentSummary", {}).get(
-                "DocumentTitle"
-            )
+            document_title = filing_document.get("DocumentSummary", {}).get("DocumentTitle")
             filenames = []
             if filing_document.get("FilesMetaData"):
                 filenames = filing_document.get("FilesMetaData", {})
             if len(filenames) == 0:
                 data_df.append(
                     [
                         document_title,
@@ -165,26 +212,26 @@
                         financial_filing_id,
                     ]
                 )
         return data_df
 
     @property
     def df(self):
-        if self._dataframe is None and self._raw and "errors" not in self._raw:
+        if self._dataframe is None and self.raw and "errors" not in self.raw:
             # generate headers for df
             headers = [
                 {"title": "DocumentTitle", "type": "string"},
                 {"title": "Filename", "type": "string"},
                 {"title": "MimeType", "type": "string"},
                 {"title": "Dcn", "type": "string"},
                 {"title": "DocId", "type": "string"},
                 {"title": "FinancialFilingId", "type": "int"},
             ]
             data_df = []
-            if self._raw.get("data") is not None:
+            if self.raw.get("data") is not None:
                 data_df = self._get_data_df()
             data_df = {"headers": headers, "data": data_df}
             self._dataframe = self._dfbuilder(data_df, **self._kwargs)
         return self._dataframe
 
     @property
     def files(self):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/_data_grid_type.py` & `refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_data_grid_type.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,29 +1,29 @@
-from enum import Enum
+from ..._base_enum import StrEnum
 from typing import TYPE_CHECKING, Tuple
 
 from ..._content_type import ContentType
 from ..._core.session.tools import is_platform_session
 from ..._tools import make_enum_arg_parser, ArgsParser, validate_bool_value
 
 if TYPE_CHECKING:
     from ..._core.session import Session
 
 
-class DataGridType(Enum):
+class DataGridType(StrEnum):
     UDF = "udf"
     RDP = "rdp"
 
 
 data_grid_types_arg_parser = make_enum_arg_parser(DataGridType)
 use_field_names_in_headers_arg_parser = ArgsParser(validate_bool_value)
 
 data_grid_type_value_by_content_type = {
-    DataGridType.UDF.value: ContentType.DATA_GRID_UDF,
-    DataGridType.RDP.value: ContentType.DATA_GRID_RDP,
+    DataGridType.UDF: ContentType.DATA_GRID_UDF,
+    DataGridType.RDP: ContentType.DATA_GRID_RDP,
 }
 
 content_type_by_data_grid_type = {
     ContentType.DATA_GRID_UDF: DataGridType.UDF,
     ContentType.DATA_GRID_RDP: DataGridType.RDP,
 }
 
@@ -37,15 +37,15 @@
     return data_grid_type
 
 
 def get_content_type(session: "Session") -> ContentType:
     from ...delivery._data._data_provider_factory import get_api_config
 
     config = get_api_config(ContentType.DATA_GRID_RDP, session.config)
-    name_platform = config.setdefault("underlying-platform", DataGridType.RDP.value)
+    name_platform = config.setdefault("underlying-platform", DataGridType.RDP)
     name_platform = data_grid_types_arg_parser.get_str(name_platform)
     content_type = data_grid_type_value_by_content_type.get(name_platform)
     return content_type
 
 
 def determine_content_type_and_flag(session: "Session") -> Tuple["ContentType", bool]:
     content_type = get_content_type(session)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_surfaces_data_provider.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,328 +1,356 @@
-from typing import TYPE_CHECKING, Iterable, Callable, List
-
-from ._data_grid_type import use_field_names_in_headers_arg_parser
-from .._content_data_provider import ContentDataProvider
-from .._content_response_factory import ContentResponseFactory
-from ..._tools import (
-    universe_arg_parser,
-    fields_arg_parser,
-    ADC_TR_PATTERN,
-    ADC_FUNC_PATTERN_IN_FIELDS,
-    cached_property,
-)
-from ...delivery._data import RequestMethod
-from ...delivery._data._data_provider import (
-    RequestFactory,
-    ContentValidator,
-    ValidatorContainer,
-    Request,
-)
+import warnings
+from dataclasses import dataclass
+from typing import Any, Callable, List, TYPE_CHECKING, Tuple
+
+import numpy as np
+import pandas as pd
+from numpy import iterable
+
+from ._models import Surface
+from .._content_provider import CurvesAndSurfacesRequestFactory, get_type_by_axis
+from .._enums import Axis
+from .._ipa_content_validator import IPAContentValidator
+from ..._content_data import Data
+from ..._content_data_provider import ContentDataProvider
+from ..._error_parser import ErrorParser
+from ...._tools import cached_property
+from ....delivery._data._data_provider import ValidatorContainer
+from ....delivery._data._endpoint_data import EndpointData
+from ....delivery._data._response_factory import ResponseFactory
 
 if TYPE_CHECKING:
-    from ...delivery._data._data_provider import ParsedData
+    from ....delivery._data._data_provider import ParsedData
 
-# --------------------------------------------------------------------------------------
-#   Response
-# --------------------------------------------------------------------------------------
-error_message_by_code = {
-    "default": "{error_message} Requested universes: {universes}. Requested fields: {"
-    "fields}",
-    412: "Unable to resolve all requested identifiers in {universes}.",
-    218: "Unable to resolve all requested fields in {fields}. The formula must "
-    "contain at least one field or function.",
-}
-
-
-class DataGridResponseFactory(ContentResponseFactory):
-    def create_fail(
-        self, parsed_data: "ParsedData", universe=None, fields=None, **kwargs
-    ):
-        error_code = parsed_data.first_error_code
-        if error_code not in error_message_by_code.keys():
-            parsed_data.error_messages = error_message_by_code["default"].format(
-                error_message=parsed_data.first_error_message,
-                fields=fields,
-                universes=universe,
-            )
-        else:
-            parsed_data.error_messages = error_message_by_code[error_code].format(
-                fields=fields, universes=universe
-            )
-
-        return super().create_fail(parsed_data, **kwargs)
-
-
-class DataGridRDPResponseFactory(DataGridResponseFactory):
-    def create_success(self, parsed_data: "ParsedData", **kwargs):
-        inst = super().create_success(parsed_data, **kwargs)
-        descriptions = (
-            self.get_raw(parsed_data).get("messages", {}).get("descriptions", [])
-        )
-        for descr in descriptions:
-            code = descr.get("code")
-            if code in {416, 413}:
-                inst.errors.append((code, descr.get("description")))
-
-        return inst
-
-
-class DataGridUDFResponseFactory(DataGridResponseFactory):
-    def get_raw(self, parsed_data: "ParsedData"):
-        return parsed_data.content_data.get("responses", [{}])[0]
-
-    def create_success(self, parsed_data: "ParsedData", **kwargs):
-        inst = super().create_success(parsed_data, **kwargs)
-        error = self.get_raw(parsed_data).get("error", [])
-        for err in error:
-            code = err.get("code")
-            if code == 416:
-                inst.errors.append((code, err.get("message")))
-
-        return inst
-
-
-# --------------------------------------------------------------------------------------
-#   Request
-# --------------------------------------------------------------------------------------
-
-
-def validate_correct_format_parameters(*_, **kwargs) -> dict:
-    parameters = kwargs.get("parameters")
-    extended_params = kwargs.get("extended_params")
-    universe = kwargs.get("universe")
-    fields = kwargs.get("fields")
-    use_field_names_in_headers = kwargs.get("use_field_names_in_headers")
-
-    if parameters is not None and not isinstance(parameters, dict):
-        raise ValueError(f"Arg parameters must be a dictionary")
-
-    extended_params = extended_params or {}
-    universe = extended_params.get("universe") or universe
-    universe = universe_arg_parser.get_list(universe)
-    universe = [value.upper() if value.islower() else value for value in universe]
-    fields = fields_arg_parser.get_list(fields)
-    use_field_names_in_headers = use_field_names_in_headers_arg_parser.get_bool(
-        use_field_names_in_headers
-    )
-
-    kwargs.update(
-        {
-            "universe": universe,
-            "fields": fields,
-            "parameters": parameters,
-            "use_field_names_in_headers": use_field_names_in_headers,
-            "extended_params": extended_params,
-        }
-    )
-    return kwargs
-
-
-class DataGridRDPRequestFactory(RequestFactory):
-    def get_body_parameters(self, *_, **kwargs) -> dict:
-        kwargs = validate_correct_format_parameters(*_, **kwargs)
-        body_parameters = {}
+SURFACE_WARNING = "Surface cannot be built for rd.content.ipa.surfaces.swaption."
+DF_WARNING = "Dataframe cannot be built for rd.content.ipa.surfaces.swaption."
 
-        universe = kwargs.get("universe")
-        if universe:
-            body_parameters["universe"] = universe
 
-        fields = kwargs.get("fields")
-        if fields:
-            body_parameters["fields"] = fields
+# ---------------------------------------------------------------------------
+#   ContentValidator
+# ---------------------------------------------------------------------------
 
-        parameters = kwargs.get("parameters")
-        if parameters:
-            body_parameters["parameters"] = parameters
 
-        layout = kwargs.get("layout")
-        if isinstance(layout, dict) and layout.get("output"):
-            body_parameters["output"] = layout["output"]
+class SurfacesContentValidator(IPAContentValidator):
+    @classmethod
+    def content_data_status_is_not_error(cls, data: "ParsedData") -> bool:
+        content_data = data.content_data
+        if isinstance(content_data.get("data"), list) and content_data.get("status") == "Error":
+            data.error_codes = content_data.get("code")
+            data.error_messages = content_data.get("message")
+            return False
 
-        return body_parameters
+        return True
 
-    def get_request_method(self, **kwargs) -> RequestMethod:
-        return RequestMethod.POST
+    @cached_property
+    def validators(self) -> List[Callable[["ParsedData"], bool]]:
+        return [
+            self.content_data_is_not_none,
+            self.content_data_status_is_not_error,
+            self.any_element_have_no_error,
+        ]
 
 
-class DataGridUDFRequestFactory(RequestFactory):
-    def create(self, session, *args, **kwargs):
-        url_root = session._get_rdp_url_root()
-        url = url_root.replace("rdp", "udf")
+# ---------------------------------------------------------------------------
+#   Data
+# ---------------------------------------------------------------------------
+
+
+def parse_axis(
+    universe: dict,
+    x_axis: Axis,
+    y_axis: Axis,
+) -> (np.array, np.array, np.array):
+    """
+      This method parsing the surface into lists row, column and matrix
+
+      >>> from refinitiv.data.content import ipa
+      >>> definition = ipa.surfaces.eti.Definition(
+      ...     underlying_definition=ipa.surfaces.eti.EtiSurfaceDefinition(
+      ...         instrument_code="BNPP.PA@RIC"
+      ...     ),
+      ...     surface_parameters=ipa.surfaces.eti.EtiCalculationParams(
+      ...         price_side=ipa.surfaces.eti.PriceSide.MID,
+      ...         volatility_model=ipa.surfaces.eti.VolatilityModel.SVI,
+      ...         x_axis=ipa.surfaces.eti.Axis.STRIKE,
+      ...         y_axis=ipa.surfaces.eti.Axis.DATE,
+      ...     ),
+      ...     surface_tag="1",
+      ...     surface_layout=ipa.surfaces.eti.SurfaceLayout(
+      ...         format=ipa.surfaces.eti.Format.MATRIX, y_point_count=10
+      ...     ),
+      ... )
+
+      This example for surface_parameters with
+      x_axis = Axis.STRIKE and y_axis = Axis.DATE
+
+      |-- column=Y
+      
+    row=X
+
+      >>> surface = universe.get("surface")
+      >>> surface
+      ... [
+      ...   [None,    '2021-08-20', '2021-09-17', '2021-12-17', '2022-03-18'],
+      ...   ['25.36',  63.76680855, 76.566676686, 514160483847, 45.563136028],
+      ...   ['30.432', 56.20802369, 64.051912234, 46.118622487, 41.540289743],
+      ...   ['35.504', 49.91436068, 51.916645386, 41.495311424, 37.870408673],
+      ... ]
+
+      Parameters
+      ----------
+      universe : dict
+          dict with surface
+      x_axis : Axis
+
+      y_axis : Axis
+
+      Returns
+      -------
+      (np.array, np.array, np.array)
+          row, column, matrix or x, y, z
+
+      Raises
+      -------
+      ValueError
+          If x_axis or y_axis not correct
+    """
+
+    if not x_axis or not y_axis:
+        raise ValueError(f"Cannot parse surface without information about x_axis={x_axis} or y_axis={y_axis}")
+
+    surface = universe.get("surface")
+
+    if surface is None:
+        # column is ["-2.00", "-1.00", "-0.50"]
+        column = universe.get("x")
+        # row is ["1Y", "2Y", "3Y"]
+        row = universe.get("y")
+        # universe has z axis too: ["1M", "2M", "3M"]
+        matrix = []
+        # z_dimension is [["129.03", "121.85", "123.85"]]
+        for z_dimension in universe.get("ndimensionalArray", []):
+            # Y dimension is ["129.03", "121.85", "123.85"]
+            # X dimension is "129.03"
+            # matrix is [["129.03", "121.85", "123.85"]]
+            matrix.extend(z_dimension)
+
+    else:
+        # column is ['2021-08-20', '2021-09-17', '2021-12-17', '2022-03-18', '2022-06-17']
+        column = surface[0][1:]
+        row = []
+        matrix = []
+        # curve is ['25.36',  63.76680855, 76.566676686, 514160483847, 41.187204258]
+        for curve in surface[1:]:
+            # row is '25.36'
+            row.append(curve[0])
+            # matrix is [63.76680855, 76.566676686, 514160483847, 41.187204258]
+            matrix.append(curve[1:])
+
+    try:
+        column = np.array(column, dtype=get_type_by_axis(y_axis))
+    except ValueError:
+        column = np.array(column, dtype=object)
+
+    try:
+        row = np.array(row, dtype=get_type_by_axis(x_axis))
+    except ValueError:
+        row = np.array(row, dtype=object)
+
+    matrix = np.array(matrix, dtype=float)
+
+    return row, column, matrix
+
+
+def create_surfaces(raw, axes_params) -> List[Surface]:
+    surfaces = []
+
+    if raw and axes_params:
+        for i, universe in enumerate(raw.get("data")):
+            x_axis, y_axis = axes_params[i]
+            row, column, matrix = parse_axis(universe, x_axis, y_axis)
+            surface = Surface(row=row, column=column, matrix=matrix)
+            surfaces.append(surface)
+
+    return surfaces
+
+
+@dataclass
+class BaseData(EndpointData):
+    _dataframe: "pd.DataFrame" = None
+    _axes_params: List = None
+
+    @property
+    def df(self):
+        if self._dataframe is None and self.raw:
+            data = self.raw.get("data")
+
+            if data:
+                surface = data[0].get("surface")
+                if isinstance(surface, dict):
+                    data_frame = pd.DataFrame([])
+                else:
+                    data_frame = pd.DataFrame(data)
+                    data_frame.set_index("surfaceTag", inplace=True)
 
-        method = self.get_request_method(**kwargs)
-        header_parameters = kwargs.get("header_parameters") or {}
-        extended_params = kwargs.get("extended_params") or {}
-        body_parameters = self.get_body_parameters(*args, **kwargs)
-        body_parameters = self.extend_body_parameters(body_parameters, extended_params)
+            else:
+                data_frame = pd.DataFrame([])
 
-        headers = {"Content-Type": "application/json"}
-        headers.update(header_parameters)
+            if not data_frame.empty:
+                data_frame = data_frame.convert_dtypes()
 
-        return Request(
-            url=url,
-            method=method,
-            headers=headers,
-            json={
-                "Entity": {
-                    "E": "DataGrid_StandardAsync",
-                    "W": {"requests": [body_parameters]},
-                }
-            },
-        )
+            self._dataframe = data_frame
 
-    def get_body_parameters(self, *_, **kwargs) -> dict:
-        ticket = kwargs.get("ticket", None)
-        if ticket:
-            return {"ticket": ticket}
+        return self._dataframe
 
-        kwargs = validate_correct_format_parameters(*_, **kwargs)
-        body_parameters = {}
 
-        instruments = kwargs.get("universe")
-        if instruments:
-            body_parameters["instruments"] = instruments
+@dataclass
+class OneSurfaceData(BaseData):
+    _surface: Surface = None
 
-        fields = kwargs.get("fields")
-        if fields:
-            body_parameters["fields"] = [
-                {"name": i}
-                for i in fields
-                if ADC_TR_PATTERN.match(i) or ADC_FUNC_PATTERN_IN_FIELDS.match(i)
-            ]
+    @property
+    def surface(self) -> Surface:
+        if self._surface is None:
+            surfaces = create_surfaces(self.raw, self._axes_params)
+            self._surface = surfaces[0]
+        return self._surface
 
-        parameters = kwargs.get("parameters")
-        if parameters:
-            body_parameters["parameters"] = parameters
+    @property
+    def df(self):
+        if self._dataframe is None:
+            data = {x: z for x, z in zip(self.surface.x, self.surface.z)}
 
-        layout = kwargs.get("layout")
-        if isinstance(layout, dict) and layout.get("layout"):
-            body_parameters["layout"] = layout["layout"]
+            if data:
+                data_frame = pd.DataFrame(data, index=self.surface.y)
+            else:
+                data_frame = super().df
 
-        return body_parameters
+            if not data_frame.empty:
+                data_frame.fillna(pd.NA, inplace=True)
+                data_frame = data_frame.convert_dtypes()
 
-    def get_request_method(self, **kwargs) -> RequestMethod:
-        return RequestMethod.POST
+            self._dataframe = data_frame
 
+        return self._dataframe
 
-# --------------------------------------------------------------------------------------
-#   Content data validator
-# --------------------------------------------------------------------------------------
 
+@dataclass
+class OneSwaptionSurfaceData(Data):
+    @property
+    def surface(self):
+        warnings.warn(SURFACE_WARNING)
+        return Surface([], [], [[]])
 
-class DataGridContentValidator(ContentValidator):
-    @classmethod
-    def status_is_not_error(cls, data: "ParsedData") -> bool:
-        status_content = data.status.get("content", "")
-        if status_content.startswith("Failed"):
-            data.error_codes = -1
-            data.error_messages = status_content
-            return False
+    @property
+    def df(self):
+        warnings.warn(DF_WARNING)
+        return pd.DataFrame()
 
-        return True
 
+@dataclass
+class SurfacesData(BaseData):
+    _surfaces: List[Surface] = None
 
-class DataGridRDPContentValidator(DataGridContentValidator):
-    @classmethod
-    def content_data_has_no_error(cls, data: "ParsedData") -> bool:
-        content_data = data.content_data
-        error = content_data.get("error")
-        if error and not content_data.get("data"):
-            data.error_codes = error.get("code", -1)
-            data.error_messages = error.get("description")
-
-            if not data.error_messages:
-                error_message = error.get("message")
-                errors = error.get("errors")
-
-                if isinstance(errors, list):
-                    error_message += ":\n"
-                    error_message += "\n".join(map(str, errors))
+    @property
+    def surfaces(self) -> List[Surface]:
+        if self._surfaces is None:
+            self._surfaces = create_surfaces(self.raw, self._axes_params)
+        return self._surfaces
 
-                data.error_messages = error_message
 
-            return False
+@dataclass
+class SwaptionSurfacesData(Data):
+    @property
+    def df(self):
+        warnings.warn(DF_WARNING)
+        return pd.DataFrame()
 
-        return True
+    @property
+    def surfaces(self):
+        warnings.warn(SURFACE_WARNING)
+        return []
 
-    @cached_property
-    def validators(self) -> List[Callable[["ParsedData"], bool]]:
-        return [
-            self.status_is_not_error,
-            self.content_data_is_not_none,
-            self.content_data_has_no_error,
-        ]
 
+# ---------------------------------------------------------------------------
+#   ResponseFactory
+# ---------------------------------------------------------------------------
 
-class DataGridUDFContentValidator(DataGridContentValidator):
-    @classmethod
-    def content_data_is_valid_type(cls, data: "ParsedData") -> bool:
-        content_data = data.content_data
-        if isinstance(content_data, str):
-            data.error_codes = -1
-            data.error_messages = content_data
-            return False
 
-        return True
+def get_surface_parameters(obj):
+    if hasattr(obj, "_kwargs"):
+        request_item = obj._kwargs.get("universe")
 
-    @classmethod
-    def content_data_has_valid_response(cls, data: "ParsedData") -> bool:
-        responses = data.content_data.get("responses", [])
-        first_response = responses[0] if responses else {}
-        error = first_response.get("error")
-        if error and not first_response.get("data"):
-            if isinstance(error, dict):
-                data.error_codes = error.get("code", -1)
-                data.error_messages = error.get("message", error)
+    else:
+        request_item = obj
 
-            else:
-                data.error_codes = -1
-                data.error_messages = error
+    return request_item.surface_parameters
 
-            return False
 
-        return True
+def get_names_axis(surface_parameters) -> Tuple[str, str]:
+    x_axis = surface_parameters._get_enum_parameter(Axis, "xAxis")
+    y_axis = surface_parameters._get_enum_parameter(Axis, "yAxis")
+    return x_axis, y_axis
 
-    @classmethod
-    def content_data_response_has_valid_data(cls, data: "ParsedData") -> bool:
-        responses = data.content_data.get("responses", [])
-        first_response = responses[0] if responses else {}
-        error = first_response.get("error")
-        if error and not any(
-            any(items[1:]) if isinstance(items, Iterable) else False
-            for items in first_response.get("data")
-        ):
-            first_error = error[0]
-            data.error_codes = first_error.get("code", -1)
-            data.error_messages = first_error.get("message", first_error)
-            return False
 
-        return True
+def get_axis_params(obj, axis_params: list = None) -> List[Tuple[str, str]]:
+    if axis_params is None:
+        axis_params = []
 
-    @cached_property
-    def validators(self) -> List[Callable[["ParsedData"], bool]]:
-        return [
-            self.status_is_not_error,
-            self.content_data_is_not_none,
-            self.content_data_is_valid_type,
-            self.content_data_has_no_error,
-            self.content_data_has_valid_response,
-            self.content_data_response_has_valid_data,
-        ]
+    surface_parameters = get_surface_parameters(obj)
+    axis_params.append(get_names_axis(surface_parameters))
+    return axis_params
+
+
+class SurfaceResponseFactory(ResponseFactory):
+    def create_data_success(self, raw: Any, **kwargs) -> EndpointData:
+        return self._do_create_data(raw, **kwargs)
+
+    def create_data_fail(self, raw: Any, **kwargs) -> EndpointData:
+        return self._do_create_data({}, **kwargs)
+
+    def _do_create_data(self, raw: Any, universe=None, **kwargs):
+        if universe:
+            if iterable(universe):
+                axes_params = []
+                for definition in universe:
+                    get_axis_params(definition, axes_params)
+
+                return SurfacesData(raw, _axes_params=axes_params)
+
+            surface_parameters = get_surface_parameters(universe)
+            if surface_parameters and all(get_names_axis(surface_parameters)):
+                axes_params = get_axis_params(universe)
+                return OneSurfaceData(raw, _axes_params=axes_params)
+
+        return BaseData(raw)
+
+
+class SwaptionSurfaceResponseFactory(SurfaceResponseFactory):
+    def _do_create_data(self, raw: Any, universe=None, **kwargs):
+        if universe:
+            if iterable(universe):
+                return SwaptionSurfacesData(raw)
+
+            surface_parameters = get_surface_parameters(universe)
+            if surface_parameters and all(get_names_axis(surface_parameters)):
+                return OneSwaptionSurfaceData(raw)
 
+        return BaseData(raw)
 
-# --------------------------------------------------------------------------------------
-#   Providers
-# --------------------------------------------------------------------------------------
 
+# ---------------------------------------------------------------------------
+#   DataProvider
+# ---------------------------------------------------------------------------
 
-data_grid_rdp_data_provider = ContentDataProvider(
-    request=DataGridRDPRequestFactory(),
-    response=DataGridRDPResponseFactory(),
-    validator=ValidatorContainer(content_validator=DataGridRDPContentValidator()),
+surfaces_data_provider = ContentDataProvider(
+    request=CurvesAndSurfacesRequestFactory(),
+    response=SurfaceResponseFactory(),
+    validator=ValidatorContainer(content_validator=SurfacesContentValidator()),
+    parser=ErrorParser(),
 )
 
-data_grid_udf_data_provider = ContentDataProvider(
-    request=DataGridUDFRequestFactory(),
-    response=DataGridUDFResponseFactory(),
-    validator=ValidatorContainer(content_validator=DataGridUDFContentValidator()),
+swaption_surfaces_data_provider = ContentDataProvider(
+    request=CurvesAndSurfacesRequestFactory(),
+    response=SwaptionSurfaceResponseFactory(),
+    validator=ValidatorContainer(content_validator=SurfacesContentValidator()),
+    parser=ErrorParser(),
 )
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/fundamental_and_reference/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/fundamental_and_reference/_definition.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,35 +1,45 @@
-# coding: utf8
-
 import asyncio
 import time
-from enum import Enum, unique
-from typing import Callable, List, Union, TYPE_CHECKING, Optional
+from enum import unique
+from typing import Callable, List, Union, TYPE_CHECKING, Optional, Any
 
 from ._data_grid_type import (
     DataGridType,
     determine_content_type_and_flag,
+    use_field_names_in_headers_arg_parser,
 )
 from .._content_data import Data
 from .._content_provider_layer import ContentUsageLoggerMixin
-from ..._content_type import ContentType
 from .._df_build_type import DFBuildType
+from ..._base_enum import StrEnum
+from ..._content_type import ContentType
 from ..._core.session import get_valid_session
-from ..._tools import ArgsParser, make_convert_to_enum_arg_parser, try_copy_to_list
+from ..._tools import (
+    ArgsParser,
+    make_convert_to_enum_arg_parser,
+    try_copy_to_list,
+    fields_arg_parser,
+    universe_arg_parser,
+)
 from ..._tools import create_repr
 from ...delivery._data._data_provider import DataProviderLayer, BaseResponse
 from ...errors import RDError
 
 if TYPE_CHECKING:
     from ..._types import ExtendedParams, OptDict, StrStrings
     from ..._core.session import Session
 
+MIN_TICKET_DURATION_MS = 15000
+
 
 @unique
-class RowHeaders(Enum):
+class RowHeaders(StrEnum):
+    """Possible values for row headers."""
+
     DATE = "date"
 
 
 OptRowHeaders = Optional[Union[str, List[str], RowHeaders, List[RowHeaders]]]
 
 row_headers_enum_arg_parser = make_convert_to_enum_arg_parser(RowHeaders)
 
@@ -72,80 +82,83 @@
             output = f"{output},date|"
             layout["output"] = output
 
     else:
         layout = ""
 
     if layout is None:
-        raise ValueError(
-            f"Layout is None, row_headers={row_headers}, content_type={content_type}"
-        )
+        raise ValueError(f"Layout is None, row_headers={row_headers}, content_type={content_type}")
 
     return layout
 
 
 def get_dfbuild_type(row_headers: List[RowHeaders]) -> DFBuildType:
     dfbuild_type = DFBuildType.INDEX
 
     if RowHeaders.DATE in row_headers:
         dfbuild_type = DFBuildType.DATE_AS_INDEX
 
     return dfbuild_type
 
 
-class Definition(
-    ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]
-):
+class Definition(ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]):
     """
-    This class describe the universe (list of instruments), the fields
-    (a.k.a. data items) and parameters that will be requested to the data platform
+    Defines the Fundamental and Reference data to retrieve.
 
     Parameters:
     ----------
     universe : str or list of str
-        The list of RICs
+        Single instrument or list of instruments.
     fields : str or list of str
-        List of fundamental field names
+        Single field or list of fields.
     parameters : dict, optional
-        Global parameters for fields
+        Fields global parameters.
     row_headers : str, list of str, list of RowHeaders enum
-        When this parameter is used, the output/layout parameter will be added
-        to the underlying request to DataGrid RDP or UDF
+        Output/layout parameters to add to the underlying request. Put headers to rows in the response.
     use_field_names_in_headers : bool, optional
-        If value is True we add field names in headers.
+        Boolean that indicates whether or not to add field names in the headers.
     extended_params : dict, optional
-        Other parameters can be provided if necessary
+        Specifies the parameters that will be merged with the request.
 
     Examples
     --------
-     >>> from refinitiv.data.content import fundamental_and_reference
-     >>> definition = fundamental_and_reference.Definition(["IBM"], ["TR.Volume"])
-     >>> definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> from refinitiv.data.content import fundamental_and_reference
+    >>> definition = fundamental_and_reference.Definition(["IBM"], ["TR.Volume"])
+    >>> definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "FundamentalAndReference.Definition"
 
     def __init__(
         self,
         universe: "StrStrings",
         fields: "StrStrings",
         parameters: "OptDict" = None,
         row_headers: OptRowHeaders = None,
         use_field_names_in_headers: bool = False,
         extended_params: "ExtendedParams" = None,
     ):
-        self.universe = try_copy_to_list(universe)
-        self.fields = try_copy_to_list(fields)
+        extended_params = extended_params or {}
+        universe = extended_params.get("universe") or try_copy_to_list(universe)
+        universe = universe_arg_parser.get_list(universe)
+        universe = [value.upper() for value in universe]
+        self.universe = universe
+        self.fields = fields_arg_parser.get_list(try_copy_to_list(fields))
+
+        if parameters is not None and not isinstance(parameters, dict):
+            raise ValueError(f"Arg parameters must be a dictionary")
+
         self.parameters = parameters
-        self.use_field_names_in_headers = use_field_names_in_headers
+        self.use_field_names_in_headers = use_field_names_in_headers_arg_parser.get_bool(use_field_names_in_headers)
         self.extended_params = extended_params
         self.row_headers = try_copy_to_list(row_headers)
         super().__init__(
             data_type=ContentType.DEFAULT,
             universe=self.universe,
             fields=self.fields,
             parameters=self.parameters,
@@ -154,15 +167,15 @@
             extended_params=self.extended_params,
         )
 
     def _update_content_type(self, session: "Session"):
         content_type, changed = determine_content_type_and_flag(session)
         changed and session.debug(
             f"UDF DataGrid service cannot be used with platform sessions, RDP DataGrid will be used instead. "
-            f"The \"/apis/data/datagrid/underlying-platform = '{DataGridType.UDF.value}'\" "
+            f"The \"/apis/data/datagrid/underlying-platform = '{DataGridType.UDF}'\" "
             f"parameter will be discarded, meaning that the regular RDP DataGrid "
             f"service will be used for Fundamental and Reference data requests."
         )
         self._initialize(content_type, **self._kwargs)
         row_headers = self._kwargs.get("row_headers")
         row_headers = row_headers_arg_parser.get_list(row_headers)
         layout = get_layout(row_headers, content_type)
@@ -176,91 +189,102 @@
             if response and response.data and response.data.raw.get("ticket"):
                 return
             callback(response, data_provider, session)
 
         return on_response
 
     @staticmethod
-    def _get_duration(raw_response: dict) -> Union[int, None]:
+    def _get_duration(raw: dict) -> int:
         """
         Compute the duration to sleep before next retry to request ticket status
 
-        :param raw_response: request's response
-        :type raw_response: dict
-        :return: duration if response contains "estimatedDuration", None otherwise
+        Parameters
+        ----------
+        raw : dict
+            e.g. {"estimatedDuration": 44000, "ticket": "78BF26B24A9D416E"}
+
+        Raises
+        ------
+        RDError
+            If raw does not contain "estimatedDuration"
+
+        Returns
+        -------
+        int
+            Duration in seconds
         """
-        ticket_duration = raw_response.get("estimatedDuration")
-        if ticket_duration:
-            ticket_duration = int(min(ticket_duration, 15000) / 1000)
-            return ticket_duration
-        return None
+        estimated_duration_ms = raw.get("estimatedDuration")
+        if estimated_duration_ms:
+            duration_sec = min(estimated_duration_ms, MIN_TICKET_DURATION_MS) // 1000
+            return duration_sec
+
+        raise RDError(-1, "Received a ticket response from DataGrid without estimatedDuration")
 
     def get_data(
         self,
         session: Optional["Session"] = None,
         on_response: Optional[Callable] = None,
     ):
         """
-        Returns a response to the data platform
+        Sends a request to the Refinitiv Data Platform to retrieve the data.
 
         Parameters
         ----------
         session : Session, optional
-            Means default session would be used
+            Session object. If it's not passed the default session will be used.
         on_response : Callable, optional
-            Callable object to process retrieved data
+            User-defined callback function to process received response.
 
         Returns
         -------
         Response
 
         Raises
         ------
         AttributeError
             If user didn't set default session.
         """
         session = get_valid_session(session)
         self._update_content_type(session)
+
         if self._content_type == ContentType.DATA_GRID_UDF:
             on_response_filter = on_response and self.make_on_response(on_response)
             response = super().get_data(session, on_response_filter)
+            raw = response.data.raw
+            ticket = raw.get("ticket")
+
+            while ticket:
+                time.sleep(self._get_duration(raw))
+                self._kwargs["ticket"] = ticket
+                response = super().get_data(session, on_response_filter)
+                raw = response.data.raw
+                ticket = raw.get("ticket")
 
-            while response.data:
-                ticket = response.data.raw.get("ticket")
-                if ticket:
-                    sleep_duration = self._get_duration(response.data.raw)
-                    if sleep_duration:
-                        time.sleep(sleep_duration)
-                    else:
-                        raise RDError(
-                            -1,
-                            "Receive ticket response from DataGrid without estimatedDuration",
-                        )
-                    self._kwargs["ticket"] = ticket
-                    response = super().get_data(session, on_response_filter)
-                else:
-                    break
         else:
             response = super().get_data(session, on_response)
+
         return response
 
     async def get_data_async(
         self,
         session: Optional["Session"] = None,
         on_response: Optional[Callable] = None,
+        closure: Optional[Any] = None,
     ):
         """
-        Returns a response asynchronously to the data platform
+        Sends an asynchronous request to the Refinitiv Data Platform to retrieve the data.
 
         Parameters
         ----------
         session : Session, optional
-            Means default session would be used
+            Session object. If it's not passed the default session will be used.
         on_response : Callable, optional
-            Callable object to process retrieved data
+            User-defined callback function to process received response.
+        closure : any, optional
+            Optional closure that will be passed to the headers and returned
 
         Returns
         -------
         Response
 
         Raises
         ------
@@ -269,34 +293,28 @@
 
         """
         session = get_valid_session(session)
         self._update_content_type(session)
 
         if self._content_type == ContentType.DATA_GRID_UDF:
             on_response_filter = on_response and self.make_on_response(on_response)
-            response = await super().get_data_async(session, on_response_filter)
-
-            while response.data:
-                ticket = response.data.raw.get("ticket")
-                if ticket:
-                    sleep_duration = self._get_duration(response.data.raw)
-                    if sleep_duration:
-                        await asyncio.sleep(sleep_duration)
-                    else:
-                        raise RDError(
-                            -1,
-                            "Receive ticket response from DataGrid without estimatedDuration",
-                        )
-                    self._kwargs["ticket"] = ticket
-                    response = await super().get_data_async(session, on_response_filter)
-                else:
-                    break
+            response = await super().get_data_async(session, on_response_filter, closure)
+            raw = response.data.raw
+            ticket = raw.get("ticket")
+
+            while ticket:
+                await asyncio.sleep(self._get_duration(raw))
+                self._kwargs["ticket"] = ticket
+                response = await super().get_data_async(session, on_response_filter, closure)
+                raw = response.data.raw
+                ticket = raw.get("ticket")
 
         else:
-            response = await super().get_data_async(session, on_response)
+            response = await super().get_data_async(session, on_response, closure)
+
         return response
 
     def __repr__(self):
         return create_repr(
             self,
             content=f"{{"
             f"universe='{self.universe}', "
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_enums.py` & `refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_enums.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,27 +1,21 @@
-from enum import Enum, unique
+from enum import unique
 from typing import Union, List, Optional
 
 from ..._tools import make_enum_arg_parser
-
+from ..._base_enum import StrEnum
 
 # --------------------------------------------------------------------------------------
 #   EventTypes
 # --------------------------------------------------------------------------------------
 
 
 @unique
-class EventTypes(Enum):
-    """
-    The list of market events (comma delimiter), supported event types are trade,
-    quote and correction.
-    Note: Currently support only single event type.
-        If request with multiple event types,
-        the backend will pick up the first event type to proceed.
-    """
+class EventTypes(StrEnum):
+    """The list of market events, supported event types are trade, quote and correction."""
 
     TRADE = "trade"
     QUOTE = "quote"
     CORRECTION = "correction"
 
 
 OptEventTypes = Optional[Union[str, List[str], EventTypes, List[EventTypes]]]
@@ -30,15 +24,15 @@
 
 # --------------------------------------------------------------------------------------
 #   Adjustments
 # --------------------------------------------------------------------------------------
 
 
 @unique
-class Adjustments(Enum):
+class Adjustments(StrEnum):
     """
     The list of adjustment types (comma delimiter) that tells the system whether
      to apply or not apply CORAX (Corporate Actions) events or
      exchange/manual corrections to historical time series data.
 
      The supported values of adjustments :
 
@@ -75,27 +69,18 @@
 
 # --------------------------------------------------------------------------------------
 #   MarketSession
 # --------------------------------------------------------------------------------------
 
 
 @unique
-class MarketSession(Enum):
+class MarketSession(StrEnum):
     """
-    The marketsession parameter represents a list of interested official durations
-        in which trade and quote activities occur for a particular universe.
-
-    The supported values of marketsession :
-
-        PRE - specifies that data returned
-              should include data during pre-market session
-        NORMAL - specifies that data returned
-                 should include data during normal market session
-        POST - specifies that data returned
-               should include data during post-market session
+    The Market Session represents a list of interested official durations in which trade and quote activities occur
+    for a particular instrument.
     """
 
     PRE = "pre"
     NORMAL = "normal"
     POST = "post"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_events_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_events_definition.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,43 @@
 from typing import TYPE_CHECKING
 
 from .._content_data import Data
 from .._content_provider_layer import ContentUsageLoggerMixin
 from ..._content_type import ContentType
-from ..._tools import hp_universe_parser, validate_types, try_copy_to_list
+from ..._tools import hp_universe_parser, validate_types, try_copy_to_list, hp_datetime_adapter
 from ...delivery._data._data_provider import BaseResponse, DataProviderLayer
 
 if TYPE_CHECKING:
     from ._enums import OptAdjustments, OptEventTypes
     from ..._types import OptDateTime, StrStrings, OptInt, ExtendedParams, OptStrStrs
 
 
-class Definition(
-    ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]
-):
+class Definition(ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]):
     """
-    Summary line of this class that defines parameters for requesting events from historical pricing
+    Defines the Historical Pricing Events to be retrieved.
 
     Parameters
     ----------
     universe : str or list of str
-        The entity universe
+        Single instrument or list of instruments.
     eventTypes : list of EventTypes or EventTypes or str, optional
-        The market events EventTypes
+        Single market event or list of events.
     start : str or date or datetime or timedelta, optional
-        The start date and timestamp of the query in ISO8601 with UTC only
+        Start time for the events query.
     end : str or date or datetime or timedelta, optional
-        The end date and timestamp of the query in ISO8601 with UTC only
+        End time for the events query.
     adjustments : list of Adjustments or Adjustments or str, optional
-        The adjustment list or Adjustments type
+        Single adjustment type or list of adjustment types to apply CORAX (Corporate Actions) events or
+        exchange/manual corrections to the historical time series data.
     count : int, optional
-        The maximum number of data returned. Values range: 1 - 10000
+        The maximum number of rows to return.
     fields : list, optional
-        The list of fields that are to be returned in the response
+        List of fields to return.
     extended_params : dict, optional
-        If necessary other parameters
+        Additional parameters to apply to the request.
 
     Examples
     --------
     >>> from refinitiv.data.content.historical_pricing import events
     >>> definition_events = events.Definition("EUR")
     >>> response = definition_events.get_data()
 
@@ -53,14 +52,16 @@
         start: "OptDateTime" = None,
         end: "OptDateTime" = None,
         adjustments: "OptAdjustments" = None,
         count: "OptInt" = None,
         fields: "OptStrStrs" = None,
         extended_params: "ExtendedParams" = None,
     ):
+        start = hp_datetime_adapter.get_localize(start)
+        end = hp_datetime_adapter.get_localize(end)
         validate_types(count, [int, type(None)], "count")
         universe = try_copy_to_list(universe)
         universe = hp_universe_parser.get_list(universe)
         event_types = try_copy_to_list(eventTypes)
         adjustments = try_copy_to_list(adjustments)
         fields = try_copy_to_list(fields)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_historical_pricing_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_historical_pricing_data_provider.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_historical_pricing_request_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_historical_pricing_request_factory.py`

 * *Files 4% similar despite different names*

```diff
@@ -25,17 +25,15 @@
 
 content_type_by_day_interval_type = {
     DayIntervalType.INTER: ContentType.HISTORICAL_PRICING_INTERDAY_SUMMARIES,
     DayIntervalType.INTRA: ContentType.HISTORICAL_PRICING_INTRADAY_SUMMARIES,
 }
 
 
-def get_content_type_by_interval(
-    interval: Union[str, Intervals, DayIntervalType]
-) -> ContentType:
+def get_content_type_by_interval(interval: Union[str, Intervals, DayIntervalType]) -> ContentType:
     day_interval_type = get_day_interval_type(interval)
     return content_type_by_day_interval_type.get(day_interval_type)
 
 
 def check_count(value):
     if value is not None and value < 1:
         raise ValueError("Count minimum value is 1")
@@ -59,22 +57,18 @@
     ),
     ValueParamItem("count", function=check_count),
     ParamItem("fields", function=get_fields_summaries),
 ]
 
 hp_events_query_params = [
     ValueParamItem("interval", function=interval_arg_parser.get_str),
-    ValueParamItem(
-        "event_types", "eventTypes", partial(event_types_arg_parser.get_str, delim=",")
-    ),
+    ValueParamItem("event_types", "eventTypes", partial(event_types_arg_parser.get_str, delim=",")),
     ValueParamItem("start", function=hp_datetime_adapter.get_str, is_true=is_date_true),
     ValueParamItem("end", function=hp_datetime_adapter.get_str, is_true=is_date_true),
-    ValueParamItem(
-        "adjustments", function=partial(adjustments_arg_parser.get_str, delim=",")
-    ),
+    ValueParamItem("adjustments", function=partial(adjustments_arg_parser.get_str, delim=",")),
     ValueParamItem("count", function=check_count),
     ParamItem("fields", function=get_fields_events),
 ]
 
 
 class HistoricalPricingRequestFactory(RequestFactory):
     def get_url(self, *args, **kwargs):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/historical_pricing/_summaries_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/historical_pricing/_summaries_definition.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 from typing import Union, TYPE_CHECKING
 
 from .._content_data import Data
 from .._intervals import DayIntervalType, Intervals, get_day_interval_type
-from ..._tools import hp_universe_parser, validate_types, try_copy_to_list
+from ..._tools import hp_universe_parser, validate_types, try_copy_to_list, hp_datetime_adapter
 from ...delivery._data._data_provider import BaseResponse, DataProviderLayer
 from ._historical_pricing_request_factory import get_content_type_by_interval
 
 if TYPE_CHECKING:
     from ._enums import OptAdjustments, OptMarketSession
     from ..._types import (
         OptInt,
@@ -15,36 +15,37 @@
         StrStrings,
         StrStrings,
     )
 
 
 class Definition(DataProviderLayer[BaseResponse[Data]]):
     """
-    Summary line of this class that defines parameters for requesting summaries from historical pricing
+    Creates a definition containing a summary of the specified historical pricing events.
 
     Parameters
     ----------
     universe : str or list of str
-        The entity universe
+        Single instrument or list of instruments.
     interval : str or Intervals, optional
-        The consolidation interval in ISO8601
+        Predefined interval for filtering historical pricing events.
     start : str or date or datetime or timedelta, optional
-        The start date and timestamp of the query in ISO8601 with UTC only
+        Start time for the events query.
     end : str or date or datetime or timedelta, optional
-        The end date and timestamp of the query in ISO8601 with UTC only
+        End time for the events query.
     adjustments : list of Adjustments or Adjustments or str, optional
-        The adjustment list or Adjustments type
+        Single adjustment type or list of adjustment types to apply CORAX (Corporate Actions) events or exchange/manual
+        corrections to the historical time series data.
     sessions : list of MarketSession or MarketSession or str, optional
-        The list of market session classification or str
+        Market session durations, such as pre-market session, normal market session and post-market session.
     count : int, optional
-        The maximum number of data returned. Values range: 1 - 10000
+        The maximum number of rows to return.
     fields : list, optional
-        The list of fields that are to be returned in the response
+        The list of fields to return.
     extended_params : dict, optional
-        If necessary other parameters
+        Additional parameters to apply to the request.
 
     Examples
     --------
     >>> from refinitiv.data.content.historical_pricing import summaries
     >>> definition_summaries = summaries.Definition("EUR")
     >>> response = definition_summaries.get_data()
 
@@ -58,14 +59,16 @@
         end: "OptDateTime" = None,
         adjustments: "OptAdjustments" = None,
         sessions: "OptMarketSession" = None,
         count: "OptInt" = None,
         fields: "StrStrings" = None,
         extended_params: "ExtendedParams" = None,
     ) -> None:
+        start = hp_datetime_adapter.get_localize(start)
+        end = hp_datetime_adapter.get_localize(end)
         # By default, if interval is not defined, interday default value is requested
         day_interval_type = get_day_interval_type(interval or DayIntervalType.INTER)
         content_type = get_content_type_by_interval(day_interval_type)
         validate_types(count, [int, type(None)], "count")
         universe = try_copy_to_list(universe)
         universe = hp_universe_parser.get_list(universe)
         adjustments = try_copy_to_list(adjustments)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_content_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_content_provider.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -218,23 +218,19 @@
         self._set_enum_parameter(IssuerType, "issuerType", value)
 
     @property
     def main_constituent_asset_class(self):
         """
         :return: enum MainConstituentAssetClass
         """
-        return self._get_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass"
-        )
+        return self._get_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass")
 
     @main_constituent_asset_class.setter
     def main_constituent_asset_class(self, value):
-        self._set_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass", value
-        )
+        self._set_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass", value)
 
     @property
     def rating(self):
         """
         Rating of the issuer. the rating can be defined by using any of: "refinitiv",
         "s&p", "moody's", "fitch", "dbrs" convention
         :return: enum Rating
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_parameters.py`

 * *Files 1% similar despite different names*

```diff
@@ -203,23 +203,19 @@
     @property
     def basis_spline_smooth_model(self):
         """
         Basis spline model. values can be: - mccullochlinearregression -
         waggonersmoothingsplinemodel - andersonsmoothingsplinemodel
         :return: enum BasisSplineSmoothModel
         """
-        return self._get_enum_parameter(
-            BasisSplineSmoothModel, "basisSplineSmoothModel"
-        )
+        return self._get_enum_parameter(BasisSplineSmoothModel, "basisSplineSmoothModel")
 
     @basis_spline_smooth_model.setter
     def basis_spline_smooth_model(self, value):
-        self._set_enum_parameter(
-            BasisSplineSmoothModel, "basisSplineSmoothModel", value
-        )
+        self._set_enum_parameter(BasisSplineSmoothModel, "basisSplineSmoothModel", value)
 
     @property
     def calendar_adjustment(self):
         """
         The cash flow adjustment according to a selected calendar. the possible values
         are:   * no   * weekend: for the cash flow pricing using the calendar 'weekend'
         * calendar: for the cash flow pricing using the calendar defined by the
@@ -361,23 +357,19 @@
         (regardless of the date of the first day or last day of the period).   *
         dcb_actual_364     a special case of actual/actual (isma) when a coupon period
         contains 91 or 182 days. actual/364 applies for some short-term instruments.
         day count basis = 364.   * dcb_30_actual_isda   * dcb_30_365_brazil   *
         dcb_actual_365p   * dcb_constant
         :return: enum InterestCalculationMethod
         """
-        return self._get_enum_parameter(
-            InterestCalculationMethod, "interestCalculationMethod"
-        )
+        return self._get_enum_parameter(InterestCalculationMethod, "interestCalculationMethod")
 
     @interest_calculation_method.setter
     def interest_calculation_method(self, value):
-        self._set_enum_parameter(
-            InterestCalculationMethod, "interestCalculationMethod", value
-        )
+        self._set_enum_parameter(InterestCalculationMethod, "interestCalculationMethod", value)
 
     @property
     def interpolation_mode(self):
         """
         The interpolation method used in zero curve bootstrapping. the possible values
         are:   * cubicdiscount: local cubic interpolation of discount factors   *
         cubicrate: local cubic interpolation of rates   * cubicspline: a natural cubic
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_request.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,50 +1,63 @@
 from typing import TYPE_CHECKING
 
+from ._credit_constituents import CreditConstituents
 from ...._object_definition import ObjectDefinition
 from ._credit_curve_definition import CreditCurveDefinition
 from ._credit_curve_parameters import CreditCurveParameters
 
 
 if TYPE_CHECKING:
     from ......_types import OptStr
-    from .._types import (
-        CurveDefinition,
-        CurveParameters,
-    )
+    from .._types import CurveDefinition, CurveParameters, OptCreditConstituents
 
 
 class RequestItem(ObjectDefinition):
     """
     Generates the credit curves for the definition provided
 
     Parameters
     ----------
+    constituents : CreditConstituents, optional
+
     curve_definition : CreditCurveDefinition, optional
 
     curve_parameters : CreditCurveParameters, optional
 
     curve_tag : str, optional
         A user-defined string to identify the interest rate curve. it can be used to
         link output results to the curve definition. limited to 40 characters. only
         alphabetic, numeric and '- _.#=@' characters are supported.
     """
 
     def __init__(
         self,
+        constituents: "OptCreditConstituents" = None,
         curve_definition: "CurveDefinition" = None,
         curve_parameters: "CurveParameters" = None,
         curve_tag: "OptStr" = None,
     ) -> None:
         super().__init__()
+        self.constituents = constituents
         self.curve_definition = curve_definition
         self.curve_parameters = curve_parameters
         self.curve_tag = curve_tag
 
     @property
+    def constituents(self):
+        """
+        :return: object CreditConstituents
+        """
+        return self._get_object_parameter(CreditConstituents, "constituents")
+
+    @constituents.setter
+    def constituents(self, value):
+        self._set_object_parameter(CreditConstituents, "constituents", value)
+
+    @property
     def curve_definition(self):
         """
         :return: object CreditCurveDefinition
         """
         return self._get_object_parameter(CreditCurveDefinition, "curveDefinition")
 
     @curve_definition.setter
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/__init__.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_business_sector.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_business_sector.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-from enum import Enum, unique
+from enum import unique
+
+from ......_base_enum import StrEnum
 
 
 @unique
-class BusinessSector(Enum):
+class BusinessSector(StrEnum):
     ACADEMIC_AND_EDUCATIONAL_SERVICES = "AcademicAndEducationalServices"
     APPLIED_RESOURCES = "AppliedResources"
     AUTOMOBILES_AND_AUTO_PARTS = "AutomobilesAndAutoParts"
     BANKING_AND_INVESTMENT_SERVICES = "BankingAndInvestmentServices"
     CHEMICALS = "Chemicals"
     COLLECTIVE_INVESTMENTS = "CollectiveInvestments"
     CONSUMER_GOODS_CONGLOMERATES = "ConsumerGoodsConglomerates"
@@ -16,23 +18,19 @@
     FINANCIAL_TECHNOLOGY_AND_INFRASTRUCTURE = "FinancialTechnologyAndInfrastructure"
     FOOD_AND_BEVERAGES = "FoodAndBeverages"
     FOOD_AND_DRUG_RETAILING = "FoodAndDrugRetailing"
     GOVERNMENT_ACTIVITY = "GovernmentActivity"
     HEALTHCARE_SERVICES_AND_EQUIPMENT = "HealthcareServicesAndEquipment"
     INDUSTRIAL_AND_COMMERCIAL_SERVICES = "IndustrialAndCommercialServices"
     INDUSTRIAL_GOODS = "IndustrialGoods"
-    INSTITUTIONS_ASSOCIATIONS_AND_ORGANIZATIONS = (
-        "InstitutionsAssociationsAndOrganizations"
-    )
+    INSTITUTIONS_ASSOCIATIONS_AND_ORGANIZATIONS = "InstitutionsAssociationsAndOrganizations"
     INSURANCE = "Insurance"
     INVESTMENT_HOLDING_COMPANIES = "InvestmentHoldingCompanies"
     MINERAL_RESOURCES = "MineralResources"
-    PERSONAL_AND_HOUSEHOLD_PRODUCTS_AND_SERVICES = (
-        "PersonalAndHouseholdProductsAndServices"
-    )
+    PERSONAL_AND_HOUSEHOLD_PRODUCTS_AND_SERVICES = "PersonalAndHouseholdProductsAndServices"
     PHARMACEUTICALS_AND_MEDICAL_RESEARCH = "PharmaceuticalsAndMedicalResearch"
     REAL_ESTATE = "RealEstate"
     RENEWABLE_ENERGY = "RenewableEnergy"
     RETAILERS = "Retailers"
     SOFTWARE_AND_IT_SERVICES = "SoftwareAndITServices"
     TECHNOLOGY_EQUIPMENT = "TechnologyEquipment"
     TELECOMMUNICATIONS_SERVICES = "TelecommunicationsServices"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_curve_sub_type.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_curve_sub_type.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-from enum import Enum, unique
+from enum import unique
+
+from ......_base_enum import StrEnum
 
 
 @unique
-class CurveSubType(Enum):
+class CurveSubType(StrEnum):
     BOND_CARRY = "BondCarry"
     BREAKEVEN_INFLATION_CURVE = "BreakevenInflationCurve"
     CDS_CREDIT_INDEX = "CDSCreditIndex"
     CAP_FLOOR_VOLATILITY = "CapFloorVolatility"
     CENTRAL_BANK_INTEREST_RATE_PROBABILITY = "CentralBankInterestRateProbability"
     COMMERCIAL_PAPER_BENCHMARK = "CommercialPaperBenchmark"
     CORPORATE_BOND_BENCHMARK = "CorporateBondBenchmark"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_economic_sector.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_economic_sector.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,20 +1,19 @@
-from enum import Enum, unique
+from enum import unique
+from ......_base_enum import StrEnum
 
 
 @unique
-class EconomicSector(Enum):
+class EconomicSector(StrEnum):
     ACADEMIC_AND_EDUCATIONAL_SERVICES = "AcademicAndEducationalServices"
     BASIC_MATERIALS = "BasicMaterials"
     CONSUMER_CYCLICALS = "ConsumerCyclicals"
     CONSUMER_NON_CYCLICALS = "ConsumerNonCyclicals"
     ENERGY = "Energy"
     FINANCIALS = "Financials"
     GOVERNMENT_ACTIVITY = "GovernmentActivity"
     HEALTHCARE = "Healthcare"
     INDUSTRIALS = "Industrials"
-    INSTITUTIONS_ASSOCIATIONS_AND_ORGANIZATIONS = (
-        "InstitutionsAssociationsAndOrganizations"
-    )
+    INSTITUTIONS_ASSOCIATIONS_AND_ORGANIZATIONS = "InstitutionsAssociationsAndOrganizations"
     REAL_ESTATE = "RealEstate"
     TECHNOLOGY = "Technology"
     UTILITIES = "Utilities"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-from enum import Enum, unique
+from enum import unique
+
+from ......_base_enum import StrEnum
 
 
 @unique
-class Industry(Enum):
+class Industry(StrEnum):
     ADVANCED_MEDICAL_EQUIPMENT_AND_TECHNOLOGY = "AdvancedMedicalEquipmentAndTechnology"
     ADVERTISING_AND_MARKETING = "AdvertisingAndMarketing"
     AEROSPACE_AND_DEFENSE = "AerospaceAndDefense"
     AGRICULTURAL_CHEMICALS = "AgriculturalChemicals"
     AIRLINES = "Airlines"
     AIRPORT_OPERATORS_AND_SERVICES = "AirportOperatorsAndServices"
     ALUMINUM = "Aluminum"
@@ -37,17 +39,15 @@
     CONSTRUCTION_AND_ENGINEERING = "ConstructionAndEngineering"
     CONSTRUCTION_MATERIALS = "ConstructionMaterials"
     CONSTRUCTION_SUPPLIES_AND_FIXTURES = "ConstructionSuppliesAndFixtures"
     CONSUMER_GOODS_CONGLOMERATES = "ConsumerGoodsConglomerates"
     CONSUMER_LENDING = "ConsumerLending"
     CONSUMER_PUBLISHING = "ConsumerPublishing"
     CORPORATE_FINANCIAL_SERVICES = "CorporateFinancialServices"
-    COURIER_POSTAL_AIR_FREIGHT_AND_LANDBASED_LOGISTICS = (
-        "CourierPostalAirFreightAndLandbasedLogistics"
-    )
+    COURIER_POSTAL_AIR_FREIGHT_AND_LANDBASED_LOGISTICS = "CourierPostalAirFreightAndLandbasedLogistics"
     CROWD_COLLABORATION = "CrowdCollaboration"
     DEPARTMENT_STORES = "DepartmentStores"
     DISCOUNT_STORES = "DiscountStores"
     DISTILLERS_AND_WINERIES = "DistillersAndWineries"
     DIVERSIFIED_CHEMICALS = "DiversifiedChemicals"
     DIVERSIFIED_INDUSTRIAL_GOODS_WHOLESALE = "DiversifiedIndustrialGoodsWholesale"
     DIVERSIFIED_INVESTMENT_SERVICES = "DiversifiedInvestmentServices"
@@ -77,17 +77,15 @@
     GROUND_FREIGHT_AND_LOGISTICS = "GroundFreightAndLogistics"
     HEALTHCARE_FACILITIES_AND_SERVICES = "HealthcareFacilitiesAndServices"
     HEAVY_ELECTRICAL_EQUIPMENT = "HeavyElectricalEquipment"
     HEAVY_MACHINERY_AND_VEHICLES = "HeavyMachineryAndVehicles"
     HIGHWAYS_AND_RAIL_TRACKS = "HighwaysAndRailTracks"
     HOME_FURNISHINGS = "HomeFurnishings"
     HOME_FURNISHINGS_RETAILERS = "HomeFurnishingsRetailers"
-    HOME_IMPROVEMENT_PRODUCTS_AND_SERVICES_RETAILERS = (
-        "HomeImprovementProductsAndServicesRetailers"
-    )
+    HOME_IMPROVEMENT_PRODUCTS_AND_SERVICES_RETAILERS = "HomeImprovementProductsAndServicesRetailers"
     HOMEBUILDING = "Homebuilding"
     HOTELS_MOTELS_AND_CRUISE_LINES = "HotelsMotelsAndCruiseLines"
     HOUSEHOLD_ELECTRONICS = "HouseholdElectronics"
     HOUSEHOLD_PRODUCTS = "HouseholdProducts"
     IT_SERVICES_AND_CONSULTING = "ITServicesAndConsulting"
     INDEPENDENT_POWER_PRODUCERS = "IndependentPowerProducers"
     INDUSTRIAL_MACHINERY_AND_EQUIPMENT = "IndustrialMachineryAndEquipment"
@@ -101,29 +99,23 @@
     IRON_AND_STEEL = "IronAndSteel"
     LEGAL_AND_SAFETY_PUBLIC_SERVICES = "LegalAndSafetyPublicServices"
     LEISURE_AND_RECREATION = "LeisureAndRecreation"
     LIFE_AND_HEALTH_INSURANCE = "LifeAndHealthInsurance"
     MANAGED_HEALTHCARE = "ManagedHealthcare"
     MARINE_FREIGHT_AND_LOGISTICS = "MarineFreightAndLogistics"
     MARINE_PORT_SERVICES = "MarinePortServices"
-    MEDICAL_EQUIPMENT_SUPPLIES_AND_DISTRIBUTION = (
-        "MedicalEquipmentSuppliesAndDistribution"
-    )
+    MEDICAL_EQUIPMENT_SUPPLIES_AND_DISTRIBUTION = "MedicalEquipmentSuppliesAndDistribution"
     MINING_SUPPORT_SERVICES_AND_EQUIPMENT = "MiningSupportServicesAndEquipment"
-    MISCELLANEOUS_EDUCATIONAL_SERVICE_PROVIDERS = (
-        "MiscellaneousEducationalServiceProviders"
-    )
+    MISCELLANEOUS_EDUCATIONAL_SERVICE_PROVIDERS = "MiscellaneousEducationalServiceProviders"
     MISCELLANEOUS_INFRASTRUCTURE = "MiscellaneousInfrastructure"
     MISCELLANEOUS_SPECIALTY_RETAILERS = "MiscellaneousSpecialtyRetailers"
     MULTILINE_INSURANCE_AND_BROKERS = "MultilineInsuranceAndBrokers"
     MULTILINE_UTILITIES = "MultilineUtilities"
     MUTUAL_FUNDS = "MutualFunds"
-    NATIONAL_SECURITY_AND_INTERNATIONAL_AFFAIRS = (
-        "NationalSecurityAndInternationalAffairs"
-    )
+    NATIONAL_SECURITY_AND_INTERNATIONAL_AFFAIRS = "NationalSecurityAndInternationalAffairs"
     NATURAL_GAS_UTILITIES = "NaturalGasUtilities"
     NON_ALCOHOLIC_BEVERAGES = "NonAlcoholicBeverages"
     NON_GOLD_PRECIOUS_METALS_AND_MINERALS = "NonGoldPreciousMetalsAndMinerals"
     NON_PAPER_CONTAINERS_AND_PACKAGING = "NonPaperContainersAndPackaging"
     OFFICE_EQUIPMENT = "OfficeEquipment"
     OIL_AND_GAS_DRILLING = "OilAndGasDrilling"
     OIL_AND_GAS_EXPLORATION_AND_PRODUCTION = "OilAndGasExplorationAndProduction"
@@ -139,17 +131,15 @@
     PERSONAL_SERVICES = "PersonalServices"
     PHARMACEUTICALS = "Pharmaceuticals"
     PHONES_AND_HANDHELD_DEVICES = "PhonesAndHandheldDevices"
     PROFESSIONAL_AND_BUSINESS_EDUCATION = "ProfessionalAndBusinessEducation"
     PROFESSIONAL_INFORMATION_SERVICES = "ProfessionalInformationServices"
     PROFESSIONAL_ORGANIZATIONS = "ProfessionalOrganizations"
     PROPERTY_AND_CASUALTY_INSURANCE = "PropertyAndCasualtyInsurance"
-    REAL_ESTATE_RENTAL_DEVELOPMENT_AND_OPERATIONS = (
-        "RealEstateRentalDevelopmentAndOperations"
-    )
+    REAL_ESTATE_RENTAL_DEVELOPMENT_AND_OPERATIONS = "RealEstateRentalDevelopmentAndOperations"
     REAL_ESTATE_SERVICES = "RealEstateServices"
     RECREATIONAL_PRODUCTS = "RecreationalProducts"
     REINSURANCE = "Reinsurance"
     RELIGIOUS_ORGANIZATIONS = "ReligiousOrganizations"
     RENEWABLE_ENERGY_EQUIPMENT_AND_SERVICES = "RenewableEnergyEquipmentAndServices"
     RENEWABLE_FUELS = "RenewableFuels"
     RESIDENTIAL_REI_TS = "ResidentialREITs"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry_group.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_industry_group.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-from enum import Enum, unique
+from enum import unique
+from ......_base_enum import StrEnum
 
 
 @unique
-class IndustryGroup(Enum):
+class IndustryGroup(StrEnum):
     AEROSPACE_AND_DEFENSE = "AerospaceAndDefense"
     AUTOMOBILES_AND_AUTO_PARTS = "AutomobilesAndAutoParts"
     BANKING_AND_INVESTMENT_SERVICES = "BankingAndInvestmentServices"
     BANKING_SERVICES = "BankingServices"
     BEVERAGES = "Beverages"
     BIOTECHNOLOGY_AND_MEDICAL_RESEARCH = "BiotechnologyAndMedicalResearch"
     CHEMICALS = "Chemicals"
     COAL = "Coal"
     COLLECTIVE_INVESTMENTS = "CollectiveInvestments"
     COMMUNICATIONS_AND_NETWORKING = "CommunicationsAndNetworking"
-    COMPUTERS_PHONES_AND_HOUSEHOLD_ELECTRONICS = (
-        "ComputersPhonesAndHouseholdElectronics"
-    )
+    COMPUTERS_PHONES_AND_HOUSEHOLD_ELECTRONICS = "ComputersPhonesAndHouseholdElectronics"
     CONSTRUCTION_AND_ENGINEERING = "ConstructionAndEngineering"
     CONSTRUCTION_MATERIALS = "ConstructionMaterials"
     CONSUMER_GOODS_CONGLOMERATES = "ConsumerGoodsConglomerates"
     CONTAINERS_AND_PACKAGING = "ContainersAndPackaging"
     DIVERSIFIED_INDUSTRIAL_GOODS_WHOLESALE = "DiversifiedIndustrialGoodsWholesale"
     DIVERSIFIED_RETAIL = "DiversifiedRetail"
     ELECTRIC_UTILITIES_AND_IP_PS = "ElectricUtilitiesAndIPPs"
@@ -30,51 +29,41 @@
     FREIGHT_AND_LOGISTICS_SERVICES = "FreightAndLogisticsServices"
     GOVERNMENT_ACTIVITY = "GovernmentActivity"
     HEALTHCARE_EQUIPMENT_AND_SUPPLIES = "HealthcareEquipmentAndSupplies"
     HEALTHCARE_PROVIDERS_AND_SERVICES = "HealthcareProvidersAndServices"
     HOMEBUILDING_AND_CONSTRUCTION_SUPPLIES = "HomebuildingAndConstructionSupplies"
     HOTELS_AND_ENTERTAINMENT_SERVICES = "HotelsAndEntertainmentServices"
     HOUSEHOLD_GOODS = "HouseholdGoods"
-    INSTITUTIONS_ASSOCIATIONS_AND_ORGANIZATIONS = (
-        "InstitutionsAssociationsAndOrganizations"
-    )
+    INSTITUTIONS_ASSOCIATIONS_AND_ORGANIZATIONS = "InstitutionsAssociationsAndOrganizations"
     INSURANCE = "Insurance"
     INTEGRATED_HARDWARE_AND_SOFTWARE = "IntegratedHardwareAndSoftware"
-    INVESTMENT_BANKING_AND_INVESTMENT_SERVICES = (
-        "InvestmentBankingAndInvestmentServices"
-    )
+    INVESTMENT_BANKING_AND_INVESTMENT_SERVICES = "InvestmentBankingAndInvestmentServices"
     INVESTMENT_HOLDING_COMPANIES = "InvestmentHoldingCompanies"
     LEISURE_PRODUCTS = "LeisureProducts"
-    MACHINERY_TOOLS_HEAVY_VEHICLES_TRAINS_AND_SHIPS = (
-        "MachineryToolsHeavyVehiclesTrainsAndShips"
-    )
+    MACHINERY_TOOLS_HEAVY_VEHICLES_TRAINS_AND_SHIPS = "MachineryToolsHeavyVehiclesTrainsAndShips"
     MEDIA_AND_PUBLISHING = "MediaAndPublishing"
     METALS_AND_MINING = "MetalsAndMining"
     MISCELLANEOUS_EDUCATIONAL_SERVICE = "MiscellaneousEducationalService"
     MULTILINE_UTILITIES = "MultilineUtilities"
     NATURAL_GAS_UTILITIES = "NaturalGasUtilities"
     OFFICE_EQUIPMENT = "OfficeEquipment"
     OIL_AND_GAS = "OilAndGas"
     OIL_AND_GAS_RELATED_EQUIPMENT_AND_SERVICES = "OilAndGasRelatedEquipmentAndServices"
     PAPER_AND_FOREST_PRODUCTS = "PaperAndForestProducts"
     PASSENGER_TRANSPORTATION_SERVICES = "PassengerTransportationServices"
-    PERSONAL_AND_HOUSEHOLD_PRODUCTS_AND_SERVICES = (
-        "PersonalAndHouseholdProductsAndServices"
-    )
+    PERSONAL_AND_HOUSEHOLD_PRODUCTS_AND_SERVICES = "PersonalAndHouseholdProductsAndServices"
     PHARMACEUTICALS = "Pharmaceuticals"
     PROFESSIONAL_AND_BUSINESS_EDUCATION = "ProfessionalAndBusinessEducation"
     PROFESSIONAL_AND_COMMERCIAL_SERVICES = "ProfessionalAndCommercialServices"
     PROVIDERS = "Providers"
     REAL_ESTATE_OPERATIONS = "RealEstateOperations"
     RENEWABLE_ENERGY = "RenewableEnergy"
     RESIDENTIAL_AND_COMMERCIAL_REI_TS = "ResidentialAndCommercialREITs"
     SCHOOL_COLLEGE_AND_UNIVERSITY = "SchoolCollegeAndUniversity"
-    SEMICONDUCTORS_AND_SEMICONDUCTOR_EQUIPMENT = (
-        "SemiconductorsAndSemiconductorEquipment"
-    )
+    SEMICONDUCTORS_AND_SEMICONDUCTOR_EQUIPMENT = "SemiconductorsAndSemiconductorEquipment"
     SOFTWARE_AND_IT_SERVICES = "SoftwareAndITServices"
     SPECIALTY_RETAILERS = "SpecialtyRetailers"
     TELECOMMUNICATIONS_SERVICES = "TelecommunicationsServices"
     TEXTILES_AND_APPAREL = "TextilesAndApparel"
     TRANSPORT_INFRASTRUCTURE = "TransportInfrastructure"
     URANIUM = "Uranium"
     WATER_AND_RELATED_UTILITIES = "WaterAndRelatedUtilities"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interest_calculation_method.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_day_count_basis.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,12 +1,16 @@
-from enum import Enum, unique
+# coding: utf8
+
+from enum import unique
+
+from ...._base_enum import StrEnum
 
 
 @unique
-class InterestCalculationMethod(Enum):
+class DayCountBasis(StrEnum):
     DCB_30_E_360_ISMA = "Dcb_30E_360_ISMA"
     DCB_30_360 = "Dcb_30_360"
     DCB_30_360_GERMAN = "Dcb_30_360_German"
     DCB_30_360_ISDA = "Dcb_30_360_ISDA"
     DCB_30_360_US = "Dcb_30_360_US"
     DCB_30_365_BRAZIL = "Dcb_30_365_Brazil"
     DCB_30_365_GERMAN = "Dcb_30_365_German"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interpolation_mode.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interpolation_mode.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-from enum import Enum, unique
+from enum import unique
+
+from ......_base_enum import StrEnum
 
 
 @unique
-class InterpolationMode(Enum):
+class InterpolationMode(StrEnum):
     AKIMA_METHOD = "AkimaMethod"
     CUBIC_DISCOUNT = "CubicDiscount"
     CUBIC_RATE = "CubicRate"
     CUBIC_SPLINE = "CubicSpline"
     FORWARD_MONOTONE_CONVEX = "ForwardMonotoneConvex"
     FRITSCH_BUTLAND_METHOD = "FritschButlandMethod"
     HERMITE = "Hermite"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_rating.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_rating.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-from enum import Enum, unique
+from enum import unique
+
+from ......_base_enum import StrEnum
 
 
 @unique
-class Rating(Enum):
+class Rating(StrEnum):
     A = "A"
     A1 = "A1"
     A2 = "A2"
     A3 = "A3"
     AA = "AA"
     AAA = "AAA"
     AA_HIGH = "AAHigh"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_butterfly_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_butterfly_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_combined_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_combined_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_curve_definition_pricing.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_curve_definition_pricing.py`

 * *Files 1% similar despite different names*

```diff
@@ -95,40 +95,32 @@
         replacedefinition: replace the default constituents by the user constituents
         from the input request,   * mergewithdefinition: merge the default constituents
         and the user constituents from the input request, the default value is
         'replacedefinition'.  if the ignoreexistingdefinition is true, the
         constituentoverridemode is set to replacedefinition.
         :return: enum ConstituentOverrideMode
         """
-        return self._get_enum_parameter(
-            ConstituentOverrideMode, "constituentOverrideMode"
-        )
+        return self._get_enum_parameter(ConstituentOverrideMode, "constituentOverrideMode")
 
     @constituent_override_mode.setter
     def constituent_override_mode(self, value):
-        self._set_enum_parameter(
-            ConstituentOverrideMode, "constituentOverrideMode", value
-        )
+        self._set_enum_parameter(ConstituentOverrideMode, "constituentOverrideMode", value)
 
     @property
     def main_constituent_asset_class(self):
         """
         The asset class used to generate the zero coupon curve. the possible values are:
         * fxforward   * swap
         :return: enum MainConstituentAssetClass
         """
-        return self._get_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass"
-        )
+        return self._get_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass")
 
     @main_constituent_asset_class.setter
     def main_constituent_asset_class(self, value):
-        self._set_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass", value
-        )
+        self._set_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass", value)
 
     @property
     def risk_type(self):
         """
         The risk type to which the generated cross currency curve is sensitive. the
         possible value is:   * 'crosscurrency'
         :return: enum RiskType
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_flattening_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_flattening_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 from typing import Optional, TYPE_CHECKING, List
 
 from ._curve_definition_pricing import (
     CrossCurrencyCurveDefinitionPricing,
 )
-from ... import ZcCurveDefinition
+from ..._zc_curve_definition import ZcCurveDefinition
 from ...._object_definition import ObjectDefinition
 from ......_tools import try_copy_to_list
 
 if TYPE_CHECKING:
     from ......_types import OptStr, OptBool, OptStrings
 
 
@@ -38,55 +38,47 @@
 
     quoted_index_name : str, optional
 
     """
 
     def __init__(
         self,
-        cross_currency_definitions: Optional[
-            List[CrossCurrencyCurveDefinitionPricing]
-        ] = None,
+        cross_currency_definitions: Optional[List[CrossCurrencyCurveDefinitionPricing]] = None,
         curve_tenors: "OptStrings" = None,
         interest_rate_curve_definitions: Optional[List[ZcCurveDefinition]] = None,
         base_currency: "OptStr" = None,
         base_index_name: "OptStr" = None,
         is_non_deliverable: "OptBool" = None,
         pivot_currency: "OptStr" = None,
         pivot_index_name: "OptStr" = None,
         quoted_currency: "OptStr" = None,
         quoted_index_name: "OptStr" = None,
     ) -> None:
         super().__init__()
         self.cross_currency_definitions = try_copy_to_list(cross_currency_definitions)
         self.curve_tenors = curve_tenors
-        self.interest_rate_curve_definitions = try_copy_to_list(
-            interest_rate_curve_definitions
-        )
+        self.interest_rate_curve_definitions = try_copy_to_list(interest_rate_curve_definitions)
         self.base_currency = base_currency
         self.base_index_name = base_index_name
         self.is_non_deliverable = is_non_deliverable
         self.pivot_currency = pivot_currency
         self.pivot_index_name = pivot_index_name
         self.quoted_currency = quoted_currency
         self.quoted_index_name = quoted_index_name
 
     @property
     def cross_currency_definitions(self):
         """
         :return: list CrossCurrencyCurveDefinitionPricing
         """
-        return self._get_list_parameter(
-            CrossCurrencyCurveDefinitionPricing, "crossCurrencyDefinitions"
-        )
+        return self._get_list_parameter(CrossCurrencyCurveDefinitionPricing, "crossCurrencyDefinitions")
 
     @cross_currency_definitions.setter
     def cross_currency_definitions(self, value):
-        self._set_list_parameter(
-            CrossCurrencyCurveDefinitionPricing, "crossCurrencyDefinitions", value
-        )
+        self._set_list_parameter(CrossCurrencyCurveDefinitionPricing, "crossCurrencyDefinitions", value)
 
     @property
     def curve_tenors(self):
         """
         List of user-defined curve tenors or dates to be computed.
         :return: list string
         """
@@ -97,23 +89,19 @@
         self._set_list_parameter(str, "curveTenors", value)
 
     @property
     def interest_rate_curve_definitions(self):
         """
         :return: list ZcCurveDefinition
         """
-        return self._get_list_parameter(
-            ZcCurveDefinition, "interestRateCurveDefinitions"
-        )
+        return self._get_list_parameter(ZcCurveDefinition, "interestRateCurveDefinitions")
 
     @interest_rate_curve_definitions.setter
     def interest_rate_curve_definitions(self, value):
-        self._set_list_parameter(
-            ZcCurveDefinition, "interestRateCurveDefinitions", value
-        )
+        self._set_list_parameter(ZcCurveDefinition, "interestRateCurveDefinitions", value)
 
     @property
     def base_currency(self):
         """
         :return: str
         """
         return self._get_parameter("baseCurrency")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_parameters.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 from typing import Optional, TYPE_CHECKING
 
-from ._valuation_time import ValuationTime
+from ..._models import ValuationTime
 from ...._enums._extrapolation_mode import ExtrapolationMode
 from ...._enums._interpolation_mode import InterpolationMode
 from ...._object_definition import ObjectDefinition
 
 
 if TYPE_CHECKING:
     from ......_types import OptStr, OptBool
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_shift_scenario.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_shift_scenario.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,10 +1,11 @@
 from typing import Optional, TYPE_CHECKING
 
 from ._shift_definition import ShiftDefinition
+from ..._models import ParRateShift
 from ...._object_definition import ObjectDefinition
 
 
 if TYPE_CHECKING:
     from ......_types import OptStr
 
 
@@ -12,27 +13,31 @@
     """
     Generates the Cross Currency curves for the definitions provided
 
     Parameters
     ----------
     fx_curve_shift : ShiftDefinition, optional
         Collection of shift parameters tenor. "all" selector supported as well.
+    par_rate_shift : ParRateShift, optional
+        Scenario of par rates shift (shift applied to constituents).
     shift_tag : str, optional
         User defined string to identify the shift scenario tag. it can be used to link
         output curve to the shift scenario. only alphabetic, numeric and '- _.#=@'
         characters are supported. optional.
     """
 
     def __init__(
         self,
         fx_curve_shift: Optional[ShiftDefinition] = None,
+        par_rate_shift: Optional[ParRateShift] = None,
         shift_tag: "OptStr" = None,
     ) -> None:
         super().__init__()
         self.fx_curve_shift = fx_curve_shift
+        self.par_rate_shift = par_rate_shift
         self.shift_tag = shift_tag
 
     @property
     def fx_curve_shift(self):
         """
         Collection of shift parameters tenor. "all" selector supported as well.
         :return: object ShiftDefinition
@@ -40,14 +45,26 @@
         return self._get_object_parameter(ShiftDefinition, "fxCurveShift")
 
     @fx_curve_shift.setter
     def fx_curve_shift(self, value):
         self._set_object_parameter(ShiftDefinition, "fxCurveShift", value)
 
     @property
+    def par_rate_shift(self):
+        """
+        Scenario of par rates shift (shift applied to constituents).
+        :return: object ParRateShift
+        """
+        return self._get_object_parameter(ParRateShift, "parRateShift")
+
+    @par_rate_shift.setter
+    def par_rate_shift(self, value):
+        self._set_object_parameter(ParRateShift, "parRateShift", value)
+
+    @property
     def shift_tag(self):
         """
         User defined string to identify the shift scenario tag. it can be used to link
         output curve to the shift scenario. only alphabetic, numeric and '- _.#=@'
         characters are supported. optional.
         :return: str
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_long_end_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_long_end_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_parallel_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_parallel_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_request.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,9 +1,10 @@
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Optional
 
+from ._fx_forward_constituents import FxForwardConstituents
 from ._fx_forward_curve_definition import FxForwardCurveDefinition
 from ._fx_forward_curve_parameters import FxForwardCurveParameters
 from ._fx_shift_scenario import FxShiftScenario
 from ...._object_definition import ObjectDefinition
 from ......_tools import try_copy_to_list
 
 if TYPE_CHECKING:
@@ -14,40 +15,55 @@
 class RequestItem(ObjectDefinition):
     """
     Generates the Cross Currency curves for the definitions provided
 
     Parameters
     ----------
 
+    constituents : FxForwardConstituents, optional
+
     curve_definition : FxForwardCurveDefinition, optional
 
     curve_parameters : FxForwardCurveParameters, optional
 
     shift_scenarios : list of FxShiftScenario, optional
         The list of attributes applied to the curve shift scenarios.
     curve_tag : str, optional
         A user-defined string to identify the interest rate curve. it can be used to
         link output results to the curve definition. limited to 40 characters. only
         alphabetic, numeric and '- _.#=@' characters are supported.
     """
 
     def __init__(
         self,
+        constituents: Optional[FxForwardConstituents] = None,
         curve_definition: "CurveDefinition" = None,
         curve_parameters: "CurveParameters" = None,
         shift_scenarios: "ShiftScenarios" = None,
         curve_tag: "OptStr" = None,
     ) -> None:
         super().__init__()
+        self.constituents = constituents
         self.curve_definition = curve_definition
         self.curve_parameters = curve_parameters
         self.shift_scenarios = try_copy_to_list(shift_scenarios)
         self.curve_tag = curve_tag
 
     @property
+    def constituents(self):
+        """
+        :return: object FxForwardConstituents
+        """
+        return self._get_object_parameter(FxForwardConstituents, "constituents")
+
+    @constituents.setter
+    def constituents(self, value):
+        self._set_object_parameter(FxForwardConstituents, "constituents", value)
+
+    @property
     def curve_definition(self):
         """
         :return: object FxForwardCurveDefinition
         """
         return self._get_object_parameter(FxForwardCurveDefinition, "curveDefinition")
 
     @curve_definition.setter
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_shift_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_shift_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_short_end_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_short_end_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_time_bucket_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_time_bucket_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_twist_shift.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_twist_shift.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_valuation_time.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_valuation_time.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,20 +1,17 @@
 from typing import TYPE_CHECKING
-
-from ...._object_definition import ObjectDefinition
+from ..._object_definition import ObjectDefinition
 
 
 if TYPE_CHECKING:
-    from ......_types import OptStr
+    from ....._types import OptStr
 
 
 class ValuationTime(ObjectDefinition):
     """
-    Generates the Cross Currency curves for the definitions provided
-
     Parameters
     ----------
     city_name : str, optional
         The city name according to market identifier code (mic) (e.g., 'new york')  see
         iso 10383 for reference.
     local_time : str, optional
         Local time or other words time in offset timezone. the value is expressed in iso
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-__version__ = "1.0.129"
+__version__ = "1.0.130"
 
 from ._bid_ask_fields_description import *
 from ._bid_ask_fields_description import *
 from ._constituents_description import *
 from ._curve_parameters import *
 from ._instrument_definition import *
 from ._constituents_description import *
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_base_definition_mixin.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_base_definition_mixin.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,23 +1,19 @@
 from .._enums import MainConstituentAssetClass, RiskType
 from ...._object_definition import ObjectDefinition
 
 
 class BaseDefinitionMixin(ObjectDefinition):
     @property
     def main_constituent_asset_class(self):
-        return self._get_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass"
-        )
+        return self._get_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass")
 
     @main_constituent_asset_class.setter
     def main_constituent_asset_class(self, value):
-        self._set_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass", value
-        )
+        self._set_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass", value)
 
     @property
     def risk_type(self):
         return self._get_enum_parameter(RiskType, "riskType")
 
     @risk_type.setter
     def risk_type(self, value):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_description.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_formula_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_bid_ask_fields_formula_description.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_constituents_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_constituents_description.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,39 +27,33 @@
 
     def __init__(
         self,
         cross_currency_swaps: Optional[List[CrossCurrencyInstrumentDescription]] = None,
         fx_forwards: Optional[List[FxForwardInstrumentDescription]] = None,
         fx_spot: Optional[FxSpotInstrumentDescription] = None,
         interest_rate_swaps: Optional[List[CrossCurrencyInstrumentDescription]] = None,
-        overnight_index_swaps: Optional[
-            List[CrossCurrencyInstrumentDescription]
-        ] = None,
+        overnight_index_swaps: Optional[List[CrossCurrencyInstrumentDescription]] = None,
     ) -> None:
         super().__init__()
         self.cross_currency_swaps = try_copy_to_list(cross_currency_swaps)
         self.fx_forwards = try_copy_to_list(fx_forwards)
         self.fx_spot = fx_spot
         self.interest_rate_swaps = try_copy_to_list(interest_rate_swaps)
         self.overnight_index_swaps = try_copy_to_list(overnight_index_swaps)
 
     @property
     def cross_currency_swaps(self):
         """
         :return: list CrossCurrencyInstrumentDescription
         """
-        return self._get_list_parameter(
-            CrossCurrencyInstrumentDescription, "crossCurrencySwaps"
-        )
+        return self._get_list_parameter(CrossCurrencyInstrumentDescription, "crossCurrencySwaps")
 
     @cross_currency_swaps.setter
     def cross_currency_swaps(self, value):
-        self._set_list_parameter(
-            CrossCurrencyInstrumentDescription, "crossCurrencySwaps", value
-        )
+        self._set_list_parameter(CrossCurrencyInstrumentDescription, "crossCurrencySwaps", value)
 
     @property
     def fx_forwards(self):
         """
         :return: list FxForwardInstrumentDescription
         """
         return self._get_list_parameter(FxForwardInstrumentDescription, "fxForwards")
@@ -80,31 +74,23 @@
         self._set_object_parameter(FxSpotInstrumentDescription, "fxSpot", value)
 
     @property
     def interest_rate_swaps(self):
         """
         :return: list CrossCurrencyInstrumentDescription
         """
-        return self._get_list_parameter(
-            CrossCurrencyInstrumentDescription, "interestRateSwaps"
-        )
+        return self._get_list_parameter(CrossCurrencyInstrumentDescription, "interestRateSwaps")
 
     @interest_rate_swaps.setter
     def interest_rate_swaps(self, value):
-        self._set_list_parameter(
-            CrossCurrencyInstrumentDescription, "interestRateSwaps", value
-        )
+        self._set_list_parameter(CrossCurrencyInstrumentDescription, "interestRateSwaps", value)
 
     @property
     def overnight_index_swaps(self):
         """
         :return: list CrossCurrencyInstrumentDescription
         """
-        return self._get_list_parameter(
-            CrossCurrencyInstrumentDescription, "overnightIndexSwaps"
-        )
+        return self._get_list_parameter(CrossCurrencyInstrumentDescription, "overnightIndexSwaps")
 
     @overnight_index_swaps.setter
     def overnight_index_swaps(self, value):
-        self._set_list_parameter(
-            CrossCurrencyInstrumentDescription, "overnightIndexSwaps", value
-        )
+        self._set_list_parameter(CrossCurrencyInstrumentDescription, "overnightIndexSwaps", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_curve_definition_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_curve_definition_description.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,15 +4,15 @@
 
 
 if TYPE_CHECKING:
     from ..._types import OptMainConstituentAssetClass, OptRiskType
     from ......._types import OptStr, OptBool
 
 
-class CrossCurrencyCurveDefinitionDescription(BaseDefinitionMixin):
+class CrossCurrencyCurveCreateDefinition(BaseDefinitionMixin):
     """
     Create a cross currency curve definition
 
     Parameters
     ----------
     main_constituent_asset_class : MainConstituentAssetClass, optional
         The asset class used to generate the zero coupon curve. the possible values are:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_create/_request.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,26 +1,26 @@
 from typing import TYPE_CHECKING
 
 from ._curve_definition_description import (
-    CrossCurrencyCurveDefinitionDescription,
+    CrossCurrencyCurveCreateDefinition,
 )
 from .._mixin_request import MixinRequest
 from ......._tools import try_copy_to_list
 
 if TYPE_CHECKING:
     from .._types import CurveCreateDefinition, OptOverrides, Segments, OptTurns
 
 
 class CreateRequest(MixinRequest):
     """
     Create a cross currency curve request
 
     Parameters
     ----------
-    curve_definition : CrossCurrencyCurveDefinitionDescription
+    curve_definition : CrossCurrencyCurveCreateDefinition
 
     overrides : list of OverrideBidAsk, optional
 
     segments : list of CrossCurrencyInstrumentsSegment
 
     turns : list of OverrideFxForwardTurn, optional
 
@@ -40,16 +40,12 @@
         self.turns = try_copy_to_list(turns)
 
     @property
     def curve_definition(self):
         """
         :return: object CrossCurrencyCurveDefinitionDescription
         """
-        return self._get_object_parameter(
-            CrossCurrencyCurveDefinitionDescription, "curveDefinition"
-        )
+        return self._get_object_parameter(CrossCurrencyCurveCreateDefinition, "curveDefinition")
 
     @curve_definition.setter
     def curve_definition(self, value):
-        self._set_object_parameter(
-            CrossCurrencyCurveDefinitionDescription, "curveDefinition", value
-        )
+        self._set_object_parameter(CrossCurrencyCurveCreateDefinition, "curveDefinition", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_curve_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_curve_parameters.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_delete/_request.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_description.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_formula_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_field_formula_description.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_formula_parameter_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_formula_parameter_description.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_instrument_description.py`

 * *Files 2% similar despite different names*

```diff
@@ -51,38 +51,30 @@
         self._set_object_parameter(BidAskFieldsDescription, "fields", value)
 
     @property
     def formula_parameters(self):
         """
         :return: list FormulaParameterDescription
         """
-        return self._get_list_parameter(
-            FormulaParameterDescription, "formulaParameters"
-        )
+        return self._get_list_parameter(FormulaParameterDescription, "formulaParameters")
 
     @formula_parameters.setter
     def formula_parameters(self, value):
-        self._set_list_parameter(
-            FormulaParameterDescription, "formulaParameters", value
-        )
+        self._set_list_parameter(FormulaParameterDescription, "formulaParameters", value)
 
     @property
     def instrument_definition(self):
         """
         :return: object FxForwardInstrumentDefinition
         """
-        return self._get_object_parameter(
-            FxForwardInstrumentDefinition, "instrumentDefinition"
-        )
+        return self._get_object_parameter(FxForwardInstrumentDefinition, "instrumentDefinition")
 
     @instrument_definition.setter
     def instrument_definition(self, value):
-        self._set_object_parameter(
-            FxForwardInstrumentDefinition, "instrumentDefinition", value
-        )
+        self._set_object_parameter(FxForwardInstrumentDefinition, "instrumentDefinition", value)
 
     @property
     def formula(self):
         """
         :return: str
         """
         return self._get_parameter("formula")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_turn_fields.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_forward_turn_fields.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_fx_spot_instrument_description.py`

 * *Files 3% similar despite different names*

```diff
@@ -51,38 +51,30 @@
         self._set_object_parameter(BidAskFieldsDescription, "fields", value)
 
     @property
     def formula_parameters(self):
         """
         :return: list FormulaParameterDescription
         """
-        return self._get_list_parameter(
-            FormulaParameterDescription, "formulaParameters"
-        )
+        return self._get_list_parameter(FormulaParameterDescription, "formulaParameters")
 
     @formula_parameters.setter
     def formula_parameters(self, value):
-        self._set_list_parameter(
-            FormulaParameterDescription, "formulaParameters", value
-        )
+        self._set_list_parameter(FormulaParameterDescription, "formulaParameters", value)
 
     @property
     def instrument_definition(self):
         """
         :return: object FxSpotInstrumentDefinition
         """
-        return self._get_object_parameter(
-            FxSpotInstrumentDefinition, "instrumentDefinition"
-        )
+        return self._get_object_parameter(FxSpotInstrumentDefinition, "instrumentDefinition")
 
     @instrument_definition.setter
     def instrument_definition(self, value):
-        self._set_object_parameter(
-            FxSpotInstrumentDefinition, "instrumentDefinition", value
-        )
+        self._set_object_parameter(FxSpotInstrumentDefinition, "instrumentDefinition", value)
 
     @property
     def formula(self):
         """
         :return: str
         """
         return self._get_parameter("formula")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_curve_definition_keys.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_curve_definition_keys.py`

 * *Files 0% similar despite different names*

```diff
@@ -75,23 +75,19 @@
     @property
     def main_constituent_asset_class(self):
         """
         The asset class used to generate the zero coupon curve. the possible values are:
         * fxforward   * swap
         :return: enum MainConstituentAssetClass
         """
-        return self._get_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass"
-        )
+        return self._get_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass")
 
     @main_constituent_asset_class.setter
     def main_constituent_asset_class(self, value):
-        self._set_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass", value
-        )
+        self._set_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass", value)
 
     @property
     def risk_type(self):
         """
         The risk type to which the generated cross currency curve is sensitive. the
         possible value is:   * 'crosscurrency'
         :return: enum RiskType
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_get/_request.py`

 * *Files 9% similar despite different names*

```diff
@@ -22,16 +22,12 @@
         self.curve_definition = curve_definition
 
     @property
     def curve_definition(self):
         """
         :return: object CrossCurrencyCurveDefinitionKeys
         """
-        return self._get_object_parameter(
-            CrossCurrencyCurveDefinitionKeys, "curveDefinition"
-        )
+        return self._get_object_parameter(CrossCurrencyCurveDefinitionKeys, "curveDefinition")
 
     @curve_definition.setter
     def curve_definition(self, value):
-        self._set_object_parameter(
-            CrossCurrencyCurveDefinitionKeys, "curveDefinition", value
-        )
+        self._set_object_parameter(CrossCurrencyCurveDefinitionKeys, "curveDefinition", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_description.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instrument_description.py`

 * *Files 2% similar despite different names*

```diff
@@ -51,38 +51,30 @@
         self._set_object_parameter(BidAskFieldsDescription, "fields", value)
 
     @property
     def formula_parameters(self):
         """
         :return: list FormulaParameterDescription
         """
-        return self._get_list_parameter(
-            FormulaParameterDescription, "formulaParameters"
-        )
+        return self._get_list_parameter(FormulaParameterDescription, "formulaParameters")
 
     @formula_parameters.setter
     def formula_parameters(self, value):
-        self._set_list_parameter(
-            FormulaParameterDescription, "formulaParameters", value
-        )
+        self._set_list_parameter(FormulaParameterDescription, "formulaParameters", value)
 
     @property
     def instrument_definition(self):
         """
         :return: object CrossCurrencyInstrumentDefinition
         """
-        return self._get_object_parameter(
-            CrossCurrencyInstrumentDefinition, "instrumentDefinition"
-        )
+        return self._get_object_parameter(CrossCurrencyInstrumentDefinition, "instrumentDefinition")
 
     @instrument_definition.setter
     def instrument_definition(self, value):
-        self._set_object_parameter(
-            CrossCurrencyInstrumentDefinition, "instrumentDefinition", value
-        )
+        self._set_object_parameter(CrossCurrencyInstrumentDefinition, "instrumentDefinition", value)
 
     @property
     def formula(self):
         """
         :return: str
         """
         return self._get_parameter("formula")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instruments_segment.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_instruments_segment.py`

 * *Files 3% similar despite different names*

```diff
@@ -37,38 +37,30 @@
         self.curve_parameters = curve_parameters
 
     @property
     def constituents(self):
         """
         :return: object CrossCurrencyConstituentsDescription
         """
-        return self._get_object_parameter(
-            CrossCurrencyConstituentsDescription, "constituents"
-        )
+        return self._get_object_parameter(CrossCurrencyConstituentsDescription, "constituents")
 
     @constituents.setter
     def constituents(self, value):
-        self._set_object_parameter(
-            CrossCurrencyConstituentsDescription, "constituents", value
-        )
+        self._set_object_parameter(CrossCurrencyConstituentsDescription, "constituents", value)
 
     @property
     def curve_parameters(self):
         """
         :return: object CrossCurrencyCurveParameters
         """
-        return self._get_object_parameter(
-            CrossCurrencyCurveParameters, "curveParameters"
-        )
+        return self._get_object_parameter(CrossCurrencyCurveParameters, "curveParameters")
 
     @curve_parameters.setter
     def curve_parameters(self, value):
-        self._set_object_parameter(
-            CrossCurrencyCurveParameters, "curveParameters", value
-        )
+        self._set_object_parameter(CrossCurrencyCurveParameters, "curveParameters", value)
 
     @property
     def start_date(self):
         """
         :return: str
         """
         return self._get_parameter("startDate")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_mixin_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_mixin_request.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask_fields.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_bid_ask_fields.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_fx_forward_turn.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_override_fx_forward_turn.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/_curve_get_definition_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_search/_curve_get_definition_item.py`

 * *Files 1% similar despite different names*

```diff
@@ -88,23 +88,19 @@
     @property
     def main_constituent_asset_class(self):
         """
         The asset class used to generate the zero coupon curve. the possible values are:
         * fxforward   * swap
         :return: enum MainConstituentAssetClass
         """
-        return self._get_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass"
-        )
+        return self._get_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass")
 
     @main_constituent_asset_class.setter
     def main_constituent_asset_class(self, value):
-        self._set_enum_parameter(
-            MainConstituentAssetClass, "mainConstituentAssetClass", value
-        )
+        self._set_enum_parameter(MainConstituentAssetClass, "mainConstituentAssetClass", value)
 
     @property
     def risk_type(self):
         """
         The risk type to which the generated cross currency curve is sensitive. the
         possible value is:   * 'crosscurrency'
         :return: enum RiskType
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_turn_adjustment.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_turn_adjustment.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_types.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_types.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,18 +1,18 @@
 from typing import Optional, List
 
 from ._create._curve_definition_description import (
-    CrossCurrencyCurveDefinitionDescription,
+    CrossCurrencyCurveCreateDefinition,
 )
 from ._instruments_segment import CrossCurrencyInstrumentsSegment
 from ._override_bid_ask import OverrideBidAsk
 from ._override_fx_forward_turn import OverrideFxForwardTurn
 from ._update._curve_update_definition import (
     CrossCurrencyCurveUpdateDefinition,
 )
 
 
-CurveCreateDefinition = CrossCurrencyCurveDefinitionDescription
+CurveCreateDefinition = CrossCurrencyCurveCreateDefinition
 CurveUpdateDefinition = CrossCurrencyCurveUpdateDefinition
 OptOverrides = Optional[List[OverrideBidAsk]]
 Segments = List[CrossCurrencyInstrumentsSegment]
 OptTurns = Optional[List[OverrideFxForwardTurn]]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_curve_update_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_curve_update_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/_update/_request.py`

 * *Files 6% similar despite different names*

```diff
@@ -39,16 +39,12 @@
         self.turns = try_copy_to_list(turns)
 
     @property
     def curve_definition(self):
         """
         :return: object CrossCurrencyCurveUpdateDefinition
         """
-        return self._get_object_parameter(
-            CrossCurrencyCurveUpdateDefinition, "curveDefinition"
-        )
+        return self._get_object_parameter(CrossCurrencyCurveUpdateDefinition, "curveDefinition")
 
     @curve_definition.setter
     def curve_definition(self, value):
-        self._set_object_parameter(
-            CrossCurrencyCurveUpdateDefinition, "curveDefinition", value
-        )
+        self._set_object_parameter(CrossCurrencyCurveUpdateDefinition, "curveDefinition", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/_request.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/_request.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_curves_builder_df.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_curves_builder_df.py`

 * *Files 1% similar despite different names*

```diff
@@ -76,15 +76,14 @@
     ...     ]
     ... }
     """
     datas = raw.get("data", [])
     datas = datas or []
     dfs = []
     for data in datas:
-
         error = data.get("error")
         if error:
             continue
 
         curve_points = data.get("curvePoints")
 
         for curve_point in curve_points:
@@ -194,22 +193,18 @@
     allcolumns = []
     for name in columns:
         if name in columns_have_level_1:
             allcolumns.extend(list(product([name], level_1)))
         else:
             allcolumns.append((name, ""))
 
-    columns_date_idxs = [
-        index for index, value in enumerate(allcolumns) if "date" in value[0].lower()
-    ]
+    columns_date_idxs = [index for index, value in enumerate(allcolumns) if "date" in value[0].lower()]
     columns = pd.MultiIndex.from_tuples(allcolumns)
     df = pd.DataFrame(data_df, columns=columns)
-    df = convert_df_columns_to_datetime_by_idx(
-        df, columns_date_idxs, utc=True, delete_tz=True
-    )
+    df = convert_df_columns_to_datetime_by_idx(df, columns_date_idxs, utc=True, delete_tz=True)
     df = convert_dtypes(df)
     return df
 
 
 def _create_data_for_df(curve_points, columns, columns_have_level_1, columns_level_1):
     data_df = []
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_curves_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_curves_data_provider.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,36 +1,34 @@
-# coding: utf8
-from typing import List, Callable, TYPE_CHECKING, Any
+from dataclasses import dataclass
+from typing import Any, Callable, List, TYPE_CHECKING
 
 import numpy as np
 import pandas as pd
 from numpy import iterable
 
 from ._models._curve import Curve, ForwardCurve, ZcCurve
 from .._content_provider import (
+    CrossCurrencyCurvesDefinitionsRequestFactory,
     CurvesAndSurfacesRequestFactory,
     get_type_by_axis,
-    CrossCurrencyCurvesDefinitionsRequestFactory,
 )
 from .._ipa_content_validator import IPAContentValidator
 from ..curves._cross_currency_curves.definitions._data_classes import (
     CurveDefinitionData,
 )
 from ..curves._cross_currency_curves.triangulate_definitions._data_provider import (
     TriangulateDefinitionsData,
 )
 from ..._content_data import Data
 from ..._content_data_provider import ContentDataProvider
 from ..._content_response_factory import ContentResponseFactory
 from ...._content_type import ContentType
 from ...._tools import cached_property
 from ...._tools._dataframe import convert_df_columns_to_datetime, convert_dtypes
-from ....delivery._data._data_provider import (
-    ValidatorContainer,
-)
+from ....delivery._data._data_provider import DataProvider, ValidatorContainer
 from ....delivery._data._response_factory import ResponseFactory
 
 if TYPE_CHECKING:
     from ....delivery._data._data_provider import ParsedData
 
 
 # ---------------------------------------------------------------------------
@@ -214,15 +212,14 @@
     ... }
     """
     datas = raw.get("data", [])
     datas = datas or []
     dfs = []
 
     for data in datas:
-
         error = data.get("error")
         if error:
             continue
 
         curves = data.get("curves")
         for value in curves.values():
             curve_points = value.get("curvePoints")
@@ -320,15 +317,14 @@
     ...     ]
     ... }
     """
     datas = raw.get("data", [])
     datas = datas or []
     dfs = []
     for data in datas:
-
         error = data.get("error")
         if error:
             continue
 
         forward_curves = data.get("forwardCurves")
 
         for forward_curve in forward_curves:
@@ -362,39 +358,36 @@
     return df
 
 
 def cross_currency_curves_definitions_search_build_df(raw, **kwargs):
     return zc_curve_definitions_build_df(raw)
 
 
+@dataclass
 class OneCurveData(Data):
-    _curve = None
-
-    def __init__(self, raw, create_curves, dfbuilder):
-        Data.__init__(self, raw, dfbuilder=dfbuilder)
-        self._create_curves = create_curves
+    _create_curves: Callable = None
+    _curve: Curve = None
 
     @property
     def curve(self) -> Curve:
         if self._curve is None:
-            curve = self._create_curves(self._raw)
+            curve = self._create_curves(self.raw)
             self._curve = curve[0]
         return self._curve
 
 
+@dataclass
 class CurvesData(Data):
-    def __init__(self, raw, create_curves, dfbuilder):
-        Data.__init__(self, raw, dfbuilder=dfbuilder)
-        self._create_curves = create_curves
-        self._curves = None
+    _create_curves: Callable = None
+    _curves: List[Curve] = None
 
     @property
     def curves(self) -> List[Curve]:
         if self._curves is None:
-            self._curves = self._create_curves(self._raw)
+            self._curves = self._create_curves(self.raw)
         return self._curves
 
 
 def make_create_forward_curves(x_axis: str, y_axis: str) -> Callable:
     """
     Parameters
     ----------
@@ -551,23 +544,17 @@
 
         return curves
 
     return create_zc_curves
 
 
 curves_maker_by_content_type = {
-    ContentType.FORWARD_CURVE: make_create_forward_curves(
-        x_axis="endDate", y_axis="discountFactor"
-    ),
-    ContentType.BOND_CURVE: make_create_bond_curves(
-        x_axis="endDate", y_axis="discountFactor"
-    ),
-    ContentType.ZC_CURVES: make_create_zc_curves(
-        x_axis="endDate", y_axis="discountFactor"
-    ),
+    ContentType.FORWARD_CURVE: make_create_forward_curves(x_axis="endDate", y_axis="discountFactor"),
+    ContentType.BOND_CURVE: make_create_bond_curves(x_axis="endDate", y_axis="discountFactor"),
+    ContentType.ZC_CURVES: make_create_zc_curves(x_axis="endDate", y_axis="discountFactor"),
 }
 
 
 def get_curves_maker(content_type):
     curves_maker = curves_maker_by_content_type.get(content_type)
 
     if not curves_maker:
@@ -578,34 +565,38 @@
 
 # ---------------------------------------------------------------------------
 #   Response factory
 # ---------------------------------------------------------------------------
 
 
 class CurvesResponseFactory(ContentResponseFactory):
-    def create_data_success(self, parsed_data: "ParsedData", **kwargs) -> Data:
-        return self._do_create_data(self.get_raw(parsed_data), **kwargs)
+    def create_data_success(self, raw: Any, **kwargs) -> Data:
+        return self._do_create_data(raw, **kwargs)
 
-    def create_data_fail(self, parsed_data: "ParsedData", **kwargs) -> Data:
+    def create_data_fail(self, raw: Any, **kwargs) -> Data:
         return self._do_create_data({}, **kwargs)
 
     def _do_create_data(self, raw: Any, universe=None, **kwargs):
         content_type = kwargs.get("__content_type__")
         dfbuilder = self.get_dfbuilder(content_type, **kwargs)
 
         if content_type is ContentType.ZC_CURVE_DEFINITIONS:
-            data = Data(raw, dfbuilder=dfbuilder)
+            data = Data(raw, _dfbuilder=dfbuilder)
 
         else:
             curves_maker = get_curves_maker(content_type)
             if iterable(universe):
-                data = CurvesData(raw, curves_maker, dfbuilder)
+                data = CurvesData(
+                    raw=raw,
+                    _dfbuilder=dfbuilder,
+                    _create_curves=curves_maker,
+                )
 
             else:
-                data = OneCurveData(raw, curves_maker, dfbuilder)
+                data = OneCurveData(raw=raw, _dfbuilder=dfbuilder, _create_curves=curves_maker)
 
         return data
 
 
 # ---------------------------------------------------------------------------
 #   Data provider
 # ---------------------------------------------------------------------------
@@ -617,22 +608,23 @@
 )
 
 curve_data_provider = ContentDataProvider(
     request=CurvesAndSurfacesRequestFactory(),
     validator=ValidatorContainer(content_validator=CurvesContentValidator()),
 )
 
-cross_currency_curves_triangulate_definitions_data_provider = ContentDataProvider(
+
+cross_currency_curves_triangulate_definitions_data_provider = DataProvider(
     request=CurvesAndSurfacesRequestFactory(),
     response=ResponseFactory(data_class=TriangulateDefinitionsData),
     validator=ValidatorContainer(content_validator=CurvesContentValidator()),
 )
 
-cross_currency_curves_definitions_data_provider = ContentDataProvider(
+cross_currency_curves_definitions_data_provider = DataProvider(
     request=CrossCurrencyCurvesDefinitionsRequestFactory(),
     response=ResponseFactory(data_class=CurveDefinitionData),
     validator=ValidatorContainer(content_validator=CurveDefinitionContentValidator()),
 )
 
-cross_currency_curves_definitions_delete_data_provider = ContentDataProvider(
+cross_currency_curves_definitions_delete_data_provider = DataProvider(
     request=CrossCurrencyCurvesDefinitionsRequestFactory(),
 )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -2,13 +2,14 @@
 from ._compounding_type import *
 from ._market_data_access_denied_fallback import *
 from ._zc_interpolation_mode import *
 from ._forward_curves_outputs import *
 from ._zc_curves_outputs import *
 from ._swap_price_side import *
 from ._instrument_type import *
+from ._constituent_override_mode import *
 from ..._enums._risk_type import *
 from ..._enums._extrapolation_mode import *
 from ..._enums._price_side import *
 from ..._enums._day_count_basis import *
 from ..._enums._interpolation_mode import *
 from ..._enums._asset_class import *
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_enums/_instrument_type.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_enums/_instrument_type.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-from enum import Enum, unique
+from enum import unique
+
+from ....._base_enum import StrEnum
 
 
 @unique
-class InstrumentType(Enum):
+class InstrumentType(StrEnum):
     BOND = "Bond"
     BOND_FUTURES = "BondFutures"
     CALENDAR_SPREAD = "CalendarSpread"
     CREDIT_DEFAULT_SWAP = "CreditDefaultSwap"
     CROSS_CURRENCY_SWAP = "CrossCurrencySwap"
     DEPOSIT = "Deposit"
     FRA = "Fra"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_forward_curve_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_forward_curve_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_forward_curve_types.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_forward_curve_types.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,22 +1,22 @@
 from typing import Optional, Iterable, Union
 
 from ._enums import ForwardCurvesOutputs
+from ._shift_scenario import ShiftScenario
 from ._forward_curve_definition import ForwardCurveDefinition
 from ._models import Step, Turn
 from ._swap_zc_curve_definition import SwapZcCurveDefinition
 from ._swap_zc_curve_parameters import SwapZcCurveParameters
 from ..curves import forward_curves
 from ...._types import Strings
 
 CurveDefinition = Optional[SwapZcCurveDefinition]
 CurveParameters = Optional[SwapZcCurveParameters]
-ForwardCurveDefinitions = Union[
-    ForwardCurveDefinition, Iterable[ForwardCurveDefinition]
-]
+ForwardCurveDefinitions = Union[ForwardCurveDefinition, Iterable[ForwardCurveDefinition]]
+ShiftScenarios = Union[ShiftScenario, Iterable[ShiftScenario]]
 OptForwardCurveDefinitions = Optional[ForwardCurveDefinitions]
 
 Outputs = Union[Strings, Iterable[ForwardCurvesOutputs]]
 Universe = Union[forward_curves.Definition, Iterable[forward_curves.Definition]]
 
 Steps = Union[Iterable[Step]]
 Turns = Union[Iterable[Turn]]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_convexity.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_convexity.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_curve.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_curve.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_instrument.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_instrument.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_interest_rate_curve_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_interest_rate_curve_parameters.py`

 * *Files 0% similar despite different names*

```diff
@@ -98,17 +98,15 @@
         interest_calculation_method: Optional[DayCountBasis] = None,
         calendar_adjustment: Optional[CalendarAdjustment] = None,
         calendars: OptStr = None,
         compounding_type: Optional[CompoundingType] = None,
         convexity_adjustment: Optional[ConvexityAdjustment] = None,
         extrapolation_mode: Optional[ExtrapolationMode] = None,
         interpolation_mode: Optional[ZcInterpolationMode] = None,
-        market_data_access_denied_fallback: Optional[
-            MarketDataAccessDeniedFallback
-        ] = None,
+        market_data_access_denied_fallback: Optional[MarketDataAccessDeniedFallback] = None,
         price_side: Optional[SwapPriceSide] = None,
         steps: "Steps" = None,
         turns: "Turns" = None,
         reference_tenor: OptStr = None,
         use_convexity_adjustment: OptBool = None,
         use_multi_dimensional_solver: OptBool = None,
         use_steps: OptBool = None,
@@ -244,23 +242,19 @@
     def market_data_access_denied_fallback(self):
         """
         - ReturnError: dont price the surface and return an error (Default value)
         - IgnoreConstituents: price the surface without the error market data
         - UseDelayedData: use delayed Market Data if possible
         :return: enum MarketDataAccessDeniedFallback
         """
-        return self._get_enum_parameter(
-            MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback"
-        )
+        return self._get_enum_parameter(MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback")
 
     @market_data_access_denied_fallback.setter
     def market_data_access_denied_fallback(self, value):
-        self._set_enum_parameter(
-            MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback", value
-        )
+        self._set_enum_parameter(MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback", value)
 
     @property
     def price_side(self):
         """
         Price side of the instrument to be used. Default value is: Mid
         :return: enum SwapPriceSide
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_step.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_step.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_models/_turn.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_models/_turn.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_swap_zc_curve_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_swap_definition.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,181 +1,167 @@
 # coding: utf8
-
 from typing import Optional
 
-from .._object_definition import ObjectDefinition
-from ._enums import AssetClass, RiskType
-from ...._types import Strings, OptStr
-from ...._tools import create_repr, try_copy_to_list
-
-
-class SwapZcCurveDefinition(ObjectDefinition):
-    """
-    Parameters
-    ----------
-    index_name : str, optional
-
-    index_tenors : string, optional
-        Defines expected rate surface tenor/slices defaults to the tenors available,
-        based on provided market data
-    main_constituent_asset_class : MainConstituentAssetClass, optional
-
-    risk_type : RiskType, optional
-
-    currency : str, optional
-
-    discounting_tenor : str, optional
+from ._swap_leg_definition import LegDefinition
+from .._instrument_definition import InstrumentDefinition
 
-    id : str, optional
-        Id of the curve definition to get
-    market_data_location : str, optional
-
-    name : str, optional
-
-    source : str, optional
-
-    """
 
+class SwapInstrumentDefinition(InstrumentDefinition):
     def __init__(
         self,
-        index_name: OptStr = None,
-        index_tenors: Strings = None,
-        main_constituent_asset_class: Optional[AssetClass] = None,
-        risk_type: Optional[RiskType] = None,
-        currency: OptStr = None,
-        discounting_tenor: OptStr = None,
-        id: OptStr = None,
-        market_data_location: OptStr = None,
-        name: OptStr = None,
-        source: OptStr = None,
+        instrument_tag: Optional[str] = None,
+        instrument_code: Optional[str] = None,
+        trade_date: Optional[str] = None,
+        start_date: Optional[str] = None,
+        end_date: Optional[str] = None,
+        tenor: Optional[str] = None,
+        legs: Optional[LegDefinition] = None,
+        is_non_deliverable: Optional[bool] = None,
+        settlement_ccy: Optional[str] = None,
+        start_tenor: Optional[str] = None,
+        template: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.index_name = index_name
-        self.index_tenors = try_copy_to_list(index_tenors)
-        self.main_constituent_asset_class = main_constituent_asset_class
-        self.risk_type = risk_type
-        self.currency = currency
-        self.discounting_tenor = discounting_tenor
-        self.id = id
-        self.market_data_location = market_data_location
-        self.name = name
-        self.source = source
+        self.instrument_tag = instrument_tag
+        self.instrument_code = instrument_code
+        self.trade_date = trade_date
+        self.start_date = start_date
+        self.end_date = end_date
+        self.tenor = tenor
+        self.legs = legs
+        self.is_non_deliverable = is_non_deliverable
+        self.settlement_ccy = settlement_ccy
+        self.start_tenor = start_tenor
+        self.template = template
 
-    def __repr__(self):
-        return create_repr(
-            self,
-            middle_path="curves.forward_curves",
-            class_name=self.__class__.__name__,
-        )
+    def get_instrument_type(self):
+        return "Swap"
 
     @property
-    def index_tenors(self):
+    def legs(self):
         """
-        Defines expected rate surface tenor/slices defaults to the tenors available,
-        based on provided market data
-        :return: list string
+        The legs of the Swap to provide a full definition of the swap if no template or instrumentCode have been provided.
+        Optional. Either InstrumentCode, Template, or Legs must be provided.
+        :return: list SwapLegDefinition
         """
-        return self._get_list_parameter(str, "indexTenors")
+        return self._get_list_parameter(LegDefinition, "legs")
 
-    @index_tenors.setter
-    def index_tenors(self, value):
-        self._set_list_parameter(str, "indexTenors", value)
+    @legs.setter
+    def legs(self, value):
+        self._set_list_parameter(LegDefinition, "legs", value)
 
     @property
-    def main_constituent_asset_class(self):
+    def end_date(self):
         """
-        :return: enum AssetClass
+        The maturity date of the swap contract.
+        Mandatory. Either the endDate or the tenor must be provided.
+        :return: str
         """
-        return self._get_enum_parameter(AssetClass, "mainConstituentAssetClass")
+        return self._get_parameter("endDate")
 
-    @main_constituent_asset_class.setter
-    def main_constituent_asset_class(self, value):
-        self._set_enum_parameter(AssetClass, "mainConstituentAssetClass", value)
+    @end_date.setter
+    def end_date(self, value):
+        self._set_parameter("endDate", value)
 
     @property
-    def risk_type(self):
+    def instrument_code(self):
         """
-        :return: enum RiskType
+        A swap RIC that is used to retrieve the description of the swap contract.
+        Optional. Either instrumentCode, template, or legs must be provided.
+        :return: str
         """
-        return self._get_enum_parameter(RiskType, "riskType")
+        return self._get_parameter("instrumentCode")
 
-    @risk_type.setter
-    def risk_type(self, value):
-        self._set_enum_parameter(RiskType, "riskType", value)
+    @instrument_code.setter
+    def instrument_code(self, value):
+        self._set_parameter("instrumentCode", value)
 
     @property
-    def currency(self):
+    def is_non_deliverable(self):
         """
-        :return: str
+        A flag that indicates if the swap is non-deliverable.
+        Optional. By defaults 'false'.
+        :return: bool
         """
-        return self._get_parameter("currency")
+        return self._get_parameter("isNonDeliverable")
 
-    @currency.setter
-    def currency(self, value):
-        self._set_parameter("currency", value)
+    @is_non_deliverable.setter
+    def is_non_deliverable(self, value):
+        self._set_parameter("isNonDeliverable", value)
 
     @property
-    def discounting_tenor(self):
+    def settlement_ccy(self):
         """
+        For non-deliverable instrument, the ISO code of the settlement currency.
+        Optional. By priority order : 'USD' if one leg denominated in USD; 'EUR' if one leg is denominated in EUR; the paidLegCcy.
         :return: str
         """
-        return self._get_parameter("discountingTenor")
+        return self._get_parameter("settlementCcy")
 
-    @discounting_tenor.setter
-    def discounting_tenor(self, value):
-        self._set_parameter("discountingTenor", value)
+    @settlement_ccy.setter
+    def settlement_ccy(self, value):
+        self._set_parameter("settlementCcy", value)
 
     @property
-    def id(self):
+    def start_date(self):
         """
-        Id of the curve definition to get
+        The date the swap starts accruing interest. Its effective date.
+        Optional. By default, it is derived from the TradeDate and the day to spot convention of the contract currency.
         :return: str
         """
-        return self._get_parameter("id")
+        return self._get_parameter("startDate")
 
-    @id.setter
-    def id(self, value):
-        self._set_parameter("id", value)
+    @start_date.setter
+    def start_date(self, value):
+        self._set_parameter("startDate", value)
 
     @property
-    def index_name(self):
+    def start_tenor(self):
         """
+        The code indicating the period from a spot date to startdate of the instrument
+        (e.g. '1m'). no default value applies.
         :return: str
         """
-        return self._get_parameter("indexName")
+        return self._get_parameter("startTenor")
 
-    @index_name.setter
-    def index_name(self, value):
-        self._set_parameter("indexName", value)
+    @start_tenor.setter
+    def start_tenor(self, value):
+        self._set_parameter("startTenor", value)
 
     @property
-    def market_data_location(self):
+    def template(self):
         """
+        A reference to a common swap contract.
+        Optional. Either InstrumentCode, Template, or Legs must be provided.
         :return: str
         """
-        return self._get_parameter("marketDataLocation")
+        return self._get_parameter("template")
 
-    @market_data_location.setter
-    def market_data_location(self, value):
-        self._set_parameter("marketDataLocation", value)
+    @template.setter
+    def template(self, value):
+        self._set_parameter("template", value)
 
     @property
-    def name(self):
+    def tenor(self):
         """
+        The period code that represents the time between the start date and end date the contract.
+        Mandatory. Either the endDate or the tenor must be provided.
         :return: str
         """
-        return self._get_parameter("name")
+        return self._get_parameter("tenor")
 
-    @name.setter
-    def name(self, value):
-        self._set_parameter("name", value)
+    @tenor.setter
+    def tenor(self, value):
+        self._set_parameter("tenor", value)
 
     @property
-    def source(self):
+    def trade_date(self):
         """
+        The date the swap contract was created.
+        Optional. By default, the valuation date.
         :return: str
         """
-        return self._get_parameter("source")
+        return self._get_parameter("tradeDate")
 
-    @source.setter
-    def source(self, value):
-        self._set_parameter("source", value)
+    @trade_date.setter
+    def trade_date(self, value):
+        self._set_parameter("tradeDate", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_swap_zc_curve_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_definition.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,370 +1,316 @@
 # coding: utf8
 
-from typing import Optional, TYPE_CHECKING
 
-from .._object_definition import ObjectDefinition
+from typing import Optional, List, Union
 
-from ._enums import (
-    DayCountBasis,
-    InterpolationMode,
-    CalendarAdjustment,
-    PriceSide,
-    CompoundingType,
-    ExtrapolationMode,
+from . import BermudanSwaptionDefinition
+from ..._enums import (
+    BuySell,
+    ExerciseStyle,
+    PremiumSettlementType,
+    SwaptionSettlementType,
+    SwaptionType,
 )
-from ._models import (
-    ConvexityAdjustment,
-    Step,
-    Turn,
-)
-from ...._types import Strings, OptBool, OptStr
-from ...._tools import create_repr, try_copy_to_list
-
-if TYPE_CHECKING:
-    from ._forward_curve_types import Steps, Turns
+from ..._models import InputFlow
+from .. import swap
+from .._instrument_definition import InstrumentDefinition
+from ..swap._swap_definition import SwapInstrumentDefinition
 
 
-class SwapZcCurveParameters(ObjectDefinition):
+class SwaptionInstrumentDefinition(InstrumentDefinition):
     """
+    API endpoint for Financial Contract analytics,
+    that returns calculations relevant to each contract type.
+
     Parameters
     ----------
-    interest_calculation_method : InterestCalculationMethod, optional
-        Day count basis of the calculated zero coupon rates.
-        Default value is: Dcb_Actual_Actual
-    calendar_adjustment : CalendarAdjustment, optional
-        Cash flow adjustment according to a calendar
-        No:
-        Null:
-        Weekend: for cash flow pricing using the calendar
-        WeekendCalendar: for cash flow pricing using the calendar defined
-                         by the parameter 'calendars'.
-    calendars : string, optional
-        A list of one or more calendar codes used to define non-working days and to
-        adjust coupon dates and values.
-    compounding_type : CompoundingType, optional
-
-    convexity_adjustment : ConvexityAdjustment, optional
-
-    extrapolation_mode : ExtrapolationMode, optional
-        None: no extrapolation
-        Constant: constant extrapolation
-        Linear: linear extrapolation
-    interpolation_mode : InterpolationMode, optional
-        Interpolation method used in swap zero curve bootstrap.
-        Default value is: CubicSpline
-
-        CubicDiscount: local cubic interpolation of discount factors
-        CubicRate: local cubic interpolation of rates
-        CubicSpline: a natural cubic spline
-        Linear: linear interpolation
-        Log: log linear interpolation
-        ForwardMonotoneConvex
-    price_side : SwapPriceSide, optional
-        Defines which data is used for the rate surface computation.
-        Default value is: Mid
-    steps : Step, optional
-        Use to calculate the swap rate surface discount curve, when OIS is selected as
-        discount curve.
-        The steps can specify overnight index stepped dates or/and rates.
-    turns : Turn, optional
-        Used to include end period rates/turns when calculating swap rate surfaces
-    ignore_existing_definition : bool, optional
-
-    reference_tenor : str, optional
-        Root tenor(s) for the xIbor dependencies
-    use_convexity_adjustment : bool, optional
-        false / true
-        Default value is: true.
-        It indicates if the system needs to retrieve the convexity adjustment
-    use_multi_dimensional_solver : bool, optional
-        false / true
-        Default value is: true.
-        Specifies the use of the multi-dimensional solver for yield curve bootstrapping.
-        This solving method is required because the bootstrapping method sometimes
-        creates a ZC curve which does not accurately reprice the input instruments used
-        to build it.
-        The multi-dimensional solver is recommended when cubic interpolation methods
-        are used in building the curve
-        (in other cases, performance might be inferior to the regular bootstrapping
-        method).
-         - true: to use multi-dimensional solver for yield curve bootstrapping
-         - false: not to use multi-dimensional solver for yield curve bootstrapping
-    use_steps : bool, optional
-        false / true
-        Default value is: false.
-        It indicates if the system needs to retrieve the overnight index
-        stepped dates or/and rates
-    valuation_date : str, optional
-        The valuation date
-        Default value is the current date
+    instrument_tag : str, optional
+        A user defined string to identify the instrument. it can be used to link output
+        results to the instrument definition.limited to 40 characters.only alphabetic,
+        numeric and '- _.#=@' characters are supported. optional. no default value
+        applies.
+    start_date : str, optional
+        The date the swaption starts. optional. by default it is derived from the
+        tradedate and the day to spot convention of the contract currency.
+    end_date : str, optional
+        The maturity or expiry date of the instrument's leg. the value is expressed in
+        iso 8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g. 2021-01-01t00:00:00z). either
+        tenor or enddate must be provided. the default value is valuationdate shifted
+        forward by tenor.
+    tenor : str, optional
+        The code indicating the period between startdate and enddate of the instrument
+        (e.g. '6m', '1y'). mandatory, if enddate is not provided. the default value is
+        calculated from enddate.
+    notional_amount : float, optional
+        The notional amount of the instrument. the default value is '1,000,000'.
+    bermudan_swaption_definition : BermudanSwaptionDefinition, optional
+
+    buy_sell : BuySell or str, optional
+        The indicator of the deal side. the possible values are:   buy: buying the
+        option,   sell: selling/writing the option.  no default value applies.
+    exercise_style : ExerciseStyle or str, optional
+        The option style based on its exercise restrictions. the possible values are:
+        amer,   euro,   berm.  note: all exercise styles may not apply to certain option
+        no default value applies.
+    payments : InputFlow, optional
+        An array of payments
+    premium_settlement_type : PremiumSettlementType or str, optional
+        The cash settlement type of the option premium   spot,   forward.
+    settlement_type : SwaptionSettlementType or str, optional
+        The settlement method for options when exercised. the possible values are:
+        physical: delivering the underlying asset, or for a swaption, physically
+        entering into the underlying swap.    cash: paying out in cash.  the default
+        value is 'physical'.
+    swaption_type : SwaptionType or str, optional
+        The indicator if the swaption is a payer or a receiver. the possible values are:
+        receiver: a right to receive a fixed rate of the underlying swap,   payer: a
+        right to pay a fixed rate of the underlying swap.  no default value applies.
+    underlying_definition : SwapDefinition, optional
+
+    spread_vs_atm_in_bp : float, optional
+        Spread between strike and atm strike, expressed in basis points (bp).
+    strike_percent : float, optional
+        The set price at which the owner of the option can buy or sell the underlying
+        asset. for a swaption, it is the fixed rate of the underlying swap at which the
+        owner of the swaption can enter the swap. the value is expressed in percentages.
+        by default, fixedratepercent of the underlying swap is used.
     """
 
     def __init__(
         self,
-        interest_calculation_method: Optional[DayCountBasis] = None,
-        calendar_adjustment: Optional[CalendarAdjustment] = None,
-        calendars: Strings = None,
-        compounding_type: Optional[CompoundingType] = None,
-        convexity_adjustment: Optional[ConvexityAdjustment] = None,
-        extrapolation_mode: Optional[ExtrapolationMode] = None,
-        interpolation_mode: Optional[InterpolationMode] = None,
-        price_side: Optional[PriceSide] = None,
-        steps: "Steps" = None,
-        turns: "Turns" = None,
-        ignore_existing_definition: OptBool = None,
-        reference_tenor: OptStr = None,
-        use_convexity_adjustment: OptBool = None,
-        use_multi_dimensional_solver: OptBool = None,
-        use_steps: OptBool = None,
-        valuation_date: OptStr = None,
+        instrument_tag: Optional[str] = None,
+        start_date: Optional[str] = None,
+        end_date: Optional[str] = None,
+        tenor: Optional[str] = None,
+        notional_amount: Optional[float] = None,
+        bermudan_swaption_definition: Optional[BermudanSwaptionDefinition] = None,
+        buy_sell: Union[BuySell, str] = None,
+        exercise_style: Union[ExerciseStyle, str] = None,
+        payments: Optional[List[InputFlow]] = None,
+        premium_settlement_type: Union[PremiumSettlementType, str] = None,
+        settlement_type: Union[SwaptionSettlementType, str] = None,
+        swaption_type: Union[SwaptionType, str] = None,
+        underlying_definition: Optional[swap.Definition] = None,
+        spread_vs_atm_in_bp: Optional[float] = None,
+        strike_percent: Optional[float] = None,
+        delivery_date: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.interest_calculation_method = interest_calculation_method
-        self.calendar_adjustment = calendar_adjustment
-        self.calendars = try_copy_to_list(calendars)
-        self.compounding_type = compounding_type
-        self.convexity_adjustment = convexity_adjustment
-        self.extrapolation_mode = extrapolation_mode
-        self.interpolation_mode = interpolation_mode
-        self.price_side = price_side
-        self.steps = try_copy_to_list(steps)
-        self.turns = try_copy_to_list(turns)
-        self.ignore_existing_definition = ignore_existing_definition
-        self.reference_tenor = reference_tenor
-        self.use_convexity_adjustment = use_convexity_adjustment
-        self.use_multi_dimensional_solver = use_multi_dimensional_solver
-        self.use_steps = use_steps
-        self.valuation_date = valuation_date
+        self.instrument_tag = instrument_tag
+        self.start_date = start_date
+        self.end_date = end_date
+        self.tenor = tenor
+        self.notional_amount = notional_amount
+        self.bermudan_swaption_definition = bermudan_swaption_definition
+        self.buy_sell = buy_sell
+        self.exercise_style = exercise_style
+        self.payments = payments
+        self.premium_settlement_type = premium_settlement_type
+        self.settlement_type = settlement_type
+        self.swaption_type = swaption_type
+        self.underlying_definition = underlying_definition
+        self.spread_vs_atm_in_bp = spread_vs_atm_in_bp
+        self.strike_percent = strike_percent
+        self.delivery_date = delivery_date
 
-    def __repr__(self):
-        return create_repr(
-            self,
-            middle_path="curves.forward_curves",
-            class_name=self.__class__.__name__,
-        )
+    def get_instrument_type(self):
+        return "Swaption"
 
     @property
-    def calendar_adjustment(self):
+    def bermudan_swaption_definition(self):
         """
-        Cash flow adjustment according to a calendar
-        No:
-        Null:
-        Weekend: for cash flow pricing using the calendar
-        WeekendCalendar: for cash flow pricing using the calendar defined
-                         by the parameter 'calendars'.
-        :return: enum CalendarAdjustment
+        :return: object BermudanSwaptionDefinition
         """
-        return self._get_enum_parameter(CalendarAdjustment, "calendarAdjustment")
+        return self._get_object_parameter(BermudanSwaptionDefinition, "bermudanSwaptionDefinition")
 
-    @calendar_adjustment.setter
-    def calendar_adjustment(self, value):
-        self._set_enum_parameter(CalendarAdjustment, "calendarAdjustment", value)
+    @bermudan_swaption_definition.setter
+    def bermudan_swaption_definition(self, value):
+        self._set_object_parameter(BermudanSwaptionDefinition, "bermudanSwaptionDefinition", value)
 
     @property
-    def calendars(self):
+    def buy_sell(self):
         """
-        A list of one or more calendar codes used to define non-working days and to
-        adjust coupon dates and values.
-        :return: list string
+        The side of the deal.
+        :return: enum BuySell
         """
-        return self._get_list_parameter(str, "calendars")
+        return self._get_enum_parameter(BuySell, "buySell")
 
-    @calendars.setter
-    def calendars(self, value):
-        self._set_list_parameter(str, "calendars", value)
+    @buy_sell.setter
+    def buy_sell(self, value):
+        self._set_enum_parameter(BuySell, "buySell", value)
 
     @property
-    def compounding_type(self):
+    def exercise_style(self):
         """
-        :return: enum CompoundingType
+        :return: enum ExerciseStyle
         """
-        return self._get_enum_parameter(CompoundingType, "compoundingType")
+        return self._get_enum_parameter(ExerciseStyle, "exerciseStyle")
 
-    @compounding_type.setter
-    def compounding_type(self, value):
-        self._set_enum_parameter(CompoundingType, "compoundingType", value)
+    @exercise_style.setter
+    def exercise_style(self, value):
+        self._set_enum_parameter(ExerciseStyle, "exerciseStyle", value)
 
     @property
-    def convexity_adjustment(self):
+    def payments(self):
         """
-        :return: object ConvexityAdjustment
+        An array of payments
+        :return: list InputFlow
         """
-        return self._get_object_parameter(ConvexityAdjustment, "convexityAdjustment")
+        return self._get_list_parameter(InputFlow, "payments")
 
-    @convexity_adjustment.setter
-    def convexity_adjustment(self, value):
-        self._set_object_parameter(ConvexityAdjustment, "convexityAdjustment", value)
+    @payments.setter
+    def payments(self, value):
+        self._set_list_parameter(InputFlow, "payments", value)
 
     @property
-    def extrapolation_mode(self):
+    def premium_settlement_type(self):
         """
-        None: no extrapolation
-        Constant: constant extrapolation
-        Linear: linear extrapolation
-        :return: enum ExtrapolationMode
+        The cash settlement type of the option premium   spot,   forward.
+        :return: enum PremiumSettlementType
         """
-        return self._get_enum_parameter(ExtrapolationMode, "extrapolationMode")
+        return self._get_enum_parameter(PremiumSettlementType, "premiumSettlementType")
 
-    @extrapolation_mode.setter
-    def extrapolation_mode(self, value):
-        self._set_enum_parameter(ExtrapolationMode, "extrapolationMode", value)
+    @premium_settlement_type.setter
+    def premium_settlement_type(self, value):
+        self._set_enum_parameter(PremiumSettlementType, "premiumSettlementType", value)
 
     @property
-    def interest_calculation_method(self):
+    def settlement_type(self):
         """
-        Day count basis of the calculated zero coupon rates.
-        Default value is: Dcb_Actual_Actual
-        :return: enum DayCountBasis
+        The settlement type of the option if the option is exercised.
+        :return: enum SwaptionSettlementType
         """
-        return self._get_enum_parameter(DayCountBasis, "interestCalculationMethod")
+        return self._get_enum_parameter(SwaptionSettlementType, "settlementType")
 
-    @interest_calculation_method.setter
-    def interest_calculation_method(self, value):
-        self._set_enum_parameter(DayCountBasis, "interestCalculationMethod", value)
+    @settlement_type.setter
+    def settlement_type(self, value):
+        self._set_enum_parameter(SwaptionSettlementType, "settlementType", value)
 
     @property
-    def interpolation_mode(self):
+    def swaption_type(self):
         """
-        Interpolation method used in swap zero curve bootstrap.
-        Default value is: CubicSpline
-
-        CubicDiscount: local cubic interpolation of discount factors
-        CubicRate: local cubic interpolation of rates
-        CubicSpline: a natural cubic spline
-        Linear: linear interpolation
-        Log: log linear interpolation
-        ForwardMonotoneConvex
-        :return: enum InterpolationMode
+        The indicator if the swaption is a payer or a receiver. the possible values are:
+        receiver: a right to receive a fixed rate of the underlying swap,   payer: a
+        right to pay a fixed rate of the underlying swap.  no default value applies.
+        :return: enum SwaptionType
         """
-        return self._get_enum_parameter(InterpolationMode, "interpolationMode")
+        return self._get_enum_parameter(SwaptionType, "swaptionType")
 
-    @interpolation_mode.setter
-    def interpolation_mode(self, value):
-        self._set_enum_parameter(InterpolationMode, "interpolationMode", value)
+    @swaption_type.setter
+    def swaption_type(self, value):
+        self._set_enum_parameter(SwaptionType, "swaptionType", value)
 
     @property
-    def price_side(self):
+    def underlying_definition(self):
         """
-        Defines which data is used for the rate surface computation.
-        Default value is: Mid
-        :return: enum PriceSide
+        :return: object SwapDefinition
         """
-        return self._get_enum_parameter(PriceSide, "priceSide")
+        return self._get_object_parameter(SwapInstrumentDefinition, "underlyingDefinition")
 
-    @price_side.setter
-    def price_side(self, value):
-        self._set_enum_parameter(PriceSide, "priceSide", value)
+    @underlying_definition.setter
+    def underlying_definition(self, value):
+        self._set_object_parameter(SwapInstrumentDefinition, "underlyingDefinition", value)
 
     @property
-    def steps(self):
+    def end_date(self):
         """
-        Use to calculate the swap rate surface discount curve, when OIS is selected as
-        discount curve.
-        The steps can specify overnight index stepped dates or/and rates.
-        :return: list Step
+        The maturity or expiry date of the instrument's leg. the value is expressed in
+        iso 8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g. 2021-01-01t00:00:00z). either
+        tenor or enddate must be provided. the default value is valuationdate shifted
+        forward by tenor.
+        :return: str
         """
-        return self._get_list_parameter(Step, "steps")
+        return self._get_parameter("endDate")
 
-    @steps.setter
-    def steps(self, value):
-        self._set_list_parameter(Step, "steps", value)
+    @end_date.setter
+    def end_date(self, value):
+        self._set_parameter("endDate", value)
 
     @property
-    def turns(self):
+    def instrument_tag(self):
         """
-        Used to include end period rates/turns when calculating swap rate surfaces
-        :return: list Turn
+        A user defined string to identify the instrument. it can be used to link output
+        results to the instrument definition.limited to 40 characters.only alphabetic,
+        numeric and '- _.#=@' characters are supported. optional. no default value
+        applies.
+        :return: str
         """
-        return self._get_list_parameter(Turn, "turns")
+        return self._get_parameter("instrumentTag")
 
-    @turns.setter
-    def turns(self, value):
-        self._set_list_parameter(Turn, "turns", value)
+    @instrument_tag.setter
+    def instrument_tag(self, value):
+        self._set_parameter("instrumentTag", value)
 
     @property
-    def ignore_existing_definition(self):
+    def notional_amount(self):
         """
-        :return: bool
+        The notional amount of the instrument. The default value is '1,000,000'.
+        :return: float
         """
-        return self._get_parameter("ignoreExistingDefinition")
+        return self._get_parameter("notionalAmount")
 
-    @ignore_existing_definition.setter
-    def ignore_existing_definition(self, value):
-        self._set_parameter("ignoreExistingDefinition", value)
+    @notional_amount.setter
+    def notional_amount(self, value):
+        self._set_parameter("notionalAmount", value)
 
     @property
-    def reference_tenor(self):
+    def spread_vs_atm_in_bp(self):
         """
-        Root tenor(s) for the xIbor dependencies
-        :return: str
+        Spread between strike and atm strike, expressed in basis points (bp).
+        :return: float
         """
-        return self._get_parameter("referenceTenor")
+        return self._get_parameter("spreadVsAtmInBp")
 
-    @reference_tenor.setter
-    def reference_tenor(self, value):
-        self._set_parameter("referenceTenor", value)
+    @spread_vs_atm_in_bp.setter
+    def spread_vs_atm_in_bp(self, value):
+        self._set_parameter("spreadVsAtmInBp", value)
 
     @property
-    def use_convexity_adjustment(self):
+    def start_date(self):
         """
-        false / true
-        Default value is: true.
-        It indicates if the system needs to retrieve the convexity adjustment
-        :return: bool
+        The date the swaption starts. optional. by default it is derived from the
+        tradedate and the day to spot convention of the contract currency.
+        :return: str
         """
-        return self._get_parameter("useConvexityAdjustment")
+        return self._get_parameter("startDate")
 
-    @use_convexity_adjustment.setter
-    def use_convexity_adjustment(self, value):
-        self._set_parameter("useConvexityAdjustment", value)
+    @start_date.setter
+    def start_date(self, value):
+        self._set_parameter("startDate", value)
 
     @property
-    def use_multi_dimensional_solver(self):
+    def strike_percent(self):
         """
-        false / true
-        Default value is: true.
-        Specifies the use of the multi-dimensional solver for yield curve bootstrapping.
-        This solving method is required because the bootstrapping method sometimes
-        creates a ZC curve which does not accurately reprice the input instruments used
-        to build it.
-        The multi-dimensional solver is recommended when cubic interpolation methods
-        are used in building the curve
-        (in other cases, performance might be inferior to the regular bootstrapping
-        method).
-         - true: to use multi-dimensional solver for yield curve bootstrapping
-         - false: not to use multi-dimensional solver for yield curve bootstrapping
-        :return: bool
+        The set price at which the owner of the option can buy or sell the underlying
+        asset. for a swaption, it is the fixed rate of the underlying swap at which the
+        owner of the swaption can enter the swap. the value is expressed in percentages.
+        by default, fixedratepercent of the underlying swap is used.
+        :return: float
         """
-        return self._get_parameter("useMultiDimensionalSolver")
+        return self._get_parameter("strikePercent")
 
-    @use_multi_dimensional_solver.setter
-    def use_multi_dimensional_solver(self, value):
-        self._set_parameter("useMultiDimensionalSolver", value)
+    @strike_percent.setter
+    def strike_percent(self, value):
+        self._set_parameter("strikePercent", value)
 
     @property
-    def use_steps(self):
+    def tenor(self):
         """
-        false / true
-        Default value is: false.
-        It indicates if the system needs to retrieve the overnight index
-        stepped dates or/and rates
-        :return: bool
+        The code indicating the period between startdate and enddate of the instrument
+        (e.g. '6m', '1y'). mandatory, if enddate is not provided. the default value is
+        calculated from enddate.
+        :return: str
         """
-        return self._get_parameter("useSteps")
+        return self._get_parameter("tenor")
 
-    @use_steps.setter
-    def use_steps(self, value):
-        self._set_parameter("useSteps", value)
+    @tenor.setter
+    def tenor(self, value):
+        self._set_parameter("tenor", value)
 
     @property
-    def valuation_date(self):
-        """
-        The valuation date
-        Default value is the current date
-        :return: str
-        """
-        return self._get_parameter("valuationDate")
+    def delivery_date(self):
+        return self._get_parameter("deliveryDate")
 
-    @valuation_date.setter
-    def valuation_date(self, value):
-        self._set_parameter("valuationDate", value)
+    @delivery_date.setter
+    def delivery_date(self, value):
+        self._set_parameter("deliveryDate", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_definition_request.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,61 +1,42 @@
-# coding: utf8
-
-from typing import Optional
-
+from typing import TYPE_CHECKING, Optional
 from .._object_definition import ObjectDefinition
-from ._enums import (
-    AssetClass,
-    RiskType,
-)
-from ...._types import OptStr
-
-
-class ZcCurveDefinition(ObjectDefinition):
-    """
-    Parameters
-    ----------
-    index_name : str, optional
-
-    main_constituent_asset_class : AssetClass, optional
+from ._enums import RiskType
+from ._enums import AssetClass
 
-    risk_type : RiskType, optional
 
-    currency : str, optional
-        The currency code of the interest rate curve
-    discounting_tenor : str, optional
-        Mono currency discounting tenor
-    id : str, optional
-        Id of the curve definition
-    name : str, optional
-        The name of the interest rate curve
-    source : str, optional
+if TYPE_CHECKING:
+    from ...._types import OptStr
 
-    """
 
+class ZcCurveDefinitionRequest(ObjectDefinition):
     def __init__(
         self,
-        index_name: OptStr = None,
+        index_name: "OptStr" = None,
         main_constituent_asset_class: Optional[AssetClass] = None,
         risk_type: Optional[RiskType] = None,
-        currency: OptStr = None,
-        discounting_tenor: OptStr = None,
-        id: OptStr = None,
-        name: OptStr = None,
-        source: OptStr = None,
-    ) -> None:
+        currency: "OptStr" = None,
+        curve_tag: "OptStr" = None,
+        id: "OptStr" = None,
+        name: "OptStr" = None,
+        source: "OptStr" = None,
+        valuation_date: "OptStr" = None,
+        market_data_location: "OptStr" = None,
+    ):
         super().__init__()
         self.index_name = index_name
         self.main_constituent_asset_class = main_constituent_asset_class
         self.risk_type = risk_type
         self.currency = currency
-        self.discounting_tenor = discounting_tenor
+        self.curve_tag = curve_tag
         self.id = id
         self.name = name
         self.source = source
+        self.valuation_date = valuation_date
+        self.market_data_location = market_data_location
 
     @property
     def main_constituent_asset_class(self):
         """
         :return: enum AssetClass
         """
         return self._get_enum_parameter(AssetClass, "mainConstituentAssetClass")
@@ -84,24 +65,25 @@
         return self._get_parameter("currency")
 
     @currency.setter
     def currency(self, value):
         self._set_parameter("currency", value)
 
     @property
-    def discounting_tenor(self):
+    def curve_tag(self):
         """
-        Mono currency discounting tenor
+        User defined string to identify the curve. It can be used to link output results to the curve definition. Only alphabetic,
+        numeric and '- _.#=@' characters are supported. Optional.
         :return: str
         """
-        return self._get_parameter("discountingTenor")
+        return self._get_parameter("curveTag")
 
-    @discounting_tenor.setter
-    def discounting_tenor(self, value):
-        self._set_parameter("discountingTenor", value)
+    @curve_tag.setter
+    def curve_tag(self, value):
+        self._set_parameter("curveTag", value)
 
     @property
     def id(self):
         """
         Id of the curve definition
         :return: str
         """
@@ -140,7 +122,32 @@
         :return: str
         """
         return self._get_parameter("source")
 
     @source.setter
     def source(self, value):
         self._set_parameter("source", value)
+
+    @property
+    def valuation_date(self):
+        """
+        :return: str
+        """
+        return self._get_parameter("valuationDate")
+
+    @valuation_date.setter
+    def valuation_date(self, value):
+        self._set_parameter("valuationDate", value)
+
+    @property
+    def market_data_location(self):
+        """
+        The identifier of the market place from which constituents come from. currently
+        the following values are supported: 'onshore' and 'emea'. the list of values can
+        be extended by a user when creating a curve.
+        :return: str
+        """
+        return self._get_parameter("marketDataLocation")
+
+    @market_data_location.setter
+    def market_data_location(self, value):
+        self._set_parameter("marketDataLocation", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_parameters.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,23 +1,27 @@
-# coding: utf8
-
 from typing import Optional, TYPE_CHECKING
 from ...._tools import create_repr, try_copy_to_list
 
 from .._object_definition import ObjectDefinition
 from ._enums import (
     CalendarAdjustment,
     CompoundingType,
     ExtrapolationMode,
     DayCountBasis,
     MarketDataAccessDeniedFallback,
     SwapPriceSide,
     ZcInterpolationMode,
 )
-from ._models import Step, Turn, InterestRateCurveParameters, ConvexityAdjustment
+from ._models import (
+    Step,
+    Turn,
+    InterestRateCurveParameters,
+    ConvexityAdjustment,
+    ValuationTime,
+)
 from ...._types import OptBool, OptStr, Strings
 
 if TYPE_CHECKING:
     from ._zc_curve_types import Steps, Turns
 
 
 class ZcCurveParameters(ObjectDefinition):
@@ -75,17 +79,14 @@
         Price side of the instrument to be used. default value is: mid
     reference_curve_parameters : InterestRateCurveParameters, optional
 
     steps : list of Step, optional
 
     turns : list of Turn, optional
         Used to include end period rates/turns when calculating swap rate surfaces
-    ignore_existing_definition : bool, optional
-        The curve definition is provided in the request, so ignore the one from data
-        base
     reference_tenor : str, optional
         Root tenor(s) for the xIbor dependencies
     use_convexity_adjustment : bool, optional
 
     use_multi_dimensional_solver : bool, optional
         Specifies the use of the multi-dimensional solver for yield curve bootstrapping.
         This solving method is required because the bootstrapping method sometimes
@@ -96,39 +97,58 @@
         Credit Curve it is only used when the calibrationModel is set to Bootstrap.
         - true: to use multi-dimensional solver for yield curve bootstrapping
         - false: not to use multi-dimensional solver for yield curve bootstrapping
     use_steps : bool, optional
 
     valuation_date : str, optional
         The valuation date. The default value is the current date.
+    valuation_time : ValuationTime, optional
+        The time identified by offsets at which the zero coupon curve is generated.
+    ignore_invalid_instrument : bool, optional
+        Ignore invalid instrument to calculate the curve.
+        if False and some instrument are invlide, the curve is not calculated and an
+        error is returned.
+        The default value is 'True'.
+    use_delayed_data_if_denied : bool, optional
+        Use delayed ric to retrieve data when not permissioned on constituent ric.
+        The default value is 'False'.
+    valuation_date_time : str, optional
+        The date and time at which the zero coupon curve is generated. the value is
+        expressed in iso 8601 format: yyyy-mm-ddt00:00:00z (e.g., '2021-01-01t14:00:00z'
+        or '2021-01-01t14:00:00+02:00'). Only one parameter of valuation_date and
+        valuation_date_time must be specified.
     """
 
+    _ignore_existing_definition = None
+
     def __init__(
         self,
         interest_calculation_method: Optional[DayCountBasis] = None,
         calendar_adjustment: Optional[CalendarAdjustment] = None,
         calendars: Strings = None,
         compounding_type: Optional[CompoundingType] = None,
         convexity_adjustment: Optional[ConvexityAdjustment] = None,
         extrapolation_mode: Optional[ExtrapolationMode] = None,
         interpolation_mode: Optional[ZcInterpolationMode] = None,
-        market_data_access_denied_fallback: Optional[
-            MarketDataAccessDeniedFallback
-        ] = None,
+        market_data_access_denied_fallback: Optional[MarketDataAccessDeniedFallback] = None,
         pivot_curve_parameters: Optional[InterestRateCurveParameters] = None,
         price_side: Optional[SwapPriceSide] = None,
         reference_curve_parameters: Optional[InterestRateCurveParameters] = None,
         steps: "Steps" = None,
         turns: "Turns" = None,
         ignore_existing_definition: OptBool = None,
         reference_tenor: OptStr = None,
         use_convexity_adjustment: OptBool = None,
         use_multi_dimensional_solver: OptBool = None,
         use_steps: OptBool = None,
         valuation_date: OptStr = None,
+        valuation_time: Optional[ValuationTime] = None,
+        ignore_invalid_instrument: OptBool = None,
+        use_delayed_data_if_denied: OptBool = None,
+        valuation_date_time: OptStr = None,
     ) -> None:
         super().__init__()
         self.interest_calculation_method = interest_calculation_method
         self.calendar_adjustment = calendar_adjustment
         self.calendars = try_copy_to_list(calendars)
         self.compounding_type = compounding_type
         self.convexity_adjustment = convexity_adjustment
@@ -142,14 +162,18 @@
         self.turns = try_copy_to_list(turns)
         self.ignore_existing_definition = ignore_existing_definition
         self.reference_tenor = reference_tenor
         self.use_convexity_adjustment = use_convexity_adjustment
         self.use_multi_dimensional_solver = use_multi_dimensional_solver
         self.use_steps = use_steps
         self.valuation_date = valuation_date
+        self.valuation_time = valuation_time
+        self.ignore_invalid_instrument = ignore_invalid_instrument
+        self.use_delayed_data_if_denied = use_delayed_data_if_denied
+        self.valuation_date_time = valuation_date_time
 
     def __repr__(self):
         return create_repr(
             self,
             middle_path="curves.zc_curves",
             class_name=self.__class__.__name__,
         )
@@ -268,38 +292,30 @@
     def market_data_access_denied_fallback(self):
         """
         - ReturnError: dont price the surface and return an error (Default value)
         - IgnoreConstituents: price the surface without the error market data
         - UseDelayedData: use delayed Market Data if possible
         :return: enum MarketDataAccessDeniedFallback
         """
-        return self._get_enum_parameter(
-            MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback"
-        )
+        return self._get_enum_parameter(MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback")
 
     @market_data_access_denied_fallback.setter
     def market_data_access_denied_fallback(self, value):
-        self._set_enum_parameter(
-            MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback", value
-        )
+        self._set_enum_parameter(MarketDataAccessDeniedFallback, "marketDataAccessDeniedFallback", value)
 
     @property
     def pivot_curve_parameters(self):
         """
         :return: object InterestRateCurveParameters
         """
-        return self._get_object_parameter(
-            InterestRateCurveParameters, "pivotCurveParameters"
-        )
+        return self._get_object_parameter(InterestRateCurveParameters, "pivotCurveParameters")
 
     @pivot_curve_parameters.setter
     def pivot_curve_parameters(self, value):
-        self._set_object_parameter(
-            InterestRateCurveParameters, "pivotCurveParameters", value
-        )
+        self._set_object_parameter(InterestRateCurveParameters, "pivotCurveParameters", value)
 
     @property
     def price_side(self):
         """
         Price side of the instrument to be used. Default value is: Mid
         :return: enum SwapPriceSide
         """
@@ -310,23 +326,19 @@
         self._set_enum_parameter(SwapPriceSide, "priceSide", value)
 
     @property
     def reference_curve_parameters(self):
         """
         :return: object InterestRateCurveParameters
         """
-        return self._get_object_parameter(
-            InterestRateCurveParameters, "referenceCurveParameters"
-        )
+        return self._get_object_parameter(InterestRateCurveParameters, "referenceCurveParameters")
 
     @reference_curve_parameters.setter
     def reference_curve_parameters(self, value):
-        self._set_object_parameter(
-            InterestRateCurveParameters, "referenceCurveParameters", value
-        )
+        self._set_object_parameter(InterestRateCurveParameters, "referenceCurveParameters", value)
 
     @property
     def steps(self):
         """
         :return: list Step
         """
         return self._get_list_parameter(Step, "steps")
@@ -345,24 +357,19 @@
 
     @turns.setter
     def turns(self, value):
         self._set_list_parameter(Turn, "turns", value)
 
     @property
     def ignore_existing_definition(self):
-        """
-        The curve definition is provided in the request, so ignore the one from data
-        base
-        :return: bool
-        """
-        return self._get_parameter("ignoreExistingDefinition")
+        return self._ignore_existing_definition
 
     @ignore_existing_definition.setter
     def ignore_existing_definition(self, value):
-        self._set_parameter("ignoreExistingDefinition", value)
+        self._ignore_existing_definition = value
 
     @property
     def reference_tenor(self):
         """
         Root tenor(s) for the xIbor dependencies
         :return: str
         """
@@ -421,7 +428,61 @@
         :return: str
         """
         return self._get_parameter("valuationDate")
 
     @valuation_date.setter
     def valuation_date(self, value):
         self._set_parameter("valuationDate", value)
+
+    @property
+    def valuation_time(self):
+        """
+        The time identified by offsets at which the zero coupon curve is generated.
+        :return: object ValuationTime
+        """
+        return self._get_object_parameter(ValuationTime, "valuationTime")
+
+    @valuation_time.setter
+    def valuation_time(self, value):
+        self._set_object_parameter(ValuationTime, "valuationTime", value)
+
+    @property
+    def ignore_invalid_instrument(self):
+        """
+        Ignore invalid instrument to calculate the curve.  if false and some instrument
+        are invlide, the curve is not calculated and an error is returned.  the default
+        value is 'true'.
+        :return: bool
+        """
+        return self._get_parameter("ignoreInvalidInstrument")
+
+    @ignore_invalid_instrument.setter
+    def ignore_invalid_instrument(self, value):
+        self._set_parameter("ignoreInvalidInstrument", value)
+
+    @property
+    def use_delayed_data_if_denied(self):
+        """
+        Use delayed ric to retrieve data when not permissioned on constituent ric. the
+        default value is 'False'.
+        :return: bool
+        """
+        return self._get_parameter("useDelayedDataIfDenied")
+
+    @use_delayed_data_if_denied.setter
+    def use_delayed_data_if_denied(self, value):
+        self._set_parameter("useDelayedDataIfDenied", value)
+
+    @property
+    def valuation_date_time(self):
+        """
+        The date and time at which the zero coupon curve is generated. the value is
+        expressed in iso 8601 format: yyyy-mm-ddt00:00:00z (e.g., '2021-01-01t14:00:00z'
+        or '2021-01-01t14:00:00+02:00'). only one parameter of valuation_date and
+        valuation_date_time must be specified.
+        :return: str
+        """
+        return self._get_parameter("valuationDateTime")
+
+    @valuation_date_time.setter
+    def valuation_date_time(self, value):
+        self._set_parameter("valuationDateTime", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_request_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_swaption_surface_request_item.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,83 +1,56 @@
-# coding: utf8
-from typing import TYPE_CHECKING
+from ._enums import UnderlyingType
+from ._surface_request_item import SurfaceRequestItem
+from ._swaption_calculation_params import SwaptionCalculationParams
+from ._swaption_surface_definition import SwaptionSurfaceDefinition
 
-from .._object_definition import ObjectDefinition
-from ._zc_curve_parameters import ZcCurveParameters
-from ._models import Constituents
-from ._zc_curve_definitions import ZcCurveDefinitions
-from ...._types import OptStr
 
-if TYPE_CHECKING:
-    from ._zc_curve_types import CurveDefinition, CurveParameters, OptConstituents
-
-
-class ZcCurveRequestItem(ObjectDefinition):
-    """
-    Parameters
-    ----------
-    constituents : Constituents, optional
-
-    curve_definition : ZcCurveDefinitions, optional
-
-    curve_parameters : ZcCurveParameters, optional
-
-    curve_tag : str, optional
-
-    """
+class SwaptionSurfaceRequestItem(SurfaceRequestItem):
+    # new name VolatilityCubeSurfaceRequestItem in version 1.0.130
+    _instrument_type = None
 
     def __init__(
         self,
-        constituents: "OptConstituents" = None,
-        curve_definition: "CurveDefinition" = None,
-        curve_parameters: "CurveParameters" = None,
-        curve_tag: OptStr = None,
-    ) -> None:
-        super().__init__()
-        self.constituents = constituents
-        self.curve_definition = curve_definition
-        self.curve_parameters = curve_parameters
-        self.curve_tag = curve_tag
+        instrument_type=None,
+        surface_layout=None,
+        surface_parameters=None,
+        underlying_definition=None,
+        surface_tag=None,
+    ):
+        super().__init__(
+            surface_layout=surface_layout,
+            surface_tag=surface_tag,
+            underlying_type=UnderlyingType.SWAPTION,
+        )
+        self.instrument_type = instrument_type
+        self.surface_parameters = surface_parameters
+        self.underlying_definition = underlying_definition
 
     @property
-    def constituents(self):
+    def surface_parameters(self):
         """
-        :return: object Constituents
+        :return: object SwaptionCalculationParams
         """
-        return self._get_object_parameter(Constituents, "constituents")
+        return self._get_object_parameter(SwaptionCalculationParams, "surfaceParameters")
 
-    @constituents.setter
-    def constituents(self, value):
-        self._set_object_parameter(Constituents, "constituents", value)
+    @surface_parameters.setter
+    def surface_parameters(self, value):
+        self._set_object_parameter(SwaptionCalculationParams, "surfaceParameters", value)
 
     @property
-    def curve_definition(self):
+    def underlying_definition(self):
         """
-        :return: object ZcCurveDefinitions
+        :return: object SwaptionSurfaceDefinition
         """
-        return self._get_object_parameter(ZcCurveDefinitions, "curveDefinition")
+        return self._get_object_parameter(SwaptionSurfaceDefinition, "underlyingDefinition")
 
-    @curve_definition.setter
-    def curve_definition(self, value):
-        self._set_object_parameter(ZcCurveDefinitions, "curveDefinition", value)
+    @underlying_definition.setter
+    def underlying_definition(self, value):
+        self._set_object_parameter(SwaptionSurfaceDefinition, "underlyingDefinition", value)
 
     @property
-    def curve_parameters(self):
-        """
-        :return: object ZcCurveParameters
-        """
-        return self._get_object_parameter(ZcCurveParameters, "curveParameters")
-
-    @curve_parameters.setter
-    def curve_parameters(self, value):
-        self._set_object_parameter(ZcCurveParameters, "curveParameters", value)
-
-    @property
-    def curve_tag(self):
-        """
-        :return: str
-        """
-        return self._get_parameter("curveTag")
+    def instrument_type(self):
+        return self._instrument_type
 
-    @curve_tag.setter
-    def curve_tag(self, value):
-        self._set_parameter("curveTag", value)
+    @instrument_type.setter
+    def instrument_type(self, value):
+        self._instrument_type = value
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_curves/_zc_curve_types.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_zc_curve_types.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,20 +1,22 @@
 from typing import Iterable, Union, Optional
 
+from ._shift_scenario import ShiftScenario
 from ._enums import ZcCurvesOutputs
 from ._zc_curve_definitions import ZcCurveDefinitions
 from ._zc_curve_parameters import ZcCurveParameters
 from ._models import (
     Step,
     Turn,
     Constituents,
 )
 from ..curves import zc_curves
 from ...._types import Strings
 
 Steps = Union[Iterable[Step]]
 Turns = Union[Iterable[Turn]]
+ShiftScenarios = Optional[Iterable[ShiftScenario]]
 OptConstituents = Optional[Constituents]
 CurveDefinition = Optional[ZcCurveDefinitions]
 CurveParameters = Optional[ZcCurveParameters]
 DefnDefns = Union[zc_curves.Definition, Iterable[zc_curves.Definition]]
 Outputs = Union[Strings, Iterable[ZcCurvesOutputs]]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/__init__.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_common_tools.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_common_tools.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_date_moving_convention.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_date_moving_convention.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,18 @@
 # coding: utf8
 
 __all__ = ["DateMovingConvention"]
 
-from enum import Enum, unique
+from enum import unique
+
+from ...._base_enum import StrEnum
 
 
 @unique
-class DateMovingConvention(Enum):
+class DateMovingConvention(StrEnum):
     """
     The method to adjust dates.
 
     The possible values are:
         - BbswModifiedFollowing - Adjusts dates according to the BBSW Modified Following convention.
         - EveryThirdWednesday - dates are adjusted to the next every third Wednesday.
         - ModifiedFollowing - Adjusts dates according to the Modified Following
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_day_count_basis.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_interest_calculation_method.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,14 @@
-# coding: utf8
+from enum import unique
 
-from enum import Enum, unique
+from ......_base_enum import StrEnum
 
 
 @unique
-class DayCountBasis(Enum):
+class InterestCalculationMethod(StrEnum):
     DCB_30_E_360_ISMA = "Dcb_30E_360_ISMA"
     DCB_30_360 = "Dcb_30_360"
     DCB_30_360_GERMAN = "Dcb_30_360_German"
     DCB_30_360_ISDA = "Dcb_30_360_ISDA"
     DCB_30_360_US = "Dcb_30_360_US"
     DCB_30_365_BRAZIL = "Dcb_30_365_Brazil"
     DCB_30_365_GERMAN = "Dcb_30_365_German"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_day_of_week.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_day_of_week.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,16 +1,17 @@
 # coding: utf8
 
 __all__ = ["DayOfWeek"]
 
-from enum import Enum, unique
+from enum import unique
+from ...._base_enum import StrEnum
 
 
 @unique
-class DayOfWeek(Enum):
+class DayOfWeek(StrEnum):
     """
     The day of week to which dates are adjusted.
     The first date in the list is defined as corresponding day of week following the start date.
     The last date in the list is defined as corresponding day of week preceding the end date.
 
     The possible values are:
         - Sunday,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_frequency.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_frequency.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # coding: utf8
 
-from enum import Enum, unique
+from enum import unique
+
+from refinitiv.data._base_enum import StrEnum
 
 
 @unique
-class Frequency(Enum):
+class Frequency(StrEnum):
     EVERYDAY = "Everyday"
     BI_MONTHLY = "BiMonthly"
     MONTHLY = "Monthly"
     QUARTERLY = "Quarterly"
     SEMI_ANNUAL = "SemiAnnual"
     ANNUAL = "Annual"
     EVERY7_DAYS = "Every7Days"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_holiday_outupts.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_holiday_outupts.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 # coding: utf8
 
 __all__ = ["HolidayOutputs"]
 
-from enum import Enum, unique
+from enum import unique
 from typing import Union, List, Optional
 
+from ...._base_enum import StrEnum
+
 
 @unique
-class HolidayOutputs(Enum):
+class HolidayOutputs(StrEnum):
     """
     Method to request additional information about the holiday.
 
     The possible values are:
         - Date (to retrieve holiday date),
         - Names (to retrieve holiday names.
             Holiday names might be different.
@@ -22,10 +24,8 @@
 
     DATE = "Date"
     NAMES = "Names"
     CALENDARS = "Calendars"
     COUNTRIES = "Countries"
 
 
-OptHolidayOutputs = Optional[
-    Union[str, List[str], HolidayOutputs, List[HolidayOutputs]]
-]
+OptHolidayOutputs = Optional[Union[str, List[str], HolidayOutputs, List[HolidayOutputs]]]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_enums/_yield_type.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_enums/_yield_type.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,14 +1,16 @@
 # coding: utf8
 
-from enum import Enum, unique
+from enum import unique
+
+from ...._base_enum import StrEnum
 
 
 @unique
-class YieldType(Enum):
+class YieldType(StrEnum):
     ANNUAL_EQUIVALENT = "Annual_Equivalent"
     BOND_ACTUAL_364 = "Bond_Actual_364"
     BRAESS_FANGMEYER = "Braess_Fangmeyer"
     DISCOUNT_ACTUAL_360 = "Discount_Actual_360"
     DISCOUNT_ACTUAL_365 = "Discount_Actual_365"
     EUROLAND = "Euroland"
     ISMA = "Isma"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_ipa_content_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_ipa_content_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,20 +35,16 @@
         self._delay_between_two_get_operation: Optional[int] = None
         self._config: Optional["_RDPConfig"] = None
         self._session: Optional["Session"] = None
 
     def _read_config(self):
         base_url = get_base_url(self._content_type, self._config)
         api_config = get_api_config(self._content_type, self._config)
-        self._operation_url = urljoin(
-            base_url, api_config.get("endpoints.async-operation")
-        )
-        self._resource_url = urljoin(
-            base_url, api_config.get("endpoints.async-resource")
-        )
+        self._operation_url = urljoin(base_url, api_config.get("endpoints.async-operation"))
+        self._resource_url = urljoin(base_url, api_config.get("endpoints.async-resource"))
         self._delay_before_first_get_operation = api_config.get(
             "delay-before-first-get-async-operation",
             DELAY_BEFORE_FIRST_GET_ASYNC_OPERATION,
         )
         self._delay_between_two_get_operation = api_config.get(
             "delay-between-two-get-async-operation",
             DELAY_BETWEEN_TWO_GET_ASYNC_OPERATION,
@@ -60,68 +56,55 @@
         # At this step, normal response should have 202 status code,
         # "Accepted" error message and location header
         status_code = initial_response.http_status.get("http_status_code")
         if status_code != 202:
             check_failed = True
             first_error = initial_response.errors[0]
             err_msg = first_error.message
-            err_msg = (
-                f"Async IPA response "
-                f"status code {status_code}|{err_msg} != 202|Accepted"
-            )
+            err_msg = f"Async IPA response status code {status_code}|{err_msg} != 202|Accepted"
             self._session.error(err_msg)
             self._write_error(initial_response, first_error.code, err_msg)
 
         location = initial_response.http_headers.get("location")
         if not location:
             check_failed = True
-            err_msg = (
-                "IPA Asynchronous request operation failed, "
-                "response doesn't contain location."
-            )
+            err_msg = "IPA Asynchronous request operation failed, response doesn't contain location."
             self._session.error(err_msg)
             self._write_error(initial_response, 0, err_msg)
 
         return check_failed
 
     def _check_operation_response(self, operation_response: "Response") -> bool:
         check_failed = False
         resource_location = operation_response.data.raw.get("resourceLocation")
         if not resource_location:
             check_failed = True
-            err_msg = (
-                "IPA Asynchronous request resource failed, "
-                "operation response doesn't contain resource location."
-            )
+            err_msg = "IPA Asynchronous request resource failed, operation response doesn't contain resource location."
             self._session.error(err_msg)
             self._write_error(operation_response, 0, err_msg)
 
         return check_failed
 
     def _write_error(self, response: "Response", err_code: int, err_msg: str):
         error = Error(err_code, err_msg)
         response.errors.append(error)
         response.is_success = False
 
     def _raise_error_no_async_url(self):
-        raise AttributeError(
-            f"Asynchronous endpoint is not available for this content provider"
-        )
+        raise AttributeError(f"Asynchronous endpoint is not available for this content provider")
 
     def get_data(
         self,
         session: Optional["Session"] = None,
         on_response: Optional[Callable] = None,
         async_mode: Optional[bool] = None,
     ):
         self._session = get_valid_session(session)
         self._config: "_RDPConfig" = self._session.config
-        self._async_url = get_url(
-            self._content_type, self._config, request_mode="async"
-        )
+        self._async_url = get_url(self._content_type, self._config, request_mode="async")
         response = None
 
         if async_mode and self._async_url:
             self._lock = threading.Lock()
             self._read_config()
             response = self._get_data_with_async_mode()
             on_response and emit_event(on_response, response, self, session)
@@ -132,17 +115,15 @@
 
         else:
             response = super().get_data(session, on_response)
 
         return response
 
     def _get_data_with_async_mode(self) -> "Response":
-        initial_response = self._provider.get_data(
-            self._session, self._async_url, **self._kwargs
-        )
+        initial_response = self._provider.get_data(self._session, self._async_url, **self._kwargs)
 
         check_failed = self._check_initial_response(initial_response)
         if check_failed:
             return initial_response
 
         operation_response = None
         location = initial_response.http_headers.get("location")
@@ -180,52 +161,47 @@
             return operation_response
 
         with self._lock:
             resource_location = operation_response.data.raw.get("resourceLocation")
             resource_id = resource_location.rsplit("/", 1)[-1]
             url = urljoin(self._resource_url, resource_id)
             self._session.debug(f"Request resource :\n {url}")
-            resource_response = self._provider.get_data(
-                self._session, url, method=RequestMethod.GET, **self._kwargs
-            )
+            resource_response = self._provider.get_data(self._session, url, method=RequestMethod.GET, **self._kwargs)
 
         return resource_response
 
     async def get_data_async(
         self,
         session: Optional["Session"] = None,
         on_response: Optional[Callable] = None,
         async_mode: Optional[bool] = None,
+        closure: Optional[str] = None,
     ):
         self._session = get_valid_session(session)
         self._config: "_RDPConfig" = self._session.config
-        self._async_url = get_url(
-            self._content_type, self._config, request_mode="async"
-        )
+        self._async_url = get_url(self._content_type, self._config, request_mode="async")
         response = None
 
         if async_mode and self._async_url:
             self._lock_async = asyncio.Lock()
             self._read_config()
             response = await self._get_data_with_async_mode_async()
             on_response and emit_event(on_response, response, self, session)
             self._check_response(response, self._config)
 
         elif async_mode and not self._async_url:
             self._raise_error_no_async_url()
 
         else:
-            response = await super().get_data_async(session, on_response)
+            response = await super().get_data_async(session, on_response, closure)
 
         return response
 
     async def _get_data_with_async_mode_async(self):
-        initial_response = await self._provider.get_data_async(
-            self._session, self._async_url, **self._kwargs
-        )
+        initial_response = await self._provider.get_data_async(self._session, self._async_url, **self._kwargs)
 
         check_failed = self._check_initial_response(initial_response)
         if check_failed:
             return initial_response
 
         operation_response = None
         location = initial_response.http_headers.get("location")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_ipa_content_validator.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_ipa_content_validator.py`

 * *Files 1% similar despite different names*

```diff
@@ -24,17 +24,15 @@
     def any_element_have_no_error(cls, data: "ParsedData") -> bool:
         elements = data.content_data.get(cls._NAME_DATA)
         if isinstance(elements, list):
             counter = len(elements) or 1
             for element in elements:
                 if not hasattr(element, "get"):
                     counter -= 1
-                    data.error_messages = (
-                        f"Invalid data type={type(element)}, data={element}"
-                    )
+                    data.error_messages = f"Invalid data type={type(element)}, data={element}"
                     continue
 
                 error = element.get("error")
 
                 if error:
                     counter -= 1
                     error_code = error.get("code")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_american_monte_carlo_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_american_monte_carlo_parameters.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,23 +19,19 @@
         self.iteration_number = iteration_number
 
     @property
     def american_monte_carlo_method(self):
         """
         :return: enum AmericanMonteCarloMethod
         """
-        return self._get_enum_parameter(
-            AmericanMonteCarloMethod, "americanMonteCarloMethod"
-        )
+        return self._get_enum_parameter(AmericanMonteCarloMethod, "americanMonteCarloMethod")
 
     @american_monte_carlo_method.setter
     def american_monte_carlo_method(self, value):
-        self._set_enum_parameter(
-            AmericanMonteCarloMethod, "americanMonteCarloMethod", value
-        )
+        self._set_enum_parameter(AmericanMonteCarloMethod, "americanMonteCarloMethod", value)
 
     @property
     def additional_points(self):
         """
         :return: int
         """
         return self._get_parameter("additionalPoints")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_amortization_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_amortization_item.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_barrier_definition_element.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_barrier_definition_element.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_basket_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_basket_item.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_bid_ask_mid.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_bid_ask_mid.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_bond_rounding_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_bond_rounding_parameters.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_day_weight.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_day_weight.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_fx_point.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_fx_point.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_input_flow.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_input_flow.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_interpolation_weight.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_interpolation_weight.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_numerical_method.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_numerical_method.py`

 * *Files 17% similar despite different names*

```diff
@@ -4,36 +4,30 @@
 
 from .._object_definition import ObjectDefinition
 from .._enums import Method
 from .._models import AmericanMonteCarloParameters, PdeParameters
 
 
 class NumericalMethod(ObjectDefinition):
-    def __init__(
-        self, american_monte_carlo_parameters=None, method=None, pde_parameters=None
-    ):
+    def __init__(self, american_monte_carlo_parameters=None, method=None, pde_parameters=None):
         super().__init__()
         self.american_monte_carlo_parameters = american_monte_carlo_parameters
         self.method = method
         self.pde_parameters = pde_parameters
 
     @property
     def american_monte_carlo_parameters(self):
         """
         :return: object AmericanMonteCarloParameters
         """
-        return self._get_object_parameter(
-            AmericanMonteCarloParameters, "americanMonteCarloParameters"
-        )
+        return self._get_object_parameter(AmericanMonteCarloParameters, "americanMonteCarloParameters")
 
     @american_monte_carlo_parameters.setter
     def american_monte_carlo_parameters(self, value):
-        self._set_object_parameter(
-            AmericanMonteCarloParameters, "americanMonteCarloParameters", value
-        )
+        self._set_object_parameter(AmericanMonteCarloParameters, "americanMonteCarloParameters", value)
 
     @property
     def method(self):
         """
         :return: enum Method
         """
         return self._get_enum_parameter(Method, "method")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_payout_scaling.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_payout_scaling.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_models/_pde_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_models/_pde_parameters.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_object_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_object_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -70,18 +70,15 @@
                 response.append(enum_type(item))
 
         return response
 
     def _get_list_parameter(self, item_type, name):
         value = self._dict.get(name, None)
         return (
-            [
-                item_type.from_json(item) if hasattr(item_type, "from_json") else item
-                for item in value
-            ]
+            [item_type.from_json(item) if hasattr(item_type, "from_json") else item for item in value]
             if value is not None
             else None
         )
 
     ####################################################
     # Set parameter values
     ####################################################
@@ -108,18 +105,15 @@
             if value_upper in upper_enum_values:
                 result = upper_enum_values[value_upper].value
 
         if result:
             self._dict[name] = result
         else:
             values = [v.value for v in enum_type]
-            raise TypeError(
-                f"Parameter '{name}' of invalid type provided:'{type(value).__name__}',"
-                f"expected: {values}"
-            )
+            raise TypeError(f"Parameter '{name}' of invalid type provided:'{type(value).__name__}', expected: {values}")
 
     def _set_object_parameter(self, object_type, name, value):
         if value is None:
             self._dict.pop(name, False)
 
         elif isinstance(value, object_type):
             self._dict[name] = value.get_dict()
@@ -133,17 +127,14 @@
 
     def _set_list_parameter(self, item_type, name, value):
         if value is None:
             self._dict.pop(name, False)
 
         elif isinstance(value, list):
             if is_all_same_type(item_type, value):
-                self._dict[name] = [
-                    item.get_dict() if hasattr(item, "get_dict") else item
-                    for item in value
-                ]
+                self._dict[name] = [item.get_dict() if hasattr(item, "get_dict") else item for item in value]
 
             else:
                 raise TypeError(f"Not all values are type of {item_type}")
 
         else:
             raise TypeError(f"{name} value must be a list of {item_type}")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_cap_surface_request_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_cap_surface_request_item.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,19 +1,19 @@
-# coding: utf8
-
-
 from ._surface_request_item import SurfaceRequestItem
 from ._enums import UnderlyingType
 from ._i_ir_vol_model_definition import IIrVolModelDefinition as CapSurfaceDefinition
 from ._i_ir_vol_model_pricing_parameters import (
     IIrVolModelPricingParameters as CapCalculationParams,
 )
 
 
 class CapSurfaceRequestItem(SurfaceRequestItem):
+    # new name CapletsStrippingSurfaceRequestItem in version 1.0.130
+    _instrument_type = None
+
     def __init__(
         self,
         instrument_type,
         surface_layout,
         surface_params,
         underlying_definition,
         surface_tag,
@@ -47,16 +47,12 @@
 
     @underlying_definition.setter
     def underlying_definition(self, value):
         self._set_object_parameter(CapSurfaceDefinition, "underlyingDefinition", value)
 
     @property
     def instrument_type(self):
-        """
-        The type of instrument being defined.
-        :return: str
-        """
-        return self._get_parameter("instrumentType")
+        return self._instrument_type
 
     @instrument_type.setter
     def instrument_type(self, value):
-        self._set_parameter("instrumentType", value)
+        self._instrument_type = value
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_eti_surface_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_eti_surface_definition.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,24 +1,45 @@
-# coding: utf8
+from typing import TYPE_CHECKING
 from .._object_definition import ObjectDefinition
 
+if TYPE_CHECKING:
+    from ...._types import OptStr, OptBool
+
 
 class EtiSurfaceDefinition(ObjectDefinition):
+    """
+    The definition of the volatility surface.
+
+    Parameters
+    ----------
+    instrument_code : str, optional
+        The code (ric for equities and indices and ricroot for futures.) that represents
+        the instrument. the format for equities and indices is xxx@ric (example:
+        vod.l@ric) the format for futures is xx@ricroot (example: cl@ricroot)
+    clean_instrument_code : str, optional
+    exchange : str, optional
+        Specifies the exchange to be used to retrieve the underlying data.
+    is_future_underlying : bool, optional
+    is_lme_future_underlying : bool, optional
+    """
+
     def __init__(
         self,
-        instrument_code=None,
-        clean_instrument_code=None,
-        exchange=None,
-        is_future_underlying=None,
+        instrument_code: "OptStr" = None,
+        clean_instrument_code: "OptStr" = None,
+        exchange: "OptStr" = None,
+        is_future_underlying: "OptBool" = None,
+        is_lme_future_underlying: "OptBool" = None,
     ):
         super().__init__()
         self.instrument_code = instrument_code
         self.clean_instrument_code = clean_instrument_code
         self.exchange = exchange
         self.is_future_underlying = is_future_underlying
+        self.is_lme_future_underlying = is_lme_future_underlying
 
     @property
     def clean_instrument_code(self):
         """
         :return: str
         """
         return self._get_parameter("cleanInstrumentCode")
@@ -59,7 +80,18 @@
         :return: bool
         """
         return self._get_parameter("isFutureUnderlying")
 
     @is_future_underlying.setter
     def is_future_underlying(self, value):
         self._set_parameter("isFutureUnderlying", value)
+
+    @property
+    def is_lme_future_underlying(self):
+        """
+        :return: bool
+        """
+        return self._get_parameter("isLmeFutureUnderlying")
+
+    @is_lme_future_underlying.setter
+    def is_lme_future_underlying(self, value):
+        self._set_parameter("isLmeFutureUnderlying", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_eti_surface_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_eti_surface_parameters.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,43 +1,97 @@
-# coding: utf8
+from typing import Optional, TYPE_CHECKING, Iterable
+
 from .._object_definition import ObjectDefinition
 
 from ._enums import EtiInputVolatilityType
 from ._enums import VolatilityModel
 from ._enums import PriceSide
 from ._enums import TimeStamp
 from ._enums import MoneynessType
 from ._enums import Axis
 from ._models import MoneynessWeight
 from ._models import SurfaceFilters
+from ...._tools import try_copy_to_list
+
+if TYPE_CHECKING:
+    from ...._types import OptStr, OptBool
 
 
 class EtiSurfaceParameters(ObjectDefinition):
+    """
+    This class property contains the properties that may be used to control the
+    calculation. It mainly covers dates, market data assumptions (e.g. interpolation),
+    and pricing model preferences. Some Parameters are common to all volatility surfaces
+    contracts, while others are specific to a particular type of volatility.
+
+    Parameters
+    ----------
+    filters : SurfaceFilters, optional
+        The parameters of options that should be used to construct the
+        volatility surface.
+    input_volatility_type : InputVolatilityType, optional
+        Specifies the type of volatility used as an input of the model (calculated
+        implied volatility, settlement)
+        - settle: [deprecated] the service uses the settlement volatility to build the
+          volatility surface
+        - quoted: the service uses the quoted volatility to build the volatility surface
+        - implied: the service internally calculates implied volatilities for the option
+          universe before building the surface default value is "implied".
+    moneyness_type : MoneynessType, optional
+        The enumerate that specifies the moneyness type to use for calibration.
+        - spot
+        - fwd
+        - sigma optional. default value is "spot".
+    price_side : PriceSide, optional
+        Specifies whether bid, ask or mid is used to build the surface.
+    time_stamp : TimeStamp, optional
+        Define how the timestamp is selected:
+        - open: the opening value of the valuationdate or if not available the close of
+          the previous day is used.
+        - default: the latest snapshot is used when valuationdate is today, the close
+          price when valuationdate is in the past.
+    volatility_model : VolatilityModel, optional
+        The quantitative model used to generate the volatility surface. this may depend
+        on the asset class.
+    weights : MoneynessWeight, optional
+        The list of calibration weights that should be applied to different
+        MoneynessWeight.
+    x_axis : Axis, optional
+        Specifies the unit for the x axis (e.g. date, tenor)
+    y_axis : Axis, optional
+        Specifies the unit for the y axis (e.g. strike, delta). this may depend on the
+        asset class. for fx volatility surface, we support both delta and strike.
+    calculation_date : str, optional
+        The date the volatility surface is generated.
+    svi_alpha_extrapolation : bool, optional
+        Svi alpha extrapolation for building the surface default value : true
+    """
+
     def __init__(
         self,
-        filters=None,
-        input_volatility_type=None,
-        moneyness_type=None,
-        price_side=None,
-        time_stamp=None,
-        volatility_model=None,
-        weights=None,
-        x_axis=None,
-        y_axis=None,
-        calculation_date=None,
-        svi_alpha_extrapolation=None,
+        filters: Optional[SurfaceFilters] = None,
+        input_volatility_type: Optional[EtiInputVolatilityType] = None,
+        moneyness_type: Optional[MoneynessType] = None,
+        price_side: Optional[PriceSide] = None,
+        time_stamp: Optional[TimeStamp] = None,
+        volatility_model: Optional[VolatilityModel] = None,
+        weights: Optional[Iterable[MoneynessWeight]] = None,
+        x_axis: Optional[Axis] = None,
+        y_axis: Optional[Axis] = None,
+        calculation_date: "OptStr" = None,
+        svi_alpha_extrapolation: "OptBool" = None,
     ):
         super().__init__()
         self.filters = filters
         self.input_volatility_type = input_volatility_type
         self.moneyness_type = moneyness_type
         self.price_side = price_side
         self.time_stamp = time_stamp
         self.volatility_model = volatility_model
-        self.weights = weights
+        self.weights = try_copy_to_list(weights)
         self.x_axis = x_axis
         self.y_axis = y_axis
         self.calculation_date = calculation_date
         self.svi_alpha_extrapolation = svi_alpha_extrapolation
 
     @property
     def filters(self):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_eti_surface_request_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_eti_surface_request_item.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,16 +1,15 @@
-# coding: utf8
-
 from ._surface_request_item import SurfaceRequestItem
 from ._enums import UnderlyingType
 from ._eti_surface_definition import EtiSurfaceDefinition
 from ._eti_surface_parameters import EtiSurfaceParameters as EtiCalculationParams
 
 
 class EtiSurfaceRequestItem(SurfaceRequestItem):
+    # new name EtiVolatilitySurfaceRequestItem into version 1.0.130
     def __init__(
         self,
         surface_layout,
         surface_parameters,
         underlying_definition,
         surface_tag,
     ):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_fx_surface_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_fx_surface_definition.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,12 +1,21 @@
-# coding: utf8
 from .._object_definition import ObjectDefinition
 
 
 class FxVolatilitySurfaceDefinition(ObjectDefinition):
+    """
+    The definition of the volatility surface.
+
+    Parameters
+    ----------
+    fx_cross_code : str
+        The currency pair of FX Cross, expressed in ISO 4217 alphabetical format
+        (e.g., 'EURCHF').
+    """
+
     def __init__(self, fx_cross_code=None):
         super().__init__()
         self.fx_cross_code = fx_cross_code
 
     @property
     def fx_cross_code(self):
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_fx_surface_request_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_fx_surface_request_item.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,31 +1,39 @@
-# coding: utf8
+from typing import Optional, TYPE_CHECKING
 
 from ._surface_request_item import SurfaceRequestItem
 
 from ._enums import UnderlyingType
+from ._fx_statistics_parameters import FxStatisticsParameters
 from ._fx_surface_parameters import FxSurfaceParameters as FxCalculationParams
 from ._fx_surface_definition import FxVolatilitySurfaceDefinition as FxSurfaceDefinition
 
 
+if TYPE_CHECKING:
+    from ...._types import OptStr
+
+
 class FxSurfaceRequestItem(SurfaceRequestItem):
+    # new name FxVolatilitySurfaceRequestItem into version 1.0.130
     def __init__(
         self,
         surface_layout=None,
-        surface_parameters=None,
-        underlying_definition=None,
-        surface_tag=None,
+        surface_parameters: Optional[FxCalculationParams] = None,
+        underlying_definition: Optional[FxSurfaceDefinition] = None,
+        surface_tag: "OptStr" = None,
+        surface_statistics_parameters: Optional[FxStatisticsParameters] = None,
     ):
         super().__init__(
             surface_layout=surface_layout,
             surface_tag=surface_tag,
             underlying_type=UnderlyingType.FX,
         )
         self.surface_parameters = surface_parameters
         self.underlying_definition = underlying_definition
+        self.surface_statistics_parameters = surface_statistics_parameters
 
     @property
     def surface_parameters(self):
         """
         The section that contains the properties that define how the volatility surface is generated
         :return: object FxCalculationParams
         """
@@ -42,7 +50,18 @@
         :return: object FxSurfaceDefinition
         """
         return self._get_object_parameter(FxSurfaceDefinition, "underlyingDefinition")
 
     @underlying_definition.setter
     def underlying_definition(self, value):
         self._set_object_parameter(FxSurfaceDefinition, "underlyingDefinition", value)
+
+    @property
+    def surface_statistics_parameters(self):
+        """
+        :return: object FxStatisticsParameters
+        """
+        return self._get_object_parameter(FxStatisticsParameters, "surfaceStatisticsParameters")
+
+    @surface_statistics_parameters.setter
+    def surface_statistics_parameters(self, value):
+        self._set_object_parameter(FxStatisticsParameters, "surfaceStatisticsParameters", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_i_ir_vol_model_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_fixing_info.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,154 +1,160 @@
 # coding: utf8
-from ._enums import VolatilityAdjustmentType
-from ._enums import Axis
-from ._enums import InputVolatilityType
-from .._object_definition import ObjectDefinition
 
+from typing import Optional, Union
+
+from .._base import Info
+from .._enums import (
+    AverageType,
+    FixingFrequency,
+)
+
+
+class EtiFixingInfo(Info):
+    """
+    Parameters
+    ----------
+    average_type : AverageType or str, optional
+        The type of average used to compute.
+    fixing_frequency : FixingFrequency or str, optional
+        The fixing's frequency.
+    average_so_far : float, optional
+        The value of the average_type
+    fixing_calendar : str, optional
+        The calendar of the underlying's currency.
+    fixing_end_date : str, optional
+        The end date of the fixing period. Should be less or equal to the expiry.
+    fixing_start_date : str, optional
+        The beginning date of the fixing period.
+    include_holidays : bool, optional
+        Include the holidays in the list of fixings
+    include_week_ends : bool, optional
+        Include the week-ends in the list of fixings
+    """
 
-class IIrVolModelPricingParameters(ObjectDefinition):
     def __init__(
         self,
-        input_volatility_type=None,
-        volatility_adjustment_type=None,
-        x_axis=None,
-        y_axis=None,
-        z_axis=None,
-        market_data_date=None,
-        shift_percent=None,
-        source=None,
-        stripping_shift_percent=None,
-        valuation_date=None,
-    ):
+        average_type: Union[AverageType, str] = None,
+        fixing_frequency: Union[FixingFrequency, str] = None,
+        average_so_far: Optional[float] = None,
+        fixing_calendar: Optional[str] = None,
+        fixing_end_date: Optional[str] = None,
+        fixing_start_date: Optional[str] = None,
+        include_holidays: Optional[bool] = None,
+        include_week_ends: Optional[bool] = None,
+    ) -> None:
         super().__init__()
-        self.input_volatility_type = input_volatility_type
-        self.volatility_adjustment_type = volatility_adjustment_type
-        self.x_axis = x_axis
-        self.y_axis = y_axis
-        self.z_axis = z_axis
-        self.market_data_date = market_data_date
-        self.shift_percent = shift_percent
-        self.source = source
-        self.stripping_shift_percent = stripping_shift_percent
-        self.valuation_date = valuation_date
-
-    @property
-    def input_volatility_type(self):
-        """
-        :return: enum InputVolatilityType
-        """
-        return self._get_enum_parameter(InputVolatilityType, "inputVolatilityType")
-
-    @input_volatility_type.setter
-    def input_volatility_type(self, value):
-        self._set_enum_parameter(InputVolatilityType, "inputVolatilityType", value)
-
-    @property
-    def volatility_adjustment_type(self):
-        """
-        Volatility Adjustment method for stripping: ConstantCaplet, ConstantCap, ShiftedCap, NormalizedCap, NormalizedCaplet
-        :return: enum VolatilityAdjustmentType
-        """
-        return self._get_enum_parameter(
-            VolatilityAdjustmentType, "volatilityAdjustmentType"
-        )
-
-    @volatility_adjustment_type.setter
-    def volatility_adjustment_type(self, value):
-        self._set_enum_parameter(
-            VolatilityAdjustmentType, "volatilityAdjustmentType", value
-        )
+        self.average_type = average_type
+        self.fixing_frequency = fixing_frequency
+        self.average_so_far = average_so_far
+        self.fixing_calendar = fixing_calendar
+        self.fixing_end_date = fixing_end_date
+        self.fixing_start_date = fixing_start_date
+        self.include_holidays = include_holidays
+        self.include_week_ends = include_week_ends
 
     @property
-    def x_axis(self):
+    def average_type(self):
         """
-        Specifies the unit for the x axis (e.g. Date, Tenor)
-        :return: enum Axis
+        The type of average used to compute. Possible values:
+        - ArithmeticRate
+        - ArithmeticStrike
+        - GeometricRate
+        - GeometricStrike
+        :return: enum AverageType
         """
-        return self._get_enum_parameter(Axis, "xAxis")
+        return self._get_enum_parameter(AverageType, "averageType")
 
-    @x_axis.setter
-    def x_axis(self, value):
-        self._set_enum_parameter(Axis, "xAxis", value)
+    @average_type.setter
+    def average_type(self, value):
+        self._set_enum_parameter(AverageType, "averageType", value)
 
     @property
-    def y_axis(self):
+    def fixing_frequency(self):
         """
-        Specifies the unit for the y axis (e.g. Strike, Delta). This may depend on the asset class.
-        For Fx Volatility Surface, we support both Delta and Strike.
-        :return: enum Axis
+        The fixing's frequency. Possible values:
+        - Daily
+        - Weekly
+        - BiWeekly
+        - Monthly
+        - Quaterly
+        - SemiAnnual
+        - Annual
+        :return: enum FixingFrequency
         """
-        return self._get_enum_parameter(Axis, "yAxis")
+        return self._get_enum_parameter(FixingFrequency, "fixingFrequency")
 
-    @y_axis.setter
-    def y_axis(self, value):
-        self._set_enum_parameter(Axis, "yAxis", value)
+    @fixing_frequency.setter
+    def fixing_frequency(self, value):
+        self._set_enum_parameter(FixingFrequency, "fixingFrequency", value)
 
     @property
-    def z_axis(self):
+    def average_so_far(self):
         """
-        Specifies the unit for the z axis (e.g. Strike, Tenor, Expiries). This applies on Ir SABR Volatility Cube.
-        :return: enum Axis
+        The value of the average_type
+        :return: float
         """
-        return self._get_enum_parameter(Axis, "zAxis")
+        return self._get_parameter("averageSoFar")
 
-    @z_axis.setter
-    def z_axis(self, value):
-        self._set_enum_parameter(Axis, "zAxis", value)
+    @average_so_far.setter
+    def average_so_far(self, value):
+        self._set_parameter("averageSoFar", value)
 
     @property
-    def market_data_date(self):
+    def fixing_calendar(self):
         """
+        The calendar of the underlying's currency.
         :return: str
         """
-        return self._get_parameter("marketDataDate")
+        return self._get_parameter("fixingCalendar")
 
-    @market_data_date.setter
-    def market_data_date(self, value):
-        self._set_parameter("marketDataDate", value)
+    @fixing_calendar.setter
+    def fixing_calendar(self, value):
+        self._set_parameter("fixingCalendar", value)
 
     @property
-    def shift_percent(self):
+    def fixing_end_date(self):
         """
-        Shift value to use in calibration(Strike/Forward). Default: 0.0
-        :return: float
+        The end date of the fixing period. Should be less or equal to the expiry.
+        :return: str
         """
-        return self._get_parameter("shiftPercent")
+        return self._get_parameter("fixingEndDate")
 
-    @shift_percent.setter
-    def shift_percent(self, value):
-        self._set_parameter("shiftPercent", value)
+    @fixing_end_date.setter
+    def fixing_end_date(self, value):
+        self._set_parameter("fixingEndDate", value)
 
     @property
-    def source(self):
+    def fixing_start_date(self):
         """
-        Requested volatility data contributor.
+        The beginning date of the fixing period.
         :return: str
         """
-        return self._get_parameter("source")
+        return self._get_parameter("fixingStartDate")
 
-    @source.setter
-    def source(self, value):
-        self._set_parameter("source", value)
+    @fixing_start_date.setter
+    def fixing_start_date(self, value):
+        self._set_parameter("fixingStartDate", value)
 
     @property
-    def stripping_shift_percent(self):
+    def include_holidays(self):
         """
-        Shift value to use in caplets stripping(Strike/Forward). Default: 0.0
-        :return: float
+        Include the holidays in the list of fixings
+        :return: bool
         """
-        return self._get_parameter("strippingShiftPercent")
+        return self._get_parameter("includeHolidays")
 
-    @stripping_shift_percent.setter
-    def stripping_shift_percent(self, value):
-        self._set_parameter("strippingShiftPercent", value)
+    @include_holidays.setter
+    def include_holidays(self, value):
+        self._set_parameter("includeHolidays", value)
 
     @property
-    def valuation_date(self):
+    def include_week_ends(self):
         """
-        :return: str
+        Include the week-ends in the list of fixings
+        :return: bool
         """
-        return self._get_parameter("valuationDate")
+        return self._get_parameter("includeWeekEnds")
 
-    @valuation_date.setter
-    def valuation_date(self, value):
-        self._set_parameter("valuationDate", value)
+    @include_week_ends.setter
+    def include_week_ends(self, value):
+        self._set_parameter("includeWeekEnds", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_strike_filter.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_strike_filter.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_surface.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_surface.py`

 * *Files 0% similar despite different names*

```diff
@@ -124,15 +124,14 @@
 
         is_axis_x = axis is Axis.X
         is_axis_y = axis is Axis.Y
         is_axis_z = axis is Axis.Z
 
         # interpolate
         if value not in values and not is_axis_z:
-
             value = value_arg_parser.get_float(value)
 
             if is_axis_x:
                 axis_x = self._column
                 axis_y = self._interp(
                     x=self._column.astype(float),
                     y=value,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_surface_filters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_surface_filters.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,43 +1,81 @@
-# coding: utf8
-
 __all__ = ["SurfaceFilters"]
 
+from typing import Optional, TYPE_CHECKING
+
 from ..._object_definition import ObjectDefinition
 
 from ._maturity_filter import MaturityFilter
 from ._strike_filter_range import StrikeFilterRange
-from ._strike_filter import StrikeFilter
+
+
+if TYPE_CHECKING:
+    from ....._types import OptBool, OptInt, OptFloat
 
 
 class SurfaceFilters(ObjectDefinition):
+    """
+    Filter object for surface.
+
+    Parameters
+    ----------
+    maturity_filter_range : MaturityFilter, optional
+        The object allows to specify the range of expiry periods of options that are
+        used to construct the surface.
+    strike_range : StrikeFilterRange, optional
+        The range allows to exclude strike levels that have implied volatilities which
+        exceed upper bound or below lower bound.
+    strike_range_percent : DEPRECATED
+        This attribute doesn't use anymore.
+    atm_tolerance_interval_percent : float, optional
+        Filter on the atm tolerance interval percent
+    ensure_prices_monotonicity : bool, optional
+        Filter on the monotonicity of price options.
+    max_of_median_bid_ask_spread : float, optional
+        Spread mutltiplier to filter the options with the same expiry
+    max_staleness_days : int, optional
+        Max staleness past days to use for building the surface
+    use_only_calls : bool, optional
+        Select only teh calls to build the surface
+    use_only_puts : bool, optional
+        Select only the puts to build the surface
+    use_weekly_options : bool, optional
+        Filter on the weekly options.
+    include_min_tick_prices : bool, optional
+        Take into account the minimum tick prices to build the surface
+    """
+
+    _strike_range_percent = None
+
     def __init__(
         self,
-        maturity_filter_range=None,
-        strike_range=None,
+        maturity_filter_range: Optional[MaturityFilter] = None,
+        strike_range: Optional[StrikeFilterRange] = None,
         strike_range_percent=None,
-        atm_tolerance_interval_percent=None,
-        ensure_prices_monotonicity=None,
-        max_of_median_bid_ask_spread=None,
-        max_staleness_days=None,
-        use_only_calls=None,
-        use_only_puts=None,
-        use_weekly_options=None,
+        atm_tolerance_interval_percent: "OptFloat" = None,
+        ensure_prices_monotonicity: "OptBool" = None,
+        max_of_median_bid_ask_spread: "OptFloat" = None,
+        max_staleness_days: "OptInt" = None,
+        use_only_calls: "OptBool" = None,
+        use_only_puts: "OptBool" = None,
+        use_weekly_options: "OptBool" = None,
+        include_min_tick_prices: "OptBool" = None,
     ):
         super().__init__()
         self.maturity_filter_range = maturity_filter_range
         self.strike_range = strike_range
         self.strike_range_percent = strike_range_percent
         self.atm_tolerance_interval_percent = atm_tolerance_interval_percent
         self.ensure_prices_monotonicity = ensure_prices_monotonicity
         self.max_of_median_bid_ask_spread = max_of_median_bid_ask_spread
         self.max_staleness_days = max_staleness_days
         self.use_only_calls = use_only_calls
         self.use_only_puts = use_only_puts
         self.use_weekly_options = use_weekly_options
+        self.include_min_tick_prices = include_min_tick_prices
 
     @property
     def maturity_filter_range(self):
         """
         Define the MaturityFilterRange
         :return: object MaturityFilter
         """
@@ -59,22 +97,20 @@
     def strike_range(self, value):
         self._set_object_parameter(StrikeFilterRange, "strikeRange", value)
 
     @property
     def strike_range_percent(self):
         """
         [DEPRECATED]
-        Define the StrikeFilterRange
-        :return: object StrikeFilter
         """
-        return self._get_object_parameter(StrikeFilter, "strikeRangePercent")
+        return self._strike_range_percent
 
     @strike_range_percent.setter
     def strike_range_percent(self, value):
-        self._set_object_parameter(StrikeFilter, "strikeRangePercent", value)
+        self._strike_range_percent = value
 
     @property
     def atm_tolerance_interval_percent(self):
         """
         Filter on the ATM tolerance interval percent
         :return: float
         """
@@ -151,7 +187,19 @@
         :return: bool
         """
         return self._get_parameter("useWeeklyOptions")
 
     @use_weekly_options.setter
     def use_weekly_options(self, value):
         self._set_parameter("useWeeklyOptions", value)
+
+    @property
+    def include_min_tick_prices(self):
+        """
+        Take into account the minimum tick prices to build the surface
+        :return: bool
+        """
+        return self._get_parameter("includeMinTickPrices")
+
+    @include_min_tick_prices.setter
+    def include_min_tick_prices(self, value):
+        self._set_parameter("includeMinTickPrices", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_models/_surface_output.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_models/_surface_output.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-# coding: utf8
-
-__all__ = ["SurfaceOutput"]
+__all__ = ["SurfaceLayout"]
 
 from ..._object_definition import ObjectDefinition
 from ._volatility_surface_point import VolatilitySurfacePoint
 from .._enums import Format
 from .._surface_types import OptFormat, OptVolatilitySurfacePoints
 from ....._types import OptStrings, OptInt
 from ....._tools import create_repr, try_copy_to_list
 
 
-class SurfaceOutput(ObjectDefinition):
+class SurfaceLayout(ObjectDefinition):
     """
+    This class property contains the properties that may be used to control how the
+    surface is displayed.
 
     Parameters
     ---------
     data_points : list of VolatilitySurfacePoint, optional
         Specifies the list of specific data points to be returned
     format : Format, option
         Specifies whether the calculated volatilities are returned as a list or a matrix
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/_surfaces/_surface_request_item.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/_surfaces/_surface_request_item.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 # coding: utf8
 
 from ._enums import UnderlyingType
-from ._models import SurfaceOutput as SurfaceLayout
+from ._models import SurfaceLayout
 from .._object_definition import ObjectDefinition
 
 
 class SurfaceRequestItem(ObjectDefinition):
     def __init__(
         self,
         surface_layout,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/curves/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_bond_curves/curves/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,9 +1,10 @@
 __all__ = (
     "Definition",
+    "CreditConstituents",
     "CreditCurveDefinition",
     "CreditCurveParameters",
     "BasisSplineSmoothModel",
     "BusinessSector",
     "ExtrapolationMode",
     "CalendarAdjustment",
     "CalibrationModel",
@@ -23,23 +24,23 @@
     "Seniority",
 )
 
 from ._definition import Definition
 from ...._curves._bond_curves import (
     CreditCurveDefinition,
     CreditCurveParameters,
+    CreditConstituents,
 )
 
-from ._enums import (
+from ...._curves._enums import CompoundingType, ExtrapolationMode
+from ...._curves._bond_curves._enums import (
     BasisSplineSmoothModel,
     BusinessSector,
-    ExtrapolationMode,
     CalendarAdjustment,
     CalibrationModel,
-    CompoundingType,
     CurveSubType,
     EconomicSector,
     Industry,
     IndustryGroup,
     InterestCalculationMethod,
     InterpolationMode,
     IssuerType,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_bond_curves/curves/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_bond_curves/curves/_definition.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,23 +6,26 @@
 from ....._content_provider_layer import ContentProviderLayer
 
 if TYPE_CHECKING:
     from ......_types import OptStr, ExtendedParams
     from ...._curves._bond_curves._types import (
         CurveDefinition,
         CurveParameters,
+        OptCreditConstituents,
     )
 
 
 class Definition(ContentProviderLayer):
     """
     Generates the Bond curves for the definitions provided.
 
     Parameters
     ----------
+    constituents : CreditConstituents, optional
+        CreditConstituents object.
     curve_definition : CreditCurveDefinition, optional
         CreditCurveDefinition object.
     curve_parameters : CreditCurveParameters, optional
         CreditCurveParameters object.
     curve_tag : str, optional
         A user-defined string to identify the interest rate curve. it can be used to
         link output results to the curve definition. limited to 40 characters. only
@@ -45,27 +48,30 @@
     ...     curve_definition=curves.CreditCurveDefinition(
     ...         reference_entity="0#EUGOVPBMK=R",
     ...         reference_entity_type=curves.ReferenceEntityType.CHAIN_RIC
     ...     ))
     >>> response = definition.get_data()
 
     Using get_data_async
+
     >>> import asyncio
     >>> task = definition.get_data_async()
     >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
+        constituents: "OptCreditConstituents" = None,
         curve_definition: "CurveDefinition" = None,
         curve_parameters: "CurveParameters" = None,
         curve_tag: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ):
         request_item = RequestItem(
+            constituents=constituents,
             curve_definition=curve_definition,
             curve_parameters=curve_parameters,
             curve_tag=curve_tag,
         )
         super().__init__(
             content_type=ContentType.BOND_CURVE,
             universe=request_item,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 __all__ = (
     "Definition",
     "ButterflyShift",
     "CombinedShift",
     "CrossCurrencyCurveDefinitionPricing",
     "FlatteningShift",
+    "FxForwardConstituents",
     "FxForwardCurveDefinition",
     "FxForwardCurveParameters",
     "FxShiftScenario",
     "LongEndShift",
+    "ParRateShift",
     "ParallelShift",
     "ShiftDefinition",
     "ShortEndShift",
     "TimeBucketShift",
     "TwistShift",
     "ValuationTime",
     "ArrayMainConstituentAssetClass",
@@ -27,29 +29,37 @@
 
 from ._definition import Definition
 from ...._curves._cross_currency_curves._curves import (
     ButterflyShift,
     CombinedShift,
     CrossCurrencyCurveDefinitionPricing,
     FlatteningShift,
+    FxForwardConstituents,
     FxForwardCurveDefinition,
     FxForwardCurveParameters,
     FxShiftScenario,
     LongEndShift,
     ParallelShift,
     ShiftDefinition,
     ShortEndShift,
     TimeBucketShift,
     TwistShift,
+)
+
+from ...._curves._models import (
+    ParRateShift,
     ValuationTime,
 )
 
-from ._enums import (
-    ArrayMainConstituentAssetClass,
-    ArrayRiskType,
+from ...._curves._cross_currency_curves._enums import (
     InterpolationMode,
-    ConstituentOverrideMode,
     MainConstituentAssetClass,
     RiskType,
+)
+
+from ...._curves._cross_currency_curves._curves._enums import (
+    ArrayMainConstituentAssetClass,
+    ArrayRiskType,
+    ConstituentOverrideMode,
     ShiftType,
     ShiftUnit,
 )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/_definition.py`

 * *Files 5% similar despite different names*

```diff
@@ -8,23 +8,26 @@
 
 if TYPE_CHECKING:
     from ......_types import OptStr, ExtendedParams
     from ...._curves._cross_currency_curves._curves._types import (
         CurveDefinition,
         CurveParameters,
         ShiftScenarios,
+        FxConstituents,
     )
 
 
 class Definition(ContentProviderLayer):
     """
     Generates the Cross Currency curves for the definitions provided
 
     Parameters
     ----------
+    constituents : FxForwardConstituents, optional
+        FxForwardConstituents object.
     curve_definition : FxForwardCurveDefinition, optional
         FxForwardCurveDefinition object.
     curve_parameters : FxForwardCurveParameters, optional
         FxForwardCurveParameters object.
     shift_scenarios : FxShiftScenario, optional
         The list of attributes applied to the curve shift scenarios.
     curve_tag : str, optional
@@ -44,32 +47,34 @@
     ...        quoted_currency="USD",
     ...        quoted_index_name="SOFR"
     ...    ),
     ...    curve_parameters=crs_currency.curves.FxForwardCurveParameters(
     ...        valuation_date="2021-10-06"
     ...    )
     ... )
-     >>> response = definition.get_data()
+    >>> response = definition.get_data()
 
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
+        constituents: "FxConstituents" = None,
         curve_definition: "CurveDefinition" = None,
         curve_parameters: "CurveParameters" = None,
         shift_scenarios: "ShiftScenarios" = None,
         curve_tag: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ) -> None:
-
         request_item = RequestItem(
+            constituents=constituents,
             curve_definition=curve_definition,
             curve_parameters=curve_parameters,
             shift_scenarios=shift_scenarios,
             curve_tag=curve_tag,
         )
         super().__init__(
             content_type=ContentType.CROSS_CURRENCY_CURVES_CURVES,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/__init__.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,11 +1,11 @@
 __all__ = (
     "search",
     "manage",
-    "CrossCurrencyCurveDefinitionDescription",
+    "CrossCurrencyCurveCreateDefinition",
     "CrossCurrencyCurveUpdateDefinition",
     "BidAskFieldsDescription",
     "BidAskFieldsFormulaDescription",
     "CrossCurrencyConstituentsDescription",
     "CrossCurrencyCurveParameters",
     "CrossCurrencyInstrumentDefinition",
     "CrossCurrencyInstrumentDescription",
@@ -50,21 +50,23 @@
     FxSpotInstrumentDescription,
     OverrideBidAsk,
     OverrideBidAskFields,
     OverrideFxForwardTurn,
     TurnAdjustment,
 )
 from ...._curves._cross_currency_curves._definitions._create import (
-    CrossCurrencyCurveDefinitionDescription,
+    CrossCurrencyCurveCreateDefinition,
 )
 
 from ...._curves._cross_currency_curves._definitions._update import (
     CrossCurrencyCurveUpdateDefinition,
 )
 
-from ._enums import (
+from ...._curves._cross_currency_curves._enums import (
     InterpolationMode,
     MainConstituentAssetClass,
-    QuotationMode,
     RiskType,
+)
+from ...._curves._cross_currency_curves._definitions._enums import (
+    QuotationMode,
     StandardTurnPeriod,
 )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_data_classes.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_data_classes.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-from typing import TYPE_CHECKING, Any
+from dataclasses import dataclass
+from typing import TYPE_CHECKING
 
 from .._arg_enums import main_constituent_asset_class_arg_parser, risk_type_arg_parser
 from .._base_data_class import BaseData
 from ......delivery._data._endpoint_data import EndpointData
 
 if TYPE_CHECKING:
     from ......_types import OptStr, OptBool
@@ -72,15 +73,15 @@
         self.creation_user_id = creation_user_id
         self.update_date_time = update_date_time
         self.update_user_id = update_user_id
         self.version = version
         super().__init__(**kwargs)
 
 
-class CurveDefinition(BaseData):
+class CrossCurrencyCurveDefinition(BaseData):
     """
     Creates the Cross Currency curve definition with the definition provided.
 
     Parameters
     ----------
     main_constituent_asset_class : MainConstituentAssetClass, optional
         The asset class used to generate the zero coupon curve. the possible values are:
@@ -138,18 +139,16 @@
         is_non_deliverable: "OptBool" = None,
         name: "OptStr" = None,
         quoted_currency: "OptStr" = None,
         quoted_index_name: "OptStr" = None,
         source: "OptStr" = None,
         **kwargs,
     ):
-        self.main_constituent_asset_class = (
-            main_constituent_asset_class_arg_parser.get_enum(
-                main_constituent_asset_class
-            )
+        self.main_constituent_asset_class = main_constituent_asset_class_arg_parser.get_enum(
+            main_constituent_asset_class
         )
         self.risk_type = risk_type_arg_parser.get_enum(risk_type)
         self.base_currency = base_currency
         self.base_index_name = base_index_name
         self.definition_expiry_date = definition_expiry_date
         self.first_historical_availability_date = first_historical_availability_date
         self.id = id
@@ -158,38 +157,29 @@
         self.name = name
         self.quoted_currency = quoted_currency
         self.quoted_index_name = quoted_index_name
         self.source = source
         super().__init__(**kwargs)
 
 
+@dataclass
 class CurveDefinitionData(EndpointData):
-    def __init__(
-        self,
-        raw: Any,
-        **kwargs,
-    ):
-        EndpointData.__init__(self, raw, **kwargs)
-        self._curve_definition = None
-        self._curve_info = None
+    _curve_definition: CrossCurrencyCurveDefinition = None
+    _curve_info: CurveInfo = None
 
     @property
     def curve_definition(self):
         if self._curve_definition is None:
             curve_definition = self.raw.get("curveDefinition")
             if curve_definition:
-                curve_definition = convert_camel_to_snake(
-                    curve_definition_camel_to_snake, curve_definition
-                )
-            self._curve_definition = CurveDefinition(**curve_definition)
+                curve_definition = convert_camel_to_snake(curve_definition_camel_to_snake, curve_definition)
+            self._curve_definition = CrossCurrencyCurveDefinition(**curve_definition)
         return self._curve_definition
 
     @property
     def curve_info(self):
         if self._curve_info is None:
             curve_info = self.raw.get("curveInfo")
             if curve_info:
-                curve_info = convert_camel_to_snake(
-                    curve_info_camel_to_snake, curve_info
-                )
+                curve_info = convert_camel_to_snake(curve_info_camel_to_snake, curve_info)
             self._curve_info = CurveInfo(**curve_info)
         return self._curve_info
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_manage.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_manage.py`

 * *Files 1% similar despite different names*

```diff
@@ -138,43 +138,44 @@
         extended_params=extended_params,
     )
     response = content_provider_layer.get_data(session=session)
     return response
 
 
 def create(
-    curve_definition: "CurveCreateDefinition" = None,
+    curve_definition: "CurveCreateDefinition",
+    segments: "Segments",
+    *,
     overrides: "OptOverrides" = None,
-    segments: "Segments" = None,
     turns: "OptTurns" = None,
     extended_params: "ExtendedParams" = None,
     session: "Session" = None,
 ) -> "Response":
     """
     Creates the Cross Currency curve definition with the definition provided.
 
     Parameters
     ----------
     curve_definition : CrossCurrencyCurveDefinitionDescription
         CrossCurrencyCurveDefinitionDescription object
-    overrides : list of OverrideBidAsk, optional
-        OverrideBidAsk object.
     segments : list of CrossCurrencyInstrumentsSegment
         list of CrossCurrencyInstrumentsSegment objects
+    overrides : list of OverrideBidAsk, optional
+        OverrideBidAsk object.
     turns : list of OverrideFxForwardTurn, optional
     extended_params : ExtendedParams, optional
         If necessary other parameters.
     session : Session, optional
         session=None - means default session would be used
 
     Examples
     --------
     >>> from refinitiv.data.content.ipa.curves._cross_currency_curves import definitions
     >>> response = definitions.manage.create(
-    ...     curve_definition=definitions.CrossCurrencyCurveDefinitionDescription(
+    ...     curve_definition=definitions.CrossCurrencyCurveCreateDefinition(
     ...         source="SourceName",
     ...         name="Name of the Curve854",
     ...         base_currency="EUR",
     ...         base_index_name="ESTR",
     ...         quoted_currency="USD",
     ...         quoted_index_name="SOFR",
     ...         is_non_deliverable=False
@@ -211,32 +212,33 @@
         extended_params=extended_params,
         session=session,
     )
     return response
 
 
 def update(
-    curve_definition: "CurveUpdateDefinition" = None,
+    curve_definition: "CurveUpdateDefinition",
+    segments: "Segments",
+    *,
     overrides: "OptOverrides" = None,
-    segments: "Segments" = None,
     turns: "OptTurns" = None,
     extended_params: "ExtendedParams" = None,
     session: "Session" = None,
 ) -> "Response":
     """
     Updates the Cross Currency curve definition with the definition provided.
 
     Parameters
     ----------
     curve_definition : CrossCurrencyCurveUpdateDefinition
         CrossCurrencyCurveUpdateDefinition object.
-    overrides : list of OverrideBidAsk, optional
-        OverrideBidAsk object.
     segments : list of CrossCurrencyInstrumentsSegment
         list of CrossCurrencyInstrumentsSegment objects
+    overrides : list of OverrideBidAsk, optional
+        OverrideBidAsk object.
     turns : list of OverrideFxForwardTurn, optional
     extended_params : ExtendedParams, optional
         If necessary other parameters.
     session : Session, optional
         session=None - means default session would be used
 
     Examples
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_search.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,15 +5,15 @@
 )
 from ......_content_type import ContentType
 from ......_tools import create_repr
 from ....._content_provider_layer import ContentProviderLayer
 
 
 if TYPE_CHECKING:
-    from ...._curves._cross_currency_curves._definitions._types import (
+    from ...._curves._cross_currency_curves._types import (
         OptMainConstituentAssetClass,
         OptRiskType,
     )
     from ......_types import OptStr, OptBool, ExtendedParams
 
 
 class Definition(ContentProviderLayer):
@@ -70,14 +70,15 @@
     >>> definition = definitions.search.Definition(
     ...     base_currency="EUR",
     ...     quoted_currency="CHF"
     >>> )
     >>> response = definition.get_data()
 
     Using get_data_async
+
     >>> import asyncio
     >>> task = definition.get_data_async()
     >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
@@ -91,15 +92,14 @@
         name: "OptStr" = None,
         quoted_currency: "OptStr" = None,
         quoted_index_name: "OptStr" = None,
         source: "OptStr" = None,
         valuation_date: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ) -> None:
-
         request_item = CrossCurrencyCurveGetDefinitionItem(
             main_constituent_asset_class=main_constituent_asset_class,
             risk_type=risk_type,
             base_currency=base_currency,
             base_index_name=base_index_name,
             curve_tag=curve_tag,
             id=id,
@@ -113,10 +113,8 @@
         super().__init__(
             content_type=ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_SEARCH,
             universe=request_item,
             extended_params=extended_params,
         )
 
     def __repr__(self):
-        return create_repr(
-            self, middle_path="_cross_currency_curves.definitions.search"
-        )
+        return create_repr(self, middle_path="_cross_currency_curves.definitions.search")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_data_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,9 @@
-from typing import TYPE_CHECKING
+from dataclasses import dataclass
+from typing import List, TYPE_CHECKING
 
 from .._arg_enums import main_constituent_asset_class_arg_parser, risk_type_arg_parser
 from .._base_data_class import BaseData
 from ......delivery._data._endpoint_data import EndpointData
 
 if TYPE_CHECKING:
     from ...._curves._cross_currency_curves._types import (
@@ -81,18 +82,16 @@
         is_non_deliverable: "OptBool" = None,
         name: "OptStr" = None,
         quoted_currency: "OptStr" = None,
         quoted_index_name: "OptStr" = None,
         source: "OptStr" = None,
         **kwargs,
     ):
-        self.main_constituent_asset_class = (
-            main_constituent_asset_class_arg_parser.get_enum(
-                main_constituent_asset_class
-            )
+        self.main_constituent_asset_class = main_constituent_asset_class_arg_parser.get_enum(
+            main_constituent_asset_class
         )
         self.risk_type = risk_type_arg_parser.get_enum(risk_type)
         self.base_currency = base_currency
         self.base_index_name = base_index_name
         self.id = id
         self.is_fallback_for_fx_curve_definition = is_fallback_for_fx_curve_definition
         self.is_non_deliverable = is_non_deliverable
@@ -110,45 +109,38 @@
         self._kwargs = kwargs
 
     @property
     def direct_curve_definitions(self):
         if self._direct_curve_definitions is None:
             self._direct_curve_definitions = []
             direct_curves = self._kwargs.get("directCurveDefinitions")
-            self._direct_curve_definitions = self._create_list_definition_triangulates(
-                direct_curves
-            )
+            self._direct_curve_definitions = self._create_list_definition_triangulates(direct_curves)
         return self._direct_curve_definitions
 
     @property
     def indirect_curve_definitions(self):
         if self._indirect_curve_definitions is None:
             self._indirect_curve_definitions = []
             indirect_curves = self._kwargs.get("indirectCurveDefinitions")
             for indirect_curve in indirect_curves:
                 cross_currencies = indirect_curve.get("crossCurrencyDefinitions", [])
-                list_triangulates = self._create_list_definition_triangulates(
-                    cross_currencies
-                )
+                list_triangulates = self._create_list_definition_triangulates(cross_currencies)
                 self._indirect_curve_definitions.append(list_triangulates)
         return self._indirect_curve_definitions
 
     @property
     def curve_tag(self):
         return self._kwargs.get("curveTag")
 
     def _create_list_definition_triangulates(self, items: list):
-        return [
-            CurveDefinitionTriangulate(**convert_camel_to_snake(item)) for item in items
-        ]
+        return [CurveDefinitionTriangulate(**convert_camel_to_snake(item)) for item in items]
 
 
+@dataclass
 class TriangulateDefinitionsData(EndpointData):
-    _curve_definitions = None
+    _curve_definitions: List[CurveDefinition] = None
 
     @property
     def curve_definitions(self):
         if self._curve_definitions is None:
-            self._curve_definitions = [
-                CurveDefinition(**item) for item in self._raw.get("data", [])
-            ]
+            self._curve_definitions = [CurveDefinition(**item) for item in self.raw.get("data", [])]
         return self._curve_definitions
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_search.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_search.py`

 * *Files 1% similar despite different names*

```diff
@@ -43,14 +43,15 @@
     ... definition = triangulate_definitions.search.Definition(
     ...     base_currency="EUR",
     ...     quoted_currency="CHF"
     ... )
     >>> response = definition.get_data()
 
     Using get_data_async
+
     >>> import asyncio
     >>> task = definition.get_data_async()
     >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
@@ -58,15 +59,14 @@
         base_index_name: "OptStr" = None,
         curve_tag: "OptStr" = None,
         quoted_currency: "OptStr" = None,
         quoted_index_name: "OptStr" = None,
         valuation_date: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ) -> None:
-
         request_item = RequestItem(
             base_currency=base_currency,
             base_index_name=base_index_name,
             curve_tag=curve_tag,
             quoted_currency=quoted_currency,
             quoted_index_name=quoted_index_name,
             valuation_date=valuation_date,
@@ -74,10 +74,8 @@
         super().__init__(
             content_type=ContentType.CROSS_CURRENCY_CURVES_TRIANGULATE_DEFINITIONS_SEARCH,
             universe=request_item,
             extended_params=extended_params,
         )
 
     def __repr__(self):
-        return create_repr(
-            self, middle_path="_cross_currency_curves.triangulate_definitions.search"
-        )
+        return create_repr(self, middle_path="_cross_currency_curves.triangulate_definitions.search")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/forward_curves/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/forward_curves/__init__.py`

 * *Files 21% similar despite different names*

```diff
@@ -10,32 +10,36 @@
     "ForwardCurveDefinition",
     "InterpolationMode",
     "PriceSide",
     "RiskType",
     "Step",
     "SwapZcCurveDefinition",
     "SwapZcCurveParameters",
+    "ShiftScenario",
     "Turn",
     "Outputs",
+    "ParRateShift",
 )
 
 from ._definition import Definition, Definitions
 from ..._curves._enums import ForwardCurvesOutputs as Outputs
-from refinitiv.data.content.ipa.curves.forward_curves._models import (
+from ..._curves._models import (
     ConvexityAdjustment,
     Step,
     Turn,
+    ParRateShift,
 )
 from ..._curves import (
     ForwardCurveDefinition,
     SwapZcCurveDefinition,
     SwapZcCurveParameters,
+    ShiftScenario,
 )
 
-from ._enums import (
+from ..._curves._enums import (
     AssetClass,
     RiskType,
     DayCountBasis,
     InterpolationMode,
     PriceSide,
     ExtrapolationMode,
     CalendarAdjustment,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/forward_curves/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/forward_curves/_definition.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Optional
 
 from numpy import iterable
 
+from ..._curves import ShiftScenario
 from ...._content_provider_layer import ContentProviderLayer
 from ....._tools import create_repr, try_copy_to_list
 from ..._curves._forward_curve_request_item import ForwardCurveRequestItem
 from ....._content_type import ContentType
 
 if TYPE_CHECKING:
     from ..._curves._forward_curve_types import (
@@ -38,73 +39,76 @@
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, async_mode=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.curves.forward_curves as forward_curves
-     >>> definition = forward_curves.Definition(
-     ...     curve_definition=forward_curves.SwapZcCurveDefinition(
-     ...         currency="EUR",
-     ...         index_name="EURIBOR",
-     ...         name="EUR EURIBOR Swap ZC Curve",
-     ...         discounting_tenor="OIS",
-     ...     ),
-     ...     forward_curve_definitions=[
-     ...         forward_curves.ForwardCurveDefinition(
-     ...             index_tenor="3M",
-     ...             forward_curve_tag="ForwardTag",
-     ...             forward_start_date="2021-02-01",
-     ...             forward_curve_tenors=[
-     ...                 "0D",
-     ...                 "1D",
-     ...                 "2D",
-     ...                 "3M",
-     ...                 "6M",
-     ...                 "9M",
-     ...                 "1Y",
-     ...                 "2Y",
-     ...                 "3Y",
-     ...                 "4Y",
-     ...                 "5Y",
-     ...                 "6Y",
-     ...                 "7Y",
-     ...                 "8Y",
-     ...                 "9Y",
-     ...                 "10Y",
-     ...                 "15Y",
-     ...                 "20Y",
-     ...                 "25Y"
-     ...             ]
-     ...         )
-     ...     ]
-     ... )
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import refinitiv.data.content.ipa.curves.forward_curves as forward_curves
+    >>> definition = forward_curves.Definition(
+    ...     curve_definition=forward_curves.SwapZcCurveDefinition(
+    ...         currency="EUR",
+    ...         index_name="EURIBOR",
+    ...         name="EUR EURIBOR Swap ZC Curve",
+    ...         discounting_tenor="OIS",
+    ...     ),
+    ...     forward_curve_definitions=[
+    ...         forward_curves.ForwardCurveDefinition(
+    ...             index_tenor="3M",
+    ...             forward_curve_tag="ForwardTag",
+    ...             forward_start_date="2021-02-01",
+    ...             forward_curve_tenors=[
+    ...                 "0D",
+    ...                 "1D",
+    ...                 "2D",
+    ...                 "3M",
+    ...                 "6M",
+    ...                 "9M",
+    ...                 "1Y",
+    ...                 "2Y",
+    ...                 "3Y",
+    ...                 "4Y",
+    ...                 "5Y",
+    ...                 "6Y",
+    ...                 "7Y",
+    ...                 "8Y",
+    ...                 "9Y",
+    ...                 "10Y",
+    ...                 "15Y",
+    ...                 "20Y",
+    ...                 "25Y"
+    ...             ]
+    ...         )
+    ...     ]
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         curve_definition: "CurveDefinition" = None,
         forward_curve_definitions: "OptForwardCurveDefinitions" = None,
         curve_parameters: "CurveParameters" = None,
         curve_tag: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
+        shift_scenarios: Optional[ShiftScenario] = None,
     ):
         forward_curve_definitions = try_copy_to_list(forward_curve_definitions)
         request_item = ForwardCurveRequestItem(
             curve_definition=curve_definition,
             forward_curve_definitions=forward_curve_definitions,
             curve_parameters=curve_parameters,
             curve_tag=curve_tag,
+            shift_scenarios=shift_scenarios,
         )
         super().__init__(
             content_type=ContentType.FORWARD_CURVE,
             universe=request_item,
             extended_params=extended_params,
         )
 
@@ -123,61 +127,62 @@
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, async_mode=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.curves.forward_curves as forward_curves
-     >>>
-     >>> forward_curve_definition = forward_curves.Definition(
-     ...     curve_definition=forward_curves.SwapZcCurveDefinition(
-     ...         currency="EUR",
-     ...         index_name="EURIBOR",
-     ...         name="EUR EURIBOR Swap ZC Curve",
-     ...         discounting_tenor="OIS",
-     ...     ),
-     ...     forward_curve_definitions=[
-     ...         forward_curves.ForwardCurveDefinition(
-     ...             index_tenor="3M",
-     ...             forward_curve_tag="ForwardTag",
-     ...             forward_start_date="2021-02-01",
-     ...             forward_curve_tenors=[
-     ...                 "0D",
-     ...                 "1D",
-     ...                 "2D",
-     ...                 "3M",
-     ...                 "6M",
-     ...                 "9M",
-     ...                 "1Y",
-     ...                 "2Y",
-     ...                 "3Y",
-     ...                 "4Y",
-     ...                 "5Y",
-     ...                 "6Y",
-     ...                 "7Y",
-     ...                 "8Y",
-     ...                 "9Y",
-     ...                 "10Y",
-     ...                 "15Y",
-     ...                 "20Y",
-     ...                 "25Y"
-     ...             ]
-     ...         )
-     ...     ]
-     ... )
-     >>> definition = forward_curves.Definitions(
-     ...     universe=[forward_curve_definition],
-     ... )
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import refinitiv.data.content.ipa.curves.forward_curves as forward_curves
+    >>>
+    >>> forward_curve_definition = forward_curves.Definition(
+    ...     curve_definition=forward_curves.SwapZcCurveDefinition(
+    ...         currency="EUR",
+    ...         index_name="EURIBOR",
+    ...         name="EUR EURIBOR Swap ZC Curve",
+    ...         discounting_tenor="OIS",
+    ...     ),
+    ...     forward_curve_definitions=[
+    ...         forward_curves.ForwardCurveDefinition(
+    ...             index_tenor="3M",
+    ...             forward_curve_tag="ForwardTag",
+    ...             forward_start_date="2021-02-01",
+    ...             forward_curve_tenors=[
+    ...                 "0D",
+    ...                 "1D",
+    ...                 "2D",
+    ...                 "3M",
+    ...                 "6M",
+    ...                 "9M",
+    ...                 "1Y",
+    ...                 "2Y",
+    ...                 "3Y",
+    ...                 "4Y",
+    ...                 "5Y",
+    ...                 "6Y",
+    ...                 "7Y",
+    ...                 "8Y",
+    ...                 "9Y",
+    ...                 "10Y",
+    ...                 "15Y",
+    ...                 "20Y",
+    ...                 "25Y"
+    ...             ]
+    ...         )
+    ...     ]
+    ... )
+    >>> definition = forward_curves.Definitions(
+    ...     universe=[forward_curve_definition],
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         universe: "DefnDefns",
     ):
         universe = try_copy_to_list(universe)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curve_definitions/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curve_definitions/_definition.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,14 +1,12 @@
 from typing import List, Optional, TYPE_CHECKING, Union
 
 from numpy import iterable
 
-from ..._curves._interest_rate_curve_get_definition import (
-    InterestRateCurveGetDefinition,
-)
+from ..._curves._zc_curve_definition_request import ZcCurveDefinitionRequest
 from ...._content_provider_layer import ContentProviderLayer
 from ....._content_type import ContentType
 from ....._tools import create_repr, try_copy_to_list
 
 if TYPE_CHECKING:
     from ....._types import OptStr, ExtendedParams
     from ..._enums._risk_type import RiskType
@@ -41,14 +39,18 @@
         Example:
             "Refinitiv"
     valuation_date: str, optional
         Example:
             "2019-08-21"
     extended_params : dict, optional
         If necessary other parameters.
+    market_data_location : str, optional
+        The identifier of the market place from which constituents come from.
+        Currently the following values are supported: 'onshore' and 'emea'.
+        The list of values can be extended by a user when creating a curve.
 
     Methods
     -------
     get_data(session=session, on_response=on_response, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
@@ -56,42 +58,45 @@
     Examples
     --------
     >>> from refinitiv.data.content.ipa.curves import zc_curve_definitions
     >>> definition = zc_curve_definitions.Definition(source="Refinitiv")
     >>> response = definition.get_data()
 
     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         index_name: "OptStr" = None,
         main_constituent_asset_class: Optional["AssetClass"] = None,
         risk_type: Optional["RiskType"] = None,
         currency: "OptStr" = None,
         curve_tag: "OptStr" = None,
         id: "OptStr" = None,
         name: "OptStr" = None,
         source: "OptStr" = None,
         valuation_date: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
+        market_data_location: "OptStr" = None,
     ) -> None:
-        request_item = InterestRateCurveGetDefinition(
+        request_item = ZcCurveDefinitionRequest(
             index_name=index_name,
             main_constituent_asset_class=main_constituent_asset_class,
             risk_type=risk_type,
             currency=currency,
             curve_tag=curve_tag,
             id=id,
             name=name,
             source=source,
             valuation_date=valuation_date,
+            market_data_location=market_data_location,
         )
         super().__init__(
             content_type=ContentType.ZC_CURVE_DEFINITIONS,
             universe=request_item,
             extended_params=extended_params,
         )
 
@@ -124,14 +129,15 @@
     >>> definition2 = zc_curve_definitions.Definition(source="Peugeot")
     >>> definitions = zc_curve_definitions.Definitions(
     ...     universe=[definition1, definition2]
     ...)
     >>> response = definitions.get_data()
 
     Using get_data_async
+
      >>> import asyncio
      >>> task = definitions.get_data_async()
      >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/curves/zc_curves/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/curves/zc_curves/_definition.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,11 +1,12 @@
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Optional
 
 from numpy import iterable
 
+from ..._curves import ShiftScenario
 from ...._content_provider_layer import ContentProviderLayer
 from ....._tools import create_repr, try_copy_to_list
 
 from ..._curves._zc_curve_request_item import ZcCurveRequestItem
 from ....._content_type import ContentType
 
 if TYPE_CHECKING:
@@ -29,55 +30,61 @@
     curve_parameters : ZcCurveParameters, optional
 
     curve_tag : str, optional
 
     extended_params : dict, optional
         If necessary other parameters.
 
+    shift_scenarios : ShiftScenario, optional
+        The list of attributes applied to the curve shift scenarios.
+
     Methods
     -------
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, async_mode=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.curves.zc_curves as zc_curves
-     >>> definition = zc_curves.Definition(
-     ...     curve_definition=zc_curves.ZcCurveDefinitions(
-     ...         currency="CHF",
-     ...         name="CHF LIBOR Swap ZC Curve",
-     ...         discounting_tenor="OIS",
-     ...     ),
-     ...     curve_parameters=zc_curves.ZcCurveParameters(
-     ...         use_steps=True
-     ...     )
-     ... )
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import refinitiv.data.content.ipa.curves.zc_curves as zc_curves
+    >>> definition = zc_curves.Definition(
+    ...     curve_definition=zc_curves.ZcCurveDefinitions(
+    ...         currency="CHF",
+    ...         name="CHF LIBOR Swap ZC Curve",
+    ...         discounting_tenor="OIS",
+    ...     ),
+    ...     curve_parameters=zc_curves.ZcCurveParameters(
+    ...         use_steps=True
+    ...     )
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         constituents: "OptConstituents" = None,
         curve_definition: "CurveDefinition" = None,
         curve_parameters: "CurveParameters" = None,
         curve_tag: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
+        shift_scenarios: Optional[ShiftScenario] = None,
     ):
         request_item = ZcCurveRequestItem(
             constituents=constituents,
             curve_definition=curve_definition,
             curve_parameters=curve_parameters,
             curve_tag=curve_tag,
+            shift_scenarios=shift_scenarios,
         )
         super().__init__(
             content_type=ContentType.ZC_CURVES,
             universe=request_item,
             extended_params=extended_params,
         )
 
@@ -96,32 +103,33 @@
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, async_mode=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.curves.zc_curves as zc_curves
-     >>> definition = zc_curves.Definition(
-     ...     curve_definition=zc_curves.ZcCurveDefinitions(
-     ...         currency="CHF",
-     ...         name="CHF LIBOR Swap ZC Curve",
-     ...         discounting_tenor="OIS",
-     ...     ),
-     ...     curve_parameters=zc_curves.ZcCurveParameters(
-     ...         use_steps=True
-     ...     )
-     ... )
-     >>> definition = zc_curves.Definitions(universe=definition)
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import refinitiv.data.content.ipa.curves.zc_curves as zc_curves
+    >>> definition = zc_curves.Definition(
+    ...     curve_definition=zc_curves.ZcCurveDefinitions(
+    ...         currency="CHF",
+    ...         name="CHF LIBOR Swap ZC Curve",
+    ...         discounting_tenor="OIS",
+    ...     ),
+    ...     curve_parameters=zc_curves.ZcCurveParameters(
+    ...         use_steps=True
+    ...     )
+    ... )
+    >>> definition = zc_curves.Definitions(universe=definition)
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         universe: "DefnDefns",
     ):
         universe = try_copy_to_list(universe)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/_base_request_items.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/_base_request_items.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/_content_data_validator.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/_content_data_validator.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/_request_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/_request_factory.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,11 +6,9 @@
     from ....delivery._data._parsed_data import ParsedData
 
 
 class DatesAndCalendarsResponseFactory(ContentResponseFactory):
     def create_fail(self, parsed_data: "ParsedData", **kwargs):
         errors = parsed_data.status.get("error", {}).get("errors")
         if errors:
-            parsed_data.error_messages = (
-                f"{parsed_data.first_error_message}. {errors[0]['reason']}"
-            )
+            parsed_data.error_messages = f"{parsed_data.first_error_message}. {errors[0]['reason']}"
         return super().create_fail(parsed_data, **kwargs)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_add_periods_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_add_periods_data_provider.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,98 +1,70 @@
-from typing import TYPE_CHECKING, List
+from dataclasses import dataclass
+from typing import List, Union
 
 from .._content_data_validator import ContentDataValidator
 from .._request_factory import DatesAndCalendarsResponseFactory
 from ..holidays._holidays_data_provider import Holiday
 from ...._content_data import Data
 from ...._content_data_provider import ContentDataProvider
 from ...._df_builder import build_dates_calendars_df
 from .....content.ipa._content_provider import DatesAndCalendarsRequestFactory
 from .....delivery._data._data_provider import ValidatorContainer
 
-if TYPE_CHECKING:
-    from .....delivery._data._data_provider import ParsedData
-
 
+@dataclass
 class Period:
-    _holidays = None
+    date: str
+    holidays: Union[list, None]
+    tag: str = ""
 
-    def __init__(self, date: str, holidays: list = None, tag: str = ""):
-        self._date = date
-        self._response_holidays_items = holidays or []
-        self._tag = tag
 
-    @property
-    def tag(self):
-        return self._tag
+def period_from_dict(datum: dict):
+    tag = datum.get("tag")
+    holidays = datum.get("holidays", [])
+    holidays = [Holiday(holiday=holiday, tag=tag) for holiday in holidays]
 
-    @property
-    def date(self):
-        return self._date
-
-    @property
-    def holidays(self):
-        if self._holidays is None:
-            self._holidays = [
-                Holiday(holiday=holiday, tag=self.tag)
-                for holiday in self._response_holidays_items
-            ]
-
-        return self._holidays
+    return Period(date=datum["date"], holidays=holidays, tag=tag)
 
 
+@dataclass
 class AddedPeriods(Data):
-    _added_periods = None
+    _added_periods: List = None
 
     @property
     def added_periods(self):
         if self._added_periods is None:
-            self._added_periods = [
-                Period(
-                    date=raw_item["date"],
-                    holidays=raw_item.get("holidays"),
-                    tag=raw_item.get("tag"),
-                )
-                for raw_item in self._raw
-                if not raw_item.get("error")
-            ]
+            self._added_periods = [period_from_dict(raw_item) for raw_item in self.raw if not raw_item.get("error")]
 
         return self._added_periods
 
     def __getitem__(self, item):
         return self.added_periods[item]
 
 
+@dataclass
 class AddedPeriod(Data):
-    def __init__(
-        self, raw: List[dict], date: str, holidays: None, tag: str = "", **kwargs
-    ):
-        super().__init__(raw, **kwargs)
-        self._period = Period(date=date, holidays=holidays, tag=tag)
+    _period: Period = None
 
     @property
     def added_period(self):
         return self._period
 
 
 class AddPeriodsResponseFactory(DatesAndCalendarsResponseFactory):
-    def create_data_success(self, parsed_data: "ParsedData", **kwargs):
-        raw: List[dict] = self.get_raw(parsed_data)
-
+    def create_data_success(self, raw: List[dict], **kwargs):
         if len(raw) > 1:
-            data = AddedPeriods(raw, build_dates_calendars_df)
+            data = AddedPeriods(raw, _dfbuilder=build_dates_calendars_df)
 
         else:
             raw_item = raw[0]
             data = AddedPeriod(
                 raw=raw,
-                date=raw_item["date"],
-                holidays=raw_item.get("holidays"),
-                tag=raw_item.get("tag"),
-                dfbuilder=build_dates_calendars_df,
+                _period=period_from_dict(raw_item),
+                _dfbuilder=build_dates_calendars_df,
             )
 
         return data
 
 
 add_period_data_provider = ContentDataProvider(
     request=DatesAndCalendarsRequestFactory(),
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/add_periods/_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -153,31 +153,32 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import add_periods
-     >>> definition = add_periods.Definition(
-     ...     start_date="2020-01-01",
-     ...     period="4D",
-     ...     calendars=["BAR", "KOR", "JAP"],
-     ...     currencies=["USD"],
-     ...     tag="my request",
-     ...     date_moving_convention=add_periods.DateMovingConvention.NEXT_BUSINESS_DAY,
-     ...     end_of_month_convention=add_periods.EndOfMonthConvention.LAST,
-     ...     holiday_outputs=["Date", "Calendars", "Names"]
-     ... )
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import add_periods
+    >>> definition = add_periods.Definition(
+    ...     start_date="2020-01-01",
+    ...     period="4D",
+    ...     calendars=["BAR", "KOR", "JAP"],
+    ...     currencies=["USD"],
+    ...     tag="my request",
+    ...     date_moving_convention=add_periods.DateMovingConvention.NEXT_BUSINESS_DAY,
+    ...     end_of_month_convention=add_periods.EndOfMonthConvention.LAST,
+    ...     holiday_outputs=["Date", "Calendars", "Names"]
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.AddPeriodsDefinition"
 
     def __init__(
         self,
         start_date: "OptDateTime" = None,
@@ -238,43 +239,44 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import add_periods
-     >>>
-     >>> first_definition = add_periods.Definition(
-     ...     tag="first",
-     ...     start_date="2020-01-01",
-     ...     period="4D",
-     ...     calendars=["BAR", "KOR", "JAP"],
-     ...     currencies=["USD"],
-     ...     date_moving_convention=add_periods.DateMovingConvention.NEXT_BUSINESS_DAY,
-     ...     end_of_month_convention=add_periods.EndOfMonthConvention.LAST,
-     ...     holiday_outputs=["Date", "Calendars", "Names"]
-     ... )
-     >>> second_definition = add_periods.Definition(
-     ...     tag="second",
-     ...     start_date="2018-01-01",
-     ...     period="4D",
-     ...     calendars=["BAR", "JAP"],
-     ...     currencies=["USD"],
-     ...     date_moving_convention=add_periods.DateMovingConvention.NEXT_BUSINESS_DAY,
-     ...     end_of_month_convention=add_periods.EndOfMonthConvention.LAST28,
-     ...     holiday_outputs=[add_periods.HolidayOutputs.DATE, add_periods.HolidayOutputs.NAMES]
-     ... )
-     >>> response = add_periods.Definitions([first_definition, second_definition]).get_data()
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import add_periods
+    >>>
+    >>> first_definition = add_periods.Definition(
+    ...     tag="first",
+    ...     start_date="2020-01-01",
+    ...     period="4D",
+    ...     calendars=["BAR", "KOR", "JAP"],
+    ...     currencies=["USD"],
+    ...     date_moving_convention=add_periods.DateMovingConvention.NEXT_BUSINESS_DAY,
+    ...     end_of_month_convention=add_periods.EndOfMonthConvention.LAST,
+    ...     holiday_outputs=["Date", "Calendars", "Names"]
+    ... )
+    >>> second_definition = add_periods.Definition(
+    ...     tag="second",
+    ...     start_date="2018-01-01",
+    ...     period="4D",
+    ...     calendars=["BAR", "JAP"],
+    ...     currencies=["USD"],
+    ...     date_moving_convention=add_periods.DateMovingConvention.NEXT_BUSINESS_DAY,
+    ...     end_of_month_convention=add_periods.EndOfMonthConvention.LAST28,
+    ...     holiday_outputs=[add_periods.HolidayOutputs.DATE, add_periods.HolidayOutputs.NAMES]
+    ... )
+    >>> response = add_periods.Definitions([first_definition, second_definition]).get_data()
 
     Using get_data_async
-     >>> import asyncio
-     >>> task = add_periods.Definitions([first_definition, second_definition]).get_data_async()
-     >>> response = asyncio.run(task)
+
+    >>> import asyncio
+    >>> task = add_periods.Definitions([first_definition, second_definition]).get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.AddPeriodsDefinitions"
 
     def __init__(self, universe: "DefnDefns"):
         universe = try_copy_to_list(universe)
         if not iterable(universe):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods.py`

 * *Files 0% similar despite different names*

```diff
@@ -125,31 +125,32 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import count_periods
-     >>> definition = count_periods.Definition(
-     ...   tag="my request",
-     ...   start_date=datetime.timedelta(-11),
-     ...   end_date=datetime.timedelta(-3),
-     ...   period_type=count_periods.PeriodType.WORKING_DAY,
-     ...   calendars=["EMU"],
-     ...   currencies=["EUR"],
-     ...   day_count_basis=count_periods.DayCountBasis.DCB_30_360
-     ... )
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import count_periods
+    >>> definition = count_periods.Definition(
+    ...   tag="my request",
+    ...   start_date=datetime.timedelta(-11),
+    ...   end_date=datetime.timedelta(-3),
+    ...   period_type=count_periods.PeriodType.WORKING_DAY,
+    ...   calendars=["EMU"],
+    ...   currencies=["EUR"],
+    ...   day_count_basis=count_periods.DayCountBasis.DCB_30_360
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.CountPeriodsDefinition"
 
     def __init__(
         self,
         start_date: "OptDateTime" = None,
@@ -205,41 +206,42 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import count_periods
-     >>> first_definition = count_periods.Definition(
-     ...   tag="my request",
-     ...   start_date=datetime.timedelta(-11),
-     ...   end_date=datetime.timedelta(-3),
-     ...   period_type=count_periods.PeriodType.WORKING_DAY,
-     ...   calendars=["EMU"],
-     ...   currencies=["EUR"],
-     ...   day_count_basis=count_periods.DayCountBasis.DCB_30_360
-     ... )
-     ...
-     >>> second_definition = count_periods.Definition(
-     ...   tag="my second request",
-     ...   start_date=datetime.timedelta(-15),
-     ...   end_date=datetime.timedelta(-10),
-     ...   period_type=count_periods.PeriodType.NON_WORKING_DAY,
-     ...   calendars=["EMU"],
-     ...   currencies=["EUR"],
-     ...   day_count_basis=count_periods.DayCountBasis.DCB_30_360
-     ... )
-     >>> response = count_periods.Definitions([first_definition, second_definition]).get_data()
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import count_periods
+    >>> first_definition = count_periods.Definition(
+    ...   tag="my request",
+    ...   start_date=datetime.timedelta(-11),
+    ...   end_date=datetime.timedelta(-3),
+    ...   period_type=count_periods.PeriodType.WORKING_DAY,
+    ...   calendars=["EMU"],
+    ...   currencies=["EUR"],
+    ...   day_count_basis=count_periods.DayCountBasis.DCB_30_360
+    ... )
+    ...
+    >>> second_definition = count_periods.Definition(
+    ...   tag="my second request",
+    ...   start_date=datetime.timedelta(-15),
+    ...   end_date=datetime.timedelta(-10),
+    ...   period_type=count_periods.PeriodType.NON_WORKING_DAY,
+    ...   calendars=["EMU"],
+    ...   currencies=["EUR"],
+    ...   day_count_basis=count_periods.DayCountBasis.DCB_30_360
+    ... )
+    >>> response = count_periods.Definitions([first_definition, second_definition]).get_data()
 
     Using get_data_async
-     >>> import asyncio
-     >>> task = count_periods.Definitions([first_definition, second_definition]).get_data_async()
-     >>> response = asyncio.run(task)
+
+    >>> import asyncio
+    >>> task = count_periods.Definitions([first_definition, second_definition]).get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.CountPeriodsDefinitions"
 
     def __init__(self, universe: "DefnDefns"):
         universe = try_copy_to_list(universe)
         if not iterable(universe):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/count_periods/_count_periods_data_provider.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,86 +1,65 @@
-from typing import TYPE_CHECKING, List
+from dataclasses import dataclass
+from typing import List
 
 from .._content_data_validator import ContentDataValidator
 from .._request_factory import DatesAndCalendarsResponseFactory
 from ...._content_data import Data
 from ...._content_data_provider import ContentDataProvider
 from ...._df_builder import default_build_df
 from .....content.ipa._content_provider import DatesAndCalendarsRequestFactory
 from .....delivery._data._data_provider import ValidatorContainer
 
-if TYPE_CHECKING:
-    from .....delivery._data._data_provider import ParsedData
-
 
+@dataclass
 class Period:
-    def __init__(self, count: float, tenor: str, tag: str = ""):
-        self._count = count
-        self._tenor = tenor
-        self._tag = tag
-
-    @property
-    def count(self):
-        return self._count
+    count: float
+    tenor: str
+    tag: str = ""
 
-    @property
-    def tenor(self):
-        return self._tenor
 
-    @property
-    def tag(self):
-        return self._tag
+def period_from_dict(datum: dict):
+    return Period(count=datum["count"], tenor=datum["tenor"], tag=datum.get("tag"))
 
 
+@dataclass
 class CountedPeriods(Data):
-    _counted_periods = None
+    _counted_periods: List[Period] = None
 
     @property
     def counted_periods(self):
         if self._counted_periods is None:
-            self._counted_periods = [
-                Period(item["count"], item["tenor"], tag=item.get("tag"))
-                for item in self._raw
-            ]
+            self._counted_periods = [period_from_dict(item) for item in self.raw]
 
         return self._counted_periods
 
     def __getitem__(self, item: int):
         return self.counted_periods[item]
 
 
+@dataclass
 class CountedPeriod(Data):
-    def __init__(self, raw: dict, count: float, tenor: str, tag: str = "", **kwargs):
-        super().__init__(raw, **kwargs)
-        self._period = Period(count, tenor, tag)
+    _period: Period = None
 
     @property
     def counted_period(self):
         return self._period
 
 
 class CountPeriodsResponseFactory(DatesAndCalendarsResponseFactory):
-    def create_data_success(self, parsed_data: "ParsedData", **kwargs):
-        raw: List[dict] = self.get_raw(parsed_data)
-        for item in raw:
-            if item.get("error"):
-                return self.create_fail(item, **kwargs)
-
+    def create_data_success(self, raw: List[dict], **kwargs):
         if len(raw) > 1:
-            data = CountedPeriods(raw, default_build_df)
+            data = CountedPeriods(raw, _dfbuilder=default_build_df)
 
         else:
-            raw_item = raw[0]
             data = CountedPeriod(
-                raw,
-                raw_item["count"],
-                raw_item["tenor"],
-                raw_item.get("tag"),
-                dfbuilder=default_build_df,
-                **kwargs,
+                raw=raw,
+                _period=period_from_dict(raw[0]),
+                _dfbuilder=default_build_df,
+                _kwargs=kwargs,
             )
 
         return data
 
 
 count_periods_data_provider = ContentDataProvider(
     request=DatesAndCalendarsRequestFactory(),
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule.py`

 * *Files 1% similar despite different names*

```diff
@@ -151,29 +151,30 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import date_schedule
-     >>> definition = date_schedule.Definition(
-     ...     start_date="2020-01-01",
-     ...     end_date=datetime.timedelta(0),
-     ...     frequency="Weekly",
-     ...     calendars=["EMU", "GER"],
-     ...     day_of_week=date_schedule.DayOfWeek.MONDAY,
-     ... )
-     >>> response = definition.get_data()
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import date_schedule
+    >>> definition = date_schedule.Definition(
+    ...     start_date="2020-01-01",
+    ...     end_date=datetime.timedelta(0),
+    ...     frequency="Weekly",
+    ...     calendars=["EMU", "GER"],
+    ...     day_of_week=date_schedule.DayOfWeek.MONDAY,
+    ... )
+    >>> response = definition.get_data()
 
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.DateScheduleDefinition"
 
     def __init__(
         self,
         frequency: Union[DateScheduleFrequency, str, None] = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/date_schedule/_date_schedule_data_provider.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,19 +1,23 @@
+from dataclasses import dataclass
+from typing import List
+
 import numpy as np
 
 from .._content_data_validator import ContentDataValidator
 from .._request_factory import DatesAndCalendarsResponseFactory
 from ...._content_data import Data
 from ...._content_data_provider import ContentDataProvider
 from .....content.ipa._content_provider import DateScheduleRequestFactory
 from .....delivery._data._data_provider import ValidatorContainer
 
 
+@dataclass
 class DateSchedule(Data):
-    _dates = None
+    _dates: List[np.datetime64] = None
 
     @property
     def dates(self):
         if self._dates is None:
             self._dates = [np.datetime64(date) for date in self.raw["dates"]]
 
         return self._dates
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-from datetime import datetime, timedelta
 from typing import List, Optional, Union, TYPE_CHECKING
 
 from numpy import iterable
 
 from ._holidays_data_provider import HolidaysData
 from .._base_request_items import StartEndDateBase
 from ..._enums import HolidayOutputs
@@ -12,17 +11,15 @@
 from .....delivery._data._data_provider import DataProviderLayer, BaseResponse
 
 if TYPE_CHECKING:
     from ....._types import ExtendedParams, OptStr, OptDateTime, OptStrStrs
 
 
 class HolidaysRequestItem(StartEndDateBase):
-    def __init__(
-        self, tag, start_date, end_date, calendars, currencies, holiday_outputs
-    ):
+    def __init__(self, tag, start_date, end_date, calendars, currencies, holiday_outputs):
         super().__init__(start_date, end_date)
         self.tag = tag
         self.calendars = calendars
         self.currencies = currencies
         self.holiday_outputs = holiday_outputs
 
     @property
@@ -102,31 +99,32 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform.
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform.
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import holidays
-     >>>
-     >>> definition = holidays.Definition(
-     ...   tag="my request",
-     ...   start_date=datetime.datetime(2020, 5, 2),
-     ...   end_date=datetime.timedelta(-30),
-     ...   calendars=["UKR", "FRA"],
-     ...   currencies=["EUR"],
-     ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
-     ... )
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import holidays
+    >>>
+    >>> definition = holidays.Definition(
+    ...   tag="my request",
+    ...   start_date=datetime.datetime(2020, 5, 2),
+    ...   end_date=datetime.timedelta(-30),
+    ...   calendars=["UKR", "FRA"],
+    ...   currencies=["EUR"],
+    ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.HolidaysDefinition"
 
     def __init__(
         self,
         start_date: "OptDateTime" = None,
@@ -181,41 +179,42 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import holidays
-     >>>
-     >>> first_definition = holidays.Definition(
-     ...   tag="my request",
-     ...   start_date=datetime.datetime(2020, 5, 2),
-     ...   end_date=datetime.timedelta(-30),
-     ...   calendars=["UKR", "FRA"],
-     ...   currencies=["EUR"],
-     ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
-     ... )
-     ...
-     >>>
-     >>> second_definition = holidays.Definition(
-     ...   tag="my second request",
-     ...   start_date="2020-01-01",
-     ...   end_date=datetime.timedelta(0),
-     ...   calendars=["UKR", "FRA"],
-     ...   currencies=["EUR"],
-     ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
-     ... )
-     >>> response = holidays.Definitions([first_definition, second_definition]).get_data()
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import holidays
+    >>>
+    >>> first_definition = holidays.Definition(
+    ...   tag="my request",
+    ...   start_date=datetime.datetime(2020, 5, 2),
+    ...   end_date=datetime.timedelta(-30),
+    ...   calendars=["UKR", "FRA"],
+    ...   currencies=["EUR"],
+    ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
+    ... )
+    ...
+    >>>
+    >>> second_definition = holidays.Definition(
+    ...   tag="my second request",
+    ...   start_date="2020-01-01",
+    ...   end_date=datetime.timedelta(0),
+    ...   calendars=["UKR", "FRA"],
+    ...   currencies=["EUR"],
+    ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
+    ... )
+    >>> response = holidays.Definitions([first_definition, second_definition]).get_data()
 
     Using get_data_async
-     >>> import asyncio
-     >>> task = holidays.Definitions([first_definition, second_definition]).get_data_async()
-     >>> response = asyncio.run(task)
+
+    >>> import asyncio
+    >>> task = holidays.Definitions([first_definition, second_definition]).get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.HolidaysDefinitions"
 
     def __init__(self, universe: "DefnDefns"):
         universe = try_copy_to_list(universe)
         if not iterable(universe):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/holidays/_holidays_data_provider.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,40 +1,37 @@
+from dataclasses import dataclass
 from typing import List
 
 import pandas as pd
 from dateutil import parser
-from pandas.tseries.holiday import nearest_workday, Holiday as PandasHoliday
+from pandas.tseries.holiday import Holiday as PandasHoliday, nearest_workday
 
 from .._content_data_validator import ContentDataValidator
 from .._request_factory import DatesAndCalendarsResponseFactory
 from ...._content_data import Data
 from ...._content_data_provider import ContentDataProvider
-from ....._tools import create_repr, add_periods_datetime_adapter
+from ....._tools import add_periods_datetime_adapter, create_repr
 from ....._types import OptDateTime
 from .....content.ipa._content_provider import DatesAndCalendarsRequestFactory
 from .....delivery._data._data_provider import ValidatorContainer
 
 
+@dataclass
 class HolidayName:
-    def __init__(self, name: str, calendars: list, countries: list):
-        self._name = name
-        self._calendars = calendars
-        self._countries = countries
+    name: str
+    calendars: list
+    countries: list
 
-    @property
-    def name(self):
-        return self._name
-
-    @property
-    def countries(self):
-        return self._countries
 
-    @property
-    def calendars(self):
-        return self._calendars
+def holiday_name_from_dict(datum: dict):
+    return HolidayName(
+        name=datum["name"],
+        calendars=datum["calendars"],
+        countries=datum["countries"],
+    )
 
 
 class Holiday(PandasHoliday):
     _holiday_names = None
 
     def __init__(
         self,
@@ -86,20 +83,15 @@
     def calendars(self):
         return self._calendars
 
     @property
     def names(self) -> List[HolidayName]:
         if self._holiday_names is None:
             self._holiday_names = [
-                HolidayName(
-                    name=holiday_name["name"],
-                    calendars=holiday_name["calendars"],
-                    countries=holiday_name["countries"],
-                )
-                for holiday_name in self._holiday.get("names", [])
+                holiday_name_from_dict(holiday_name) for holiday_name in self._holiday.get("names", [])
             ]
         return self._holiday_names
 
     @property
     def tag(self):
         return self._tag
 
@@ -107,16 +99,17 @@
         return create_repr(
             self,
             class_name="HolidayData",
             content="representation of 'holidayOutputs' response",
         )
 
 
+@dataclass
 class HolidaysData(Data):
-    _holidays = None
+    _holidays: List[Holiday] = None
 
     @property
     def holidays(self) -> List[Holiday]:
         if self._holidays is None:
             self._holidays = [
                 Holiday(holiday=holiday, tag=raw_item.get("tag"))
                 for raw_item in self.raw
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/_is_working_day.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/dates_and_calendars/is_working_day/_is_working_day.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,7 @@
-from datetime import datetime, timedelta
 from typing import List, Optional, Union, TYPE_CHECKING
 
 from numpy import iterable
 
 from ._is_working_day_data_provider import IsWorkingDay, IsWorkingDays
 from .._base_request_items import DateBase
 from ..._enums import HolidayOutputs
@@ -98,29 +97,30 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import is_working_day
-     >>>
-     >>> definition = is_working_day.Definition(
-     ...   tag="my request",
-     ...   date=datetime.timedelta(0),
-     ...   currencies=["EUR"],
-     ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
-     ... )
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import is_working_day
+    >>>
+    >>> definition = is_working_day.Definition(
+    ...   tag="my request",
+    ...   date=datetime.timedelta(0),
+    ...   currencies=["EUR"],
+    ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.IsWorkingDayDefinition"
 
     def __init__(
         self,
         date: "OptDateTime" = None,
@@ -173,37 +173,38 @@
     get_data(session=None, on_response=None, **kwargs)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, **kwargs)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> import datetime
-     >>> from refinitiv.data.content.ipa.dates_and_calendars import is_working_day
-     >>>
-     >>> first_definition = is_working_day.Definition(
-     ...   tag="my request",
-     ...   date=datetime.timedelta(0),
-     ...   currencies=["EUR"],
-     ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
-     ... )
-     ...
-     >>>
-     >>> second_definition = is_working_day.Definition(
-     ...   tag="my second request",
-     ...   date="2020-01-01",
-     ...   currencies=["EUR"],
-     ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
-     ... )
-     >>> response = is_working_day.Definitions([first_definition, second_definition]).get_data()
+    >>> import datetime
+    >>> from refinitiv.data.content.ipa.dates_and_calendars import is_working_day
+    >>>
+    >>> first_definition = is_working_day.Definition(
+    ...   tag="my request",
+    ...   date=datetime.timedelta(0),
+    ...   currencies=["EUR"],
+    ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
+    ... )
+    ...
+    >>>
+    >>> second_definition = is_working_day.Definition(
+    ...   tag="my second request",
+    ...   date="2020-01-01",
+    ...   currencies=["EUR"],
+    ...   holiday_outputs=["Date", "Names", "Calendars", "Countries"]
+    ... )
+    >>> response = is_working_day.Definitions([first_definition, second_definition]).get_data()
 
     Using get_data_async
-     >>> import asyncio
-     >>> task = is_working_day.Definitions([first_definition, second_definition]).get_data_async()
-     >>> response = asyncio.run(task)
+
+    >>> import asyncio
+    >>> task = is_working_day.Definitions([first_definition, second_definition]).get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     _USAGE_CLS_NAME = "IPA.DatesAndCalendars.IsWorkingDayDefinitions"
 
     def __init__(self, universe: "DefnDefns"):
         universe = try_copy_to_list(universe)
         if not iterable(universe):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_base_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_base_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_contracts_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_contracts_data_provider.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,22 +1,22 @@
+from dataclasses import dataclass
 from itertools import zip_longest
-from typing import TYPE_CHECKING, Any
+from typing import Any, TYPE_CHECKING
 
-import numpy
 import pandas as pd
 from pandas import DataFrame
 
 from ..._content_data import Data as ContentData
 from ..._content_data_provider import ContentDataProvider
 from ..._content_response_factory import ContentResponseFactory
 from ..._error_parser import ErrorParser
 from ...._tools import ArgsParser, fields_arg_parser, merge_dict_to_dict
 from ...._tools._dataframe import convert_df_columns_to_datetime_by_idx, convert_dtypes
-from ....delivery._data._data_provider import ContentValidator
 from ....delivery._data._data_provider import (
+    ContentValidator,
     RequestFactory,
     ValidatorContainer,
 )
 from ....delivery.endpoint_request import RequestMethod
 
 if TYPE_CHECKING:
     from ....delivery._data._data_provider import ParsedData
@@ -65,55 +65,48 @@
 
 # ---------------------------------------------------------------------------
 #   Content data
 # ---------------------------------------------------------------------------
 
 
 def convert_data_items_to_datetime(df: pd.DataFrame, headers: dict) -> pd.DataFrame:
-    columns_indexes = [
-        index
-        for index, header in enumerate(headers)
-        if header.get("type", "") in ["DateTime", "Date"]
-    ]
-
-    df = convert_df_columns_to_datetime_by_idx(
-        df, columns_indexes, utc=True, delete_tz=True
-    )
+    columns_indexes = [index for index, header in enumerate(headers) if header.get("type", "") in ["DateTime", "Date"]]
+
+    df = convert_df_columns_to_datetime_by_idx(df, columns_indexes, utc=True, delete_tz=True)
     return df
 
 
 def financial_contracts_build_df(raw: dict, **kwargs) -> pd.DataFrame:
     """
     Convert "data" from raw response bond to dataframe format
     """
     data = raw.get("data")
     headers = raw.get("headers")
     if data:
-        numpy_array = numpy.array(data, dtype=object)
-        df = DataFrame(numpy_array)
+        columns = [header["name"] for header in headers]
+        df = DataFrame(data, columns=columns)
         df = convert_data_items_to_datetime(df, headers)
-        df.columns = [header["name"] for header in headers]
         df = convert_dtypes(df)
     else:
         df = DataFrame()
     return df
 
 
+@dataclass
 class Data(ContentData):
     """
     This class is designed for storing and managing the response instrument data
     """
 
-    _analytics_headers = None
-    _analytics_data = None
-    _analytics_market_data = None
-    _analytics_statuses = None
+    _analytics_headers: Any = None
+    _analytics_data: Any = None
+    _analytics_market_data: Any = None
+    _analytics_statuses: Any = None
 
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
+    def __post_init__(self):
         if self.raw:
             #   get headers
             self._analytics_headers = self.raw.get("headers")
             #   get data
             self._analytics_data = self.raw.get("data")
             #   get marketData
             self._analytics_market_data = self.raw.get("marketData")
@@ -251,18 +244,15 @@
     return data
 
 
 def process_bond_instrument_code(code: Any) -> str:
     if code is None or isinstance(code, str):
         return code
     else:
-        raise ValueError(
-            f"Invalid type of instrument_code, string is expected."
-            f"type: {type(code)} is given"
-        )
+        raise ValueError(f"Invalid type of instrument_code, string is expected. type: {type(code)} is given")
 
 
 bond_instrument_code_arg_parser = ArgsParser(process_bond_instrument_code)
 
 # ---------------------------------------------------------------------------
 #   Data provider
 # ---------------------------------------------------------------------------
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_instrument_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_instrument_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,16 +7,15 @@
 
 class InstrumentDefinition(ObjectDefinition, abc.ABC):
     """
     This class is designed to represent
     the instrument definition templates for QPS request.
     """
 
-    @classmethod
-    def get_instrument_type(cls):
+    def get_instrument_type(self):
         return ""
 
     def __init__(self, instrument_tag=None, *args, **kwargs):
         super().__init__(*args, **kwargs)
         self.instrument_tag = instrument_tag
 
     @property
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_quantitative_data_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_quantitative_data_stream.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,17 +10,15 @@
 
 if TYPE_CHECKING:
     from ...._types import ExtendedParams
     from ...._core.session import Session
     from ....delivery._stream import StreamState
 
 
-class QuantitativeDataStream(
-    StreamStateManager, RDPStreamListener["QuantitativeDataStream"]
-):
+class QuantitativeDataStream(StreamStateManager, RDPStreamListener["QuantitativeDataStream"]):
     def __init__(
         self,
         universe: dict,
         session: "Session",
         fields: list = None,
         extended_params: "ExtendedParams" = None,
     ):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/_stream_facade.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/_stream_facade.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/__init__.py`

 * *Files 11% similar despite different names*

```diff
@@ -11,14 +11,15 @@
     "DayCountBasis",
     "Definition",
     "Direction",
     "DividendType",
     "Frequency",
     "IndexAverageMethod",
     "IndexCompoundingMethod",
+    "IndexObservationMethod",
     "InflationMode",
     "InterestType",
     "PriceSide",
     "PricingParameters",
     "ProjectedIndexCalculationMethod",
     "QuoteFallbackLogic",
     "RedemptionDateType",
@@ -28,38 +29,36 @@
     "VolatilityTermStructureType",
     "VolatilityType",
     "YieldType",
 )
 
 from ._bond_pricing_parameters import PricingParameters
 from ._definition import Definition
-from ._enums import (
+from ..._enums import (
+    AdjustInterestToPaymentDate,
     AmortizationFrequency,
     AmortizationType,
     BenchmarkYieldSelectionMode,
     BusinessDayConvention,
     CreditSpreadType,
     DateRollingConvention,
     DayCountBasis,
+    Direction,
     DividendType,
     Frequency,
     IndexAverageMethod,
+    IndexCompoundingMethod,
+    IndexObservationMethod,
     InflationMode,
+    InterestType,
     PriceSide,
     ProjectedIndexCalculationMethod,
     QuoteFallbackLogic,
     RedemptionDateType,
     Rounding,
     RoundingType,
+    StubRule,
     VolatilityTermStructureType,
     VolatilityType,
     YieldType,
 )
-from ._models import BondRoundingParameters
-from ..._enums import (
-    AdjustInterestToPaymentDate,
-    Direction,
-    IndexCompoundingMethod,
-    InterestType,
-    StubRule,
-)
-from ..._models import AmortizationItem
+from ..._models import BondRoundingParameters, AmortizationItem
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_bond_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_swap_leg_definition.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,715 +1,782 @@
 # coding: utf8
 
-from typing import Optional
 
-from .._instrument_definition import InstrumentDefinition
-from ._enums import (
-    DateRollingConvention,
-    DayCountBasis,
-    InterestType,
-    StubRule,
-    Frequency,
+from typing import Optional, List, Union
+
+from ..._enums import (
     AdjustInterestToPaymentDate,
-    IndexCompoundingMethod,
     BusinessDayConvention,
+    DateRollingConvention,
+    DayCountBasis,
     Direction,
+    Frequency,
     IndexAverageMethod,
+    IndexCompoundingMethod,
+    IndexObservationMethod,
+    IndexResetType,
+    IndexSpreadCompoundingMethod,
+    InterestCalculationConvention,
+    InterestType,
+    NotionalExchange,
+    StubRule,
+    PriceSide,
 )
-from ._models import AmortizationItem
+from ..._models import AmortizationItem
+from ..._object_definition import ObjectDefinition
+from ....._tools import try_copy_to_list
 
 
-class BondInstrumentDefinition(InstrumentDefinition):
+class LegDefinition(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
     instrument_tag : str, optional
-        User defined string to identify the instrument.It can be used to link output
-        results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
-        characters are supported. Optional.
-    instrument_code : str, optional
-        Code to define the bond instrument. It can be an ISIN, a RIC, a CUSIP or an
-        AssetId .
-    end_date : str, optional
-        Maturity date of the bond to override. Mandatory if instrument code has not been
-        defined and is_perpetual flag has been set to false. In case an instrument code
-        has been defined, value comes from bond reference data.
-    direction : Direction, optional
-        The direction of the leg. the possible values are:
-        - 'Paid' (the cash flows of the leg are paid to the counterparty),
-        - 'Received' (the cash flows of the leg are received from the counterparty).
-          Optional for a single leg instrument (like a bond), in that case default value
-          is Received. It is mandatory for a multi-instrument leg instrument (like Swap
-          or CDS leg).
-    interest_type : InterestType, optional
-        A flag that indicates whether the leg is fixed or float. Possible values are:
-        - 'Fixed' (the leg has a fixed coupon),
-        - 'Float' (the leg has a floating rate index). Mandatory.
+
+    leg_tag : str, optional
+        A user-defined string to identify the direction of the leg: 'paid' or
+        'received'. optional. no default value applies.
+    direction : Direction or str, optional
+        The indication whether the cash flows of the instrument's leg are paid or
+        received. the possible values are:   paid: the cash flows are paid to the
+        counterparty,   received: the cash flows are received from the counterparty.  no
+        default value applies.
+    interest_type : InterestType or str, optional
+        An indicator whether the instrument pays a fixed or floating interest. the
+        possible values are: fixed, float. no default value applies.
     notional_ccy : str, optional
-        The ISO code of the notional currency. Mandatory if instrument code or
-        instrument style has not been defined. In case an instrument code/style has been
-        defined, value may comes from the reference data.
+        The currency of the instrument's notional amount. the value is expressed in iso
+        4217 alphabetical format (e.g. 'usd'). no default value applies.
     notional_amount : float, optional
-        The notional amount of the leg at the period start date. Optional. By default
-        1,000,000 is used.
+        The notional amount of the instrument's leg. the default value is '1,000,000'.
     fixed_rate_percent : float, optional
-        The fixed coupon rate in percentage. It is mandatory in case of a single leg
-        instrument. Otherwise, in case of multi leg instrument, it can be computed as
-        the Par rate.
+        The interest rate of the instrument. the value is expressed in percentages.
+        mandatory if no instrumentcode is defined. if instrumentcode is defined, the
+        value comes from the instrument reference data.
+    index_name : str, optional
+        The name of the floating rate index (e.g. 'euribor'). no default value applies.
+    index_tenor : str, optional
+        The period code indicating the maturity of the floating rate index. the default
+        value is the tenor equivalent toindexresetfrequency or interestpaymentfrequency.
     spread_bp : float, optional
-        The spread in basis point that is added to the floating rate index index value.
-        Optional. By default, 0 is used.
-    interest_payment_frequency : Frequency, optional
-        The frequency of the interest payments. Optional if an instrument code/style
-        have been defined : in that case, value comes from reference data. Otherwise, it
-        is mandatory.
-    interest_calculation_method : DayCountBasis, optional
-        The Day Count Basis method used to calculate the coupon interest payments.
-        Mandatory.
-    accrued_calculation_method : DayCountBasis, optional
-        The Day Count Basis method used to calculate the accrued interest payments.
-        Optional. By default, the same value than interest_calculation_method is used.
-    payment_business_day_convention : BusinessDayConvention, optional
-        The method to adjust dates to a working day. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. In case an instrument code/style has been defined,
-          value comes from bond reference data. Otherwise 'ModifiedFollowing' is used.
-    payment_roll_convention : DateRollingConvention, optional
-        Method to adjust payment dates when they fall at the end of the month (28th of
-        February, 30th, 31st). The possible values are:
-        - Last (For setting the calculated date to the last working day),
-        - Same (For setting the calculated date to the same day . In this latter case,
-          the date may be moved according to the date moving convention if it is a
-          non-working day),
-        - Last28 (For setting the calculated date to the last working day. 28FEB being
-          always considered as the last working day),
-        - Same28 (For setting the calculated date to the same day .28FEB being always
-          considered as the last working day). Optional. In case an instrument code has
-          been defined, value comes from bond reference data. Otherwise, 'SameDay' is
-          used.
-    index_reset_frequency : Frequency, optional
-        The reset frequency in case the leg Type is Float. Optional. By default the
-        IndexTenor is used.
+        The interest spread in basis points that is added to the floating rate index
+        value. optional. if instrumentcode is defined, the value comes from the
+        instrument reference data. in case of a user-defined instrument, the default
+        value is '0'.
+    interest_payment_frequency : Frequency or str, optional
+        The frequency of the interest payment. either indexresetfrequency or
+        interestpaymentfrequency must be provided (e.g. annual, semiannual).   the
+        default value is indexresetfrequency.
+    interest_calculation_method : DayCountBasis or str, optional
+        The day count basis method used to calculate the interest payments(e.g.
+        dcb_30_360, dcb_30_actual). the default value is selected based onnotionalccy.
+    accrued_calculation_method : DayCountBasis or str, optional
+        The day count basis method used to calculate the accrued interest payments (e.g.
+        dcb_30_360, dcb_30_actual).   if instrumentcode is defined, the value comes from
+        the instrument reference data. in case of a user-defined instrument,
+        interestcalculationmethod is used.
+    payment_business_day_convention : BusinessDayConvention or str, optional
+        The method to adjust dates to working days. the possible values are:
+        previousbusinessday,    nextbusinessday,    modified following,    nomoving,
+        bbswmodifiedfollowing.   if instrumentcode is defined, the value comes from the
+        instrument reference data. in case of a user-defined instrument, the default
+        value is'modifiedfollowing'.
+    payment_roll_convention : DateRollingConvention or str, optional
+        The method to adjust payment dates when they fall at the end of the month (e.g.
+        28th of february, 30th, 31st). the possible values are:    last,    same,
+        last28,    same28.   if instrumentcode is defined, the value comes from the
+        instrument reference data. in case of a user-defined instrument, the default
+        value is'same'.
+    index_reset_frequency : Frequency or str, optional
+        The reset frequency for the floating instrument (e.g. annual, semiannual).   the
+        default value is interestpaymentfrequency.
+    index_reset_type : IndexResetType or str, optional
+        A type indicating if the floating rate index is reset before the coupon period
+        starts or at the end of the coupon period. the possible values are: inadvance:
+        resets the index before the start of the interest period, inarrears: resets the
+        index at the end of the interest period. the default value is 'inadvance'.
     index_fixing_lag : int, optional
-        Defines the number of working days between the fixing date and the start of the
-        coupon period ('InAdvance') or the end of the coupon period ('InArrears').
-        Optional. By default 0 is used.
+        The number of working daysbetween the fixing date and the start of the coupon
+        period ('inadvance') or the end of the coupon period ('inarrears'). the
+        inadvance/inarrears mode is set in the indexresettype parameter. the default
+        value is the fixing lag associated to the index defined/determined by default on
+        the floating instrument.
     first_regular_payment_date : str, optional
-        The first regular coupon payment date for leg with an odd first coupon.
-        Optional.
+        The first regular interest payment date used for the odd first interest period.
+        the value is expressed in iso 8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g.
+        2021-01-01t00:00:00z). no default value applies.
     last_regular_payment_date : str, optional
-        The last regular coupon payment date for leg with an odd last coupon. Optional.
+        The last regular interest payment date used for the odd last interest period.
+        the value is expressed in iso 8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g.
+        2021-01-01t00:00:00z). no default value applies.
     amortization_schedule : AmortizationItem, optional
-        Definition of amortizations
+        The amortization schedule of the instrument. it contains the following
+        information:   startdate,   enddate,   remainingnotional,
+        amortizationfrequency,   amount,   amortizationtype.  optional. no default value
+        applies.
     payment_business_days : str, optional
-        A list of coma-separated calendar codes to adjust dates (e.g. 'EMU' or 'USA').
-        Optional. By default the calendar associated to notional_ccy is used.
-    adjust_interest_to_payment_date : AdjustInterestToPaymentDate, optional
-        A flag that indicates if the coupon dates are adjusted to the payment dates.
-        Optional. By default 'false' is used.
-    index_compounding_method : IndexCompoundingMethod, optional
-        A flag that defines how the coupon rate is calculated from the reset floating
-        rates when the reset frequency is higher than the interest payment frequency
-        (e.g. daily index reset with quarterly interest payment). The possible values
-        are:
-        - Compounded (uses the compounded average rate from multiple fixings),
-        - Average (uses the arithmetic average rate from multiple fixings),
-        - Constant (uses the last published rate among multiple fixings),
-        - AdjustedCompounded (uses Chinese 7-day repo fixing),
-        - MexicanCompounded (uses Mexican Bremse fixing). Optional. By default
-          'Constant' is used.
+        A list of comma-separated calendar codes to adjust dates (e.g. 'emu' or 'usa').
+        the default value is the calendar associated to the market conventions of the
+        interestpaymentccy for the corresponding leg.
+    notional_exchange : NotionalExchange or str, optional
+        An indicator if the notional amount is exchanged and when it is exchanged. the
+        possible values are:    none,    start,    end,    both,    endadjustment.   the
+        default value is 'none'.
+    adjust_interest_to_payment_date : AdjustInterestToPaymentDate or str, optional
+        An indication if the coupon dates are adjusted to the payment dates. the
+        possible values are:   adjusted,   unadjusted.  if instrumentcode is defined,
+        the value comes from the instrument reference data. in case of a user-defined
+        instrument, the default value is 'adjusted'.
+    index_compounding_method : IndexCompoundingMethod or str, optional
+        The method how the interest rate is calculated from the reset floating rates
+        when the reset frequency is higher than the interest payment frequency (e.g.
+        daily index reset with quarterly interest payments). the possible values are:
+        compounded, average, constant, adjustedcompounded, mexicancompounded. if
+        instrumentcode is defined, the value comes from the instrument reference data.
+        in case of a user-defined instrument, the default value is 'constant'.
     interest_payment_delay : int, optional
-        The number of working days between the end of coupon period and the actual
-        interest payment date. Optional. By default no delay (0) is applied.
-    stub_rule : StubRule, optional
-        The rule that defines whether coupon roll dates are aligned on the  maturity or
-        the issue date.  The possible values are:
-        - ShortFirstProRata (to create a short period between the start date and the
-          first coupon date, and pay a smaller amount of interest for the short
-          period.All coupon dates are calculated backward from the maturity date),
-        - ShortFirstFull (to create a short period between the start date and the first
-          coupon date, and pay a regular coupon on the first coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - LongFirstFull (to create a long period between the start date and the second
-          coupon date, and pay a regular coupon on the second coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - ShortLastProRata (to create a short period between the last payment date and
-          maturity, and pay a smaller amount of interest for the short period. All
-          coupon dates are calculated forward from the start date). This property may
-          also be used in conjunction with first_regular_payment_date and
-          last_regular_payment_date; in that case the following values can be defined:
-        - Issue (all dates are aligned on the issue date),
-        - Maturity (all dates are aligned on the maturity date). Optional. By default
-          'Maturity' is used.
-    issue_date : str, optional
-        Date of issuance of the bond to override. Mandatory if instrument code has not
-        been defined. In case an instrument code has been defined, value comes from bond
-        reference data.
-    first_accrual_date : str, optional
-        Date at which bond starts accruing. Optional. In case an instrument code has
-        been defined, value comes from bond reference data. Otherwise default value is
-        the issue date of the bond.
+        The number of working days between the end of the interest accrual period and
+        the interest payment date. by default, no delay (0) is applied.
+    stub_rule : StubRule or str, optional
+        The rule that defines whether coupon roll dates are aligned to the maturity or
+        issue date. the possible values are:   issue,   maturity,   shortfirstprorata,
+        shortfirstfull,   longfirstfull,   shortlastprorata.  the default value is
+        'maturity'.
+    index_average_method : IndexAverageMethod or str, optional
+        The value of the average index calculation method. the possible values are:
+        compoundedactual,      dailycompoundedaverage,      compoundedaveragerate,
+        arithmeticaverage
+    index_observation_method : IndexObservationMethod or str, optional
+        (rfr) method for determining the accrual observation period. the possible values
+        are:      lookback: use the interest period for both rate accrual and interest
+        payment.      periodshift: use the observation period for both rate accrual and
+        interest payment.      mixed: use the observation period for rate accrual and
+        the interest period for interest payment.
+    index_spread_compounding_method : IndexSpreadCompoundingMethod or str, optional
+        The method defining how the computed float leg spread is applied to compounded
+        rate. it applies only when indexcompoundingmethod= compounded. the possible
+        values are:    isdacompounding,    nocompounding,    isdaflatcompounding.  the
+        default value is 'isdacompounding'.
+    interest_calculation_convention : InterestCalculationConvention or str, optional
+        The day count basis method convention used to calculate the interest payments.
+        optional. defaults to moneymarket. if instrumentcode is defined, the value comes
+        from the instrument reference data.
+    cms_template : str, optional
+        A reference to a common swap contract that represents the underlying swap in
+        case of a constant maturity swap contract (cms). example: eur_ab6e. no default
+        value applies.
+    floor_strike_percent : float, optional
+        The contractual strike rate of the floor. the value is expressed in percentages.
+        if this parameter is set, the floor will apply to the leg with the same
+        parameters set in the swaplegdefinition (e.g.maturity, frequency, index,
+        discounting rule). no default value applies.
     index_fixing_ric : str, optional
-        The RIC that carries the fixing value. This value overrides the RIC associated
-        by default with the IndexName and IndexTenor. Optional.
-    is_perpetual : bool, optional
-        Flag the defines wether the bond is perpetual or not in case of user defined
-        bond. Optional. In case an instrument code has been defined, value comes from
-        bond reference data. In case of user defined bond, default value is 'false'.
-    template : str, optional
-        A reference to a Adfin instrument contract or the Adfin detailed contract.
-        Optional. Either instrument_code, template, or full definition must be provided.
+        The ric that carries the fixing value if the instrument has a floating interest.
+        optional. mandatory for floating rate instruments if no instrumentcode is
+        defined. if instrumentcode is defined, the value comes from the instrument
+        reference data. no default value applies.
+    upfront_amount : float, optional
+        The amount which represents the net present value of the swap. it is computed as
+        [(100  dirtypricepercent / 100) x notionalamount]. the value is expressed in
+        upfrontamountccy. by default, no payment (0) applies.
+    index_price_side : PriceSide or str, optional
+        The side that is selected for an index supporting Bid/Ask/Mid (which is the case of deposits).
+    fixed_rate_percent_schedule : dict, optional
+        The step structure: a list of pre-determined future coupon rates indexed by their dates.
+        Either fixedRatePercent or fixedRatePercentSchedule is used.
+        No default value applies.
     """
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
-        instrument_code: Optional[str] = None,
-        end_date: Optional[str] = None,
-        direction: Optional[Direction] = None,
-        interest_type: Optional[InterestType] = None,
+        leg_tag: Optional[str] = None,
+        direction: Union[Direction, str] = None,
+        interest_type: Union[InterestType, str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
         fixed_rate_percent: Optional[float] = None,
+        index_name: Optional[str] = None,
+        index_tenor: Optional[str] = None,
         spread_bp: Optional[float] = None,
-        interest_payment_frequency: Optional[Frequency] = None,
-        interest_calculation_method: Optional[DayCountBasis] = None,
-        accrued_calculation_method: Optional[DayCountBasis] = None,
-        payment_business_day_convention: Optional[BusinessDayConvention] = None,
-        payment_roll_convention: Optional[DateRollingConvention] = None,
-        index_reset_frequency: Optional[Frequency] = None,
+        interest_payment_frequency: Union[Frequency, str] = None,
+        interest_calculation_method: Union[DayCountBasis, str] = None,
+        accrued_calculation_method: Union[DayCountBasis, str] = None,
+        payment_business_day_convention: Union[BusinessDayConvention, str] = None,
+        payment_roll_convention: Union[DateRollingConvention, str] = None,
+        index_reset_frequency: Union[Frequency, str] = None,
+        index_reset_type: Union[IndexResetType, str] = None,
         index_fixing_lag: Optional[int] = None,
         first_regular_payment_date: Optional[str] = None,
         last_regular_payment_date: Optional[str] = None,
-        amortization_schedule: Optional[AmortizationItem] = None,
+        amortization_schedule: Optional[List[AmortizationItem]] = None,
         payment_business_days: Optional[str] = None,
-        adjust_interest_to_payment_date: Optional[AdjustInterestToPaymentDate] = None,
-        index_compounding_method: Optional[IndexCompoundingMethod] = None,
+        notional_exchange: Union[NotionalExchange, str] = None,
+        adjust_interest_to_payment_date: Union[AdjustInterestToPaymentDate, str] = None,
+        index_compounding_method: Union[IndexCompoundingMethod, str] = None,
         interest_payment_delay: Optional[int] = None,
-        stub_rule: Optional[StubRule] = None,
-        issue_date: Optional[str] = None,
-        index_average_method: Optional[IndexAverageMethod] = None,
-        first_accrual_date: Optional[str] = None,
+        stub_rule: Union[StubRule, str] = None,
+        index_average_method: Union[IndexAverageMethod, str] = None,
+        index_observation_method: Union[IndexObservationMethod, str] = None,
+        index_spread_compounding_method: Union[IndexSpreadCompoundingMethod, str] = None,
+        interest_calculation_convention: Union[InterestCalculationConvention, str] = None,
+        cms_template: Optional[str] = None,
         floor_strike_percent: Optional[float] = None,
         index_fixing_ric: Optional[str] = None,
-        is_perpetual: Optional[bool] = None,
-        template: Optional[str] = None,
+        upfront_amount: Optional[float] = None,
+        index_price_side: Union[PriceSide, str] = None,
+        fixed_rate_percent_schedule: Optional[dict] = None,
     ) -> None:
         super().__init__()
         self.instrument_tag = instrument_tag
-        self.instrument_code = instrument_code
-        self.end_date = end_date
+        self.leg_tag = leg_tag
         self.direction = direction
         self.interest_type = interest_type
         self.notional_ccy = notional_ccy
         self.notional_amount = notional_amount
         self.fixed_rate_percent = fixed_rate_percent
+        self.index_name = index_name
+        self.index_tenor = index_tenor
         self.spread_bp = spread_bp
         self.interest_payment_frequency = interest_payment_frequency
         self.interest_calculation_method = interest_calculation_method
         self.accrued_calculation_method = accrued_calculation_method
         self.payment_business_day_convention = payment_business_day_convention
         self.payment_roll_convention = payment_roll_convention
         self.index_reset_frequency = index_reset_frequency
+        self.index_reset_type = index_reset_type
         self.index_fixing_lag = index_fixing_lag
         self.first_regular_payment_date = first_regular_payment_date
         self.last_regular_payment_date = last_regular_payment_date
-        self.amortization_schedule = amortization_schedule
+        self.amortization_schedule = try_copy_to_list(amortization_schedule)
         self.payment_business_days = payment_business_days
+        self.notional_exchange = notional_exchange
         self.adjust_interest_to_payment_date = adjust_interest_to_payment_date
         self.index_compounding_method = index_compounding_method
         self.interest_payment_delay = interest_payment_delay
         self.stub_rule = stub_rule
-        self.issue_date = issue_date
         self.index_average_method = index_average_method
-        self.first_accrual_date = first_accrual_date
+        self.index_observation_method = index_observation_method
+        self.index_spread_compounding_method = index_spread_compounding_method
+        self.interest_calculation_convention = interest_calculation_convention
+        self.cms_template = cms_template
         self.floor_strike_percent = floor_strike_percent
         self.index_fixing_ric = index_fixing_ric
-        self.is_perpetual = is_perpetual
-        self.template = template
-
-    @classmethod
-    def get_instrument_type(cls):
-        return "Bond"
+        self.upfront_amount = upfront_amount
+        self.index_price_side = index_price_side
+        self.fixed_rate_percent_schedule = fixed_rate_percent_schedule
 
     @property
     def accrued_calculation_method(self):
         """
-        The Day Count Basis method used to calculate the accrued interest payments.
-        Optional. By default, the same value than InterestCalculationMethod is used.
+        The day count basis method used to calculate the accrued interest payments (e.g.
+        dcb_30_360, dcb_30_actual).   if instrumentcode is defined, the value comes from
+        the instrument reference data. in case of a user-defined instrument,
+        interestcalculationmethod is used.
         :return: enum DayCountBasis
         """
         return self._get_enum_parameter(DayCountBasis, "accruedCalculationMethod")
 
     @accrued_calculation_method.setter
     def accrued_calculation_method(self, value):
         self._set_enum_parameter(DayCountBasis, "accruedCalculationMethod", value)
 
     @property
     def adjust_interest_to_payment_date(self):
         """
-        A flag that indicates if the coupon dates are adjusted to the payment dates.
-        Optional. By default 'false' is used.
+        An indication if the coupon dates are adjusted to the payment dates. the
+        possible values are:   adjusted,   unadjusted.  if instrumentcode is defined,
+        the value comes from the instrument reference data. in case of a user-defined
+        instrument, the default value is 'adjusted'.
         :return: enum AdjustInterestToPaymentDate
         """
-        return self._get_enum_parameter(
-            AdjustInterestToPaymentDate, "adjustInterestToPaymentDate"
-        )
+        return self._get_enum_parameter(AdjustInterestToPaymentDate, "adjustInterestToPaymentDate")
 
     @adjust_interest_to_payment_date.setter
     def adjust_interest_to_payment_date(self, value):
-        self._set_enum_parameter(
-            AdjustInterestToPaymentDate, "adjustInterestToPaymentDate", value
-        )
+        self._set_enum_parameter(AdjustInterestToPaymentDate, "adjustInterestToPaymentDate", value)
 
     @property
     def amortization_schedule(self):
         """
-        Definition of amortizations
+        The amortization schedule of the instrument. it contains the following
+        information:   startdate,   enddate,   remainingnotional,
+        amortizationfrequency,   amount,   amortizationtype.  optional. no default value
+        applies.
         :return: list AmortizationItem
         """
         return self._get_list_parameter(AmortizationItem, "amortizationSchedule")
 
     @amortization_schedule.setter
     def amortization_schedule(self, value):
         self._set_list_parameter(AmortizationItem, "amortizationSchedule", value)
 
     @property
     def direction(self):
         """
-        The direction of the leg. the possible values are:
-        - 'Paid' (the cash flows of the leg are paid to the counterparty),
-        - 'Received' (the cash flows of the leg are received from the counterparty).
-          Optional for a single leg instrument (like a bond), in that case default value
-          is Received. It is mandatory for a multi-instrument leg instrument (like Swap
-          or CDS leg).
+        The indication whether the cash flows of the instrument's leg are paid or
+        received. the possible values are:   paid: the cash flows are paid to the
+        counterparty,   received: the cash flows are received from the counterparty.  no
+        default value applies.
         :return: enum Direction
         """
         return self._get_enum_parameter(Direction, "direction")
 
     @direction.setter
     def direction(self, value):
         self._set_enum_parameter(Direction, "direction", value)
 
     @property
     def index_average_method(self):
         """
+        The value of the average index calculation method. the possible values are:
+        compoundedactual,      dailycompoundedaverage,      compoundedaveragerate,
+        arithmeticaverage
         :return: enum IndexAverageMethod
         """
         return self._get_enum_parameter(IndexAverageMethod, "indexAverageMethod")
 
     @index_average_method.setter
     def index_average_method(self, value):
         self._set_enum_parameter(IndexAverageMethod, "indexAverageMethod", value)
 
     @property
     def index_compounding_method(self):
         """
-        A flag that defines how the coupon rate is calculated from the reset floating
-        rates when the reset frequency is higher than the interest payment frequency
-        (e.g. daily index reset with quarterly interest payment). The possible values
-        are:
-        - Compounded (uses the compounded average rate from multiple fixings),
-        - Average (uses the arithmetic average rate from multiple fixings),
-        - Constant (uses the last published rate among multiple fixings),
-        - AdjustedCompounded (uses Chinese 7-day repo fixing),
-        - MexicanCompounded (uses Mexican Bremse fixing). Optional. By default
-          'Constant' is used.
+        The method how the interest rate is calculated from the reset floating rates
+        when the reset frequency is higher than the interest payment frequency (e.g.
+        daily index reset with quarterly interest payments). the possible values are:
+        compounded, average, constant, adjustedcompounded, mexicancompounded. if
+        instrumentcode is defined, the value comes from the instrument reference data.
+        in case of a user-defined instrument, the default value is 'constant'.
         :return: enum IndexCompoundingMethod
         """
-        return self._get_enum_parameter(
-            IndexCompoundingMethod, "indexCompoundingMethod"
-        )
+        return self._get_enum_parameter(IndexCompoundingMethod, "indexCompoundingMethod")
 
     @index_compounding_method.setter
     def index_compounding_method(self, value):
-        self._set_enum_parameter(
-            IndexCompoundingMethod, "indexCompoundingMethod", value
-        )
+        self._set_enum_parameter(IndexCompoundingMethod, "indexCompoundingMethod", value)
+
+    @property
+    def index_observation_method(self):
+        """
+        (rfr) method for determining the accrual observation period. the possible values
+        are:      lookback: use the interest period for both rate accrual and interest
+        payment.      periodshift: use the observation period for both rate accrual and
+        interest payment.      mixed: use the observation period for rate accrual and
+        the interest period for interest payment.
+        :return: enum IndexObservationMethod
+        """
+        return self._get_enum_parameter(IndexObservationMethod, "indexObservationMethod")
+
+    @index_observation_method.setter
+    def index_observation_method(self, value):
+        self._set_enum_parameter(IndexObservationMethod, "indexObservationMethod", value)
 
     @property
     def index_reset_frequency(self):
         """
-        The reset frequency in case the leg Type is Float. Optional. By default the
-        IndexTenor is used.
+        The reset frequency for the floating instrument (e.g. annual, semiannual).   the
+        default value is interestpaymentfrequency.
         :return: enum Frequency
         """
         return self._get_enum_parameter(Frequency, "indexResetFrequency")
 
     @index_reset_frequency.setter
     def index_reset_frequency(self, value):
         self._set_enum_parameter(Frequency, "indexResetFrequency", value)
 
     @property
+    def index_reset_type(self):
+        """
+        A type indicating if the floating rate index is reset before the coupon period
+        starts or at the end of the coupon period. the possible values are: inadvance:
+        resets the index before the start of the interest period, inarrears: resets the
+        index at the end of the interest period. the default value is 'inadvance'.
+        :return: enum IndexResetType
+        """
+        return self._get_enum_parameter(IndexResetType, "indexResetType")
+
+    @index_reset_type.setter
+    def index_reset_type(self, value):
+        self._set_enum_parameter(IndexResetType, "indexResetType", value)
+
+    @property
+    def index_spread_compounding_method(self):
+        """
+        The method defining how the computed float leg spread is applied to compounded
+        rate. it applies only when indexcompoundingmethod= compounded. the possible
+        values are:    isdacompounding,    nocompounding,    isdaflatcompounding.  the
+        default value is 'isdacompounding'.
+        :return: enum IndexSpreadCompoundingMethod
+        """
+        return self._get_enum_parameter(IndexSpreadCompoundingMethod, "indexSpreadCompoundingMethod")
+
+    @index_spread_compounding_method.setter
+    def index_spread_compounding_method(self, value):
+        self._set_enum_parameter(IndexSpreadCompoundingMethod, "indexSpreadCompoundingMethod", value)
+
+    @property
+    def interest_calculation_convention(self):
+        """
+        The day count basis method convention used to calculate the interest payments.
+        optional. defaults to moneymarket. if instrumentcode is defined, the value comes
+        from the instrument reference data.
+        :return: enum InterestCalculationConvention
+        """
+        return self._get_enum_parameter(InterestCalculationConvention, "interestCalculationConvention")
+
+    @interest_calculation_convention.setter
+    def interest_calculation_convention(self, value):
+        self._set_enum_parameter(InterestCalculationConvention, "interestCalculationConvention", value)
+
+    @property
     def interest_calculation_method(self):
         """
-        The Day Count Basis method used to calculate the coupon interest payments.
-        Mandatory.
+        The day count basis method used to calculate the interest payments(e.g.
+        dcb_30_360, dcb_30_actual). the default value is selected based onnotionalccy.
         :return: enum DayCountBasis
         """
         return self._get_enum_parameter(DayCountBasis, "interestCalculationMethod")
 
     @interest_calculation_method.setter
     def interest_calculation_method(self, value):
         self._set_enum_parameter(DayCountBasis, "interestCalculationMethod", value)
 
     @property
     def interest_payment_frequency(self):
         """
-        The frequency of the interest payments. Optional if an instrument code/style
-        have been defined : in that case, value comes from reference data. Otherwise, it
-        is mandatory.
+        The frequency of the interest payment. either indexresetfrequency or
+        interestpaymentfrequency must be provided (e.g. annual, semiannual).   the
+        default value is indexresetfrequency.
         :return: enum Frequency
         """
         return self._get_enum_parameter(Frequency, "interestPaymentFrequency")
 
     @interest_payment_frequency.setter
     def interest_payment_frequency(self, value):
         self._set_enum_parameter(Frequency, "interestPaymentFrequency", value)
 
     @property
     def interest_type(self):
         """
-        A flag that indicates whether the leg is fixed or float. Possible values are:
-        - 'Fixed' (the leg has a fixed coupon),
-        - 'Float' (the leg has a floating rate index). Mandatory.
+        An indicator whether the instrument pays a fixed or floating interest. the
+        possible values are: fixed, float. no default value applies.
         :return: enum InterestType
         """
         return self._get_enum_parameter(InterestType, "interestType")
 
     @interest_type.setter
     def interest_type(self, value):
         self._set_enum_parameter(InterestType, "interestType", value)
 
     @property
+    def notional_exchange(self):
+        """
+        An indicator if the notional amount is exchanged and when it is exchanged. the
+        possible values are:    none,    start,    end,    both,    endadjustment.   the
+        default value is 'none'.
+        :return: enum NotionalExchange
+        """
+        return self._get_enum_parameter(NotionalExchange, "notionalExchange")
+
+    @notional_exchange.setter
+    def notional_exchange(self, value):
+        self._set_enum_parameter(NotionalExchange, "notionalExchange", value)
+
+    @property
     def payment_business_day_convention(self):
         """
-        The method to adjust dates to a working day. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. In case an instrument code/style has been defined,
-          value comes from bond reference data. Otherwise 'ModifiedFollowing' is used.
+        The method to adjust dates to working days. the possible values are:
+        previousbusinessday,    nextbusinessday,    modified following,    nomoving,
+        bbswmodifiedfollowing.   if instrumentcode is defined, the value comes from the
+        instrument reference data. in case of a user-defined instrument, the default
+        value is'modifiedfollowing'.
         :return: enum BusinessDayConvention
         """
-        return self._get_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention"
-        )
+        return self._get_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention")
 
     @payment_business_day_convention.setter
     def payment_business_day_convention(self, value):
-        self._set_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention", value
-        )
+        self._set_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention", value)
 
     @property
     def payment_roll_convention(self):
         """
-        Method to adjust payment dates when they fall at the end of the month (28th of
-        February, 30th, 31st). The possible values are:
-        - Last (For setting the calculated date to the last working day),
-        - Same (For setting the calculated date to the same day . In this latter case,
-          the date may be moved according to the date moving convention if it is a
-          non-working day),
-        - Last28 (For setting the calculated date to the last working day. 28FEB being
-          always considered as the last working day),
-        - Same28 (For setting the calculated date to the same day .28FEB being always
-          considered as the last working day). Optional. In case an instrument code has
-          been defined, value comes from bond reference data. Otherwise, 'SameDay' is
-          used.
+        The method to adjust payment dates when they fall at the end of the month (e.g.
+        28th of february, 30th, 31st). the possible values are:    last,    same,
+        last28,    same28.   if instrumentcode is defined, the value comes from the
+        instrument reference data. in case of a user-defined instrument, the default
+        value is'same'.
         :return: enum DateRollingConvention
         """
         return self._get_enum_parameter(DateRollingConvention, "paymentRollConvention")
 
     @payment_roll_convention.setter
     def payment_roll_convention(self, value):
         self._set_enum_parameter(DateRollingConvention, "paymentRollConvention", value)
 
     @property
     def stub_rule(self):
         """
-        The rule that defines whether coupon roll dates are aligned on the  maturity or
-        the issue date.  The possible values are:
-        - ShortFirstProRata (to create a short period between the start date and the
-          first coupon date, and pay a smaller amount of interest for the short
-          period.All coupon dates are calculated backward from the maturity date),
-        - ShortFirstFull (to create a short period between the start date and the first
-          coupon date, and pay a regular coupon on the first coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - LongFirstFull (to create a long period between the start date and the second
-          coupon date, and pay a regular coupon on the second coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - ShortLastProRata (to create a short period between the last payment date and
-          maturity, and pay a smaller amount of interest for the short period. All
-          coupon dates are calculated forward from the start date). This property may
-          also be used in conjunction with firstRegularPaymentDate and
-          lastRegularPaymentDate; in that case the following values can be defined:
-        - Issue (all dates are aligned on the issue date),
-        - Maturity (all dates are aligned on the maturity date). Optional. By default
-          'Maturity' is used.
+        The rule that defines whether coupon roll dates are aligned to the maturity or
+        issue date. the possible values are:   issue,   maturity,   shortfirstprorata,
+        shortfirstfull,   longfirstfull,   shortlastprorata.  the default value is
+        'maturity'.
         :return: enum StubRule
         """
         return self._get_enum_parameter(StubRule, "stubRule")
 
     @stub_rule.setter
     def stub_rule(self, value):
         self._set_enum_parameter(StubRule, "stubRule", value)
 
     @property
-    def end_date(self):
-        """
-        Maturity date of the bond to override. Mandatory if instrument code has not been
-        defined and is_perpetual flag has been set to false. In case an instrument code
-        has been defined, value comes from bond reference data.
-        :return: str
-        """
-        return self._get_parameter("endDate")
-
-    @end_date.setter
-    def end_date(self, value):
-        self._set_parameter("endDate", value)
-
-    @property
-    def first_accrual_date(self):
+    def cms_template(self):
         """
-        Date at which bond starts accruing. Optional. In case an instrument code has
-        been defined, value comes from bond reference data. Otherwise default value is
-        the issue date of the bond.
+        A reference to a common swap contract that represents the underlying swap in
+        case of a constant maturity swap contract (cms). example: eur_ab6e. no default
+        value applies.
         :return: str
         """
-        return self._get_parameter("firstAccrualDate")
+        return self._get_parameter("cmsTemplate")
 
-    @first_accrual_date.setter
-    def first_accrual_date(self, value):
-        self._set_parameter("firstAccrualDate", value)
+    @cms_template.setter
+    def cms_template(self, value):
+        self._set_parameter("cmsTemplate", value)
 
     @property
     def first_regular_payment_date(self):
         """
-        The first regular coupon payment date for leg with an odd first coupon.
-        Optional.
+        The first regular interest payment date used for the odd first interest period.
+        the value is expressed in iso 8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g.
+        2021-01-01t00:00:00z). no default value applies.
         :return: str
         """
         return self._get_parameter("firstRegularPaymentDate")
 
     @first_regular_payment_date.setter
     def first_regular_payment_date(self, value):
         self._set_parameter("firstRegularPaymentDate", value)
 
     @property
     def fixed_rate_percent(self):
         """
-        The fixed coupon rate in percentage. It is mandatory in case of a single leg
-        instrument. Otherwise, in case of multi leg instrument, it can be computed as
-        the Par rate.
+        The interest rate of the instrument. the value is expressed in percentages.
+        mandatory if no instrumentcode is defined. if instrumentcode is defined, the
+        value comes from the instrument reference data.
         :return: float
         """
         return self._get_parameter("fixedRatePercent")
 
     @fixed_rate_percent.setter
     def fixed_rate_percent(self, value):
         self._set_parameter("fixedRatePercent", value)
 
     @property
     def floor_strike_percent(self):
         """
         The contractual strike rate of the floor. the value is expressed in percentages.
         if this parameter is set, the floor will apply to the leg with the same
-        parameters set in the swapLegDefinition (e.g.maturity, frequency, index,
+        parameters set in the swaplegdefinition (e.g.maturity, frequency, index,
         discounting rule). no default value applies.
         :return: float
         """
         return self._get_parameter("floorStrikePercent")
 
     @floor_strike_percent.setter
     def floor_strike_percent(self, value):
         self._set_parameter("floorStrikePercent", value)
 
     @property
     def index_fixing_lag(self):
         """
-        Defines the number of working days between the fixing date and the start of the
-        coupon period ('InAdvance') or the end of the coupon period ('InArrears').
-        Optional. By default 0 is used.
+        The number of working daysbetween the fixing date and the start of the coupon
+        period ('inadvance') or the end of the coupon period ('inarrears'). the
+        inadvance/inarrears mode is set in the indexresettype parameter. the default
+        value is the fixing lag associated to the index defined/determined by default on
+        the floating instrument.
         :return: int
         """
         return self._get_parameter("indexFixingLag")
 
     @index_fixing_lag.setter
     def index_fixing_lag(self, value):
         self._set_parameter("indexFixingLag", value)
 
     @property
     def index_fixing_ric(self):
         """
-        The RIC that carries the fixing value. This value overrides the RIC associated
-        by default with the IndexName and IndexTenor. Optional.
+        The ric that carries the fixing value if the instrument has a floating interest.
+        optional. mandatory for floating rate instruments if no instrumentcode is
+        defined. if instrumentcode is defined, the value comes from the instrument
+        reference data. no default value applies.
         :return: str
         """
         return self._get_parameter("indexFixingRic")
 
     @index_fixing_ric.setter
     def index_fixing_ric(self, value):
         self._set_parameter("indexFixingRic", value)
 
     @property
-    def instrument_code(self):
+    def index_name(self):
         """
-        Code to define the bond instrument. It can be an ISIN, a RIC, a CUSIP or an
-        AssetId .
+        The name of the floating rate index (e.g. 'euribor'). no default value applies.
         :return: str
         """
-        return self._get_parameter("instrumentCode")
+        return self._get_parameter("indexName")
 
-    @instrument_code.setter
-    def instrument_code(self, value):
-        self._set_parameter("instrumentCode", value)
+    @index_name.setter
+    def index_name(self, value):
+        self._set_parameter("indexName", value)
+
+    @property
+    def index_tenor(self):
+        """
+        The period code indicating the maturity of the floating rate index. the default
+        value is the tenor equivalent toindexresetfrequency or interestpaymentfrequency.
+        :return: str
+        """
+        return self._get_parameter("indexTenor")
+
+    @index_tenor.setter
+    def index_tenor(self, value):
+        self._set_parameter("indexTenor", value)
 
     @property
     def instrument_tag(self):
         """
-        User defined string to identify the instrument.It can be used to link output
-        results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
-        characters are supported. Optional.
         :return: str
         """
         return self._get_parameter("instrumentTag")
 
     @instrument_tag.setter
     def instrument_tag(self, value):
         self._set_parameter("instrumentTag", value)
 
     @property
     def interest_payment_delay(self):
         """
-        The number of working days between the end of coupon period and the actual
-        interest payment date. Optional. By default no delay (0) is applied.
+        The number of working days between the end of the interest accrual period and
+        the interest payment date. by default, no delay (0) is applied.
         :return: int
         """
         return self._get_parameter("interestPaymentDelay")
 
     @interest_payment_delay.setter
     def interest_payment_delay(self, value):
         self._set_parameter("interestPaymentDelay", value)
 
     @property
-    def is_perpetual(self):
-        """
-        Flag the defines wether the bond is perpetual or not in case of user defined
-        bond. Optional. In case an instrument code has been defined, value comes from
-        bond reference data. In case of user defined bond, default value is 'false'.
-        :return: bool
-        """
-        return self._get_parameter("isPerpetual")
-
-    @is_perpetual.setter
-    def is_perpetual(self, value):
-        self._set_parameter("isPerpetual", value)
-
-    @property
-    def issue_date(self):
+    def last_regular_payment_date(self):
         """
-        Date of issuance of the bond to override. Mandatory if instrument code has not
-        been defined. In case an instrument code has been defined, value comes from bond
-        reference data.
+        The last regular interest payment date used for the odd last interest period.
+        the value is expressed in iso 8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g.
+        2021-01-01t00:00:00z). no default value applies.
         :return: str
         """
-        return self._get_parameter("issueDate")
+        return self._get_parameter("lastRegularPaymentDate")
 
-    @issue_date.setter
-    def issue_date(self, value):
-        self._set_parameter("issueDate", value)
+    @last_regular_payment_date.setter
+    def last_regular_payment_date(self, value):
+        self._set_parameter("lastRegularPaymentDate", value)
 
     @property
-    def last_regular_payment_date(self):
+    def leg_tag(self):
         """
-        The last regular coupon payment date for leg with an odd last coupon. Optional.
+        A user-defined string to identify the direction of the leg: 'paid' or
+        'received'. optional. no default value applies.
         :return: str
         """
-        return self._get_parameter("lastRegularPaymentDate")
+        return self._get_parameter("legTag")
 
-    @last_regular_payment_date.setter
-    def last_regular_payment_date(self, value):
-        self._set_parameter("lastRegularPaymentDate", value)
+    @leg_tag.setter
+    def leg_tag(self, value):
+        self._set_parameter("legTag", value)
 
     @property
     def notional_amount(self):
         """
-        The notional amount of the leg at the period start date. Optional. By default
-        1,000,000 is used.
+        The notional amount of the instrument's leg. the default value is '1,000,000'.
         :return: float
         """
         return self._get_parameter("notionalAmount")
 
     @notional_amount.setter
     def notional_amount(self, value):
         self._set_parameter("notionalAmount", value)
 
     @property
     def notional_ccy(self):
         """
-        The ISO code of the notional currency. Mandatory if instrument code or
-        instrument style has not been defined. In case an instrument code/style has been
-        defined, value may comes from the reference data.
+        The currency of the instrument's notional amount. the value is expressed in iso
+        4217 alphabetical format (e.g. 'usd'). no default value applies.
         :return: str
         """
         return self._get_parameter("notionalCcy")
 
     @notional_ccy.setter
     def notional_ccy(self, value):
         self._set_parameter("notionalCcy", value)
 
     @property
     def payment_business_days(self):
         """
-        A list of coma-separated calendar codes to adjust dates (e.g. 'EMU' or 'USA').
-        Optional. By default the calendar associated to notional_ccy is used.
+        A list of comma-separated calendar codes to adjust dates (e.g. 'emu' or 'usa').
+        the default value is the calendar associated to the market conventions of the
+        interestpaymentccy for the corresponding leg.
         :return: str
         """
         return self._get_parameter("paymentBusinessDays")
 
     @payment_business_days.setter
     def payment_business_days(self, value):
         self._set_parameter("paymentBusinessDays", value)
 
     @property
     def spread_bp(self):
         """
-        The spread in basis point that is added to the floating rate index index value.
-        Optional. By default 0 is used.
+        The interest spread in basis points that is added to the floating rate index
+        value. optional. if instrumentcode is defined, the value comes from the
+        instrument reference data. in case of a user-defined instrument, the default
+        value is '0'.
         :return: float
         """
         return self._get_parameter("spreadBp")
 
     @spread_bp.setter
     def spread_bp(self, value):
         self._set_parameter("spreadBp", value)
 
     @property
-    def template(self):
+    def upfront_amount(self):
         """
-        A reference to a Adfin instrument contract or the Adfin detailed contract.
-        Optional. Either instrument_code, template, or full definition must be provided.
-        :return: str
+        The amount which represents the net present value of the swap. it is computed as
+        [(100  dirtypricepercent / 100) x notionalamount]. the value is expressed in
+        upfrontamountccy. by default, no payment (0) applies.
+        :return: float
         """
-        return self._get_parameter("template")
+        return self._get_parameter("upfrontAmount")
+
+    @upfront_amount.setter
+    def upfront_amount(self, value):
+        self._set_parameter("upfrontAmount", value)
+
+    @property
+    def index_price_side(self):
+        return self._get_enum_parameter(PriceSide, "indexPriceSide")
+
+    @index_price_side.setter
+    def index_price_side(self, value):
+        self._set_enum_parameter(PriceSide, "indexPriceSide", value)
+
+    @property
+    def fixed_rate_percent_schedule(self):
+        return self._get_parameter("fixedRatePercentSchedule")
 
-    @template.setter
-    def template(self, value):
-        self._set_parameter("template", value)
+    @fixed_rate_percent_schedule.setter
+    def fixed_rate_percent_schedule(self, value):
+        self._set_parameter("fixedRatePercentSchedule", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_bond_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/_bond_pricing_parameters.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,101 +1,79 @@
 # coding: utf8
 
 from typing import Optional, Union
 
-from ._enums import (
+from ..._enums import (
     DividendType,
     ProjectedIndexCalculationMethod,
     CreditSpreadType,
     PriceSide,
     RedemptionDateType,
     VolatilityType,
     VolatilityTermStructureType,
     BenchmarkYieldSelectionMode,
     YieldType,
     QuoteFallbackLogic,
     InflationMode,
 )
-from ._models import BondRoundingParameters
+from ..._models import BondRoundingParameters
 from ..._object_definition import ObjectDefinition
 
 
 class PricingParameters(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
     trade_date : str, optional
         Trade date of the bond. The trade_date is used to compute the default
         valuation_date : By default the rule is that valuation_date = trade_date +
         settlement_convention. Optional. By default, it is equal to market_data_date.
-    benchmark_yield_selection_mode : BenchmarkYieldSelectionMode, optional
+    benchmark_yield_selection_mode : BenchmarkYieldSelectionMode or str, optional
         The benchmark yield.
         Default value is "Interpolate".
-    credit_spread_type : CreditSpreadType, optional
+    credit_spread_type : CreditSpreadType or str, optional
         Credit curve spread type to use during pricing. Applicable for Convertible
         Bonds.
-    dividend_type : DividendType, optional
+    dividend_type : DividendType or str, optional
         Underlying stock dividend type used during pricing convertible bond. Applicable
         for Convertible Bonds.
-    fx_price_side : PriceSide, optional
+    fx_price_side : PriceSide or str, optional
         FX price side to consider when retrieving FX rates (Mid, Bid, Ask, Last, Close)
-    inflation_mode : InflationMode, optional
+    inflation_mode : InflationMode or str, optional
         The indicator used to define whether instrument parameters should be adjusted
         from inflation or not. Available only for inflation-linked instruments.
         optional. By default, 'default' is used. That means it depends on the instrument
         quotation convention.
-    price_side : PriceSide, optional
+    price_side : PriceSide or str, optional
         Quoted price side of the bond to use for pricing Analysis: Bid(Bid value),
         Ask(Ask value), Mid(Mid value) Optional. By default the "Mid" price of the bond
         is used.
-    projected_index_calculation_method : ProjectedIndexCalculationMethod, optional
+    projected_index_calculation_method : ProjectedIndexCalculationMethod or str, optional
         Flag used to define how projected index is computed.
         Default value is "ConstantIndex". It is defaulted to "ForwardIndex"
         for Preferreds and Brazilian Debenture bonds.
-    quote_fallback_logic : QuoteFallbackLogic, optional
+    quote_fallback_logic : QuoteFallbackLogic or str, optional
         Enumeration used to define the fallback logic for the quotation of the
-        instrument. Available values are:
-        - "None": it means that there's no fallback logic. For example, if the user asks
-          for a "Ask" price and instrument is only quoted with a "Bid" price, it is an
-          error case.
-        - "BestField" : it means that there's a fallback logic to use another market
-          data field as quoted price. For example, if the user asks for a "Ask" price
-          and instrument is only quoted with a "Bid" price, "Bid" price can be used.
-    redemption_date_type : RedemptionDateType, optional
-        Redemption type of the bond. It is used to compute the default redemption date:
-        - RedemptionAtMaturityDate : yield and price are computed at maturity date.
-        - RedemptionAtCallDate : yield and price are computed at call date (next call
-          date by default).
-        - RedemptionAtPutDate : yield and price are computed at put date (next put date
-          by default)..
-        - RedemptionAtWorstDate : yield and price are computed at the lowest yield date.
-        - RedemptionAtSinkDate : yield and price are computed at sinking fund date.
-        - RedemptionAtParDate : yield and price are computed at next par.
-        - RedemptionAtPremiumDate : yield and price are computed at next premium.
-        - RedemptionAtMakeWholeCallDate : yield and price are computed at Make Whole
-          Call date.
-        - RedemptionAtAverageLife : yield and price are computed at average life (case
-          of sinkable bonds)
-        - RedemptionAtNextDate : yield and price are computed at next redemption date
-          available.
-
-          Default value is "RedemptionAtWorstDate" for callable
-          bond, "RedemptionAtBestDate" for puttable bond or "RedemptionAtMaturityDate".
+        instrument.
+    redemption_date_type : RedemptionDateType or str, optional
+        Redemption type of the bond. It is used to compute the default redemption date.
+        Default value is "RedemptionAtWorstDate" for callable bond,
+        "RedemptionAtBestDate" for puttable bond or "RedemptionAtMaturityDate".
     rounding_parameters : BondRoundingParameters, optional
         Definition of rounding parameters to be applied on accrued, price or yield.
         By default, rounding parameters are the ones defined in the bond structure.
-    volatility_term_structure_type : VolatilityTermStructureType, optional
+    volatility_term_structure_type : VolatilityTermStructureType or str, optional
         Stock volatility trem structure type to use during pricing. Applicable for
         Convertible Bonds.
-    volatility_type : VolatilityType, optional
+    volatility_type : VolatilityType or str, optional
         Volatility type to use during pricing. Applicable for Convertible Bonds.
-    yield_type : YieldType, optional
+    yield_type : YieldType or str, optional
         yield_type that specifies the rate structure.
         The default value is Native.
     adjusted_clean_price : float, optional
         Inflation Adjusted Clean price to override and that will be used as pricing
         analysis input. The currency of the clean price is the cash flow currency (that
         can be different to deal currency especially if "ComputeCashFlowWithReportCcy"
         flag has been set to true). No override is applied by default. Note that only
@@ -428,29 +406,27 @@
     ...)
     >>> response = definition.get_data()
     """
 
     def __init__(
         self,
         trade_date: Optional[str] = None,
-        benchmark_yield_selection_mode: Optional[BenchmarkYieldSelectionMode] = None,
-        credit_spread_type: Optional[CreditSpreadType] = None,
-        dividend_type: Optional[DividendType] = None,
-        fx_price_side: Optional[PriceSide] = None,
-        inflation_mode: Optional[InflationMode] = None,
-        price_side: Optional[PriceSide] = None,
-        projected_index_calculation_method: Optional[
-            ProjectedIndexCalculationMethod
-        ] = None,
-        quote_fallback_logic: Optional[QuoteFallbackLogic] = None,
-        redemption_date_type: Optional[RedemptionDateType] = None,
+        benchmark_yield_selection_mode: Union[BenchmarkYieldSelectionMode, str] = None,
+        credit_spread_type: Union[CreditSpreadType, str] = None,
+        dividend_type: Union[DividendType, str] = None,
+        fx_price_side: Union[PriceSide, str] = None,
+        inflation_mode: Union[InflationMode, str] = None,
+        price_side: Union[PriceSide, str] = None,
+        projected_index_calculation_method: Union[ProjectedIndexCalculationMethod, str] = None,
+        quote_fallback_logic: Union[QuoteFallbackLogic, str] = None,
+        redemption_date_type: Union[RedemptionDateType, str] = None,
         rounding_parameters: Union[BondRoundingParameters, dict] = None,
-        volatility_term_structure_type: Optional[VolatilityTermStructureType] = None,
-        volatility_type: Optional[VolatilityType] = None,
-        yield_type: Optional[YieldType] = None,
+        volatility_term_structure_type: Union[VolatilityTermStructureType, str] = None,
+        volatility_type: Union[VolatilityType, str] = None,
+        yield_type: Union[YieldType, str] = None,
         adjusted_clean_price: Optional[float] = None,
         adjusted_dirty_price: Optional[float] = None,
         adjusted_yield_percent: Optional[float] = None,
         apply_tax_to_full_pricing: Optional[bool] = None,
         asset_swap_spread_bp: Optional[float] = None,
         benchmark_at_issue_price: Optional[float] = None,
         benchmark_at_issue_ric: Optional[str] = None,
@@ -552,17 +528,15 @@
         self.asset_swap_spread_bp = asset_swap_spread_bp
         self.benchmark_at_issue_price = benchmark_at_issue_price
         self.benchmark_at_issue_ric = benchmark_at_issue_ric
         self.benchmark_at_issue_spread_bp = benchmark_at_issue_spread_bp
         self.benchmark_at_issue_yield_percent = benchmark_at_issue_yield_percent
         self.benchmark_at_redemption_price = benchmark_at_redemption_price
         self.benchmark_at_redemption_spread_bp = benchmark_at_redemption_spread_bp
-        self.benchmark_at_redemption_yield_percent = (
-            benchmark_at_redemption_yield_percent
-        )
+        self.benchmark_at_redemption_yield_percent = benchmark_at_redemption_yield_percent
         self.bond_recovery_rate_percent = bond_recovery_rate_percent
         self.cash_amount = cash_amount
         self.cds_recovery_rate_percent = cds_recovery_rate_percent
         self.clean_price = clean_price
         self.compute_cash_flow_from_issue_date = compute_cash_flow_from_issue_date
         self.compute_cash_flow_with_report_ccy = compute_cash_flow_with_report_ccy
         self.concession_fee = concession_fee
@@ -579,26 +553,20 @@
         self.efp_spread_bp = efp_spread_bp
         self.flat_credit_spread_bp = flat_credit_spread_bp
         self.flat_credit_spread_tenor = flat_credit_spread_tenor
         self.fx_stock_correlation = fx_stock_correlation
         self.fx_volatility_percent = fx_volatility_percent
         self.fx_volatility_tenor = fx_volatility_tenor
         self.gov_country_benchmark_curve_price = gov_country_benchmark_curve_price
-        self.gov_country_benchmark_curve_yield_percent = (
-            gov_country_benchmark_curve_yield_percent
-        )
+        self.gov_country_benchmark_curve_yield_percent = gov_country_benchmark_curve_yield_percent
         self.gov_country_spread_bp = gov_country_spread_bp
         self.government_benchmark_curve_price = government_benchmark_curve_price
-        self.government_benchmark_curve_yield_percent = (
-            government_benchmark_curve_yield_percent
-        )
+        self.government_benchmark_curve_yield_percent = government_benchmark_curve_yield_percent
         self.government_spread_bp = government_spread_bp
-        self.is_coupon_payment_adjustedfor_leap_year = (
-            is_coupon_payment_adjustedfor_leap_year
-        )
+        self.is_coupon_payment_adjustedfor_leap_year = is_coupon_payment_adjustedfor_leap_year
         self.issuer_benchmark_curve_yield_percent = issuer_benchmark_curve_yield_percent
         self.issuer_spread_bp = issuer_spread_bp
         self.market_data_date = market_data_date
         self.market_value_in_deal_ccy = market_value_in_deal_ccy
         self.market_value_in_report_ccy = market_value_in_report_ccy
         self.net_price = net_price
         self.neutral_yield_percent = neutral_yield_percent
@@ -609,17 +577,15 @@
         self.price = price
         self.projected_index_percent = projected_index_percent
         self.quoted_price = quoted_price
         self.rating_benchmark_curve_yield_percent = rating_benchmark_curve_yield_percent
         self.rating_spread_bp = rating_spread_bp
         self.redemption_date = redemption_date
         self.report_ccy = report_ccy
-        self.sector_rating_benchmark_curve_yield_percent = (
-            sector_rating_benchmark_curve_yield_percent
-        )
+        self.sector_rating_benchmark_curve_yield_percent = sector_rating_benchmark_curve_yield_percent
         self.sector_rating_spread_bp = sector_rating_spread_bp
         self.settlement_convention = settlement_convention
         self.simple_margin_bp = simple_margin_bp
         self.stock_borrow_rate_percent = stock_borrow_rate_percent
         self.stock_flat_volatility_percent = stock_flat_volatility_percent
         self.stock_flat_volatility_tenor = stock_flat_volatility_tenor
         self.stock_price_on_default = stock_price_on_default
@@ -644,23 +610,19 @@
         The benchmark yield selection mode:
         - Interpolate : do an interpolatation on yield curve to compute the reference
           yield.
         - Nearest : use the nearest point to find the reference yield. Optional. Default
           value is "Interpolate".
         :return: enum BenchmarkYieldSelectionMode
         """
-        return self._get_enum_parameter(
-            BenchmarkYieldSelectionMode, "benchmarkYieldSelectionMode"
-        )
+        return self._get_enum_parameter(BenchmarkYieldSelectionMode, "benchmarkYieldSelectionMode")
 
     @benchmark_yield_selection_mode.setter
     def benchmark_yield_selection_mode(self, value):
-        self._set_enum_parameter(
-            BenchmarkYieldSelectionMode, "benchmarkYieldSelectionMode", value
-        )
+        self._set_enum_parameter(BenchmarkYieldSelectionMode, "benchmarkYieldSelectionMode", value)
 
     @property
     def credit_spread_type(self):
         """
         Credit curve spread type to use during pricing. Applicable for Convertible
         Bonds.
         :return: enum CreditSpreadType
@@ -732,23 +694,19 @@
         - "ConstantIndex" : future index values are considered as constant and equal to
           projected index value.
         - "ForwardIndex" : future index values are computed using a forward curve.
           Optional. Default value is "ConstantIndex". It is defaulted to "ForwardIndex"
           for Preferreds and Brazilian Debenture bonds.
         :return: enum ProjectedIndexCalculationMethod
         """
-        return self._get_enum_parameter(
-            ProjectedIndexCalculationMethod, "projectedIndexCalculationMethod"
-        )
+        return self._get_enum_parameter(ProjectedIndexCalculationMethod, "projectedIndexCalculationMethod")
 
     @projected_index_calculation_method.setter
     def projected_index_calculation_method(self, value):
-        self._set_enum_parameter(
-            ProjectedIndexCalculationMethod, "projectedIndexCalculationMethod", value
-        )
+        self._set_enum_parameter(ProjectedIndexCalculationMethod, "projectedIndexCalculationMethod", value)
 
     @property
     def quote_fallback_logic(self):
         """
         Enumeration used to define the fallback logic for the quotation of the
         instrument. Available values are:
         - "None": it means that there's no fallback logic. For example, if the user asks
@@ -810,23 +768,19 @@
     @property
     def volatility_term_structure_type(self):
         """
         Stock volatility trem structure type to use during pricing. Applicable for
         Convertible Bonds.
         :return: enum VolatilityTermStructureType
         """
-        return self._get_enum_parameter(
-            VolatilityTermStructureType, "volatilityTermStructureType"
-        )
+        return self._get_enum_parameter(VolatilityTermStructureType, "volatilityTermStructureType")
 
     @volatility_term_structure_type.setter
     def volatility_term_structure_type(self, value):
-        self._set_enum_parameter(
-            VolatilityTermStructureType, "volatilityTermStructureType", value
-        )
+        self._set_enum_parameter(VolatilityTermStructureType, "volatilityTermStructureType", value)
 
     @property
     def volatility_type(self):
         """
         Volatility type to use during pricing. Applicable for Convertible Bonds.
         :return: enum VolatilityType
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/bond/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/_definition.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,28 +1,27 @@
-# coding: utf8
-
 from typing import Optional, Union, TYPE_CHECKING
 
 from ._bond_definition import BondInstrumentDefinition
 from ._bond_pricing_parameters import PricingParameters
-from ._enums import (
+from ..._enums import (
     AdjustInterestToPaymentDate,
     BusinessDayConvention,
     DayCountBasis,
     Direction,
     Frequency,
     IndexCompoundingMethod,
     InterestType,
     DateRollingConvention,
     StubRule,
     IndexAverageMethod,
 )
-from ._models import AmortizationItem
+from ..._models import AmortizationItem
 from .._base_definition import BaseDefinition
 from .._contracts_data_provider import bond_instrument_code_arg_parser
+from ..._enums import IndexObservationMethod
 from ....._tools import validate_types, try_copy_to_list
 
 if TYPE_CHECKING:
     from ....._types import ExtendedParams, OptStrStrs
 
 
 class Definition(BaseDefinition):
@@ -38,19 +37,19 @@
         User defined string to identify the instrument. It can be used to link output
         results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
         characters are supported.
     end_date : str, optional
         Maturity date of the bond to override. Mandatory if instrument code has not been
         defined and is_perpetual flag has been set to false. In case an instrument code
         has been defined, value comes from bond reference data.
-    direction : Direction, optional
+    direction : Direction or str, optional
         The direction of the leg. Optional for a single leg instrument (like a bond),
         in that case default value is Received. It is mandatory for a multi-instrument
         leg instrument (like Swap or CDS leg).
-    interest_type : InterestType, optional
+    interest_type : InterestType or str, optional
         A flag that indicates whether the leg is fixed or float.
     notional_ccy : str, optional
         The ISO code of the notional currency. Mandatory if instrument code or
         instrument style has not been defined. In case an instrument code/style has been
         defined, value may comes from the reference data.
     notional_amount : float, optional
         The notional amount of the leg at the period start date.
@@ -64,26 +63,26 @@
         By default 0 is used.
     interest_payment_frequency : Frequency or str, optional
         The frequency of the interest payments. Optional if an instrument code/style
         have been defined : in that case, value comes from reference data. Otherwise, it
         is mandatory.
     interest_calculation_method : DayCountBasis or str, optional
         The Day Count Basis method used to calculate the coupon interest payments.
-    accrued_calculation_method : DayCountBasis, optional
+    accrued_calculation_method : DayCountBasis or str, optional
         The Day Count Basis method used to calculate the accrued interest payments.
         By default, the same value than interest_calculation_method is used.
-    payment_business_day_convention : BusinessDayConvention, optional
+    payment_business_day_convention : BusinessDayConvention or str, optional
         The method to adjust dates to a working day.
         In case an instrument code/style has been defined, value comes from bond
         reference data. Otherwise 'ModifiedFollowing' is used.
-    payment_roll_convention : DateRollingConvention, optional
+    payment_roll_convention : DateRollingConvention or str, optional
         Method to adjust payment dates when they fall at the end of the month (28th of
         February, 30th, 31st). In case an instrument code has been defined,
         value comes from bond reference data. Otherwise, 'SameDay' is used.
-    index_reset_frequency : Frequency, optional
+    index_reset_frequency : Frequency or str, optional
         The reset frequency in case the leg Type is Float.
         By default, the IndexTenor is used.
     index_fixing_lag : int, optional
         Defines the number of working days between the fixing date and the start of the
         coupon period ('InAdvance') or the end of the coupon period ('InArrears').
         By default 0 is used.
     first_regular_payment_date : str, optional
@@ -91,35 +90,35 @@
     last_regular_payment_date : str, optional
         The last regular coupon payment date for leg with an odd last coupon.
     amortization_schedule : AmortizationItem, optional
         Definition of amortizations.
     payment_business_days : str, optional
         A list of coma-separated calendar codes to adjust dates (e.g. 'EMU' or 'USA').
         By default the calendar associated to notional_ccy is used.
-    adjust_interest_to_payment_date : AdjustInterestToPaymentDate, optional
+    adjust_interest_to_payment_date : AdjustInterestToPaymentDate or str, optional
         A flag that indicates if the coupon dates are adjusted to the payment dates.
         By default, 'false' is used.
-    index_compounding_method : IndexCompoundingMethod, optional
+    index_compounding_method : IndexCompoundingMethod or str, optional
         A flag that defines how the coupon rate is calculated from the reset floating
         rates when the reset frequency is higher than the interest payment frequency
         (e.g. daily index reset with quarterly interest payment).
         By default 'Constant' is used.
     interest_payment_delay : int, optional
         The number of working days between the end of coupon period and the actual
         interest payment date.
         By default, no delay (0) is applied.
-    stub_rule : StubRule, optional
+    stub_rule : StubRule or str, optional
         The rule that defines whether coupon roll dates are aligned on the  maturity or
         the issue date.
         By default, 'Maturity' is used.
     issue_date : str, optional
         Date of issuance of the bond to override. Mandatory if instrument code has not
         been defined. In case an instrument code has been defined, value comes from bond
         reference data.
-    index_average_method : IndexAverageMethod, optional
+    index_average_method :  or str, optional
         The value of the average index calculation method. The possible values are:
         ArithmeticAverage, CompoundedActual, CompoundedAverageRate, DailyCompoundedAverage
     first_accrual_date : str, optional
         Date at which bond starts accruing. In case an instrument code has
         been defined, value comes from bond reference data. Otherwise, default value is
         the issue date of the bond.
     floor_strike_percent : float, optional
@@ -143,91 +142,112 @@
     pricing_parameters : PricingParameters, optional
         The pricing parameters to apply to this instrument. If pricing
         parameters are not provided at this level parameters defined globally at the
         request level are used. If no pricing parameters are provided globally default
         values apply.
     extended_params : dict, optional
         If necessary other parameters.
+    index_observation_method : IndexObservationMethod or str, optional
+        (RFR) Method for determining the accrual observation period.
+    fixed_rate_percent_schedule : dict, optional
+        The step structure: a list of pre-determined future coupon rates indexed by their dates.
+        Either fixedRatePercent or fixedRatePercentSchedule is used. No default value applies.
+    instrument_type : str, optional
+        Instrument type definition for bond.
+        Optional. Definition can be provided. Otherwise, default value is "Bond".
 
     Methods
     -------
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_data_async(session=None, on_response=None, async_mode=None)
         Returns a response asynchronously to the data platform
     get_stream(session=session)
         Get stream quantitative analytic service subscription
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> definition = rdf.bond.Definition(
-     ...    issue_date = "2002-02-28",
-     ...    end_date = "2032-02-28",
-     ...    notional_ccy = "USD",
-     ...    interest_payment_frequency = "Annual",
-     ...    fixed_rate_percent = 7,
-     ...    interest_calculation_method = rdf.bond.DayCountBasis.DCB_ACTUAL_ACTUAL
-     ...)
-     >>> response = definition.get_data()
-
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> definition = rdf.bond.Definition(
+    ...    issue_date = "2002-02-28",
+    ...    end_date = "2032-02-28",
+    ...    notional_ccy = "USD",
+    ...    interest_payment_frequency = "Annual",
+    ...    fixed_rate_percent = 7,
+    ...    interest_calculation_method = rdf.bond.DayCountBasis.DCB_ACTUAL_ACTUAL
+    ... )
+    >>> response = definition.get_data()
+
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> definition = rdf.bond.Definition(
+    ...    fields = ["YieldPercent", "Duration", "RedemptionDateType", "RedemptionDate"],
+    ...    instrument_type = "Bond",
+    ...    instrument_code="250847EF3=TWBL",
+    ...    pricing_parameters=PricingParameters(
+    ...        redemption_date_type="RedemptionAtCallDate",
+    ...        price = 100,
+    ...        trade_date = "2020-05-01"
+    ...    )
+    ... )
+    >>> response = definition.get_data()
+
+    Using get_data_async
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
 
-     Using get_stream
-     >>> response = definition.get_stream()
+    Using get_stream
+    >>> response = definition.get_stream()
     """
 
     def __init__(
         self,
         instrument_code: Optional[str] = None,
         instrument_tag: Optional[str] = None,
         end_date: Optional[str] = None,
-        direction: Optional[Direction] = None,
-        interest_type: Optional[InterestType] = None,
+        direction: Union[Direction, str] = None,
+        interest_type: Union[InterestType, str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
         fixed_rate_percent: Optional[float] = None,
         spread_bp: Optional[float] = None,
         interest_payment_frequency: Union[Frequency, str] = None,
         interest_calculation_method: Union[DayCountBasis, str] = None,
-        accrued_calculation_method: Optional[DayCountBasis] = None,
-        payment_business_day_convention: Optional[BusinessDayConvention] = None,
-        payment_roll_convention: Optional[DateRollingConvention] = None,
-        index_reset_frequency: Optional[Frequency] = None,
+        accrued_calculation_method: Union[DayCountBasis, str] = None,
+        payment_business_day_convention: Union[BusinessDayConvention, str] = None,
+        payment_roll_convention: Union[DateRollingConvention, str] = None,
+        index_reset_frequency: Union[Frequency, str] = None,
         index_fixing_lag: Optional[int] = None,
         first_regular_payment_date: Optional[str] = None,
         last_regular_payment_date: Optional[str] = None,
         amortization_schedule: Optional[AmortizationItem] = None,
         payment_business_days: Optional[str] = None,
-        adjust_interest_to_payment_date: Optional[AdjustInterestToPaymentDate] = None,
-        index_compounding_method: Optional[IndexCompoundingMethod] = None,
+        adjust_interest_to_payment_date: Union[AdjustInterestToPaymentDate, str] = None,
+        index_compounding_method: Union[IndexCompoundingMethod, str] = None,
         interest_payment_delay: Optional[int] = None,
-        stub_rule: Optional[StubRule] = None,
+        stub_rule: Union[StubRule, str] = None,
         issue_date: Optional[str] = None,
-        index_average_method: Optional[IndexAverageMethod] = None,
+        index_average_method: Union[IndexAverageMethod, str] = None,
         first_accrual_date: Optional[str] = None,
         floor_strike_percent: Optional[float] = None,
         index_fixing_ric: Optional[str] = None,
         is_perpetual: Optional[bool] = None,
         template: Optional[str] = None,
         fields: "OptStrStrs" = None,
         pricing_parameters: Optional[PricingParameters] = None,
         extended_params: "ExtendedParams" = None,
+        index_observation_method: Union[IndexObservationMethod, str] = None,
+        fixed_rate_percent_schedule: Optional[dict] = None,
+        instrument_type: Optional[str] = "Bond",
     ) -> None:
-
         if instrument_code:
             instrument_code = bond_instrument_code_arg_parser.get_str(instrument_code)
 
         validate_types(index_fixing_lag, [int, type(None)], "index_fixing_lag")
-        validate_types(
-            interest_payment_delay, [int, type(None)], "interest_payment_delay"
-        )
+        validate_types(interest_payment_delay, [int, type(None)], "interest_payment_delay")
         fields = try_copy_to_list(fields)
 
         definition = BondInstrumentDefinition(
             accrued_calculation_method=accrued_calculation_method,
             adjust_interest_to_payment_date=adjust_interest_to_payment_date,
             amortization_schedule=amortization_schedule,
             direction=direction,
@@ -254,14 +274,17 @@
             issue_date=issue_date,
             last_regular_payment_date=last_regular_payment_date,
             notional_amount=notional_amount,
             notional_ccy=notional_ccy,
             payment_business_days=payment_business_days,
             spread_bp=spread_bp,
             template=template,
+            index_observation_method=index_observation_method,
+            fixed_rate_percent_schedule=fixed_rate_percent_schedule,
+            instrument_type=instrument_type,
         )
         super().__init__(
             definition=definition,
             fields=fields,
             pricing_parameters=pricing_parameters,
             extended_params=extended_params,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -20,15 +20,15 @@
     "PriceSide",
     "PricingParameters",
     "StubRule",
 )
 
 from ._cap_floor_pricing_parameters import PricingParameters
 from ._definition import Definition
-from ._enums import (
+from ..._enums import (
     AdjustInterestToPaymentDate,
     AmortizationFrequency,
     AmortizationType,
     BarrierType,
     BusinessDayConvention,
     BuySell,
     DateRollingConvention,
@@ -39,8 +39,8 @@
     IndexResetType,
     InterestCalculationConvention,
     PremiumSettlementType,
     PriceSide,
     StubRule,
 )
 
-from ._models import AmortizationItem, BarrierDefinitionElement, InputFlow
+from ..._models import AmortizationItem, BarrierDefinitionElement, InputFlow
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/bond/_bond_definition.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,694 +1,575 @@
-# coding: utf8
+from typing import Optional, Union
 
-from typing import Optional
-
-from .._instrument_definition import InstrumentDefinition
-from ._enums import (
-    AdjustInterestToPaymentDate,
-    BusinessDayConvention,
-    BuySell,
+from ..._enums import (
     DateRollingConvention,
     DayCountBasis,
-    Frequency,
-    IndexResetType,
-    InterestCalculationConvention,
+    InterestType,
     StubRule,
+    Frequency,
+    AdjustInterestToPaymentDate,
+    IndexCompoundingMethod,
+    BusinessDayConvention,
+    Direction,
+    IndexAverageMethod,
 )
-from ._models import (
-    AmortizationItem,
-    BarrierDefinitionElement,
-    InputFlow,
-)
-
-
-class CapFloorInstrumentDefinition(InstrumentDefinition):
-    """
-    API endpoint for Financial Contract analytics,
-    that returns calculations relevant to each contract type.
-
-    Parameters
-    ----------
-    instrument_tag : str, optional
-        User defined string to identify the instrument.It can be used to link output
-        results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
-        characters are supported. Optional.
-    start_date : str, optional
-        The option start date
-    end_date : str, optional
-        The maturity date of the CapFloor
-    tenor : str, optional
-        Tenor of the option
-    notional_ccy : str, optional
-        The ISO code of the notional currency. Mandatory if instrument code or
-        instrument style has not been defined. In case an instrument code/style has been
-        defined, value may comes from the reference data.
-    notional_amount : float, optional
-        The notional amount of the leg at the period start date. Optional. By default
-        1,000,000 is used.
-    index_name : str, optional
-        The name of the floating rate index.
-    index_tenor : str, optional
-        The period code that represents the maturity of the floating rate index.
-        Mandatory when the leg is float.
-    interest_payment_frequency : Frequency, optional
-        The frequency of the interest payments. Optional if an instrument code/style
-        have been defined : in that case, value comes from reference data. Otherwise, it
-        is mandatory.
-    interest_calculation_method : DayCountBasis, optional
-        The Day Count Basis method used to calculate the coupon interest payments.
-        Mandatory.
-    payment_business_day_convention : BusinessDayConvention, optional
-        The method to adjust dates to a working day. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. In case an instrument code/style has been defined,
-          value comes from bond reference data. Otherwise 'ModifiedFollowing' is used.
-    payment_roll_convention : DateRollingConvention, optional
-        The method to adjust payment dates whn they fall at the end of the month (28th
-        of February, 30th, 31st). The possible values are:
-        - Last (For setting the calculated date to the last working day),
-        - Same (For setting the calculated date to the same day . In this latter case,
-          the date may be moved according to the date moving convention if it is a
-          non-working day),
-        - Last28 (For setting the calculated date to the last working day. 28FEB being
-          always considered as the last working day),
-        - Same28 (For setting the calculated date to the same day .28FEB being always
-          considered as the last working day). Optional. By default 'SameDay' is used.
-    index_reset_frequency : Frequency, optional
-        The reset frequency in case the leg Type is Float. Optional. By default the
-        IndexTenor is used.
-    index_reset_type : IndexResetType, optional
-        A flag that indicates if the floating rate index is reset before the coupon
-        period starts or at the end of the coupon period. The possible values are:
-        - InAdvance (resets the index before the start of the interest period),
-        - InArrears (resets the index at the end of the interest period). Optional. By
-          default 'InAdvance' is used.
-    index_fixing_lag : int, optional
-        Defines the positive number of working days between the fixing date and the
-        start of the coupon period ('InAdvance') or the end of the coupon period
-        ('InArrears'). Optional. By default 0 is used.
-    amortization_schedule : AmortizationItem, optional
-        Definition of amortizations
-    payment_business_days : str, optional
-        A list of coma-separated calendar codes to adjust dates (e.g. 'EMU' or 'USA').
-        Optional. By default the calendar associated to NotionalCcy is used.
-    adjust_interest_to_payment_date : AdjustInterestToPaymentDate, optional
-        A flag that indicates if the coupon dates are adjusted to the payment dates.
-        Optional. By default 'false' is used.
-    stub_rule : StubRule, optional
-        The rule that defines whether coupon roll dates are aligned on the  maturity or
-        the issue date. The possible values are:
-        - ShortFirstProRata (to create a short period between the start date and the
-          first coupon date, and pay a smaller amount of interest for the short
-          period.All coupon dates are calculated backward from the maturity date),
-        - ShortFirstFull (to create a short period between the start date and the first
-          coupon date, and pay a regular coupon on the first coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - LongFirstFull (to create a long period between the start date and the second
-          coupon date, and pay a regular coupon on the second coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - ShortLastProRata (to create a short period between the last payment date and
-          maturity, and pay a smaller amount of interest for the short period. All
-          coupon dates are calculated forward from the start date). This property may
-          also be used in conjunction with firstRegularPaymentDate and
-          lastRegularPaymentDate; in that case the following values can be defined:
-        - Issue (all dates are aligned on the issue date),
-        - Maturity (all dates are aligned on the maturity date). Optional. By default
-          'Maturity' is used.
-    barrier_definition : BarrierDefinitionElement, optional
+from ..._models import AmortizationItem
+from .._instrument_definition import InstrumentDefinition
+from ..._enums import IndexObservationMethod
 
-    buy_sell : BuySell, optional
-        The side of the deal. Possible values:
-        - Buy
-        - Sell
-    annualized_rebate : bool, optional
-
-    cap_digital_payout_percent : float, optional
-
-    cap_strike_percent : float, optional
-        Cap leg strike expressed in %
-    cms_template : str, optional
-        A reference to a common swap contract that represents the underlying swap in
-        case of a Constant Maturity Swap contract (CMS). Mandatory for CMS contract.
-    floor_digital_payout_percent : float, optional
-
-    floor_strike_percent : float, optional
-        Floor leg strike expressed in %
-    index_fixing_ric : str, optional
-        The RIC that carries the fixing value. This value overrides the RIC associated
-        by default with the IndexName and IndexTenor. Optional.
-    """
 
+class BondInstrumentDefinition(InstrumentDefinition):
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
-        start_date: Optional[str] = None,
+        instrument_code: Optional[str] = None,
         end_date: Optional[str] = None,
-        tenor: Optional[str] = None,
+        direction: Union[Direction, str] = None,
+        interest_type: Union[InterestType, str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
-        index_name: Optional[str] = None,
-        index_tenor: Optional[str] = None,
-        interest_payment_frequency: Optional[Frequency] = None,
-        interest_calculation_method: Optional[DayCountBasis] = None,
-        payment_business_day_convention: Optional[BusinessDayConvention] = None,
-        payment_roll_convention: Optional[DateRollingConvention] = None,
-        index_reset_frequency: Optional[Frequency] = None,
-        index_reset_type: Optional[IndexResetType] = None,
+        fixed_rate_percent: Optional[float] = None,
+        spread_bp: Optional[float] = None,
+        interest_payment_frequency: Union[Frequency, str] = None,
+        interest_calculation_method: Union[DayCountBasis, str] = None,
+        accrued_calculation_method: Union[DayCountBasis, str] = None,
+        payment_business_day_convention: Union[BusinessDayConvention, str] = None,
+        payment_roll_convention: Union[DateRollingConvention, str] = None,
+        index_reset_frequency: Union[Frequency, str] = None,
         index_fixing_lag: Optional[int] = None,
+        first_regular_payment_date: Optional[str] = None,
+        last_regular_payment_date: Optional[str] = None,
         amortization_schedule: Optional[AmortizationItem] = None,
         payment_business_days: Optional[str] = None,
-        adjust_interest_to_payment_date: Optional[AdjustInterestToPaymentDate] = None,
-        stub_rule: Optional[StubRule] = None,
-        barrier_definition: Optional[BarrierDefinitionElement] = None,
-        buy_sell: Optional[BuySell] = None,
-        interest_calculation_convention: Optional[InterestCalculationConvention] = None,
-        payments: Optional[InputFlow] = None,
-        annualized_rebate: Optional[bool] = None,
-        cap_digital_payout_percent: Optional[float] = None,
-        cap_strike_percent: Optional[float] = None,
-        cms_template: Optional[str] = None,
-        floor_digital_payout_percent: Optional[float] = None,
+        adjust_interest_to_payment_date: Union[AdjustInterestToPaymentDate, str] = None,
+        index_compounding_method: Union[IndexCompoundingMethod, str] = None,
+        interest_payment_delay: Optional[int] = None,
+        stub_rule: Union[StubRule, str] = None,
+        issue_date: Optional[str] = None,
+        index_average_method: Union[IndexAverageMethod, str] = None,
+        first_accrual_date: Optional[str] = None,
         floor_strike_percent: Optional[float] = None,
         index_fixing_ric: Optional[str] = None,
-        is_backward_looking_index: Optional[bool] = None,
-        is_rfr: Optional[bool] = None,
-        is_term_rate: Optional[bool] = None,
+        is_perpetual: Optional[bool] = None,
+        template: Optional[str] = None,
+        index_observation_method: Union[IndexObservationMethod, str] = None,
+        fixed_rate_percent_schedule: Optional[dict] = None,
+        instrument_type: Optional[str] = "Bond",
     ) -> None:
         super().__init__()
         self.instrument_tag = instrument_tag
-        self.start_date = start_date
+        self.instrument_code = instrument_code
         self.end_date = end_date
-        self.tenor = tenor
+        self.direction = direction
+        self.interest_type = interest_type
         self.notional_ccy = notional_ccy
         self.notional_amount = notional_amount
-        self.index_name = index_name
-        self.index_tenor = index_tenor
+        self.fixed_rate_percent = fixed_rate_percent
+        self.spread_bp = spread_bp
         self.interest_payment_frequency = interest_payment_frequency
         self.interest_calculation_method = interest_calculation_method
+        self.accrued_calculation_method = accrued_calculation_method
         self.payment_business_day_convention = payment_business_day_convention
         self.payment_roll_convention = payment_roll_convention
         self.index_reset_frequency = index_reset_frequency
-        self.index_reset_type = index_reset_type
         self.index_fixing_lag = index_fixing_lag
+        self.first_regular_payment_date = first_regular_payment_date
+        self.last_regular_payment_date = last_regular_payment_date
         self.amortization_schedule = amortization_schedule
         self.payment_business_days = payment_business_days
         self.adjust_interest_to_payment_date = adjust_interest_to_payment_date
+        self.index_compounding_method = index_compounding_method
+        self.interest_payment_delay = interest_payment_delay
         self.stub_rule = stub_rule
-        self.barrier_definition = barrier_definition
-        self.buy_sell = buy_sell
-        self.interest_calculation_convention = interest_calculation_convention
-        self.payments = payments
-        self.annualized_rebate = annualized_rebate
-        self.cap_digital_payout_percent = cap_digital_payout_percent
-        self.cap_strike_percent = cap_strike_percent
-        self.cms_template = cms_template
-        self.floor_digital_payout_percent = floor_digital_payout_percent
+        self.issue_date = issue_date
+        self.index_average_method = index_average_method
+        self.first_accrual_date = first_accrual_date
         self.floor_strike_percent = floor_strike_percent
         self.index_fixing_ric = index_fixing_ric
-        self.is_backward_looking_index = is_backward_looking_index
-        self.is_rfr = is_rfr
-        self.is_term_rate = is_term_rate
-
-    @classmethod
-    def get_instrument_type(cls):
-        return "CapFloor"
+        self.is_perpetual = is_perpetual
+        self.template = template
+        self.index_observation_method = index_observation_method
+        self.fixed_rate_percent_schedule = fixed_rate_percent_schedule
+        self._instrument_type = instrument_type
+
+    def get_instrument_type(self):
+        return self._instrument_type
+
+    @property
+    def accrued_calculation_method(self):
+        """
+        The Day Count Basis method used to calculate the accrued interest payments.
+        Optional. By default, the same value than InterestCalculationMethod is used.
+        :return: enum DayCountBasis
+        """
+        return self._get_enum_parameter(DayCountBasis, "accruedCalculationMethod")
+
+    @accrued_calculation_method.setter
+    def accrued_calculation_method(self, value):
+        self._set_enum_parameter(DayCountBasis, "accruedCalculationMethod", value)
 
     @property
     def adjust_interest_to_payment_date(self):
         """
-        An indication if the coupon dates are adjusted to the payment dates. the
-        possible values are:    adjusted,    unadjusted.  optional. the default value is
-        'unadjusted'.
+        A flag that indicates if the coupon dates are adjusted to the payment dates.
+        Optional. By default 'false' is used.
         :return: enum AdjustInterestToPaymentDate
         """
-        return self._get_enum_parameter(
-            AdjustInterestToPaymentDate, "adjustInterestToPaymentDate"
-        )
+        return self._get_enum_parameter(AdjustInterestToPaymentDate, "adjustInterestToPaymentDate")
 
     @adjust_interest_to_payment_date.setter
     def adjust_interest_to_payment_date(self, value):
-        self._set_enum_parameter(
-            AdjustInterestToPaymentDate, "adjustInterestToPaymentDate", value
-        )
+        self._set_enum_parameter(AdjustInterestToPaymentDate, "adjustInterestToPaymentDate", value)
 
     @property
     def amortization_schedule(self):
         """
-        The amortization schedule of the instrument. it contains the following
-        information:      startdate,      enddate,      remainingnotional,
-        amortizationfrequency,      amount,      amortizationtype.   no default value
-        applies.
+        Definition of amortizations
         :return: list AmortizationItem
         """
         return self._get_list_parameter(AmortizationItem, "amortizationSchedule")
 
     @amortization_schedule.setter
     def amortization_schedule(self, value):
         self._set_list_parameter(AmortizationItem, "amortizationSchedule", value)
 
     @property
-    def barrier_definition(self):
+    def direction(self):
+        """
+        The direction of the leg. the possible values are:
+        - 'Paid' (the cash flows of the leg are paid to the counterparty),
+        - 'Received' (the cash flows of the leg are received from the counterparty).
+          Optional for a single leg instrument (like a bond), in that case default value
+          is Received. It is mandatory for a multi-instrument leg instrument (like Swap
+          or CDS leg).
+        :return: enum Direction
+        """
+        return self._get_enum_parameter(Direction, "direction")
+
+    @direction.setter
+    def direction(self, value):
+        self._set_enum_parameter(Direction, "direction", value)
+
+    @property
+    def index_average_method(self):
         """
-        :return: object BarrierDefinitionElement
+        :return: enum IndexAverageMethod
         """
-        return self._get_object_parameter(BarrierDefinitionElement, "barrierDefinition")
+        return self._get_enum_parameter(IndexAverageMethod, "indexAverageMethod")
 
-    @barrier_definition.setter
-    def barrier_definition(self, value):
-        self._set_object_parameter(BarrierDefinitionElement, "barrierDefinition", value)
+    @index_average_method.setter
+    def index_average_method(self, value):
+        self._set_enum_parameter(IndexAverageMethod, "indexAverageMethod", value)
 
     @property
-    def buy_sell(self):
+    def index_compounding_method(self):
         """
-        The indicator of the deal side. the possible values are:   buy: buying the
-        option,   sell: selling/writing the option.  mandatory. no default value
-        applies.
-        :return: enum BuySell
+        A flag that defines how the coupon rate is calculated from the reset floating
+        rates when the reset frequency is higher than the interest payment frequency
+        (e.g. daily index reset with quarterly interest payment). The possible values
+        are:
+        - Compounded (uses the compounded average rate from multiple fixings),
+        - Average (uses the arithmetic average rate from multiple fixings),
+        - Constant (uses the last published rate among multiple fixings),
+        - AdjustedCompounded (uses Chinese 7-day repo fixing),
+        - MexicanCompounded (uses Mexican Bremse fixing). Optional. By default
+          'Constant' is used.
+        :return: enum IndexCompoundingMethod
         """
-        return self._get_enum_parameter(BuySell, "buySell")
+        return self._get_enum_parameter(IndexCompoundingMethod, "indexCompoundingMethod")
 
-    @buy_sell.setter
-    def buy_sell(self, value):
-        self._set_enum_parameter(BuySell, "buySell", value)
+    @index_compounding_method.setter
+    def index_compounding_method(self, value):
+        self._set_enum_parameter(IndexCompoundingMethod, "indexCompoundingMethod", value)
 
     @property
     def index_reset_frequency(self):
         """
-        The reset frequency for the floating instrument. optional. by default, the reset
-        frequency associated to  the index defined for the floating leg is used.
+        The reset frequency in case the leg Type is Float. Optional. By default the
+        IndexTenor is used.
         :return: enum Frequency
         """
         return self._get_enum_parameter(Frequency, "indexResetFrequency")
 
     @index_reset_frequency.setter
     def index_reset_frequency(self, value):
         self._set_enum_parameter(Frequency, "indexResetFrequency", value)
 
     @property
-    def index_reset_type(self):
-        """
-        A flag that indicates if the floating rate index is reset before the coupon
-        period starts or at the end of the coupon period. the possible values are:
-        inadvance (resets the index before the start of the interest period),
-        inarrears (resets the index at the end of the interest period).  optional. the
-        default value is 'inadvance'.
-        :return: enum IndexResetType
-        """
-        return self._get_enum_parameter(IndexResetType, "indexResetType")
-
-    @index_reset_type.setter
-    def index_reset_type(self, value):
-        self._set_enum_parameter(IndexResetType, "indexResetType", value)
-
-    @property
-    def interest_calculation_convention(self):
-        """
-        The day count basis method convention used to calculate the interest payments.
-        optional. defaults to moneymarket.
-        :return: enum InterestCalculationConvention
-        """
-        return self._get_enum_parameter(
-            InterestCalculationConvention, "interestCalculationConvention"
-        )
-
-    @interest_calculation_convention.setter
-    def interest_calculation_convention(self, value):
-        self._set_enum_parameter(
-            InterestCalculationConvention, "interestCalculationConvention", value
-        )
-
-    @property
     def interest_calculation_method(self):
         """
-        The interest payment frequency (e.g : annual, semiannual, quartely). the default
-        value is selected based onnotionalccy.
+        The Day Count Basis method used to calculate the coupon interest payments.
+        Mandatory.
         :return: enum DayCountBasis
         """
         return self._get_enum_parameter(DayCountBasis, "interestCalculationMethod")
 
     @interest_calculation_method.setter
     def interest_calculation_method(self, value):
         self._set_enum_parameter(DayCountBasis, "interestCalculationMethod", value)
 
     @property
     def interest_payment_frequency(self):
         """
-        The interest payment frequency. by default, indextenor is used, if it is
-        defined. otherwise, the default value is indexresetfrequency.
+        The frequency of the interest payments. Optional if an instrument code/style
+        have been defined : in that case, value comes from reference data. Otherwise, it
+        is mandatory.
         :return: enum Frequency
         """
         return self._get_enum_parameter(Frequency, "interestPaymentFrequency")
 
     @interest_payment_frequency.setter
     def interest_payment_frequency(self, value):
         self._set_enum_parameter(Frequency, "interestPaymentFrequency", value)
 
     @property
+    def interest_type(self):
+        """
+        A flag that indicates whether the leg is fixed or float. Possible values are:
+        - 'Fixed' (the leg has a fixed coupon),
+        - 'Float' (the leg has a floating rate index). Mandatory.
+        :return: enum InterestType
+        """
+        return self._get_enum_parameter(InterestType, "interestType")
+
+    @interest_type.setter
+    def interest_type(self, value):
+        self._set_enum_parameter(InterestType, "interestType", value)
+
+    @property
     def payment_business_day_convention(self):
         """
-        The method to adjust dates to working days. the possible values are:
-        previousbusinessday,    nextbusinessday,    modified following,    nomoving,
-        bswmodifiedfollowing.  optional. if instrumentcode is defined, the value comes
-        from the instrument reference data. in case of a user-defined instrument, the
-        default value is'modifiedfollowing'.
+        The method to adjust dates to a working day. The possible values are:
+        - ModifiedFollowing (adjusts dates according to the Modified Following
+          convention - next business day unless is it goes into the next month,
+          preceeding is used in that  case),
+        - NextBusinessDay (adjusts dates according to the Following convention - Next
+          Business Day),
+        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
+          Previous Business Day),
+        - NoMoving (does not adjust dates),
+        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
+          convention). Optional. In case an instrument code/style has been defined,
+          value comes from bond reference data. Otherwise 'ModifiedFollowing' is used.
         :return: enum BusinessDayConvention
         """
-        return self._get_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention"
-        )
+        return self._get_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention")
 
     @payment_business_day_convention.setter
     def payment_business_day_convention(self, value):
-        self._set_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention", value
-        )
+        self._set_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention", value)
 
     @property
     def payment_roll_convention(self):
         """
-        The method to adjust payment dates when they fall at the end of the month (e.g.
-        28th of february, 30th, 31st). the possible values are:    last,    same,
-        last28,    same28.  optional. if instrumentcode is defined, the value comes from
-        the instrument reference data. in case of a user-defined instrument, the default
-        value is'last'.
+        Method to adjust payment dates when they fall at the end of the month (28th of
+        February, 30th, 31st). The possible values are:
+        - Last (For setting the calculated date to the last working day),
+        - Same (For setting the calculated date to the same day . In this latter case,
+          the date may be moved according to the date moving convention if it is a
+          non-working day),
+        - Last28 (For setting the calculated date to the last working day. 28FEB being
+          always considered as the last working day),
+        - Same28 (For setting the calculated date to the same day .28FEB being always
+          considered as the last working day). Optional. In case an instrument code has
+          been defined, value comes from bond reference data. Otherwise, 'SameDay' is
+          used.
         :return: enum DateRollingConvention
         """
         return self._get_enum_parameter(DateRollingConvention, "paymentRollConvention")
 
     @payment_roll_convention.setter
     def payment_roll_convention(self, value):
         self._set_enum_parameter(DateRollingConvention, "paymentRollConvention", value)
 
     @property
-    def payments(self):
-        """
-        An array of historical payments
-        :return: list InputFlow
-        """
-        return self._get_list_parameter(InputFlow, "payments")
-
-    @payments.setter
-    def payments(self, value):
-        self._set_list_parameter(InputFlow, "payments", value)
-
-    @property
     def stub_rule(self):
         """
-        The rule that defines whether coupon roll dates are aligned to the maturity or
-        issue date. the possible values are:    issue,    maturity,
-        shortfirstprorata,    shortfirstfull,    longfirstfull,    shortlastprorata.
-        optional. the default value is 'maturity'.
+        The rule that defines whether coupon roll dates are aligned on the  maturity or
+        the issue date.  The possible values are:
+        - ShortFirstProRata (to create a short period between the start date and the
+          first coupon date, and pay a smaller amount of interest for the short
+          period.All coupon dates are calculated backward from the maturity date),
+        - ShortFirstFull (to create a short period between the start date and the first
+          coupon date, and pay a regular coupon on the first coupon date. All coupon
+          dates are calculated backward from the maturity date),
+        - LongFirstFull (to create a long period between the start date and the second
+          coupon date, and pay a regular coupon on the second coupon date. All coupon
+          dates are calculated backward from the maturity date),
+        - ShortLastProRata (to create a short period between the last payment date and
+          maturity, and pay a smaller amount of interest for the short period. All
+          coupon dates are calculated forward from the start date). This property may
+          also be used in conjunction with firstRegularPaymentDate and
+          lastRegularPaymentDate; in that case the following values can be defined:
+        - Issue (all dates are aligned on the issue date),
+        - Maturity (all dates are aligned on the maturity date). Optional. By default
+          'Maturity' is used.
         :return: enum StubRule
         """
         return self._get_enum_parameter(StubRule, "stubRule")
 
     @stub_rule.setter
     def stub_rule(self, value):
         self._set_enum_parameter(StubRule, "stubRule", value)
 
     @property
-    def annualized_rebate(self):
-        """
-        An indicator if the rebate is adjusted according to the frequency of the cap.
-        for example, if the frequency of the cap is quarterly (frq:4), all rebates are
-        divided by four (approximately). the possible values are:   true: the rebates
-        are adjusted,   false: there is no rebate adjustment(the rebate value is assumed
-        to be annualized).  the default value is 'false'.
-        :return: bool
-        """
-        return self._get_parameter("annualizedRebate")
-
-    @annualized_rebate.setter
-    def annualized_rebate(self, value):
-        self._set_parameter("annualizedRebate", value)
-
-    @property
-    def cap_digital_payout_percent(self):
-        """
-        A percentage of notionalamount that is received (paid) by the option buyer
-        (seller) if the option expires on or above the cap strike. no default value
-        applies.
-        :return: float
-        """
-        return self._get_parameter("capDigitalPayoutPercent")
-
-    @cap_digital_payout_percent.setter
-    def cap_digital_payout_percent(self, value):
-        self._set_parameter("capDigitalPayoutPercent", value)
-
-    @property
-    def cap_strike_percent(self):
+    def end_date(self):
         """
-        The contractual strike rate of the cap. the value is expressed in percentages.
-        either capstrikepercent or capstrikepercentschedule must be provided. no default
-        value applies.
-        :return: float
+        Maturity date of the bond to override. Mandatory if instrument code has not been
+        defined and is_perpetual flag has been set to false. In case an instrument code
+        has been defined, value comes from bond reference data.
+        :return: str
         """
-        return self._get_parameter("capStrikePercent")
+        return self._get_parameter("endDate")
 
-    @cap_strike_percent.setter
-    def cap_strike_percent(self, value):
-        self._set_parameter("capStrikePercent", value)
+    @end_date.setter
+    def end_date(self, value):
+        self._set_parameter("endDate", value)
 
     @property
-    def cms_template(self):
+    def first_accrual_date(self):
         """
-        A reference to a common swap contract that represents the underlying swap in
-        case of a constant maturity swap contract (cms). example: eur_ab6e. no default
-        value applies.
+        Date at which bond starts accruing. Optional. In case an instrument code has
+        been defined, value comes from bond reference data. Otherwise default value is
+        the issue date of the bond.
         :return: str
         """
-        return self._get_parameter("cmsTemplate")
+        return self._get_parameter("firstAccrualDate")
 
-    @cms_template.setter
-    def cms_template(self, value):
-        self._set_parameter("cmsTemplate", value)
+    @first_accrual_date.setter
+    def first_accrual_date(self, value):
+        self._set_parameter("firstAccrualDate", value)
 
     @property
-    def end_date(self):
+    def first_regular_payment_date(self):
         """
-        The maturity or expiry date of the instrument. the value is expressed in iso
-        8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g. 2021-01-01t00:00:00z). either
-        tenor or enddate must be provided. the default value is valuationdate shifted
-        forward by tenor.
+        The first regular coupon payment date for leg with an odd first coupon.
+        Optional.
         :return: str
         """
-        return self._get_parameter("endDate")
+        return self._get_parameter("firstRegularPaymentDate")
 
-    @end_date.setter
-    def end_date(self, value):
-        self._set_parameter("endDate", value)
+    @first_regular_payment_date.setter
+    def first_regular_payment_date(self, value):
+        self._set_parameter("firstRegularPaymentDate", value)
 
     @property
-    def floor_digital_payout_percent(self):
+    def fixed_rate_percent(self):
         """
-        A percentage of notionalamount that is received (paid) by the option buyer
-        (seller) if the option expires on or below the floor strike. no default value
-        applies.
+        The fixed coupon rate in percentage. It is mandatory in case of a single leg
+        instrument. Otherwise, in case of multi leg instrument, it can be computed as
+        the Par rate.
         :return: float
         """
-        return self._get_parameter("floorDigitalPayoutPercent")
+        return self._get_parameter("fixedRatePercent")
 
-    @floor_digital_payout_percent.setter
-    def floor_digital_payout_percent(self, value):
-        self._set_parameter("floorDigitalPayoutPercent", value)
+    @fixed_rate_percent.setter
+    def fixed_rate_percent(self, value):
+        self._set_parameter("fixedRatePercent", value)
 
     @property
     def floor_strike_percent(self):
         """
         The contractual strike rate of the floor. the value is expressed in percentages.
-        either floorstrikepercent or floorstrikepercentschedule must be provided. no
-        default value applies.
+        if this parameter is set, the floor will apply to the leg with the same
+        parameters set in the swapLegDefinition (e.g.maturity, frequency, index,
+        discounting rule). no default value applies.
         :return: float
         """
         return self._get_parameter("floorStrikePercent")
 
     @floor_strike_percent.setter
     def floor_strike_percent(self, value):
         self._set_parameter("floorStrikePercent", value)
 
     @property
     def index_fixing_lag(self):
         """
-        The number of working days between the fixing date of the index and the start of
-        the coupon period ('inadvance') or the end of the coupon period ('inarrears').
-        optional. if indexfixingric or indexname is defined, the associated fixing lag
-        is selected. otherwise, the value is based on the first fixing that matches
-        currency and indextenor.
+        Defines the number of working days between the fixing date and the start of the
+        coupon period ('InAdvance') or the end of the coupon period ('InArrears').
+        Optional. By default 0 is used.
         :return: int
         """
         return self._get_parameter("indexFixingLag")
 
     @index_fixing_lag.setter
     def index_fixing_lag(self, value):
         self._set_parameter("indexFixingLag", value)
 
     @property
     def index_fixing_ric(self):
         """
-        The ric that carries the fixing value if the instrument has a floating interest.
-        optional. no default value applies.
+        The RIC that carries the fixing value. This value overrides the RIC associated
+        by default with the IndexName and IndexTenor. Optional.
         :return: str
         """
         return self._get_parameter("indexFixingRic")
 
     @index_fixing_ric.setter
     def index_fixing_ric(self, value):
         self._set_parameter("indexFixingRic", value)
 
     @property
-    def index_name(self):
+    def instrument_code(self):
         """
-        The name of the floating rate index (e.g. 'euribor'). optional. no default value
-        applies.
+        Code to define the bond instrument. It can be an ISIN, a RIC, a CUSIP or an
+        AssetId .
         :return: str
         """
-        return self._get_parameter("indexName")
+        return self._get_parameter("instrumentCode")
 
-    @index_name.setter
-    def index_name(self, value):
-        self._set_parameter("indexName", value)
-
-    @property
-    def index_tenor(self):
-        """
-        The period code indicating the maturity of the floating rate index. if
-        indexfixingric is defined, the values comes from the instrument reference data.
-        otherwise, the default value is the tenor equivalent toindexresetfrequency or
-        interestpaymentfrequency.
-        :return: str
-        """
-        return self._get_parameter("indexTenor")
-
-    @index_tenor.setter
-    def index_tenor(self, value):
-        self._set_parameter("indexTenor", value)
+    @instrument_code.setter
+    def instrument_code(self, value):
+        self._set_parameter("instrumentCode", value)
 
     @property
     def instrument_tag(self):
         """
-        A user defined string to identify the instrument. it can be used to link output
-        results to the instrument definition.limited to 40 characters. only alphabetic,
-        numeric and '- _.#=@' characters are supported. optional. no default value
-        applies.
+        User defined string to identify the instrument.It can be used to link output
+        results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
+        characters are supported. Optional.
         :return: str
         """
         return self._get_parameter("instrumentTag")
 
     @instrument_tag.setter
     def instrument_tag(self, value):
         self._set_parameter("instrumentTag", value)
 
     @property
-    def is_backward_looking_index(self):
+    def interest_payment_delay(self):
         """
-        An indicator if the underlying fixing rate is backward-looking (mostly
-        risk-free-rate). the possible values are: false: the underlying fixing rate is
-        forward-looking and the rate of each caplet is fixed at the start of the caplet
-        period. true: the underlying fixing rate is backward-looking. for backward
-        looking caps/floors the rate is accreted up to the end of the caplet period.
-        :return: bool
+        The number of working days between the end of coupon period and the actual
+        interest payment date. Optional. By default no delay (0) is applied.
+        :return: int
         """
-        return self._get_parameter("isBackwardLookingIndex")
+        return self._get_parameter("interestPaymentDelay")
 
-    @is_backward_looking_index.setter
-    def is_backward_looking_index(self, value):
-        self._set_parameter("isBackwardLookingIndex", value)
+    @interest_payment_delay.setter
+    def interest_payment_delay(self, value):
+        self._set_parameter("interestPaymentDelay", value)
 
     @property
-    def is_rfr(self):
+    def is_perpetual(self):
         """
+        Flag the defines wether the bond is perpetual or not in case of user defined
+        bond. Optional. In case an instrument code has been defined, value comes from
+        bond reference data. In case of user defined bond, default value is 'false'.
         :return: bool
         """
-        return self._get_parameter("isRfr")
+        return self._get_parameter("isPerpetual")
 
-    @is_rfr.setter
-    def is_rfr(self, value):
-        self._set_parameter("isRfr", value)
+    @is_perpetual.setter
+    def is_perpetual(self, value):
+        self._set_parameter("isPerpetual", value)
 
     @property
-    def is_term_rate(self):
+    def issue_date(self):
         """
-        :return: bool
+        Date of issuance of the bond to override. Mandatory if instrument code has not
+        been defined. In case an instrument code has been defined, value comes from bond
+        reference data.
+        :return: str
         """
-        return self._get_parameter("isTermRate")
+        return self._get_parameter("issueDate")
 
-    @is_term_rate.setter
-    def is_term_rate(self, value):
-        self._set_parameter("isTermRate", value)
+    @issue_date.setter
+    def issue_date(self, value):
+        self._set_parameter("issueDate", value)
+
+    @property
+    def last_regular_payment_date(self):
+        """
+        The last regular coupon payment date for leg with an odd last coupon. Optional.
+        :return: str
+        """
+        return self._get_parameter("lastRegularPaymentDate")
+
+    @last_regular_payment_date.setter
+    def last_regular_payment_date(self, value):
+        self._set_parameter("lastRegularPaymentDate", value)
 
     @property
     def notional_amount(self):
         """
-        The notional amount of the instrument. optional. the default value is
-        '1,000,000' in notionalccy.
+        The notional amount of the leg at the period start date. Optional. By default
+        1,000,000 is used.
         :return: float
         """
         return self._get_parameter("notionalAmount")
 
     @notional_amount.setter
     def notional_amount(self, value):
         self._set_parameter("notionalAmount", value)
 
     @property
     def notional_ccy(self):
         """
-        The currency of the instrument's notional amount. the value is expressed in iso
-        4217 alphabetical format (e.g. 'usd'). no default value applies. mandatory if
-        instrument code or instrument style has not been defined. in case an instrument
-        code/style has been defined, value may comes from the reference data.
+        The ISO code of the notional currency. Mandatory if instrument code or
+        instrument style has not been defined. In case an instrument code/style has been
+        defined, value may comes from the reference data.
         :return: str
         """
         return self._get_parameter("notionalCcy")
 
     @notional_ccy.setter
     def notional_ccy(self, value):
         self._set_parameter("notionalCcy", value)
 
     @property
     def payment_business_days(self):
         """
-        A list of comma-separated calendar codes to adjust dates (e.g. 'emu' or 'usa').
-        no default value applies.
+        A list of coma-separated calendar codes to adjust dates (e.g. 'EMU' or 'USA').
+        Optional. By default the calendar associated to notional_ccy is used.
         :return: str
         """
         return self._get_parameter("paymentBusinessDays")
 
     @payment_business_days.setter
     def payment_business_days(self, value):
         self._set_parameter("paymentBusinessDays", value)
 
     @property
-    def start_date(self):
+    def spread_bp(self):
         """
-        The start date of the instrument. this value is expressed in iso 8601 format:
-        yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g. 2021-01-01t00:00:00z). the default value is
-        valuationdate shifted forward by a month using market conventions.
-        :return: str
+        The spread in basis point that is added to the floating rate index index value.
+        Optional. By default 0 is used.
+        :return: float
         """
-        return self._get_parameter("startDate")
+        return self._get_parameter("spreadBp")
 
-    @start_date.setter
-    def start_date(self, value):
-        self._set_parameter("startDate", value)
+    @spread_bp.setter
+    def spread_bp(self, value):
+        self._set_parameter("spreadBp", value)
 
     @property
-    def tenor(self):
+    def template(self):
         """
-        The code indicating the tenor of the instrument (e.g. '5m'). mandatory if
-        enddate is not provided. no default value applies.
+        A reference to a Adfin instrument contract or the Adfin detailed contract.
+        Optional. Either instrument_code, template, or full definition must be provided.
         :return: str
         """
-        return self._get_parameter("tenor")
+        return self._get_parameter("template")
+
+    @template.setter
+    def template(self, value):
+        self._set_parameter("template", value)
 
-    @tenor.setter
-    def tenor(self, value):
-        self._set_parameter("tenor", value)
+    @property
+    def index_observation_method(self):
+        return self._get_enum_parameter(IndexObservationMethod, "indexObservationMethod")
+
+    @index_observation_method.setter
+    def index_observation_method(self, value):
+        self._set_enum_parameter(IndexObservationMethod, "indexObservationMethod", value)
+
+    @property
+    def fixed_rate_percent_schedule(self):
+        return self._get_parameter("fixedRatePercentSchedule")
+
+    @fixed_rate_percent_schedule.setter
+    def fixed_rate_percent_schedule(self, value):
+        self._set_parameter("fixedRatePercentSchedule", value)
+
+    @property
+    def instrument_type(self):
+        return self._instrument_type
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_pricing_parameters.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 # coding: utf8
 
-from typing import Optional
-from ._enums import (
+from typing import Optional, Union
+
+from ..._enums import (
     IndexConvexityAdjustmentIntegrationMethod,
     IndexConvexityAdjustmentMethod,
     PriceSide,
 )
 from ..._object_definition import ObjectDefinition
 
 
@@ -16,15 +17,15 @@
 
     Parameters
     ----------
     index_convexity_adjustment_integration_method : IndexConvexityAdjustmentIntegrationMethod, optional
 
     index_convexity_adjustment_method : IndexConvexityAdjustmentMethod, optional
 
-    price_side : PriceSide, optional
+    price_side : PriceSide or str, optional
         The quoted price side of the instrument. optional. default value is 'mid'.
     market_data_date : str, optional
         The market data date for pricing. Optional. By default, the marketDataDate date
         is the ValuationDate or Today
     market_value_in_deal_ccy : float, optional
         MarketValueInDealCcy to override and that will be used as pricing analysis input
         to compute VolatilityPercent. Optional. No override is applied by default. Note
@@ -35,42 +36,44 @@
     skip_first_cap_floorlet : bool, optional
         Indicates whether to take in consideration the first caplet
     valuation_date : str, optional
         The valuation date for pricing.  Optional. If not set the valuation date is
         equal to MarketDataDate or Today. For assets that contains a
         settlementConvention, the default valuation date  is equal to the settlementdate
         of the Asset that is usually the TradeDate+SettlementConvention.
+    implied_volatility_bp : float, optional
+        User defined implied normal volatility, expressed in basis points.
+    implied_volatility_percent : float, optional
+        User defined implied lognormal volatility, expressed in percent.
     """
 
     def __init__(
         self,
-        index_convexity_adjustment_integration_method: Optional[
-            IndexConvexityAdjustmentIntegrationMethod
-        ] = None,
-        index_convexity_adjustment_method: Optional[
-            IndexConvexityAdjustmentMethod
-        ] = None,
-        price_side: Optional[PriceSide] = None,
+        index_convexity_adjustment_integration_method: Optional[IndexConvexityAdjustmentIntegrationMethod] = None,
+        index_convexity_adjustment_method: Optional[IndexConvexityAdjustmentMethod] = None,
+        price_side: Union[PriceSide, str] = None,
         market_data_date: Optional[str] = None,
         market_value_in_deal_ccy: Optional[float] = None,
         report_ccy: Optional[str] = None,
         skip_first_cap_floorlet: Optional[bool] = None,
         valuation_date: Optional[str] = None,
+        implied_volatility_bp: Optional[float] = None,
+        implied_volatility_percent: Optional[float] = None,
     ) -> None:
         super().__init__()
-        self.index_convexity_adjustment_integration_method = (
-            index_convexity_adjustment_integration_method
-        )
+        self.index_convexity_adjustment_integration_method = index_convexity_adjustment_integration_method
         self.index_convexity_adjustment_method = index_convexity_adjustment_method
         self.price_side = price_side
         self.market_data_date = market_data_date
         self.market_value_in_deal_ccy = market_value_in_deal_ccy
         self.report_ccy = report_ccy
         self.skip_first_cap_floorlet = skip_first_cap_floorlet
         self.valuation_date = valuation_date
+        self.implied_volatility_bp = implied_volatility_bp
+        self.implied_volatility_percent = implied_volatility_percent
 
     @property
     def index_convexity_adjustment_integration_method(self):
         """
         :return: enum IndexConvexityAdjustmentIntegrationMethod
         """
         return self._get_enum_parameter(
@@ -87,23 +90,19 @@
         )
 
     @property
     def index_convexity_adjustment_method(self):
         """
         :return: enum IndexConvexityAdjustmentMethod
         """
-        return self._get_enum_parameter(
-            IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod"
-        )
+        return self._get_enum_parameter(IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod")
 
     @index_convexity_adjustment_method.setter
     def index_convexity_adjustment_method(self, value):
-        self._set_enum_parameter(
-            IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod", value
-        )
+        self._set_enum_parameter(IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod", value)
 
     @property
     def price_side(self):
         """
         The quoted price side of the instrument. Optional. Default value is 'mid'.
         :return: enum PriceSide
         """
@@ -175,7 +174,23 @@
         :return: str
         """
         return self._get_parameter("valuationDate")
 
     @valuation_date.setter
     def valuation_date(self, value):
         self._set_parameter("valuationDate", value)
+
+    @property
+    def implied_volatility_bp(self):
+        return self._get_parameter("impliedVolatilityBp")
+
+    @implied_volatility_bp.setter
+    def implied_volatility_bp(self, value):
+        self._set_parameter("impliedVolatilityBp", value)
+
+    @property
+    def implied_volatility_percent(self):
+        return self._get_parameter("impliedVolatilityPercent")
+
+    @implied_volatility_percent.setter
+    def implied_volatility_percent(self, value):
+        self._set_parameter("impliedVolatilityPercent", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cap_floor/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cap_floor/_definition.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 # coding: utf8
 
 from typing import Optional, Union, List, TYPE_CHECKING
 
 from ._cap_floor_definition import CapFloorInstrumentDefinition
 from ._cap_floor_pricing_parameters import PricingParameters
-from ._enums import (
+from ..._enums import (
     AdjustInterestToPaymentDate,
     IndexResetType,
     Frequency,
     DateRollingConvention,
     DayCountBasis,
     StubRule,
     BuySell,
     BusinessDayConvention,
+    PriceSide,
 )
-from ._models import (
+from ..._models import (
     AmortizationItem,
     BarrierDefinitionElement,
 )
 from .._base_definition import BaseDefinition
 from ..._enums import InterestCalculationConvention
 from ..._models import InputFlow
 from ....._tools import validate_types, try_copy_to_list
@@ -52,86 +53,48 @@
         The notional amount of the leg at the period start date. Optional. By default
         1,000,000 is used.
     index_name : str, optional
         The name of the floating rate index.
     index_tenor : str, optional
         The period code that represents the maturity of the floating rate index.
         Mandatory when the leg is float.
-    interest_payment_frequency : Frequency, optional
+    interest_payment_frequency : Frequency or str, optional
         The frequency of the interest payments. Optional if an instrument code/style
         have been defined : in that case, value comes from reference data. Otherwise, it
         is mandatory.
-    interest_calculation_method : DayCountBasis, str, optional
+    interest_calculation_method : DayCountBasis or str, optional
         The Day Count Basis method used to calculate the coupon interest payments.
         Mandatory.
-    payment_business_day_convention : BusinessDayConvention, optional
-        The method to adjust dates to a working day. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. In case an instrument code/style has been defined,
-          value comes from bond reference data. Otherwise 'ModifiedFollowing' is used.
-    payment_roll_convention : DateRollingConvention, optional
+    payment_business_day_convention : BusinessDayConvention or str, optional
+        The method to adjust dates to a working day.
+    payment_roll_convention : DateRollingConvention or str, optional
         The method to adjust payment dates whn they fall at the end of the month (28th
-        of February, 30th, 31st). The possible values are:
-        - Last (For setting the calculated date to the last working day),
-        - Same (For setting the calculated date to the same day . In this latter case,
-          the date may be moved according to the date moving convention if it is a
-          non-working day),
-        - Last28 (For setting the calculated date to the last working day. 28FEB being
-          always considered as the last working day),
-        - Same28 (For setting the calculated date to the same day .28FEB being always
-          considered as the last working day). Optional. By default 'SameDay' is used.
-    index_reset_frequency : Frequency, optional
+        of February, 30th, 31st). Optional. By default 'SameDay' is used.
+    index_reset_frequency : Frequency or str, optional
         The reset frequency in case the leg Type is Float. Optional. By default the
         IndexTenor is used.
-    index_reset_type : IndexResetType, optional
+    index_reset_type : IndexResetType or str, optional
         A flag that indicates if the floating rate index is reset before the coupon
-        period starts or at the end of the coupon period. The possible values are:
-        - InAdvance (resets the index before the start of the interest period),
-        - InArrears (resets the index at the end of the interest period). Optional. By
+        period starts or at the end of the coupon period. Optional. By
           default 'InAdvance' is used.
     index_fixing_lag : int, optional
         Defines the positive number of working days between the fixing date and the
         start of the coupon period ('InAdvance') or the end of the coupon period
         ('InArrears'). Optional. By default 0 is used.
     amortization_schedule : list of AmortizationItem, optional
         Definition of amortizations
     payment_business_days : str, optional
         A list of coma-separated calendar codes to adjust dates (e.g. 'EMU' or 'USA').
-        Optional. By default the calendar associated to NotionalCcy is used.
-    adjust_interest_to_payment_date : AdjustInterestToPaymentDate, optional
+        Optional. By default, the calendar associated to NotionalCcy is used.
+    adjust_interest_to_payment_date : AdjustInterestToPaymentDate or str, optional
         A flag that indicates if the coupon dates are adjusted to the payment dates.
-        Optional. By default 'false' is used.
-    stub_rule : StubRule, optional
+        Optional. By default, 'false' is used.
+    stub_rule : StubRule or str, optional
         The rule that defines whether coupon roll dates are aligned on the  maturity or
-        the issue date. The possible values are:
-        - ShortFirstProRata (to create a short period between the start date and the
-          first coupon date, and pay a smaller amount of interest for the short
-          period.All coupon dates are calculated backward from the maturity date),
-        - ShortFirstFull (to create a short period between the start date and the first
-          coupon date, and pay a regular coupon on the first coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - LongFirstFull (to create a long period between the start date and the second
-          coupon date, and pay a regular coupon on the second coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - ShortLastProRata (to create a short period between the last payment date and
-          maturity, and pay a smaller amount of interest for the short period. All
-          coupon dates are calculated forward from the start date). This property may
-          also be used in conjunction with firstRegularPaymentDate and
-          lastRegularPaymentDate; in that case the following values can be defined:
-        - Issue (all dates are aligned on the issue date),
-        - Maturity (all dates are aligned on the maturity date). Optional. By default
-          'Maturity' is used.
+        the issue date. Optional. By default, 'Maturity' is used.
     barrier_definition : BarrierDefinitionElement, optional
 
     buy_sell : BuySell, optional
         The side of the deal. Possible values:
         - Buy
         - Sell
     annualized_rebate : bool, optional
@@ -156,14 +119,24 @@
     pricing_parameters : PricingParameters, optional
         The pricing parameters to apply to this instrument. Optional. If pricing
         parameters are not provided at this level parameters defined globally at the
         request level are used. If no pricing parameters are provided globally default
         values apply.
     extended_params : dict, optional
         If necessary other parameters
+    index_price_side : PriceSide, optional
+        The side that is selected for an index supporting Bid/Ask/Mid (which is the case of deposits).
+    cap_strike_percent_schedule : dict, optional
+        The schedule of the dates and cap strike rates. The rates are expressed in percentages.
+        Either capStrikePercent or capStrikePercentSchedule must be provided.
+        No default value applies.
+    floor_strike_percent_schedule : dict, optional
+        The schedule of the dates and floor strike rates. The rates are expressed in percentages.
+        Either FloorStrikePercent or FloorStrikePercentSchedule must be provided.
+        No default value applies.
 
     Methods
     -------
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_stream(session=session)
         Get stream object of this definition
@@ -203,56 +176,60 @@
     ...        "MarketValueInDealCcy",
     ...        "MarketValueInReportCcy",
     ...        "ErrorMessage",
     ...    ],
     ...)
     >>> response = definition.get_data()
 
-     Using get_stream
-     >>> response = definition.get_stream()
+    Using get_stream
+
+    >>> response = definition.get_stream()
     """
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
         index_name: Optional[str] = None,
         index_tenor: Optional[str] = None,
         interest_payment_frequency: Union[Frequency, str] = None,
         interest_calculation_method: Union[DayCountBasis, str] = None,
-        payment_business_day_convention: Optional[BusinessDayConvention] = None,
-        payment_roll_convention: Optional[DateRollingConvention] = None,
-        index_reset_frequency: Optional[Frequency] = None,
-        index_reset_type: Optional[IndexResetType] = None,
+        payment_business_day_convention: Union[BusinessDayConvention, str] = None,
+        payment_roll_convention: Union[DateRollingConvention, str] = None,
+        index_reset_frequency: Union[Frequency, str] = None,
+        index_reset_type: Union[IndexResetType, str] = None,
         index_fixing_lag: Optional[int] = None,
         amortization_schedule: Optional[List[AmortizationItem]] = None,
         payment_business_days: Optional[str] = None,
-        adjust_interest_to_payment_date: Optional[AdjustInterestToPaymentDate] = None,
-        stub_rule: Optional[StubRule] = None,
+        adjust_interest_to_payment_date: Union[AdjustInterestToPaymentDate, str] = None,
+        stub_rule: Union[StubRule, str] = None,
         barrier_definition: Optional[BarrierDefinitionElement] = None,
         buy_sell: Union[BuySell, str] = None,
-        interest_calculation_convention: Optional[InterestCalculationConvention] = None,
+        interest_calculation_convention: Union[InterestCalculationConvention, str] = None,
         payments: Optional[List[InputFlow]] = None,
         annualized_rebate: Optional[bool] = None,
         cap_digital_payout_percent: Optional[float] = None,
         cap_strike_percent: Optional[float] = None,
         cms_template: Optional[str] = None,
         floor_digital_payout_percent: Optional[float] = None,
         floor_strike_percent: Optional[float] = None,
         index_fixing_ric: Optional[str] = None,
         is_backward_looking_index: Optional[bool] = None,
         is_rfr: Optional[bool] = None,
         is_term_rate: Optional[bool] = None,
         fields: "OptStrStrs" = None,
         pricing_parameters: Optional[PricingParameters] = None,
         extended_params: "ExtendedParams" = None,
+        index_price_side: Optional[PriceSide] = None,
+        cap_strike_percent_schedule: Optional[dict] = None,
+        floor_strike_percent_schedule: Optional[dict] = None,
     ):
         validate_types(index_fixing_lag, [int, type(None)], "index_fixing_lag")
 
         amortization_schedule = try_copy_to_list(amortization_schedule)
         payments = try_copy_to_list(payments)
         fields = try_copy_to_list(fields)
         definition = CapFloorInstrumentDefinition(
@@ -285,14 +262,17 @@
             cms_template=cms_template,
             floor_digital_payout_percent=floor_digital_payout_percent,
             floor_strike_percent=floor_strike_percent,
             index_fixing_ric=index_fixing_ric,
             is_backward_looking_index=is_backward_looking_index,
             is_rfr=is_rfr,
             is_term_rate=is_term_rate,
+            index_price_side=index_price_side,
+            cap_strike_percent_schedule=cap_strike_percent_schedule,
+            floor_strike_percent_schedule=floor_strike_percent_schedule,
         )
         super().__init__(
             definition=definition,
             fields=fields,
             pricing_parameters=pricing_parameters,
             extended_params=extended_params,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -11,20 +11,21 @@
     "ProtectionLegDefinition",
     "Seniority",
     "StubRule",
 )
 
 from ._cds_pricing_parameters import PricingParameters
 from ._definition import Definition
-from ._enums import (
+from ..._enums import (
     BusinessDayConvention,
     BusinessDayConvention,
     CdsConvention,
     DayCountBasis,
     Direction,
     DocClause,
     Frequency,
     Seniority,
     StubRule,
 )
 
-from ._models import PremiumLegDefinition, ProtectionLegDefinition
+from ._premium_leg_definition import PremiumLegDefinition
+from ._protection_leg_definition import ProtectionLegDefinition
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_cds_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_cds_definition.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,20 +1,18 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._instrument_definition import InstrumentDefinition
-from ._enums import (
+from ..._enums import (
     BusinessDayConvention,
     CdsConvention,
 )
-from ._models import (
-    PremiumLegDefinition,
-    ProtectionLegDefinition,
-)
+from ._premium_leg_definition import PremiumLegDefinition
+from ._protection_leg_definition import ProtectionLegDefinition
 
 
 class CdsInstrumentDefinition(InstrumentDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
@@ -23,20 +21,16 @@
     instrument_tag : str, optional
         User defined string to identify the instrument.It can be used to link output
         results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
         characters are supported. Optional.
     instrument_code : str, optional
         A cds RIC that is used to retrieve the description of the cds contract.
         Optional. If null, the protection_leg and the premium_leg  must be provided.
-    cds_convention : CdsConvention, optional
-        Define the cds convention. Possible values are:
-        - 'ISDA' (start_date will default to accrued_begin_date, end_date will be
-          adjusted to IMM Date),
-        - 'UserDefined' (start_date will default to step_in_date, end_date will not be
-          adjusted). Optional. Defaults to 'ISDA'.
+    cds_convention : CdsConvention or str, optional
+        Define the cds convention. Optional. Defaults to 'ISDA'.
     trade_date : str, optional
         The date the cds contract was created. Optional. By default the valuation date.
     step_in_date : str, optional
         The effective protection date. Optional. By default the trade_date + 1 calendar.
     start_date : str, optional
         The date the cds starts accruing interest. Its effective date. Optional. By
         default it is the accrued_begin_date (the last IMM date before trade_date) if
@@ -44,38 +38,18 @@
     end_date : str, optional
         The maturity date of the cds contract. Mandatory if instrument_code is null.
         Either the end_date or the tenor must be provided.
     tenor : str, optional
         The period code that represents the time between the start date and end date the
         contract. Mandatory if instrument_code is null. Either the end_date or the tenor
         must be provided.
-    start_date_moving_convention : BusinessDayConvention, optional
-        The method to adjust the start_date. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. By default 'NoMoving' is used.
+    start_date_moving_convention : BusinessDayConvention or str, optional
+        The method to adjust the start_date. Optional. By default 'NoMoving' is used.
     end_date_moving_convention : BusinessDayConvention, optional
-        The method to adjust the end_date.. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. By default 'NoMoving' is used.
+        The method to adjust the end_date. Optional. By default 'NoMoving' is used.
     adjust_to_isda_end_date : bool, optional
         The way the end_date is adjusted if computed from tenor input.    The possible
         values are:
         - true ( the end_date is an IMM date computed from start_date according to ISDA
           rules, ),
         - false ( the end_date is computed from start_date according to
           end_dateMovingConvention), Optional. By default true is used if cds_convention
@@ -90,22 +64,22 @@
         The last cashflow date. Optional. By default it is the last cashflow date.
     """
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
         instrument_code: Optional[str] = None,
-        cds_convention: Optional[CdsConvention] = None,
+        cds_convention: Union[CdsConvention, str] = None,
         trade_date: Optional[str] = None,
         step_in_date: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
-        start_date_moving_convention: Optional[BusinessDayConvention] = None,
-        end_date_moving_convention: Optional[BusinessDayConvention] = None,
+        start_date_moving_convention: Union[BusinessDayConvention, str] = None,
+        end_date_moving_convention: Union[BusinessDayConvention, str] = None,
         adjust_to_isda_end_date: Optional[bool] = None,
         protection_leg: Optional[ProtectionLegDefinition] = None,
         premium_leg: Optional[PremiumLegDefinition] = None,
         accrued_begin_date: Optional[str] = None,
     ) -> None:
         super().__init__()
         self.instrument_tag = instrument_tag
@@ -119,16 +93,15 @@
         self.start_date_moving_convention = start_date_moving_convention
         self.end_date_moving_convention = end_date_moving_convention
         self.adjust_to_isda_end_date = adjust_to_isda_end_date
         self.protection_leg = protection_leg
         self.premium_leg = premium_leg
         self.accrued_begin_date = accrued_begin_date
 
-    @classmethod
-    def get_instrument_type(cls):
+    def get_instrument_type(self):
         return "Cds"
 
     @property
     def cds_convention(self):
         """
         Define the cds convention. Possible values are:
         - 'ISDA' (start_date will default to accrued_begin_date, end_date will be
@@ -155,23 +128,19 @@
         - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
           Previous Business Day),
         - NoMoving (does not adjust dates),
         - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
           convention). Optional. By default 'NoMoving' is used.
         :return: enum BusinessDayConvention
         """
-        return self._get_enum_parameter(
-            BusinessDayConvention, "endDateMovingConvention"
-        )
+        return self._get_enum_parameter(BusinessDayConvention, "endDateMovingConvention")
 
     @end_date_moving_convention.setter
     def end_date_moving_convention(self, value):
-        self._set_enum_parameter(
-            BusinessDayConvention, "endDateMovingConvention", value
-        )
+        self._set_enum_parameter(BusinessDayConvention, "endDateMovingConvention", value)
 
     @property
     def premium_leg(self):
         """
         The Premium Leg of the CDS. It is a swap leg paying a fixed coupon. Mandatory if
         instrument_code is null. Optional if instrument_code not null.
         :return: object PremiumLegDefinition
@@ -207,23 +176,19 @@
         - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
           Previous Business Day),
         - NoMoving (does not adjust dates),
         - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
           convention). Optional. By default 'NoMoving' is used.
         :return: enum BusinessDayConvention
         """
-        return self._get_enum_parameter(
-            BusinessDayConvention, "startDateMovingConvention"
-        )
+        return self._get_enum_parameter(BusinessDayConvention, "startDateMovingConvention")
 
     @start_date_moving_convention.setter
     def start_date_moving_convention(self, value):
-        self._set_enum_parameter(
-            BusinessDayConvention, "startDateMovingConvention", value
-        )
+        self._set_enum_parameter(BusinessDayConvention, "startDateMovingConvention", value)
 
     @property
     def accrued_begin_date(self):
         """
         The last cashflow date. Optional. By default it is the last cashflow date.
         :return: str
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_cds_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_cds_pricing_parameters.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_definition.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,21 +1,19 @@
 # coding: utf8
 
 from typing import Optional, TYPE_CHECKING
 
 from . import PricingParameters
 from ._cds_definition import CdsInstrumentDefinition
-from ._enums import (
+from ..._enums import (
     BusinessDayConvention,
     CdsConvention,
 )
-from ._models import (
-    PremiumLegDefinition,
-    ProtectionLegDefinition,
-)
+from ._premium_leg_definition import PremiumLegDefinition
+from ._protection_leg_definition import ProtectionLegDefinition
 from .._base_definition import BaseDefinition
 from ....._tools import try_copy_to_list
 
 if TYPE_CHECKING:
     from ....._types import ExtendedParams, OptStrStrs
 
 
@@ -30,19 +28,15 @@
         User defined string to identify the instrument.It can be used to link output
         results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
         characters are supported. Optional.
     instrument_code : str, optional
         A cds RIC that is used to retrieve the description of the cds contract.
         Optional. If null, the protection_leg and the premium_leg  must be provided.
     cds_convention : CdsConvention, optional
-        Define the cds convention. Possible values are:
-        - 'ISDA' (start_date will default to accrued_begin_date, end_date will be
-          adjusted to IMM Date),
-        - 'UserDefined' (start_date will default to step_in_date, end_date will not be
-          adjusted). Optional. Defaults to 'ISDA'.
+        Define the cds convention. Optional. Defaults to 'ISDA'.
     trade_date : str, optional
         The date the cds contract was created. Optional. By default the valuation date.
     step_in_date : str, optional
         The effective protection date. Optional. By default the trade_date + 1 calendar.
     start_date : str, optional
         The date the cds starts accruing interest. Its effective date. Optional. By
         default it is the accrued_begin_date (the last IMM date before trade_date) if
@@ -51,45 +45,20 @@
         The maturity date of the cds contract. Mandatory if instrument_code is null.
         Either the end_date or the tenor must be provided.
     tenor : str, optional
         The period code that represents the time between the start date and end date the
         contract. Mandatory if instrument_code is null. Either the end_date or the tenor
         must be provided.
     start_date_moving_convention : BusinessDayConvention, optional
-        The method to adjust the start_date. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. By default 'NoMoving' is used.
+        The method to adjust the start_date. Optional. By default 'NoMoving' is used.
     end_date_moving_convention : BusinessDayConvention, optional
-        The method to adjust the end_date.. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. By default 'NoMoving' is used.
+        The method to adjust the end_date. Optional. By default 'NoMoving' is used.
     adjust_to_isda_end_date : bool, optional
-        The way the end_date is adjusted if computed from tenor input.    The possible
-        values are:
-        - true ( the end_date is an IMM date computed from start_date according to ISDA
-          rules, ),
-        - false ( the end_date is computed from start_date according to
-          end_dateMovingConvention), Optional. By default true is used if cds_convention
-          is ISDA, else false is used.
+        The way the end_date is adjusted if computed from tenor input. Optional.
+        By default true is used if cds_convention is ISDA, else false is used.
     protection_leg : ProtectionLegDefinition, optional
         The Protection Leg of the CDS. It is the default leg. Mandatory if instrumenCode
         is null. Optional if instrument_code not null.
     premium_leg : PremiumLegDefinition, optional
         The Premium Leg of the CDS. It is a swap leg paying a fixed coupon. Mandatory if
         instrument_code is null. Optional if instrument_code not null.
     accrued_begin_date : str, optional
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_premium_leg_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_premium_leg_definition.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._instrument_definition import ObjectDefinition
-from ._enums import (
+from ..._enums import (
     BusinessDayConvention,
     DayCountBasis,
     Direction,
     Frequency,
     StubRule,
 )
 
@@ -15,107 +15,76 @@
 class PremiumLegDefinition(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    direction : Direction, optional
-        The direction of the leg. the possible values are:
-        - 'Paid' (the cash flows of the leg are paid to the counterparty),
-        - 'Received' (the cash flows of the leg are received from the counterparty).
-          Optional for a single leg instrument (like a bond), in that case default value
-          is Received. It is mandatory for a multi-instrument leg instrument (like Swap
-          or CDS leg).
+    direction : Direction or str, optional
+        The direction of the leg.
     notional_ccy : str, optional
         The ISO code of the notional currency. Mandatory if instrument code or
         instrument style has not been defined. In case an instrument code/style has been
         defined, value may comes from the reference data.
     notional_amount : float, optional
         The notional amount of the leg at the period start date. Optional. By default
         1,000,000 is used.
     fixed_rate_percent : float, optional
         The fixed coupon rate in percentage. It is mandatory in case of a single leg
         instrument. Otherwise, in case of multi leg instrument, it can be computed as
         the Par rate.
-    interest_payment_frequency : Frequency, optional
+    interest_payment_frequency : Frequency or str, optional
         The frequency of the interest payments. Optional if an instrument code/style
         have been defined : in that case, value comes from reference data. Otherwise, it
         is mandatory.
-    interest_calculation_method : DayCountBasis, optional
+    interest_calculation_method : DayCountBasis or str, optional
         The Day Count Basis method used to calculate the coupon interest payments.
         Mandatory.
-    accrued_calculation_method : DayCountBasis, optional
+    accrued_calculation_method : DayCountBasis or str, optional
         The Day Count Basis method used to calculate the accrued interest payments.
         Optional. By default, the same value than interest_calculation_method is used.
-    payment_business_day_convention : BusinessDayConvention, optional
-        The method to adjust dates to a working day. The possible values are:
-        - ModifiedFollowing (adjusts dates according to the Modified Following
-          convention - next business day unless is it goes into the next month,
-          preceeding is used in that  case),
-        - NextBusinessDay (adjusts dates according to the Following convention - Next
-          Business Day),
-        - PreviousBusinessDay (adjusts dates  according to the Preceeding convention -
-          Previous Business Day),
-        - NoMoving (does not adjust dates),
-        - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
-          convention). Optional. In case an instrument code/style has been defined,
-          value comes from bond reference data. Otherwise 'ModifiedFollowing' is used.
+    payment_business_day_convention : BusinessDayConvention or str, optional
+        The method to adjust dates to a working day. Optional.
+        In case an instrument code/style has been defined, value comes from
+        bond reference data. Otherwise 'ModifiedFollowing' is used.
     first_regular_payment_date : str, optional
         The first regular coupon payment date for leg with an odd first coupon.
         Optional.
     last_regular_payment_date : str, optional
         The last regular coupon payment date for leg with an odd last coupon. Optional.
     payment_business_days : str, optional
         A list of coma-separated calendar codes to adjust dates (e.g. 'EMU' or 'USA').
         Optional. By default the calendar associated to notional_ccy is used.
-    stub_rule : StubRule, optional
+    stub_rule : StubRule or str, optional
         The rule that defines whether coupon roll dates are aligned on the  maturity or
-        the issue date.  The possible values are:
-        - ShortFirstProRata (to create a short period between the start date and the
-          first coupon date, and pay a smaller amount of interest for the short
-          period.All coupon dates are calculated backward from the maturity date),
-        - ShortFirstFull (to create a short period between the start date and the first
-          coupon date, and pay a regular coupon on the first coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - LongFirstFull (to create a long period between the start date and the second
-          coupon date, and pay a regular coupon on the second coupon date. All coupon
-          dates are calculated backward from the maturity date),
-        - ShortLastProRata (to create a short period between the last payment date and
-          maturity, and pay a smaller amount of interest for the short period. All
-          coupon dates are calculated forward from the start date). This property may
-          also be used in conjunction with first_regular_payment_date and
-          last_regular_payment_date; in that case the following values can be defined:
-        - Issue (all dates are aligned on the issue date),
-        - Maturity (all dates are aligned on the maturity date). Optional. By default
-          'Maturity' is used.
+        the issue date. Optional. By default 'Maturity' is used.
     accrued_paid_on_default : bool, optional
         Specifies whether the accrued is paid at the credit event date or not.
         - true :  the accrued is paid at the credit event date
         - false :  the accrued is not paid at the credit event date Optional. Defaults
           to false.
     interest_payment_ccy : str, optional
         The ISO code of the interest payment currency. Mandatory.
     """
 
     def __init__(
         self,
         *,
-        direction: Optional[Direction] = None,
+        direction: Union[Direction, str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
         fixed_rate_percent: Optional[float] = None,
-        interest_payment_frequency: Optional[Frequency] = None,
-        interest_calculation_method: Optional[DayCountBasis] = None,
-        accrued_calculation_method: Optional[DayCountBasis] = None,
-        payment_business_day_convention: Optional[BusinessDayConvention] = None,
+        interest_payment_frequency: Union[Frequency, str] = None,
+        interest_calculation_method: Union[DayCountBasis, str] = None,
+        accrued_calculation_method: Union[DayCountBasis, str] = None,
+        payment_business_day_convention: Union[BusinessDayConvention, str] = None,
         first_regular_payment_date: Optional[str] = None,
         last_regular_payment_date: Optional[str] = None,
         payment_business_days: Optional[str] = None,
-        stub_rule: Optional[StubRule] = None,
+        stub_rule: Union[StubRule, str] = None,
         accrued_paid_on_default: Optional[bool] = None,
         interest_payment_ccy: Optional[str] = None,
     ) -> None:
         super().__init__()
         self.direction = direction
         self.notional_ccy = notional_ccy
         self.notional_amount = notional_amount
@@ -201,23 +170,19 @@
           Previous Business Day),
         - NoMoving (does not adjust dates),
         - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following
           convention). Optional. In case an instrument code/style has been defined,
           value comes from bond reference data. Otherwise 'ModifiedFollowing' is used.
         :return: enum BusinessDayConvention
         """
-        return self._get_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention"
-        )
+        return self._get_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention")
 
     @payment_business_day_convention.setter
     def payment_business_day_convention(self, value):
-        self._set_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention", value
-        )
+        self._set_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention", value)
 
     @property
     def stub_rule(self):
         """
         The rule that defines whether coupon roll dates are aligned on the  maturity or
         the issue date.  The possible values are:
         - ShortFirstProRata (to create a short period between the start date and the
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cds/_protection_leg_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cds/_protection_leg_definition.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,63 +1,44 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._instrument_definition import ObjectDefinition
-from ._enums import (
+from ..._enums import (
     Direction,
     DocClause,
     Seniority,
 )
 
 
 class ProtectionLegDefinition(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    direction : Direction, optional
-        The direction of the leg. the possible values are:
-        - 'Paid' (the cash flows of the leg are paid to the counterparty),
-        - 'Received' (the cash flows of the leg are received from the counterparty).
-          Optional for a single leg instrument (like a bond), in that case default value
-          is Received. It is mandatory for a multi-instrument leg instrument (like Swap
-          or CDS leg).
+    direction : Direction or str, optional
+        The direction of the leg. Optional for a single leg instrument (like a bond), in that case default value
+        is Received. It is mandatory for a multi-instrument leg instrument (like Swap
+        or CDS leg).
     notional_ccy : str, optional
         The ISO code of the notional currency. Mandatory if instrument code or
         instrument style has not been defined. In case an instrument code/style has been
         defined, value may comes from the reference data.
     notional_amount : float, optional
         The notional amount of the leg at the period start date. Optional. By default
         1,000,000 is used.
-    doc_clause : DocClause, optional
-        The restructuring clause or credit event for Single Name Cds. The possible
-        values are:
-        - CumRestruct14,
-        - ModifiedRestruct14,
-        - ModModRestruct14,
-        - ExRestruct14,
-        - CumRestruct03,
-        - ModifiedRestruct03,
-        - ModModRestruct03,
-        - ExRestruct03. Optional. By default the doc_clause of the reference_entity's
-          Primary Ric is used.
-    seniority : Seniority, optional
-        The order of repayment in the case of a credit event for Single Name Cds. The
-        possible values are:
-        - Secured (Secured Debt (Corporate/Financial) or Domestic Currency Sovereign
-          Debt (Government)),
-        - SeniorUnsecured (Senior Unsecured Debt (Corporate/Financial) or Foreign
-          Currency Sovereign Debt (Government)),
-        - Subordinated (Subordinated or Lower Tier 2 Debt (Banks)),
-        - JuniorSubordinated (Junior Subordinated or Upper Tier 2 Debt (Banks)),
-        - Preference (Preference Shares or Tier 1 Capital (Banks)). Optional. By default
-          the seniority of the reference_entity's Primary Ric is used.
+    doc_clause : DocClause or str, optional
+        The restructuring clause or credit event for Single Name Cds. Optional.
+        By default the doc_clause of the reference_entity's
+        Primary Ric is used.
+    seniority : Seniority or str, optional
+        The order of repayment in the case of a credit event for Single Name Cds. Optional. By default
+        the seniority of the reference_entity's Primary Ric is used.
     index_factor : float, optional
         The factor that is applied to the notional in case a credit event happens in one
         of the constituents of the Cds Index. Optional. By default no factor (1)
         applies.
     index_series : int, optional
         The series of the Cds Index.  Optional. By default the series of the BenchmarkRic
         is used.
@@ -77,19 +58,19 @@
         The cashSettlementRule of the CDS. Optional. By default "3WD" (3 week days) is
         used.
     """
 
     def __init__(
         self,
         *,
-        direction: Optional[Direction] = None,
+        direction: Union[Direction, str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
-        doc_clause: Optional[DocClause] = None,
-        seniority: Optional[Seniority] = None,
+        doc_clause: Union[DocClause, str] = None,
+        seniority: Union[Seniority, str] = None,
         index_factor: Optional[float] = None,
         index_series: Optional[int] = None,
         recovery_rate: Optional[float] = None,
         recovery_rate_percent: Optional[float] = None,
         reference_entity: Optional[str] = None,
         settlement_convention: Optional[str] = None,
     ) -> None:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,18 +9,18 @@
     "LegDefinition",
     "PriceSide",
     "PricingParameters",
 )
 
 
 from ._definition import Definition
-from ._enums import (
+from ..._enums import (
     BuySell,
     FxCrossType,
     FxLegType,
     FxSwapCalculationMethod,
     ImpliedDepositDateConvention,
     PriceSide,
 )
 from ._fx_cross_leg_definition import LegDefinition
 from ._fx_cross_pricing_parameters import PricingParameters
-from ._models import FxPoint
+from ..._models import FxPoint
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_definition.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # coding: utf8
 
 from typing import Optional, List, TYPE_CHECKING
 
-from ._enums import FxCrossType
+from ..._enums import FxCrossType
 from ._fx_cross_definition import FxCrossInstrumentDefinition
 from ._fx_cross_leg_definition import LegDefinition
 from ._fx_cross_pricing_parameters import PricingParameters
 from .._base_definition import BaseDefinition
 from ....._tools import try_copy_to_list
 
 if TYPE_CHECKING:
@@ -24,18 +24,16 @@
         User defined string to identify the instrument.It can be used to link output
         results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
         characters are supported. Optional.
     legs : list of LegDefinition, optional
         Extra parameters to describe further the contract. 1 leg is mandatory for
         Forwards and NDFs contracts. 2 legs are required for Swaps, and FwdFwdSwaps
         contracts. Optional for Spot contracts.
-    fx_cross_type : FxCrossType, optional
-        The type of the Fx Cross instrument :  'FxSpot', 'FxForward',
-        'FxNonDeliverableForward', 'FxSwap', 'MultiLeg' or 'FxForwardForward'.
-        Mandatory.
+    fx_cross_type : FxCrossType or str, optional
+        The type of the Fx Cross instrument. Mandatory.
     fx_cross_code : str, optional
         The ISO code of the cross currency (e.g. 'EURCHF'). Mandatory.
     ndf_fixing_settlement_ccy : str, optional
         In case of a NDF contract, the ISO code of the settlement currency (e.g. 'EUR'
         ). Optional.
     reference_spot_rate : float, optional
         Contractual Spot Rate the counterparties agreed. It is used to compute the
@@ -64,47 +62,52 @@
         compute.
     pricing_parameters : PricingParameters, optional
         The pricing parameters to apply to this instrument. Optional. If pricing
         parameters are not provided at this level parameters defined globally at the
         request level are used. If no pricing parameters are provided globally default
         values apply.
     extended_params : dict, optional
-        If necessary other parameters
+        If necessary other parameters.
+    settlement_ccy : str, optional
+        This settlement currency code in case of an FxNonDeliverableForward (NDF) contract.
+        The value is expressed in ISO 4217 alphabetical format (e.g., 'USD').
+        Default value is 'USD'.
 
     Methods
     -------
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_stream(session=session)
         Get stream quantitative analytic service subscription
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> definition = rdf.cross.Definition(
-     ...     instrument_tag="00102700008910C",
-     ...     fx_cross_type=rdf.cross.FxCrossType.FX_FORWARD,
-     ...     fx_cross_code="USDEUR",
-     ...     legs=[rdf.cross.LegDefinition(end_date="2015-04-09T00:00:00Z")],
-     ...     pricing_parameters=rdf.cross.PricingParameters(
-     ...         valuation_date="2015-02-02T00:00:00Z",
-     ...         price_side=rdf.cross.PriceSide.MID,
-     ...     ),
-     ...     fields=[
-     ...         "InstrumentTag",
-     ...         "ValuationDate",
-     ...         "InstrumentDescription",
-     ...         "FxOutrightCcy1Ccy2",
-     ...     ],
-     ... )
-     >>> response = definition.get_data()
-     >>> response.data.df
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> definition = rdf.cross.Definition(
+    ...     instrument_tag="00102700008910C",
+    ...     fx_cross_type=rdf.cross.FxCrossType.FX_FORWARD,
+    ...     fx_cross_code="USDEUR",
+    ...     legs=[rdf.cross.LegDefinition(end_date="2015-04-09T00:00:00Z")],
+    ...     pricing_parameters=rdf.cross.PricingParameters(
+    ...         valuation_date="2015-02-02T00:00:00Z",
+    ...         price_side=rdf.cross.PriceSide.MID,
+    ...     ),
+    ...     fields=[
+    ...         "InstrumentTag",
+    ...         "ValuationDate",
+    ...         "InstrumentDescription",
+    ...         "FxOutrightCcy1Ccy2",
+    ...     ],
+    ... )
+    >>> response = definition.get_data()
+    >>> response.data.df
 
-     Using get_stream
-     >>> response = definition.get_stream()
+    Using get_stream
+
+    >>> response = definition.get_stream()
     """
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
         legs: Optional[List[LegDefinition]] = None,
         fx_cross_type: Optional[FxCrossType] = None,
@@ -112,26 +115,28 @@
         ndf_fixing_settlement_ccy: Optional[str] = None,
         reference_spot_rate: Optional[float] = None,
         traded_cross_rate: Optional[float] = None,
         traded_swap_points: Optional[float] = None,
         fields: "OptStrStrs" = None,
         pricing_parameters: Optional[PricingParameters] = None,
         extended_params: "ExtendedParams" = None,
+        settlement_ccy: Optional[str] = None,
     ):
         legs = try_copy_to_list(legs)
         fields = try_copy_to_list(fields)
         definition = FxCrossInstrumentDefinition(
             fx_cross_type=fx_cross_type,
             legs=legs,
             fx_cross_code=fx_cross_code,
             instrument_tag=instrument_tag,
             ndf_fixing_settlement_ccy=ndf_fixing_settlement_ccy,
             reference_spot_rate=reference_spot_rate,
             traded_cross_rate=traded_cross_rate,
             traded_swap_points=traded_swap_points,
+            settlement_ccy=settlement_ccy,
         )
         super().__init__(
             definition=definition,
             fields=fields,
             pricing_parameters=pricing_parameters,
             extended_params=extended_params,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_definition.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
-from ._enums import FxCrossType
+from ..._enums import FxCrossType
 from ._fx_cross_leg_definition import LegDefinition
 from .._instrument_definition import InstrumentDefinition
 
 
 class FxCrossInstrumentDefinition(InstrumentDefinition):
     """
     API endpoint for Financial Contract analytics,
@@ -18,18 +18,16 @@
         User defined string to identify the instrument.It can be used to link output
         results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
         characters are supported. Optional.
     legs : list of LegDefinition, optional
         Extra parameters to describe further the contract. 1 leg is mandatory for
         Forwards and NDFs contracts. 2 legs are required for Swaps, and FwdFwdSwaps
         contracts. Optional for Spot contracts.
-    fx_cross_type : FxCrossType, optional
-        The type of the Fx Cross instrument :  'FxSpot', 'FxForward',
-        'FxNonDeliverableForward', 'FxSwap', 'MultiLeg' or 'FxForwardForward'.
-        Mandatory.
+    fx_cross_type : FxCrossType or str, optional
+        The type of the Fx Cross instrument. Mandatory.
     fx_cross_code : str, optional
         The ISO code of the cross currency (e.g. 'EURCHF'). Mandatory.
     ndf_fixing_settlement_ccy : str, optional
         In case of a NDF contract, the ISO code of the settlement currency (e.g. 'EUR'
         ). Optional.
     reference_spot_rate : float, optional
         Contractual Spot Rate the counterparties agreed. It is used to compute the
@@ -56,33 +54,34 @@
     """
 
     def __init__(
         self,
         *,
         instrument_tag: Optional[str] = None,
         legs: Optional[LegDefinition] = None,
-        fx_cross_type: Optional[FxCrossType] = None,
+        fx_cross_type: Union[FxCrossType, str] = None,
         fx_cross_code: Optional[str] = None,
         ndf_fixing_settlement_ccy: Optional[str] = None,
         reference_spot_rate: Optional[float] = None,
         traded_cross_rate: Optional[float] = None,
         traded_swap_points: Optional[float] = None,
+        settlement_ccy: Optional[str] = None,
     ) -> None:
         super().__init__()
         self.instrument_tag = instrument_tag
         self.legs = legs
         self.fx_cross_type = fx_cross_type
         self.fx_cross_code = fx_cross_code
         self.ndf_fixing_settlement_ccy = ndf_fixing_settlement_ccy
         self.reference_spot_rate = reference_spot_rate
         self.traded_cross_rate = traded_cross_rate
         self.traded_swap_points = traded_swap_points
+        self.settlement_ccy = settlement_ccy
 
-    @classmethod
-    def get_instrument_type(cls):
+    def get_instrument_type(self):
         return "FxCross"
 
     @property
     def fx_cross_type(self):
         """
         The type of the Fx Cross instrument :  'FxSpot', 'FxForward',
         'FxNonDeliverableForward', 'FxSwap', 'MultiLeg' or 'FxForwardForward'.
@@ -195,7 +194,15 @@
         :return: float
         """
         return self._get_parameter("tradedSwapPoints")
 
     @traded_swap_points.setter
     def traded_swap_points(self, value):
         self._set_parameter("tradedSwapPoints", value)
+
+    @property
+    def settlement_ccy(self):
+        return self._get_parameter("settlementCcy")
+
+    @settlement_ccy.setter
+    def settlement_ccy(self, value):
+        self._set_parameter("settlementCcy", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_leg_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_leg_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._instrument_definition import ObjectDefinition
-from ._enums import (
+from ..._enums import (
     FxLegType,
     BuySell,
 )
 
 
 class LegDefinition(ObjectDefinition):
     """
@@ -22,20 +22,19 @@
         The maturity date of the contract that is the date the amounts are exchanged.
         Either the end_date or the tenor must be provided.
     tenor : str, optional
         The tenor representing the maturity date of the contract (e.g. '1Y' or '6M' ).
         Either the end_date or the tenor must be provided.
     leg_tag : str, optional
         A user defined string to identify the leg. Optional.
-    deal_ccy_buy_sell : BuySell, optional
-        The direction of the trade in terms of the deal currency : 'Buy' or 'Sell'.
+    deal_ccy_buy_sell : BuySell or str, optional
+        The direction of the trade in terms of the deal currency.
         Optional. Defaults to 'Buy'
-    fx_leg_type : FxLegType, optional
-        The enumeration that specifies the type of the leg : 'Spot', 'FxForward',
-        'FxNonDeliverableForward', 'SwapNear' or 'SwapFar'. Mandatory for MultiLeg,
+    fx_leg_type : FxLegType or str, optional
+        The enumeration that specifies the type of the leg. Mandatory for MultiLeg,
         FwdFwdSwap, or Swap contracts. Optional for Spot and Forwards contracts.
     contra_amount : float, optional
         The unsigned amount exchanged to buy or sell the traded amount. Optional. By
         default, it is calculated from the traded rate and the deal_amount. If no traded
         rate is provided the market rate will be used.
     contra_ccy : str, optional
         The currency that is exchanged. Optional. By default, the second currency in the
@@ -54,16 +53,16 @@
 
     def __init__(
         self,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
         leg_tag: Optional[str] = None,
-        deal_ccy_buy_sell: Optional[BuySell] = None,
-        fx_leg_type: Optional[FxLegType] = None,
+        deal_ccy_buy_sell: Union[BuySell, str] = None,
+        fx_leg_type: Union[FxLegType, str] = None,
         contra_amount: Optional[float] = None,
         contra_ccy: Optional[str] = None,
         deal_amount: Optional[float] = None,
         deal_ccy: Optional[str] = None,
         start_tenor: Optional[str] = None,
     ) -> None:
         super().__init__()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_pricing_parameters.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,43 +1,33 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
-from ._enums import PriceSide, FxSwapCalculationMethod, ImpliedDepositDateConvention
+from ..._enums import PriceSide, FxSwapCalculationMethod, ImpliedDepositDateConvention
 from ..._object_definition import ObjectDefinition
 
 
 class PricingParameters(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    fx_swap_calculation_method : FxSwapCalculationMethod, optional
-        The method used to calculate an outright price or deposit rates. the possible
-        values are:
-        - fxswap: outrights are computed using swap points,
-        - fxswapimpliedfromdeposit: implied fx swap points are computed from deposit
-          rates.  optional. defaults to 'fxswap'.
-    implied_deposit_date_convention : ImpliedDepositDateConvention, optional
+    fx_swap_calculation_method : FxSwapCalculationMethod or str, optional
+        The method used to calculate an outright price or deposit rates. The possible
+        values are. Optional. defaults to 'fxswap'.
+    implied_deposit_date_convention : ImpliedDepositDateConvention or str, optional
         An indicator for the "depositccy1impliedfromfxswap",
         "depositccy2impliedfromfxswap" calculation methods, used to check whether dts or
-        dtm period is selected for the implied deposits calculation. by default, for the
+        dtm period is selected for the implied deposits calculation. By default, for the
         implied deposits calculation we use the dts(day to spot) period, while for the
-        deposits the dtm(day to money market) period is used. the possible values are:
-        - fxmarketconvention:    dts case, traditional method getperiodsfromtenor is
-          used with the parameter "from:fxtrade"
-        - moneymarketconvention: dtm case, getperiodsfromtenor with the parameter
-          "from:mmtrade" is used optional.
-    price_side : PriceSide, optional
-        The quoted price side of the instrument. the possible values are:
-        - bid,
-        - ask,
-        - mid. optional. defaults to 'mid'.
+        deposits the dtm(day to money market) period is used.
+    price_side : PriceSide or str, optional
+        The quoted price side of the instrument. Optional. defaults to 'mid'.
     adjust_all_deposit_points_to_cross_calendars : bool, optional
         An indicator if depositccy1marketdata and depositccy2marketdata are adjusted to
         the cross calendar dates. the possible values are:
         - true: the market data is adjusted according to the cross calendar dates,
         - false: unmodified market data is returned, cross calendar is ignored.
           optional. defaults to 'false'.
     adjust_all_swap_points_to_cross_calendars : bool, optional
@@ -71,35 +61,31 @@
         equal to market_data_date or Today. For assets that contains a
         settlementConvention, the default valuation date  is equal to the settlementdate
         of the Asset that is usually the TradeDate+SettlementConvention.
     """
 
     def __init__(
         self,
-        fx_swap_calculation_method: Optional[FxSwapCalculationMethod] = None,
-        implied_deposit_date_convention: Optional[ImpliedDepositDateConvention] = None,
-        price_side: Optional[PriceSide] = None,
+        fx_swap_calculation_method: Union[FxSwapCalculationMethod, str] = None,
+        implied_deposit_date_convention: Union[ImpliedDepositDateConvention, str] = None,
+        price_side: Union[PriceSide, str] = None,
         adjust_all_deposit_points_to_cross_calendars: Optional[bool] = None,
         adjust_all_swap_points_to_cross_calendars: Optional[bool] = None,
         ignore_ref_ccy_holidays: Optional[bool] = None,
         market_data_date: Optional[str] = None,
         report_ccy: Optional[str] = None,
         use_direct_quote: Optional[bool] = None,
         valuation_date: Optional[str] = None,
     ) -> None:
         super().__init__()
         self.fx_swap_calculation_method = fx_swap_calculation_method
         self.implied_deposit_date_convention = implied_deposit_date_convention
         self.price_side = price_side
-        self.adjust_all_deposit_points_to_cross_calendars = (
-            adjust_all_deposit_points_to_cross_calendars
-        )
-        self.adjust_all_swap_points_to_cross_calendars = (
-            adjust_all_swap_points_to_cross_calendars
-        )
+        self.adjust_all_deposit_points_to_cross_calendars = adjust_all_deposit_points_to_cross_calendars
+        self.adjust_all_swap_points_to_cross_calendars = adjust_all_swap_points_to_cross_calendars
         self.ignore_ref_ccy_holidays = ignore_ref_ccy_holidays
         self.market_data_date = market_data_date
         self.report_ccy = report_ccy
         self.use_direct_quote = use_direct_quote
         self.valuation_date = valuation_date
 
     @property
@@ -108,38 +94,30 @@
         The method we chose to price outright using or not implied deposits. Possible
         values are:   FxSwap (compute outright using swap points),
         DepositCcy1ImpliedFromFxSwap (compute currency1 deposits using swap points),
         DepositCcy2ImpliedFromFxSwap (compute currency2 deposits using swap points).
         Optional. Defaults to 'FxSwap'.
         :return: enum FxSwapCalculationMethod
         """
-        return self._get_enum_parameter(
-            FxSwapCalculationMethod, "fxSwapCalculationMethod"
-        )
+        return self._get_enum_parameter(FxSwapCalculationMethod, "fxSwapCalculationMethod")
 
     @fx_swap_calculation_method.setter
     def fx_swap_calculation_method(self, value):
-        self._set_enum_parameter(
-            FxSwapCalculationMethod, "fxSwapCalculationMethod", value
-        )
+        self._set_enum_parameter(FxSwapCalculationMethod, "fxSwapCalculationMethod", value)
 
     @property
     def implied_deposit_date_convention(self):
         """
         :return: object FxPoint
         """
-        return self._get_enum_parameter(
-            ImpliedDepositDateConvention, "impliedDepositDateConvention"
-        )
+        return self._get_enum_parameter(ImpliedDepositDateConvention, "impliedDepositDateConvention")
 
     @implied_deposit_date_convention.setter
     def implied_deposit_date_convention(self, value):
-        self._set_enum_parameter(
-            ImpliedDepositDateConvention, "impliedDepositDateConvention", value
-        )
+        self._set_enum_parameter(ImpliedDepositDateConvention, "impliedDepositDateConvention", value)
 
     @property
     def price_side(self):
         """
         The type of price returned for pricing Analysis:
         Bid(Bid value),
         Ask(Ask value),
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -89,15 +89,15 @@
     FxDoubleBarrierDefinition,
     FxDoubleBarrierInfo,
     FxDoubleBinaryDefinition,
     FxDualCurrencyDefinition,
     FxForwardStart,
     FxUnderlyingDefinition,
 )
-from ._models import (
+from ..._models import (
     BidAskMid,
+    DayWeight,
     InputFlow,
     InterpolationWeight,
     PayoutScaling,
 )
 from ._option_pricing_parameters import PricingParameters
-from ..._models import DayWeight
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -39,28 +39,22 @@
     ----------
     instrument_tag : str, optional
         User defined string to identify the instrument.It can be used to link output
         results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
         characters are supported. Optional.
     end_date : str, optional
         Expiry date of the option
-    buy_sell : BuySell, optional
-        The side of the deal. Possible values:
-        - Buy
-        - Sell
-    call_put : CallPut, optional
-        Tells if the option is a call or a put. Possible values:
-        - Call
-        - Put
-    exercise_style : ExerciseStyle, optional
+    buy_sell : BuySell or str, optional
+        The side of the deal.
+    call_put : CallPut or str, optional
+        Tells if the option is a call or a put.
+    exercise_style : ExerciseStyle or str, optional
         EURO or AMER
     underlying_type : UnderlyingType, optional
-        Underlying type of the option. Possible values:
-        - Eti
-        - Fx
+        Underlying type of the option.
     strike : float, optional
         strike of the option
     tenor : str, optional
         tenor of the option
     notional_ccy : str, optional
         Currency of the notional amount If the option is a EURGBP Call option,
         notional_ccy can be expressed in EUR OR GBP
@@ -118,69 +112,68 @@
     get_data(session=session, on_response=on_response)
         Returns a response to the data platform
     get_stream(session=session)
         Get stream quantitative analytic service subscription
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> definition = rdf.option.Definition(
-     ...    instrument_code="FCHI560000L1.p",
-     ...    underlying_type=rdf.option.UnderlyingType.ETI,
-     ...    fields=[
-     ...        "MarketValueInDealCcy",
-     ...        "DeltaPercent",
-     ...        "GammaPercent",
-     ...        "RhoPercent",
-     ...        "ThetaPercent",
-     ...        "VegaPercent",
-     ...        "ErrorCode",
-     ...        "ErrorMessage",
-     ...    ],
-     ... )
-     >>> response = definition.get_data()
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> definition = rdf.option.Definition(
+    ...    instrument_code="FCHI560000L1.p",
+    ...    underlying_type=rdf.option.UnderlyingType.ETI,
+    ...    fields=[
+    ...        "MarketValueInDealCcy",
+    ...        "DeltaPercent",
+    ...        "GammaPercent",
+    ...        "RhoPercent",
+    ...        "ThetaPercent",
+    ...        "VegaPercent",
+    ...        "ErrorCode",
+    ...        "ErrorMessage",
+    ...    ],
+    ... )
+    >>> response = definition.get_data()
 
-     Using get_stream
-     >>> response = definition.get_stream()
+    Using get_stream
+
+    >>> response = definition.get_stream()
     """
 
     def __init__(
         self,
         asian_definition: Union[EtiFixingInfo, FxAverageInfo] = None,
         barrier_definition: Union[FxBarrierDefinition, EtiBarrierDefinition] = None,
         binary_definition: Union[FxBinaryDefinition, EtiBinaryDefinition] = None,
-        buy_sell: Optional[BuySell] = None,
-        call_put: Optional[CallPut] = None,
+        buy_sell: Union[BuySell, str] = None,
+        call_put: Union[CallPut, str] = None,
         cbbc_definition: Optional[EtiCbbcDefinition] = None,
         deal_contract: Optional[int] = None,
         delivery_date: Optional[str] = None,
         double_barrier_definition: Optional[FxDoubleBarrierDefinition] = None,
         double_barriers_definition: Optional[EtiDoubleBarriersDefinition] = None,
         double_binary_definition: Union[FxDoubleBinaryDefinition] = None,
         dual_currency_definition: Optional[FxDualCurrencyDefinition] = None,
         end_date: Optional[str] = None,
         end_date_time: Optional[str] = None,
-        exercise_style: Optional[ExerciseStyle] = None,
+        exercise_style: Union[ExerciseStyle, str] = None,
         forward_start_definition: Optional[FxForwardStart] = None,
         instrument_code: Optional[str] = None,
         instrument_tag: Optional[str] = None,
         lot_size: Optional[float] = None,
         notional_amount: Optional[float] = None,
         notional_ccy: Optional[str] = None,
         payments: Optional[InputFlow] = None,
         settlement_ccy: Optional[str] = None,
-        settlement_type: Optional[SettlementType] = None,
+        settlement_type: Union[SettlementType, str] = None,
         start_date: Optional[str] = None,
         strike: Optional[float] = None,
         tenor: Optional[str] = None,
         time_zone_offset: Optional[int] = None,
-        underlying_definition: Union[
-            FxUnderlyingDefinition, EtiUnderlyingDefinition
-        ] = None,
-        underlying_type: Optional[UnderlyingType] = None,
+        underlying_definition: Union[FxUnderlyingDefinition, EtiUnderlyingDefinition] = None,
+        underlying_type: Union[UnderlyingType, str] = None,
         fields: "OptStrStrs" = None,
         pricing_parameters: Optional[PricingParameters] = None,
         extended_params: "ExtendedParams" = None,
     ):
         validate_types(deal_contract, [int, type(None)], "deal_contract")
         fields = try_copy_to_list(fields)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_enums/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_enums/__init__.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_barrier_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_barrier_definition.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,38 +1,38 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._base import BarrierDefinition
 from .._enums import (
     BarrierStyle,
     InOrOut,
     UpOrDown,
 )
 
 
 class EtiBarrierDefinition(BarrierDefinition):
     """
     Parameters
     ----------
-    barrier_style : BarrierStyle, optional
+    barrier_style : BarrierStyle or str, optional
 
-    in_or_out : InOrOut, optional
+    in_or_out : InOrOut or str, optional
 
-    up_or_down : UpOrDown, optional
+    up_or_down : UpOrDown or str, optional
 
     level : float, optional
 
     """
 
     def __init__(
         self,
-        barrier_style: Optional[BarrierStyle] = None,
-        in_or_out: Optional[InOrOut] = None,
-        up_or_down: Optional[UpOrDown] = None,
+        barrier_style: Union[BarrierStyle, str] = None,
+        in_or_out: Union[InOrOut, str] = None,
+        up_or_down: Union[UpOrDown, str] = None,
         level: Optional[float] = None,
     ) -> None:
         super().__init__()
         self.barrier_style = barrier_style
         self.in_or_out = in_or_out
         self.up_or_down = up_or_down
         self.level = level
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_binary_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_binary_definition.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,37 +1,37 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._base import BinaryDefinition
 from .._enums import (
     BinaryType,
     UpOrDown,
 )
 
 
 class EtiBinaryDefinition(BinaryDefinition):
     """
     Parameters
     ----------
     notional_amount : float, optional
 
-    binary_type : BinaryType, optional
+    binary_type : BinaryType or str, optional
 
-    up_or_down : UpOrDown, optional
+    up_or_down : UpOrDown or str, optional
 
     level : float, optional
 
     """
 
     def __init__(
         self,
         notional_amount: Optional[float] = None,
-        binary_type: Optional[BinaryType] = None,
-        up_or_down: Optional[UpOrDown] = None,
+        binary_type: Union[BinaryType, str] = None,
+        up_or_down: Union[UpOrDown, str] = None,
         level: Optional[float] = None,
     ) -> None:
         super().__init__()
         self.notional_amount = notional_amount
         self.binary_type = binary_type
         self.up_or_down = up_or_down
         self.level = level
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_cbbc_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_cbbc_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._base import UnderlyingDefinition
 from ..._instrument_definition import InstrumentDefinition
 from .._enums import (
     BuySell,
     CallPut,
     ExerciseStyle,
@@ -34,34 +34,28 @@
         Expiry date of the option
     asian_definition : EtiOptionFixingInfo, optional
         Fixing details for asian options
     barrier_definition : EtiOptionBarrierDefinition, optional
         Details for barrier option.
     binary_definition : EtiOptionBinaryDefinition, optional
         Details for binary option.
-    buy_sell : BuySell, optional
-        The side of the deal. Possible values:
-        - Buy
-        - Sell
-    call_put : CallPut, optional
-        Tells if the option is a call or a put. Possible values:
-        - Call
-        - Put
+    buy_sell : BuySell or str, optional
+        The side of the deal.
+    call_put : CallPut or str, optional
+        Tells if the option is a call or a put.
     cbbc_definition : EtiOptionCbbcDefinition, optional
         Details for CBBC (Call Bear/Bull Contract) option.
     double_barriers_definition : EtiOptionDoubleBarriersDefinition, optional
         Details for double barriers option.
-    exercise_style : ExerciseStyle, optional
+    exercise_style : ExerciseStyle or str, optional
         EURO or AMER
     underlying_definition : EtiUnderlyingDefinition, optional
         Details of the underlying. Can be used to override some data of the underlying.
-    underlying_type : UnderlyingType, optional
-        Underlying type of the option. Possible values:
-        - Eti
-        - Fx
+    underlying_type : UnderlyingType or str, optional
+        Underlying type of the option.
     deal_contract : int, optional
         deal_contract. It is the number of contracts bought or sold in the deal.
     end_date_time : str, optional
         Expiry date time of the option
     lot_size : float, optional
         The lot size. It is the number of options bought or sold in one transaction.
     offset : int, optional
@@ -76,21 +70,21 @@
         instrument_tag: Optional[str] = None,
         instrument_code: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         asian_definition: Optional[EtiFixingInfo] = None,
         barrier_definition: Optional[EtiBarrierDefinition] = None,
         binary_definition: Optional[EtiBinaryDefinition] = None,
-        buy_sell: Optional[BuySell] = None,
-        call_put: Optional[CallPut] = None,
+        buy_sell: Union[BuySell, str] = None,
+        call_put: Union[CallPut, str] = None,
         cbbc_definition: Optional[EtiCbbcDefinition] = None,
         double_barriers_definition: Optional[EtiDoubleBarriersDefinition] = None,
-        exercise_style: Optional[ExerciseStyle] = None,
+        exercise_style: Union[ExerciseStyle, str] = None,
         underlying_definition: Optional[EtiUnderlyingDefinition] = None,
-        underlying_type: Optional[UnderlyingType] = None,
+        underlying_type: Union[UnderlyingType, str] = None,
         deal_contract: Optional[int] = None,
         end_date_time: Optional[str] = None,
         lot_size: Optional[float] = None,
         strike: Optional[float] = None,
         time_zone_offset: Optional[int] = None,
         **kwargs,
     ) -> None:
@@ -192,23 +186,19 @@
         self._set_object_parameter(EtiCbbcDefinition, "cbbcDefinition", value)
 
     @property
     def double_barriers_definition(self):
         """
         :return: object EtiOptionDoubleBarriersDefinition
         """
-        return self._get_object_parameter(
-            EtiDoubleBarriersDefinition, "doubleBarriersDefinition"
-        )
+        return self._get_object_parameter(EtiDoubleBarriersDefinition, "doubleBarriersDefinition")
 
     @double_barriers_definition.setter
     def double_barriers_definition(self, value):
-        self._set_object_parameter(
-            EtiDoubleBarriersDefinition, "doubleBarriersDefinition", value
-        )
+        self._set_object_parameter(EtiDoubleBarriersDefinition, "doubleBarriersDefinition", value)
 
     @property
     def exercise_style(self):
         """
         The option style based on its exercise restrictions. the possible values are:
         - amer: the owner has the right to exercise on any date before the option
           expires,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_double_barriers_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_double_barriers_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_fixing_info.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_average_info.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,68 +1,54 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._base import Info
 from .._enums import (
     AverageType,
     FixingFrequency,
 )
 
 
-class EtiFixingInfo(Info):
+class FxAverageInfo(Info):
     """
     Parameters
     ----------
-    average_type : AverageType, optional
-        The type of average used to compute. Possible values:
-        - ArithmeticRate
-        - ArithmeticStrike
-        - GeometricRate
-        - GeometricStrike
-    fixing_frequency : FixingFrequency, optional
-        The fixing's frequency. Possible values:
-        - Daily
-        - Weekly
-        - BiWeekly
-        - Monthly
-        - Quaterly
-        - SemiAnnual
-        - Annual
+    average_type : AverageType or str, optional
+        The type of average used to compute.
+    fixing_frequency : FixingFrequency or str, optional
+        The fixing's frequency.
     average_so_far : float, optional
         The value of the average_type
-    fixing_calendar : str, optional
-        The calendar of the underlying's currency.
-    fixing_end_date : str, optional
-        The end date of the fixing period. Should be less or equal to the expiry.
+    fixing_ric_source : str, optional
+        The fixing's RIC source. Default value: the first available source RIC of the Fx
+        Cross Code
     fixing_start_date : str, optional
         The beginning date of the fixing period.
     include_holidays : bool, optional
         Include the holidays in the list of fixings
     include_week_ends : bool, optional
         Include the week-ends in the list of fixings
     """
 
     def __init__(
         self,
-        average_type: Optional[AverageType] = None,
-        fixing_frequency: Optional[FixingFrequency] = None,
+        average_type: Union[AverageType, str] = None,
+        fixing_frequency: Union[FixingFrequency, str] = None,
         average_so_far: Optional[float] = None,
-        fixing_calendar: Optional[str] = None,
-        fixing_end_date: Optional[str] = None,
+        fixing_ric: Optional[str] = None,
         fixing_start_date: Optional[str] = None,
         include_holidays: Optional[bool] = None,
         include_week_ends: Optional[bool] = None,
     ) -> None:
         super().__init__()
         self.average_type = average_type
         self.fixing_frequency = fixing_frequency
         self.average_so_far = average_so_far
-        self.fixing_calendar = fixing_calendar
-        self.fixing_end_date = fixing_end_date
+        self.fixing_ric = fixing_ric
         self.fixing_start_date = fixing_start_date
         self.include_holidays = include_holidays
         self.include_week_ends = include_week_ends
 
     @property
     def average_type(self):
         """
@@ -107,36 +93,25 @@
         return self._get_parameter("averageSoFar")
 
     @average_so_far.setter
     def average_so_far(self, value):
         self._set_parameter("averageSoFar", value)
 
     @property
-    def fixing_calendar(self):
-        """
-        The calendar of the underlying's currency.
-        :return: str
-        """
-        return self._get_parameter("fixingCalendar")
-
-    @fixing_calendar.setter
-    def fixing_calendar(self, value):
-        self._set_parameter("fixingCalendar", value)
-
-    @property
-    def fixing_end_date(self):
+    def fixing_ric(self):
         """
-        The end date of the fixing period. Should be less or equal to the expiry.
+        The fixing's ric source. default value: the first available source ric of the fx
+        cross code
         :return: str
         """
-        return self._get_parameter("fixingEndDate")
+        return self._get_parameter("fixingRic")
 
-    @fixing_end_date.setter
-    def fixing_end_date(self, value):
-        self._set_parameter("fixingEndDate", value)
+    @fixing_ric.setter
+    def fixing_ric(self, value):
+        self._set_parameter("fixingRic", value)
 
     @property
     def fixing_start_date(self):
         """
         The beginning date of the fixing period.
         :return: str
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_underlying_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_eti/_eti_underlying_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/__init__.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_barrier_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_barrier_definition.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,44 +1,44 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._base import BarrierDefinition
 from .._enums import (
     BarrierMode,
     InOrOut,
     UpOrDown,
 )
 
 
 class FxBarrierDefinition(BarrierDefinition):
     """
     Parameters
     ----------
-    barrier_mode : BarrierMode, optional
+    barrier_mode : BarrierMode or str, optional
         Barrier Mode of the barrier option
-    in_or_out : InOrOut, optional
+    in_or_out : InOrOut or str, optional
         In/Out property of the barrier option
-    up_or_down : UpOrDown, optional
+    up_or_down : UpOrDown or str, optional
         Up/Down property of the barrier option
     level : float, optional
         Barrier of the barrier option
     rebate_amount : float, optional
         Rebate of the barrier option
     window_end_date : str, optional
         Window Start date of the barrier option
     window_start_date : str, optional
         Window Start date of the barrier option
     """
 
     def __init__(
         self,
-        barrier_mode: Optional[BarrierMode] = None,
-        in_or_out: Optional[InOrOut] = None,
-        up_or_down: Optional[UpOrDown] = None,
+        barrier_mode: Union[BarrierMode, str] = None,
+        in_or_out: Union[InOrOut, str] = None,
+        up_or_down: Union[UpOrDown, str] = None,
         level: Optional[float] = None,
         rebate_amount: Optional[float] = None,
         window_end_date: Optional[str] = None,
         window_start_date: Optional[str] = None,
     ) -> None:
         super().__init__()
         self.barrier_mode = barrier_mode
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_binary_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_binary_definition.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,46 +1,38 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._base import BinaryDefinition
 from .._enums import (
     FxBinaryType,
 )
 
 
 class FxBinaryDefinition(BinaryDefinition):
     """
     Parameters
     ----------
-    binary_type : FxBinaryType, optional
-        The type of a binary option. the possible values are:
-        - onetouchimmediate: the option expires in-the-money if the trigger is reached
-          at any time prior to option expiration.the option premium is paid immediately.
-        - onetouchdeferred: the option expires in-the-money if the trigger is reached at
-          any time prior to option expiration. the option premium payment is deferred.
-        - notouch: the option expires in-the-money if the trigger is not reached prior
-          to expiration.
-        - digital: the option expires in-the-money if the trigger is reached at the
-          option expiry date. mandatory for binary options.
+    binary_type : FxBinaryType or str, optional
+        The type of a binary option.
     payout_amount : float, optional
         The payout amount of the option. the default value is '1,000,000'.
     payout_ccy : str, optional
         The trade currency, which is either a domestic or foreign currency. either
         payoutccy or settlementtype can be used at a time. payoutccy="foreign currency"
         is equivalent to settlementtype ="physical", and payoutccy="domestic currency"
         is equivalent to settlementtype ="cash". the value is expressed in iso 4217
         alphabetical format (e.g. 'usd').
     trigger : float, optional
         The trigger of the binary option.
     """
 
     def __init__(
         self,
-        binary_type: Optional[FxBinaryType] = None,
+        binary_type: Union[FxBinaryType, str] = None,
         payout_amount: Optional[float] = None,
         payout_ccy: Optional[str] = None,
         trigger: Optional[float] = None,
     ) -> None:
         super().__init__()
         self.binary_type = binary_type
         self.payout_amount = payout_amount
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,11 +1,12 @@
 # coding: utf8
 
-from typing import Optional, List
+from typing import Optional, List, Union
 
+from ...._models import InputFlow
 from .._base import UnderlyingDefinition
 from ..._instrument_definition import InstrumentDefinition
 from .._enums import (
     BuySell,
     CallPut,
     ExerciseStyle,
     UnderlyingType,
@@ -17,15 +18,14 @@
     FxBarrierDefinition,
     FxBinaryDefinition,
     FxDoubleBarrierDefinition,
     FxDoubleBinaryDefinition,
     FxForwardStart,
     FxUnderlyingDefinition,
 )
-from .._models import InputFlow
 
 
 class FxDefinition(InstrumentDefinition):
     """
     Parameters
     ----------
     instrument_tag : str, optional
@@ -52,56 +52,39 @@
         amount of eur or gbp of the contract
     asian_definition : FxOptionAverageInfo, optional
 
     barrier_definition : FxOptionBarrierDefinition, optional
 
     binary_definition : FxOptionBinaryDefinition, optional
 
-    buy_sell : BuySell, optional
-        The indicator of the deal side. the possible values are:
-        - buy: buying the option,
-        - sell: selling/writing the option. the output amounts calculated with taking
-          buysell into consideration are returned with a reversed sign when the value
-          'sell' is used. optional. the default value is 'buy'.
-    call_put : CallPut, optional
-        The indicator if the option is a call or a put. the possible values are:
-        - call: the right to buy the underlying asset,
-        - put: the right to sell the underlying asset. optional. if instrumentcode of
-          listed eti option is defined, the value comes from the instrument reference
-          data.the default value is 'call' for otc eti options and fx options.
+    buy_sell : BuySell or str, optional
+        The indicator of the deal side.
+    call_put : CallPu or strt, optional
+        The indicator if the option is a call or a put.
+        The default value is 'call' for otc eti options and fx options.
     double_barrier_definition : FxOptionDoubleBarrierDefinition, optional
 
     double_binary_definition : FxOptionDoubleBinaryDefinition, optional
 
     dual_currency_definition : FxDualCurrencyDefinition, optional
 
-    exercise_style : ExerciseStyle, optional
-        The option style based on its exercise restrictions. the possible values are:
-        - amer: the owner has the right to exercise on any date before the option
-          expires,
-        - euro: the owner has the right to exercise only on enddate,
-        - berm: the owner has the right to exercise on any of several specified dates
-          before the option expires. all exercise styles may not apply to certain option
-          types. optional. if instrumentcode of listed eti option is defined, the value
-          comes from the instrument reference data. the default value is 'euro' for otc
+    exercise_style : ExerciseStyle or str, optional
+        The option style based on its exercise restrictions.
+        The default value is 'euro' for otc
           eti options and fx options.
     forward_start_definition : FxOptionForwardStart, optional
 
     payments : InputFlow, optional
         An array of payments
-    settlement_type : SettlementType, optional
-        The settlement method for options when exercised. the possible values are:
-        - physical(asset): delivering the underlying asset.
-        - cash: paying out in cash.
+    settlement_type : SettlementType or str, optional
+        The settlement method for options when exercised.
     underlying_definition : FxUnderlyingDefinition, optional
 
-    underlying_type : UnderlyingType, optional
-        The type of the option based on the underlying asset. the possible values are:
-        - eti: eti(exchanged traded instruments) options,
-        - fx: fx options. mandatory. no default value applies.
+    underlying_type : UnderlyingType or str, optional
+        The type of the option based on the underlying asset. Mandatory. No default value applies.
     delivery_date : str, optional
         The date when the underlylng asset is delivered. the value is expressed in iso
         8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g. '2021-01-01t00:00:00z').
     settlement_ccy : str, optional
         The currency of the instrument's settlement. the value is expressed in iso 4217
         alphabetical format (e.g. 'usd'). if the option is a eurgbp call option,
         settlementccy can be expressed in eur or gbp
@@ -120,25 +103,25 @@
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
         asian_definition: Optional[FxAverageInfo] = None,
         barrier_definition: Optional[FxBarrierDefinition] = None,
         binary_definition: Optional[FxBinaryDefinition] = None,
-        buy_sell: Optional[BuySell] = None,
-        call_put: Optional[CallPut] = None,
+        buy_sell: Union[BuySell, str] = None,
+        call_put: Union[CallPut, str] = None,
         double_barrier_definition: Optional[FxDoubleBarrierDefinition] = None,
         double_binary_definition: Optional[FxDoubleBinaryDefinition] = None,
         dual_currency_definition: Optional[FxDualCurrencyDefinition] = None,
-        exercise_style: Optional[ExerciseStyle] = None,
+        exercise_style: Union[ExerciseStyle, str] = None,
         forward_start_definition: Optional[FxForwardStart] = None,
         payments: Optional[List[InputFlow]] = None,
-        settlement_type: Optional[SettlementType] = None,
+        settlement_type: Union[SettlementType, str] = None,
         underlying_definition: Optional[FxUnderlyingDefinition] = None,
-        underlying_type: Optional[UnderlyingType] = None,
+        underlying_type: Union[UnderlyingType, str] = None,
         delivery_date: Optional[str] = None,
         settlement_ccy: Optional[str] = None,
         strike: Optional[float] = None,
         **kwargs,
     ) -> None:
         super().__init__(instrument_tag, **kwargs)
         self.instrument_tag = instrument_tag
@@ -231,53 +214,41 @@
         self._set_enum_parameter(CallPut, "callPut", value)
 
     @property
     def double_barrier_definition(self):
         """
         :return: object FxOptionDoubleBarrierDefinition
         """
-        return self._get_object_parameter(
-            FxDoubleBarrierDefinition, "doubleBarrierDefinition"
-        )
+        return self._get_object_parameter(FxDoubleBarrierDefinition, "doubleBarrierDefinition")
 
     @double_barrier_definition.setter
     def double_barrier_definition(self, value):
-        self._set_object_parameter(
-            FxDoubleBarrierDefinition, "doubleBarrierDefinition", value
-        )
+        self._set_object_parameter(FxDoubleBarrierDefinition, "doubleBarrierDefinition", value)
 
     @property
     def double_binary_definition(self):
         """
         :return: object FxOptionDoubleBinaryDefinition
         """
-        return self._get_object_parameter(
-            FxDoubleBinaryDefinition, "doubleBinaryDefinition"
-        )
+        return self._get_object_parameter(FxDoubleBinaryDefinition, "doubleBinaryDefinition")
 
     @double_binary_definition.setter
     def double_binary_definition(self, value):
-        self._set_object_parameter(
-            FxDoubleBinaryDefinition, "doubleBinaryDefinition", value
-        )
+        self._set_object_parameter(FxDoubleBinaryDefinition, "doubleBinaryDefinition", value)
 
     @property
     def dual_currency_definition(self):
         """
         :return: object FxDualCurrencyDefinition
         """
-        return self._get_object_parameter(
-            FxDualCurrencyDefinition, "dualCurrencyDefinition"
-        )
+        return self._get_object_parameter(FxDualCurrencyDefinition, "dualCurrencyDefinition")
 
     @dual_currency_definition.setter
     def dual_currency_definition(self, value):
-        self._set_object_parameter(
-            FxDualCurrencyDefinition, "dualCurrencyDefinition", value
-        )
+        self._set_object_parameter(FxDualCurrencyDefinition, "dualCurrencyDefinition", value)
 
     @property
     def exercise_style(self):
         """
         The option style based on its exercise restrictions. the possible values are:
         - amer: the owner has the right to exercise on any date before the option
           expires,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from ..._instrument_definition import ObjectDefinition
 from .._enums import BarrierMode
 from ._fx_double_barrier_info import FxDoubleBarrierInfo
 
 
 class FxDoubleBarrierDefinition(ObjectDefinition):
@@ -12,24 +12,24 @@
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
     barrier_down : FxDoubleBarrierInfo, optional
         Barrier Information for the lower barrier
-    barrier_mode : BarrierMode, optional
+    barrier_mode : BarrierMode or str, optional
         Barrier Mode of the double barrier option
     barrier_up : FxDoubleBarrierInfo, optional
         Barrier Information for the upper barrier
     """
 
     def __init__(
         self,
         barrier_down: Optional[FxDoubleBarrierInfo] = None,
-        barrier_mode: Optional[BarrierMode] = None,
+        barrier_mode: Union[BarrierMode, str] = None,
         barrier_up: Optional[FxDoubleBarrierInfo] = None,
     ) -> None:
         super().__init__()
         self.barrier_down = barrier_down
         self.barrier_mode = barrier_mode
         self.barrier_up = barrier_up
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_info.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_info.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,20 @@
-# coding: utf8
+from typing import Optional, Union
+
+from .._enums import InOrOut
 from ..._instrument_definition import ObjectDefinition
 
 
 class FxDoubleBarrierInfo(ObjectDefinition):
-    """ """
-
-    def __init__(self, in_or_out=None, level=None, rebate_amount=None):
+    def __init__(
+        self,
+        in_or_out: Union[InOrOut, str] = None,
+        level: Optional[float] = None,
+        rebate_amount: Optional[float] = None,
+    ):
         super().__init__()
         self.in_or_out = in_or_out
         self.level = level
         self.rebate_amount = rebate_amount
 
     @property
     def in_or_out(self):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_binary_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_binary_definition.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,27 +1,24 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._enums import DoubleBinaryType
 from ..._instrument_definition import ObjectDefinition
 
 
 class FxDoubleBinaryDefinition(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    double_binary_type : DoubleBinaryType, optional
-        The type of a double binary option. the possible values are:
-        - doublenotouch: the option expires in-the-money if the price of the underlying
-          asset fails to breach either the lower trigger or the upper trigger at any
-          time prior to option expiration.
+    double_binary_type : DoubleBinaryType or str, optional
+        The type of a double binary option.
     payout_amount : float, optional
         The payout amount of the option. the default value is '1,000,000'.
     payout_ccy : str, optional
         The trade currency, which is either a domestic or foreign currency. either
         payoutccy or settlementtype can be used at a time. payoutccy="foreign currency"
         is equivalent to settlementtype ="physical", and payoutccy="domestic currency"
         is equivalent to settlementtype ="cash". the value is expressed in iso 4217
@@ -30,15 +27,15 @@
         The lower trigger of the binary option.
     trigger_up : float, optional
         The upper trigger of the binary option.
     """
 
     def __init__(
         self,
-        double_binary_type: Optional[DoubleBinaryType] = None,
+        double_binary_type: Union[DoubleBinaryType, str] = None,
         payout_amount: Optional[float] = None,
         payout_ccy: Optional[str] = None,
         trigger_down: Optional[float] = None,
         trigger_up: Optional[float] = None,
     ) -> None:
         super().__init__()
         self.double_binary_type = double_binary_type
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_dual_currency_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_dual_currency_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_forward_start.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_forward_start.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_underlying_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_underlying_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_option_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_option_definition.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._instrument_definition import InstrumentDefinition
 from ._enums import (
     BuySell,
     CallPut,
     ExerciseStyle,
     UnderlyingType,
@@ -16,57 +16,50 @@
     Parameters
     ----------
     instrument_tag : str, optional
         User defined string to identify the instrument.It can be used to link output
         results to the instrument definition. Only alphabetic, numeric and '- _.#=@'
         characters are supported. Optional.
     end_date : str, optional
-        Expiry date of the option
-    buy_sell : BuySell, optional
-        The side of the deal. Possible values:
-        - Buy
-        - Sell
-    call_put : CallPut, optional
-        Tells if the option is a call or a put. Possible values:
-        - Call
-        - Put
-    exercise_style : ExerciseStyle, optional
-        EURO or AMER
-    underlying_type : UnderlyingType, optional
-        Underlying type of the option. Possible values:
-        - Eti
-        - Fx
+        Expiry date of the option.
+    buy_sell : BuySell or str, optional
+        The side of the deal.
+    call_put : CallPut or str, optional
+        Tells if the option is a call or a put.
+    exercise_style : ExerciseStyle or str, optional
+        The option style based on its exercise restrictions.
+    underlying_type : UnderlyingType or str, optional
+        Underlying type of the option.
     strike : float, optional
         strike of the option
     """
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
-        buy_sell: Optional[BuySell] = None,
-        call_put: Optional[CallPut] = None,
-        exercise_style: Optional[ExerciseStyle] = None,
-        underlying_type: Optional[UnderlyingType] = None,
+        buy_sell: Union[BuySell, str] = None,
+        call_put: Union[CallPut, str] = None,
+        exercise_style: Union[ExerciseStyle, str] = None,
+        underlying_type: Union[UnderlyingType, str] = None,
         strike: Optional[float] = None,
         **kwargs,
     ) -> None:
         super().__init__(instrument_tag, **kwargs)
         self.instrument_tag = instrument_tag
         self.start_date = start_date
         self.end_date = end_date
         self.buy_sell = buy_sell
         self.call_put = call_put
         self.exercise_style = exercise_style
         self.underlying_type = underlying_type
         self.strike = strike
 
-    @classmethod
-    def get_instrument_type(cls):
+    def get_instrument_type(self):
         return "Option"
 
     @property
     def buy_sell(self):
         """
         The side of the deal. Possible values:
         - Buy
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_option_instrument_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_option_instrument_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -16,15 +16,15 @@
     FxDefinition,
     FxDualCurrencyDefinition,
     FxDoubleBarrierDefinition,
     FxDoubleBinaryDefinition,
     FxForwardStart,
 )
 from ._option_definition import OptionDefinition
-from ._models import InputFlow
+from ..._models import InputFlow
 
 
 class OptionInstrumentDefinition(EtiDefinition, FxDefinition, OptionDefinition):
     def __init__(
         self,
         asian_definition: Optional[EtiFixingInfo] = None,
         barrier_definition: Optional[EtiBarrierDefinition] = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/option/_option_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/option/_option_pricing_parameters.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 # coding: utf8
 
-from typing import Optional, List
+from typing import Optional, List, Union
 
 from ....._tools import try_copy_to_list
 from ..._object_definition import ObjectDefinition
 
 from ._enums import (
     FxSwapCalculationMethod,
     OptionVolatilityType,
     PriceSide,
     PricingModelType,
     TimeStamp,
     VolatilityModel,
 )
-from ._models import (
+from ..._models import (
     BidAskMid,
     InterpolationWeight,
     PayoutScaling,
 )
 
 
 class PricingParameters(ObjectDefinition):
@@ -37,103 +37,58 @@
 
     foreign_deposit_rate_percent_object : BidAskMid, optional
 
     forward_points_object : BidAskMid, optional
 
     fx_spot_object : BidAskMid, optional
 
-    fx_swap_calculation_method : FxSwapCalculationMethod, optional
-        The method used to calculate an outright price or deposit rates. the possible
-        values are:
-        - fxswapimpliedfromdeposit: implied fx swap points are computed from deposit
-          rates.
-        - depositccy1impliedfromfxswap: currency 1 deposit rates are computed using swap
-          points,
-        - depositccy2impliedfromfxswap: currency 2 deposit rates are computed using swap
-          points.  the default value is 'depositccy2impliedfromfxswap'.
+    fx_swap_calculation_method : FxSwapCalculationMethod or str, optional
+        The method used to calculate an outright price or deposit rates.
     implied_volatility_object : BidAskMid, optional
 
     interpolation_weight : InterpolationWeight, optional
 
-    option_price_side : PriceSide, optional
-        The quoted price side of the instrument.the possible values are:
-        - bid,
-        - ask,
-        - mid,
-        - last. optional. the default values for listed options are:
+    option_price_side : PriceSide or str, optional
+        The quoted price side of the instrument. Optional. the default values for listed options are:
         - ask: if buysell is set to 'buy',
         - bid: if buysell is set to 'sell',
         - last: if buysell is not provided. the default value for otc options is 'mid'.
     option_time_stamp : TimeStamp, optional
-        The mode of the instrument's timestamp selection. the possible values are:
-        - open: the opening value of valuationdate, or if it is not available, the close
-          of the previous day is used,
-        - close: the close value of valuationdate is used,
-        - default: the latest snapshot is used when valuationdate is today, and the
-          close price when valuationdate is in the past. optional.the default value is
-          'default'.
+        The mode of the instrument's timestamp selection. Optional.the default value is 'default'.
     payout_custom_dates : string, optional
         The array of dates set by a user for the payout/volatility chart. optional.no
         default value applies.
     payout_scaling_interval : PayoutScaling, optional
 
     price_side : PriceSide, optional
-        The quoted price side of the instrument. the possible values are:
-        - bid,
-        - ask,
-        - mid.
+        The quoted price side of the instrument.
     pricing_model_type : PricingModelType, optional
-        The model type of the option pricing. the possible values are:
-        - blackscholes
-        - bachelier (available for commodity options including calendar spread options)
-        - whaley
-        - binomial
-        - trinomial
-        - localvolatility (applicable only for barrier options, cbbc options and binary
-          options)
-        - vannavolga (only applicable for fxbarrieroption, fxdigitaloption and
-          fxtouchesoption)  optional. the default value depends on the option type.
+        The model type of the option pricing. Optional. the default value depends on the option type.
     risk_reversal10_d_object : BidAskMid, optional
 
     risk_reversal25_d_object : BidAskMid, optional
 
-    underlying_price_side : PriceSide, optional
-        The quoted price side of the underlying asset.the possible values are:
-        - bid,
-        - ask,
-        - mid,
-        - last. optional. the default values are:
+    underlying_price_side : PriceSide or str, optional
+        The quoted price side of the underlying asset. Optional. the default values are:
         - ask: if buysell is set to 'buy',
         - bid: if buysell is set to 'sell',
         - last: if buysell is not provided.
-    underlying_time_stamp : TimeStamp, optional
-        The mode of the underlying asset's timestamp selection. the possible values are:
-        - open: the opening value of valuationdate, or if it is not available, the close
-          of the previous day is used,
-        - close: the close value of valuationdate is used,
-        - default: the latest snapshot is used when valuationdate is today, and the
-          close price when valuationdate is in the past. optional.the default value is
+    underlying_time_stamp : TimeStamp or str, optional
+        The mode of the underlying asset's timestamp selection. Optional.the default value is
           'default'.
     volatility_model : VolatilityModel, optional
         The model used to build the volatility surface. the possible values are:
         - sabr,
         - cubicspline,
         - svi,
         - twinlognormal,
         - vannavolga10d,
         - vannavolga25d.
-    volatility_type : OptionVolatilityType, optional
-        The type of volatility for the option pricing. the possible values are:
-        - implied: the volatility anticipated for the underlying asset for the remaining
-          life of the option(implied by an option premium),
-        - svisurface: the volatility computed from volsurface service,
-        - historical: the volatility of the underlying asset during a period in the
-          past. the value 'implied' is available only for listed options. if
-          volatilitypercent is defined, volatilitytype is not taken into account.
-          optional. the default value is 'implied'.
+    volatility_type : OptionVolatilityType or str, optional
+        The type of volatility for the option pricing. Optional. the default value is 'implied'.
     compute_payout_chart : bool, optional
         Define whether the payout chart must be computed or not
     compute_volatility_payout : bool, optional
         Define whether the volatility payout chart must be computed or not
     cutoff_time : str, optional
         The cutoff time
     cutoff_time_zone : str, optional
@@ -202,28 +157,28 @@
         atm_volatility_object: Optional[BidAskMid] = None,
         butterfly10_d_object: Optional[BidAskMid] = None,
         butterfly25_d_object: Optional[BidAskMid] = None,
         domestic_deposit_rate_percent_object: Optional[BidAskMid] = None,
         foreign_deposit_rate_percent_object: Optional[BidAskMid] = None,
         forward_points_object: Optional[BidAskMid] = None,
         fx_spot_object: Optional[BidAskMid] = None,
-        fx_swap_calculation_method: Optional[FxSwapCalculationMethod] = None,
+        fx_swap_calculation_method: Union[FxSwapCalculationMethod, str] = None,
         implied_volatility_object: Optional[BidAskMid] = None,
         interpolation_weight: Optional[InterpolationWeight] = None,
-        option_price_side: Optional[PriceSide] = None,
-        option_time_stamp: Optional[TimeStamp] = None,
+        option_price_side: Union[PriceSide, str] = None,
+        option_time_stamp: Union[TimeStamp, str] = None,
         payout_custom_dates: Optional[List[str]] = None,
         payout_scaling_interval: Optional[PayoutScaling] = None,
-        price_side: Optional[PriceSide] = None,
-        pricing_model_type: Optional[PricingModelType] = None,
+        price_side: Union[PriceSide, str] = None,
+        pricing_model_type: Union[PricingModelType, str] = None,
         risk_reversal10_d_object: Optional[BidAskMid] = None,
         risk_reversal25_d_object: Optional[BidAskMid] = None,
-        underlying_price_side: Optional[PriceSide] = None,
+        underlying_price_side: Union[PriceSide, str] = None,
         underlying_time_stamp: Optional[TimeStamp] = None,
-        volatility_model: Optional[VolatilityModel] = None,
+        volatility_model: Union[VolatilityModel, str] = None,
         volatility_type: Optional[OptionVolatilityType] = None,
         compute_payout_chart: Optional[bool] = None,
         compute_volatility_payout: Optional[bool] = None,
         cutoff_time: Optional[str] = None,
         cutoff_time_zone: Optional[str] = None,
         market_data_date: Optional[str] = None,
         market_value_in_deal_ccy: Optional[float] = None,
@@ -362,23 +317,19 @@
           rates.
         - depositccy1impliedfromfxswap: currency 1 deposit rates are computed using swap
           points,
         - depositccy2impliedfromfxswap: currency 2 deposit rates are computed using swap
           points.  the default value is 'depositccy2impliedfromfxswap'.
         :return: enum FxSwapCalculationMethod
         """
-        return self._get_enum_parameter(
-            FxSwapCalculationMethod, "fxSwapCalculationMethod"
-        )
+        return self._get_enum_parameter(FxSwapCalculationMethod, "fxSwapCalculationMethod")
 
     @fx_swap_calculation_method.setter
     def fx_swap_calculation_method(self, value):
-        self._set_enum_parameter(
-            FxSwapCalculationMethod, "fxSwapCalculationMethod", value
-        )
+        self._set_enum_parameter(FxSwapCalculationMethod, "fxSwapCalculationMethod", value)
 
     @property
     def implied_volatility_object(self):
         """
         :return: object BidAskMid
         """
         return self._get_object_parameter(BidAskMid, "impliedVolatilityObject")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,16 +6,13 @@
     "RepoCurveType",
     "RepoParameters",
     "UnderlyingContract",
     "UnderlyingPricingParameters",
 )
 
 from ._definition import Definition
-from ._enums import (
-    BuySell,
-    DayCountBasis,
-    RepoCurveType,
-)
+from ..._enums import BuySell, DayCountBasis, RepoCurveType
+
 from ._repo_parameters import RepoParameters
 from ._repo_pricing_parameters import PricingParameters
 from ._repo_underlying_contract import UnderlyingContract
 from ._repo_underlying_pricing_parameters import UnderlyingPricingParameters
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # coding: utf8
 
-from typing import Optional, List, TYPE_CHECKING
+from typing import Optional, List, TYPE_CHECKING, Union
 
 from ._repo_definition import RepoInstrumentDefinition
 from ._repo_pricing_parameters import PricingParameters
 from ._repo_underlying_contract import UnderlyingContract
 from .._base_definition import BaseDefinition
 from ..._enums import DayCountBasis, BuySell
 from ....._tools import create_repr, try_copy_to_list
@@ -30,18 +30,17 @@
     end_date : str, optional
         End date of the repo, that means when the borrower repurchases the security
         back. Either end_date or tenor field are requested.
     tenor : str, optional
         tenor that defines the duration of the Repo in case no end_date has been
         provided. In that case, end_date is computed from start_date and tenor. Either
         end_date or tenor field are requested.
-     buy_sell : BuySell, optional
-        The indicator of the deal side. the possible values are:   buy: buying the repo,
-        sell: selling the repo.  optional. the default value is "buy".
-    day_count_basis : DayCountBasis, optional
+     buy_sell : BuySell or str, optional
+        The indicator of the deal side. Optional. the default value is "buy".
+    day_count_basis : DayCountBasis or str, optional
         Day Count Basis convention to apply to the custom Repo rate.
         By default "Dcb_Actual_360".
     underlying_instruments : list of UnderlyingContract
         Definition of the underlying instruments. Only Bond Contracts are supported for
         now, and only one Bond can be used.
     is_coupon_exchanged : bool, optional
         Specifies whether or not intermediate coupons are exchanged.
@@ -74,37 +73,38 @@
     get_data_async(session=session, on_response=on_response, async_mode=None)
         Returns a response to the async data platform
     get_stream(session=session)
         Get stream quantitative analytic service subscription
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> definition = rdf.repo.Definition(
-     ...   underlying_instruments=[
-     ...   rdf.repo.UnderlyingContract(
-     ...   instrument_type="Bond",
-     ...   instrument_definition=rdf.bond.Definition(instrument_code="US191450264="),
-     ...   )],
-     ...)
-     >>> response = definition.get_data()
-
-     Using get_stream
-     >>> stream = definition.get_stream()
-     >>> stream.open()
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> definition = rdf.repo.Definition(
+    ...   underlying_instruments=[
+    ...   rdf.repo.UnderlyingContract(
+    ...   instrument_type="Bond",
+    ...   instrument_definition=rdf.bond.Definition(instrument_code="US191450264="),
+    ...   )],
+    ...)
+    >>> response = definition.get_data()
+
+    Using get_stream
+
+    >>> stream = definition.get_stream()
+    >>> stream.open()
     """
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
-        buy_sell: Optional[BuySell] = None,
-        day_count_basis: Optional[DayCountBasis] = None,
+        buy_sell: Union[BuySell, str] = None,
+        day_count_basis: Union[DayCountBasis, str] = None,
         underlying_instruments: Optional[List[UnderlyingContract]] = None,
         is_coupon_exchanged: Optional[bool] = None,
         repo_rate_percent: Optional[float] = None,
         fields: "OptStrStrs" = None,
         pricing_parameters: Optional[PricingParameters] = None,
         extended_params: "ExtendedParams" = None,
     ) -> None:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
 from .._instrument_definition import InstrumentDefinition
-from ._enums import (
+from ..._enums import (
     BuySell,
     DayCountBasis,
 )
 from ._repo_underlying_contract import UnderlyingContract
 
 
 class RepoInstrumentDefinition(InstrumentDefinition):
@@ -27,18 +27,18 @@
     end_date : str, optional
         End date of the repo, that means when the borrower repurchases the security
         back. either enddate or tenor field are requested.
     tenor : str, optional
         Tenor that defines the duration of the repo in case no enddate has been
         provided. in that case, enddate is computed from startdate and tenor. either
         enddate or tenor field are requested.
-    buy_sell : BuySell, optional
+    buy_sell : BuySell or str, optional
         The indicator of the deal side. the possible values are:   buy: buying the repo,
         sell: selling the repo.  optional. the default value is "buy".
-    day_count_basis : DayCountBasis, optional
+    day_count_basis : DayCountBasis or str, optional
         Day count basis convention to apply to the custom repo rate. optional,
         "dcb_actual_360" by default.
     underlying_instruments : RepoUnderlyingContract, optional
         Definition of the underlying instruments. only bond contracts are supported for
         now, and only one bond can be used. mandatory.
     is_coupon_exchanged : bool, optional
         Specifies whether or not intermediate coupons are exchanged.
@@ -57,16 +57,16 @@
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
-        buy_sell: Optional[BuySell] = None,
-        day_count_basis: Optional[DayCountBasis] = None,
+        buy_sell: Union[BuySell, str] = None,
+        day_count_basis: Union[DayCountBasis, str] = None,
         underlying_instruments: Optional[UnderlyingContract] = None,
         is_coupon_exchanged: Optional[bool] = None,
         repo_rate_percent: Optional[float] = None,
     ) -> None:
         super().__init__()
         self.instrument_tag = instrument_tag
         self.start_date = start_date
@@ -74,16 +74,15 @@
         self.tenor = tenor
         self.buy_sell = buy_sell
         self.day_count_basis = day_count_basis
         self.underlying_instruments = underlying_instruments
         self.is_coupon_exchanged = is_coupon_exchanged
         self.repo_rate_percent = repo_rate_percent
 
-    @classmethod
-    def get_instrument_type(cls):
+    def get_instrument_type(self):
         return "Repo"
 
     @property
     def buy_sell(self):
         """
         The indicator of the deal side. the possible values are:   buy: buying the repo,
         sell: selling the repo.  optional. the default value is "buy".
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_parameters.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_pricing_parameters.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,23 +1,23 @@
 # coding: utf8
 
-from typing import Optional
+from typing import Optional, Union
 
-from ._enums import RepoCurveType
+from ..._enums import RepoCurveType
 from ..._object_definition import ObjectDefinition
 
 
 class PricingParameters(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    repo_curve_type : RepoCurveType, optional
+    repo_curve_type : RepoCurveType or str, optional
         Curve used to compute the repo rate. it can be computed using following methods:
         - repocurve : rate is computed by interpolating a repo curve.     - depositcurve
         : rate is computed by interpolating a deposit curve.     - fixinglibor : rate is
         computed by interpolating libor rates.  if no curve can be found, the rate is
         computed using a deposit curve.
     market_data_date : str, optional
         The date at which the market data is retrieved. the value is expressed in iso
@@ -33,15 +33,15 @@
         format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g., '2021-01-01t00:00:00z'). by default,
         marketdatadate is used. if marketdatadate is not specified, the default value is
         today.
     """
 
     def __init__(
         self,
-        repo_curve_type: Optional[RepoCurveType] = None,
+        repo_curve_type: Union[RepoCurveType, str] = None,
         market_data_date: Optional[str] = None,
         report_ccy: Optional[str] = None,
         valuation_date: Optional[str] = None,
     ) -> None:
         super().__init__()
         self.repo_curve_type = repo_curve_type
         self.market_data_date = market_data_date
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_contract.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_contract.py`

 * *Files 2% similar despite different names*

```diff
@@ -59,23 +59,19 @@
     def pricing_parameters(self):
         """
         The pricing parameters to apply to this instrument. Optional.
         If pricing parameters are not provided at this level parameters defined globally at the request level are used. If no
         pricing parameters are provided globally default values apply.
         :return: object RepoUnderlyingPricingParameters
         """
-        return self._get_object_parameter(
-            UnderlyingPricingParameters, "pricingParameters"
-        )
+        return self._get_object_parameter(UnderlyingPricingParameters, "pricingParameters")
 
     @pricing_parameters.setter
     def pricing_parameters(self, value):
-        self._set_object_parameter(
-            UnderlyingPricingParameters, "pricingParameters", value
-        )
+        self._set_object_parameter(UnderlyingPricingParameters, "pricingParameters", value)
 
     @property
     def instrument_type(self):
         """
         The type of instrument being defined.
         :return: str
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_pricing_parameters.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,38 +55,30 @@
         self.valuation_date = valuation_date
 
     @property
     def pricing_parameters_at_end(self):
         """
         :return: object BondPricingParameters
         """
-        return self._get_object_parameter(
-            BondPricingParameters, "pricingParametersAtEnd"
-        )
+        return self._get_object_parameter(BondPricingParameters, "pricingParametersAtEnd")
 
     @pricing_parameters_at_end.setter
     def pricing_parameters_at_end(self, value):
-        self._set_object_parameter(
-            BondPricingParameters, "pricingParametersAtEnd", value
-        )
+        self._set_object_parameter(BondPricingParameters, "pricingParametersAtEnd", value)
 
     @property
     def pricing_parameters_at_start(self):
         """
         :return: object BondPricingParameters
         """
-        return self._get_object_parameter(
-            BondPricingParameters, "pricingParametersAtStart"
-        )
+        return self._get_object_parameter(BondPricingParameters, "pricingParametersAtStart")
 
     @pricing_parameters_at_start.setter
     def pricing_parameters_at_start(self, value):
-        self._set_object_parameter(
-            BondPricingParameters, "pricingParametersAtStart", value
-        )
+        self._set_object_parameter(BondPricingParameters, "pricingParametersAtStart", value)
 
     @property
     def repo_parameters(self):
         """
         :return: object RepoParameters
         """
         return self._get_object_parameter(RepoParameters, "repoParameters")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/__init__.py`

 * *Files 7% similar despite different names*

```diff
@@ -26,15 +26,15 @@
     "PricingParameters",
     "StubRule",
     "SwaptionType",
     "TenorReferenceDate",
 )
 
 from ._definition import Definition
-from ._enums import (
+from ..._enums import (
     AdjustInterestToPaymentDate,
     AmortizationFrequency,
     AmortizationType,
     BusinessDayConvention,
     DateRollingConvention,
     DayCountBasis,
     Direction,
@@ -51,10 +51,10 @@
     NotionalExchange,
     PremiumSettlementType,
     PriceSide,
     StubRule,
     SwaptionType,
     TenorReferenceDate,
 )
-from ._models import AmortizationItem, InputFlow
+from ..._models import AmortizationItem, InputFlow
 from ._swap_leg_definition import LegDefinition
 from ._swap_pricing_parameters import PricingParameters
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_swap_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_stream_facade.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,168 +1,160 @@
-# coding: utf8
-from typing import Optional
+import copy
+from typing import Any, Callable, List, TYPE_CHECKING
 
-from ._swap_leg_definition import LegDefinition
-from .._instrument_definition import InstrumentDefinition
+from ._stream import StreamingChain
+from ...._core.session import get_valid_session
+from ...._tools import cached_property, create_repr
+from ....delivery._stream.base_stream import StreamOpenWithUpdatesMixin
+
+if TYPE_CHECKING:
+    from ...._types import ExtendedParams, OptBool, OptInt, OptStr
+    from ...._core.session import Session
+
+
+class Stream(StreamOpenWithUpdatesMixin):
+    """
+    Stream is designed to request streaming chains and decode it dynamically.
+    This class also act like a cache for each part of the chain record.
+
+    Parameters
+    ----------
+    name : str
+        Single instrument name
+    session : Session, optional
+        The Session defines the source where you want to retrieve your data
+    service : str, optional
+        Name service
+    skip_summary_links : bool, optional
+        Store skip summary links
+    skip_empty : bool, optional
+        Store skip empty
+    override_summary_links : int, optional
+        Store the override number of summary links
+    extended_params : dict, optional
+        If necessary other parameters
+
+    Methods
+    -------
+    open(**kwargs)
+        Open the Stream connection
+
+    close()
+        Closes the Stream connection, releases resources
+
+    is_chain
+        True - stream was decoded as a chain
+        False - stream wasn't identified as a chain
+
+    Attributes
+    __________
+    constituents: list
+        A list of constituents in the chain record or empty list
 
+    """
 
-class SwapInstrumentDefinition(InstrumentDefinition):
     def __init__(
         self,
-        instrument_tag: Optional[str] = None,
-        instrument_code: Optional[str] = None,
-        trade_date: Optional[str] = None,
-        start_date: Optional[str] = None,
-        end_date: Optional[str] = None,
-        tenor: Optional[str] = None,
-        legs: Optional[LegDefinition] = None,
-        is_non_deliverable: Optional[bool] = None,
-        settlement_ccy: Optional[str] = None,
-        start_tenor: Optional[str] = None,
-        template: Optional[str] = None,
+        name: str,
+        session: "Session" = None,
+        service: "OptStr" = None,
+        skip_summary_links: "OptBool" = True,
+        skip_empty: "OptBool" = True,
+        override_summary_links: "OptInt" = None,
+        extended_params: "ExtendedParams" = None,
     ) -> None:
-        super().__init__()
-        self.instrument_tag = instrument_tag
-        self.instrument_code = instrument_code
-        self.trade_date = trade_date
-        self.start_date = start_date
-        self.end_date = end_date
-        self.tenor = tenor
-        self.legs = legs
-        self.is_non_deliverable = is_non_deliverable
-        self.settlement_ccy = settlement_ccy
-        self.start_tenor = start_tenor
-        self.template = template
-
-    @classmethod
-    def get_instrument_type(cls):
-        return "Swap"
-
-    @property
-    def legs(self):
-        """
-        The legs of the Swap to provide a full definition of the swap if no template or instrumentCode have been provided.
-        Optional. Either InstrumentCode, Template, or Legs must be provided.
-        :return: list SwapLegDefinition
-        """
-        return self._get_list_parameter(LegDefinition, "legs")
-
-    @legs.setter
-    def legs(self, value):
-        self._set_list_parameter(LegDefinition, "legs", value)
-
-    @property
-    def end_date(self):
-        """
-        The maturity date of the swap contract.
-        Mandatory. Either the endDate or the tenor must be provided.
-        :return: str
-        """
-        return self._get_parameter("endDate")
-
-    @end_date.setter
-    def end_date(self, value):
-        self._set_parameter("endDate", value)
-
-    @property
-    def instrument_code(self):
-        """
-        A swap RIC that is used to retrieve the description of the swap contract.
-        Optional. Either instrumentCode, template, or legs must be provided.
-        :return: str
-        """
-        return self._get_parameter("instrumentCode")
-
-    @instrument_code.setter
-    def instrument_code(self, value):
-        self._set_parameter("instrumentCode", value)
-
-    @property
-    def is_non_deliverable(self):
-        """
-        A flag that indicates if the swap is non-deliverable.
-        Optional. By defaults 'false'.
-        :return: bool
-        """
-        return self._get_parameter("isNonDeliverable")
-
-    @is_non_deliverable.setter
-    def is_non_deliverable(self, value):
-        self._set_parameter("isNonDeliverable", value)
-
-    @property
-    def settlement_ccy(self):
-        """
-        For non-deliverable instrument, the ISO code of the settlement currency.
-        Optional. By priority order : 'USD' if one leg denominated in USD; 'EUR' if one leg is denominated in EUR; the paidLegCcy.
-        :return: str
-        """
-        return self._get_parameter("settlementCcy")
-
-    @settlement_ccy.setter
-    def settlement_ccy(self, value):
-        self._set_parameter("settlementCcy", value)
-
-    @property
-    def start_date(self):
-        """
-        The date the swap starts accruing interest. Its effective date.
-        Optional. By default, it is derived from the TradeDate and the day to spot convention of the contract currency.
-        :return: str
-        """
-        return self._get_parameter("startDate")
-
-    @start_date.setter
-    def start_date(self, value):
-        self._set_parameter("startDate", value)
-
-    @property
-    def start_tenor(self):
-        """
-        The code indicating the period from a spot date to startdate of the instrument
-        (e.g. '1m'). no default value applies.
-        :return: str
-        """
-        return self._get_parameter("startTenor")
-
-    @start_tenor.setter
-    def start_tenor(self, value):
-        self._set_parameter("startTenor", value)
-
-    @property
-    def template(self):
-        """
-        A reference to a common swap contract.
-        Optional. Either InstrumentCode, Template, or Legs must be provided.
-        :return: str
-        """
-        return self._get_parameter("template")
-
-    @template.setter
-    def template(self, value):
-        self._set_parameter("template", value)
-
-    @property
-    def tenor(self):
-        """
-        The period code that represents the time between the start date and end date the contract.
-        Mandatory. Either the endDate or the tenor must be provided.
-        :return: str
-        """
-        return self._get_parameter("tenor")
-
-    @tenor.setter
-    def tenor(self, value):
-        self._set_parameter("tenor", value)
-
-    @property
-    def trade_date(self):
-        """
-        The date the swap contract was created.
-        Optional. By default, the valuation date.
-        :return: str
-        """
-        return self._get_parameter("tradeDate")
-
-    @trade_date.setter
-    def trade_date(self, value):
-        self._set_parameter("tradeDate", value)
+        self._session = get_valid_session(session)
+        self._always_use_default_session = session is None
+        self._name = name
+        self._service = service
+        self._skip_summary_links = skip_summary_links
+        self._skip_empty = skip_empty
+        self._override_summary_links = override_summary_links
+        self._extended_params = extended_params
+
+    @cached_property
+    def _stream(self) -> StreamingChain:
+        streaming_chain = StreamingChain(
+            name=self._name,
+            session=self._session,
+            service=self._service,
+            skip_summary_links=self._skip_summary_links,
+            skip_empty=self._skip_empty,
+            override_summary_links=self._override_summary_links,
+            extended_params=self._extended_params,
+        )
+        return streaming_chain
+
+    @property
+    def name(self) -> str:
+        return self._stream.name
+
+    @property
+    def is_chain(self) -> bool:
+        return self._stream.is_chain
+
+    @property
+    def num_summary_links(self) -> int:
+        return self._stream.num_summary_links
+
+    @property
+    def summary_links(self) -> List[str]:
+        return self._stream.summary_links
+
+    @property
+    def display_name(self) -> str:
+        return self._stream.display_name
+
+    @property
+    def constituents(self) -> List[str]:
+        return copy.deepcopy(self._stream.get_constituents())
+
+    def on_add(self, func: Callable[[int, str, "Stream"], Any]) -> "Stream":
+        func = make_callback(self, func)
+        self._stream.on_add(func)
+        return self
+
+    def on_remove(self, func: Callable[[str, int, "Stream"], Any]) -> "Stream":
+        func = make_callback(self, func)
+        self._stream.on_remove(func)
+        return self
+
+    def on_update(self, func: Callable[[str, str, int, "Stream"], Any]) -> "Stream":
+        func = make_callback(self, func)
+        self._stream.on_update(func)
+        return self
+
+    def on_complete(self, func: Callable[[list, "Stream"], Any]) -> "Stream":
+        func = make_callback(self, func)
+        self._stream.on_complete(func)
+        return self
+
+    def on_error(self, func: Callable[[tuple, str, "Stream"], Any]) -> "Stream":
+        func = make_error_callback(self, func)
+        self._stream.on_error(func)
+        return self
+
+    def __repr__(self):
+        return create_repr(
+            self,
+            content=f"{{name='{self._name}'}}",
+            class_name=self.__class__.__name__,
+        )
+
+
+def make_callback(stream: Stream, func: Callable) -> Callable:
+    """Return a callback functions with correct arguments order."""
+
+    def callback(*args):
+        func(*args, stream)
+
+    return callback
+
+
+def make_error_callback(stream: Stream, func: Callable) -> Callable:
+    """Return a callback function with correct arguments order for error handling."""
+
+    def callback(*args):
+        args = reversed(args)
+        func(*args, stream)
+
+    return callback
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swap/_swap_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swap/_swap_pricing_parameters.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,13 +1,13 @@
 # coding: utf8
 
 
-from typing import Optional
+from typing import Optional, Union
 
-from ._enums import (
+from ..._enums import (
     IndexConvexityAdjustmentIntegrationMethod,
     IndexConvexityAdjustmentMethod,
     PriceSide,
     TenorReferenceDate,
 )
 from ..._object_definition import ObjectDefinition
 
@@ -15,24 +15,24 @@
 class PricingParameters(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    index_convexity_adjustment_integration_method : IndexConvexityAdjustmentIntegrationMethod, optional
+    index_convexity_adjustment_integration_method : IndexConvexityAdjustmentIntegrationMethod or str, optional
         The integration method used for static replication method. the possible values
         are: riemannsum, rungekutta. the default value is 'riemannsum'.
-    index_convexity_adjustment_method : IndexConvexityAdjustmentMethod, optional
+    index_convexity_adjustment_method : IndexConvexityAdjustmentMethod or str, optional
         The convexity adjustment type for constant maturity swaps (cms) and libor
         in-arrears swaps. the possible values are: none, blackscholes, linearswapmodel,
         replication. the default value is 'blackscholes'.
-    price_side : PriceSide, optional
+    price_side : PriceSide or str, optional
         The quoted price side of the instrument. optional. default value is 'mid'.
-    tenor_reference_date : TenorReferenceDate, optional
+    tenor_reference_date : TenorReferenceDate or str, optional
         In case the swap definition uses 'starttenor', 'starttenor' defines whether the
         swap start date is calculated from valuation date or from spot date
     discounting_ccy : str, optional
         The currency code, which defines the choice of the discounting curve. the value
         is expressed in iso 4217 alphabetical format (e.g. 'usd'). by default,
         settlementccy or the paid leg currency is used.
     discounting_tenor : str, optional
@@ -58,34 +58,28 @@
         format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g., '2021-01-01t00:00:00z'). by default,
         marketdatadate is used. if marketdatadate is not specified, the default value is
         today.
     """
 
     def __init__(
         self,
-        index_convexity_adjustment_integration_method: Optional[
-            IndexConvexityAdjustmentIntegrationMethod
-        ] = None,
-        index_convexity_adjustment_method: Optional[
-            IndexConvexityAdjustmentMethod
-        ] = None,
-        price_side: Optional[PriceSide] = None,
-        tenor_reference_date: Optional[TenorReferenceDate] = None,
+        index_convexity_adjustment_integration_method: Union[IndexConvexityAdjustmentIntegrationMethod, str] = None,
+        index_convexity_adjustment_method: Union[IndexConvexityAdjustmentMethod, str] = None,
+        price_side: Union[PriceSide, str] = None,
+        tenor_reference_date: Union[TenorReferenceDate, str] = None,
         discounting_ccy: Optional[str] = None,
         discounting_tenor: Optional[str] = None,
         market_data_date: Optional[str] = None,
         market_value_in_deal_ccy: Optional[float] = None,
         report_ccy: Optional[str] = None,
         use_legs_signing: Optional[bool] = None,
         valuation_date: Optional[str] = None,
     ) -> None:
         super().__init__()
-        self.index_convexity_adjustment_integration_method = (
-            index_convexity_adjustment_integration_method
-        )
+        self.index_convexity_adjustment_integration_method = index_convexity_adjustment_integration_method
         self.index_convexity_adjustment_method = index_convexity_adjustment_method
         self.price_side = price_side
         self.tenor_reference_date = tenor_reference_date
         self.discounting_ccy = discounting_ccy
         self.discounting_tenor = discounting_tenor
         self.market_data_date = market_data_date
         self.market_value_in_deal_ccy = market_value_in_deal_ccy
@@ -117,23 +111,19 @@
     def index_convexity_adjustment_method(self):
         """
         The convexity adjustment type for constant maturity swaps (cms) and libor
         in-arrears swaps. the possible values are: none, blackscholes, linearswapmodel,
         replication. the default value is 'blackscholes'.
         :return: enum IndexConvexityAdjustmentMethod
         """
-        return self._get_enum_parameter(
-            IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod"
-        )
+        return self._get_enum_parameter(IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod")
 
     @index_convexity_adjustment_method.setter
     def index_convexity_adjustment_method(self, value):
-        self._set_enum_parameter(
-            IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod", value
-        )
+        self._set_enum_parameter(IndexConvexityAdjustmentMethod, "indexConvexityAdjustmentMethod", value)
 
     @property
     def price_side(self):
         """
         The quoted price side of the instrument. optional. default value is 'mid'.
         :return: enum PriceSide
         """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/__init__.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,20 +12,20 @@
     "SwaptionMarketDataRule",
     "SwaptionSettlementType",
     "SwaptionType",
 )
 
 from ._bermudan_swaption_definition import BermudanSwaptionDefinition
 from ._definition import Definition
-from ._enums import (
+from ..._enums import (
     BuySell,
     CallPut,
     ExerciseScheduleType,
     ExerciseStyle,
     PremiumSettlementType,
     PriceSide,
     SwaptionSettlementType,
     SwaptionType,
 )
 from ._swaption_market_data_rule import SwaptionMarketDataRule
 from ._swaption_pricing_parameters import PricingParameters
-from ._models import InputFlow
+from ..._models import InputFlow
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_bermudan_swaption_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_bermudan_swaption_definition.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 # coding: utf8
-from typing import Optional
+from typing import Optional, Union
 
 from ....._tools import try_copy_to_list
 from ....._types import Strings
-from ._enums import ExerciseScheduleType
+from ..._enums import ExerciseScheduleType
 from ..._object_definition import ObjectDefinition
 
 
 class BermudanSwaptionDefinition(ObjectDefinition):
     def __init__(
         self,
         exercise_schedule: Optional[Strings] = None,
-        exercise_schedule_type: Optional[ExerciseScheduleType] = None,
+        exercise_schedule_type: Union[ExerciseScheduleType, str] = None,
         notification_days: Optional[int] = None,
     ):
         super().__init__()
         self.exercise_schedule = try_copy_to_list(exercise_schedule)
         self.exercise_schedule_type = exercise_schedule_type
         self.notification_days = notification_days
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_definition.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 # coding: utf8
 
-from typing import Optional, TYPE_CHECKING
+from typing import Optional, TYPE_CHECKING, Union
 
 from ._bermudan_swaption_definition import BermudanSwaptionDefinition
 from ._swaption_definition import SwaptionInstrumentDefinition
 from ._swaption_pricing_parameters import PricingParameters
 from .. import swap
 from .._base_definition import BaseDefinition
 from ..._enums import (
@@ -54,22 +54,22 @@
         option,   sell: selling/writing the option.  no default value applies.
     exercise_style : ExerciseStyle, optional
         The option style based on its exercise restrictions. the possible values are:
         amer,   euro,   berm.  note: all exercise styles may not apply to certain option
         no default value applies.
     payments : InputFlow, optional
         An array of payments
-    premium_settlement_type : PremiumSettlementType, optional
+    premium_settlement_type : PremiumSettlementType or str, optional
         The cash settlement type of the option premium   spot,   forward.
-    settlement_type : SwaptionSettlementType, optional
+    settlement_type : SwaptionSettlementType or str, optional
         The settlement method for options when exercised. the possible values are:
         physical: delivering the underlying asset, or for a swaption, physically
         entering into the underlying swap.    cash: paying out in cash.  the default
         value is 'physical'.
-    swaption_type : SwaptionType, optional
+    swaption_type : SwaptionType or str, optional
         The indicator if the swaption is a payer or a receiver. the possible values are:
         receiver: a right to receive a fixed rate of the underlying swap,   payer: a
         right to pay a fixed rate of the underlying swap.  no default value applies.
     underlying_definition : SwapDefinition, optional
 
     spread_vs_atm_in_bp : float, optional
         Spread between strike and atm strike, expressed in basis points (bp).
@@ -84,60 +84,65 @@
     pricing_parameters : PricingParameters, optional
         The pricing parameters to apply to this instrument. If pricing parameters
         are not provided at this level parameters defined globally at the request
         level are used. If no pricing parameters are provided globally default
         values apply.
     extended_params : dict, optional
         If necessary other parameters.
+    delivery_date : str, optional
+        The date when the underlying asset is delivered.
+        The value is expressed in ISO 8601 format: YYYY-MM-DDT[hh]:[mm]:[ss]Z (e.g. '2021-01-01T00:00:00Z').
 
     Methods
     -------
     get_data(session=session, on_response=on_response, async_mode=None)
         Returns a response to the data platform
     get_data_async(session=session, on_response=on_response, async_mode=None)
         Returns a response to the async data platform
     get_stream(session=session)
         Get stream quantitative analytic service subscription
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> definition = rdf.swaption.Definition(
-     ...   buy_sell=rdf.swaption.BuySell.BUY,
-     ...   call_put=rdf.swaption.CallPut.CALL,
-     ...   exercise_style=rdf.swaption.ExerciseStyle.BERM,
-     ...   underlying_definition=rdf.swap.Definition(tenor="3Y", template="EUR_AB6E"),
-     ...)
-     >>> response = definition.get_data()
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> definition = rdf.swaption.Definition(
+    ...   buy_sell=rdf.swaption.BuySell.BUY,
+    ...   call_put=rdf.swaption.CallPut.CALL,
+    ...   exercise_style=rdf.swaption.ExerciseStyle.BERM,
+    ...   underlying_definition=rdf.swap.Definition(tenor="3Y", template="EUR_AB6E"),
+    ...)
+    >>> response = definition.get_data()
 
     Using get_stream
-     >>> stream = definition.get_stream()
-     >>> stream.open()
+
+    >>> stream = definition.get_stream()
+    >>> stream.open()
     """
 
     def __init__(
         self,
         instrument_tag: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
         notional_amount: Optional[float] = None,
         bermudan_swaption_definition: Optional[BermudanSwaptionDefinition] = None,
         buy_sell: Optional[BuySell] = None,
         exercise_style: Optional[ExerciseStyle] = None,
         payments: Optional[InputFlow] = None,
-        premium_settlement_type: Optional[PremiumSettlementType] = None,
-        settlement_type: Optional[SwaptionSettlementType] = None,
-        swaption_type: Optional[SwaptionType] = None,
+        premium_settlement_type: Union[PremiumSettlementType, str] = None,
+        settlement_type: Union[SwaptionSettlementType, str] = None,
+        swaption_type: Union[SwaptionType, str] = None,
         underlying_definition: Optional[swap.Definition] = None,
         spread_vs_atm_in_bp: Optional[float] = None,
         strike_percent: Optional[float] = None,
         fields: "OptStrStrs" = None,
         pricing_parameters: Optional[PricingParameters] = None,
         extended_params: "ExtendedParams" = None,
+        delivery_date: Optional[str] = None,
     ):
         fields = try_copy_to_list(fields)
         self.underlying_definition = underlying_definition
         definition = SwaptionInstrumentDefinition(
             bermudan_swaption_definition=bermudan_swaption_definition,
             buy_sell=buy_sell,
             exercise_style=exercise_style,
@@ -149,14 +154,15 @@
             end_date=end_date,
             instrument_tag=instrument_tag,
             notional_amount=notional_amount,
             spread_vs_atm_in_bp=spread_vs_atm_in_bp,
             start_date=start_date,
             strike_percent=strike_percent,
             tenor=tenor,
+            delivery_date=delivery_date,
         )
         super().__init__(
             definition=definition,
             fields=fields,
             pricing_parameters=pricing_parameters,
             extended_params=extended_params,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_market_data_rule.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_market_data_rule.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,31 +1,24 @@
-# coding: utf8
 from ..._object_definition import ObjectDefinition
 
 
 class SwaptionMarketDataRule(ObjectDefinition):
     def __init__(self, discount=None, forward=None):
         super().__init__()
         self.discount = discount
         self.forward = forward
 
     @property
     def discount(self):
-        """
-        :return: str
-        """
         return self._get_parameter("discount")
 
     @discount.setter
     def discount(self, value):
         self._set_parameter("discount", value)
 
     @property
     def forward(self):
-        """
-        :return: str
-        """
         return self._get_parameter("forward")
 
     @forward.setter
     def forward(self, value):
         self._set_parameter("forward", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_pricing_parameters.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,24 @@
 # coding: utf8
 
 
-from typing import Optional
+from typing import Optional, Union
 
 from ..._object_definition import ObjectDefinition
-from ._enums import PriceSide
+from ..._enums import PriceSide
 
 
 class PricingParameters(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    price_side : PriceSide, optional
+    price_side : PriceSide or str, optional
         The quoted price side of the instrument. optional. default value is 'mid'.
     exercise_date : str, optional
 
     market_data_date : str, optional
         The date at which the market data is retrieved. the value is expressed in iso
         8601 format: yyyy-mm-ddt[hh]:[mm]:[ss]z (e.g., '2021-01-01t00:00:00z'). it
         should be less or equal tovaluationdate). optional. by
@@ -41,21 +41,21 @@
         The valuation date for pricing. If not set the valuation date is equal to
         market_data_date or Today. For assets that contains a settlementConvention,
         the default valuation date  is equal to the settlementdate of the Asset that is
         usually the TradeDate+SettlementConvention.
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> rdf.swaption.PricingParameters(valuation_date="2020-04-24", nb_iterations=80)
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> rdf.swaption.PricingParameters(valuation_date="2020-04-24", nb_iterations=80)
     """
 
     def __init__(
         self,
-        price_side: Optional[PriceSide] = None,
+        price_side: Union[PriceSide, str] = None,
         exercise_date: Optional[str] = None,
         market_data_date: Optional[str] = None,
         market_value_in_deal_ccy: Optional[float] = None,
         nb_iterations: Optional[int] = None,
         report_ccy: Optional[str] = None,
         simulate_exercise: Optional[bool] = None,
         valuation_date: Optional[str] = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/_definition.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,15 +1,20 @@
 # coding: utf8
 
-from typing import Optional, TYPE_CHECKING
+from typing import Optional, TYPE_CHECKING, Union
 
 from ._term_deposit_definition import TermDepositInstrumentDefinition
 from ._term_deposit_pricing_parameters import PricingParameters
 from .._base_definition import BaseDefinition
-from ..._enums import BusinessDayConvention, DateRollingConvention, DayCountBasis
+from ..._enums import (
+    BusinessDayConvention,
+    DateRollingConvention,
+    DayCountBasis,
+    Frequency,
+)
 from ....._tools import create_repr, try_copy_to_list
 
 if TYPE_CHECKING:
     from ....._types import ExtendedParams, OptStrStrs
 
 
 class Definition(BaseDefinition):
@@ -50,15 +55,15 @@
         Mandatory if instrument_code is None.
     payment_business_day_convention : BusinessDayConvention, optional
         The method to adjust dates to a working day.
         By default 'ModifiedFollowing'.
     payment_roll_convention : DateRollingConvention, optional
         Method to adjust payment dates when they fall at the end of the month.
         By default 'Last'.
-    year_basis : DayCountBasis, optional
+    year_basis : DayCountBasis or str, optional
         The Day Count Basis method used to calculate the interest payments.
         By default 'Dcb_Actual_365'.
     calendar : str, optional
         Calendar used to adjust deposit duration calculation.
         By default the calendar corresponding to notional currency is used.
     fields : list of str, optional
         Contains the list of Analytics that the quantitative analytic service will
@@ -66,56 +71,73 @@
     pricing_parameters : PricingParameters, optional
         The pricing parameters to apply to this instrument. If pricing parameters
         are not provided at this level parameters defined globally at the request
         level are used. If no pricing parameters are provided globally default
         values apply.
     extended_params : dict, optional
         If necessary other parameters.
+    interest_payment_frequency : Frequency or str, optional
+        The frequency of the interest payment.
+        The default value is zero.
+    interest_calculation_method : DayCountBasis or str, optional
+        The day count basis method used to calculate the interest payments. The possible values are listed here.
+        Mandatory if no instrumentCode is defined.
+    payment_business_days : str, optional
+        A comma-separated calendar code used to adjust dates(e.g., 'EMU' or 'USA').
+        The default value is the calendar associated to the market conventions of NotionalCcy.
+    start_tenor : str, optional
+        The code indicating the period from a spot date to startDate of the instrument (e.g., '1M').
+        Either startDate or startTenor can be specified, but not both.
 
     Methods
     -------
     get_data(session=session, on_response=on_response, async_mode=None)
         Returns a response to the data platform
     get_data_async(session=session, on_response=on_response, async_mode=None)
         Returns a response to the async data platform
     get_stream(session=session)
         Get stream quantitative analytic service subscription
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> definition = rdf.term_deposit.Definition(
-     ...   tenor="5Y",
-     ...   notional_ccy="EUR",
-     ...   fixed_rate_percent=11
-     ...)
-     >>> response = definition.get_data()
-
-     Using get_stream
-     >>> stream = definition.get_stream()
-     >>> stream.open()
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> definition = rdf.term_deposit.Definition(
+    ...   tenor="5Y",
+    ...   notional_ccy="EUR",
+    ...   fixed_rate_percent=11
+    ...)
+    >>> response = definition.get_data()
+
+    Using get_stream
+
+    >>> stream = definition.get_stream()
+    >>> stream.open()
     """
 
     def __init__(
         self,
         instrument_code: Optional[str] = None,
         instrument_tag: Optional[str] = None,
         start_date: Optional[str] = None,
         end_date: Optional[str] = None,
         tenor: Optional[str] = None,
         notional_ccy: Optional[str] = None,
         notional_amount: Optional[float] = None,
         fixed_rate_percent: Optional[float] = None,
-        payment_business_day_convention: Optional[BusinessDayConvention] = None,
-        payment_roll_convention: Optional[DateRollingConvention] = None,
-        year_basis: Optional[DayCountBasis] = None,
+        payment_business_day_convention: Union[BusinessDayConvention, str] = None,
+        payment_roll_convention: Union[DateRollingConvention, str] = None,
+        year_basis: Union[DayCountBasis, str] = None,
         calendar: Optional[str] = None,
         fields: "OptStrStrs" = None,
         pricing_parameters: Optional[PricingParameters] = None,
         extended_params: "ExtendedParams" = None,
+        interest_payment_frequency: Union[Frequency, str] = None,
+        interest_calculation_method: Union[DayCountBasis, str] = None,
+        payment_business_days: Optional[str] = None,
+        start_tenor: Optional[str] = None,
     ):
         fields = try_copy_to_list(fields)
         definition = TermDepositInstrumentDefinition(
             payment_business_day_convention=payment_business_day_convention,
             payment_roll_convention=payment_roll_convention,
             year_basis=year_basis,
             calendar=calendar,
@@ -123,14 +145,18 @@
             fixed_rate_percent=fixed_rate_percent,
             instrument_code=instrument_code,
             instrument_tag=instrument_tag,
             notional_amount=notional_amount,
             notional_ccy=notional_ccy,
             start_date=start_date,
             tenor=tenor,
+            interest_payment_frequency=interest_payment_frequency,
+            interest_calculation_method=interest_calculation_method,
+            payment_business_days=payment_business_days,
+            start_tenor=start_tenor,
         )
         super().__init__(
             definition=definition,
             fields=fields,
             pricing_parameters=pricing_parameters,
             extended_params=extended_params,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_definition.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,13 +1,17 @@
 # coding: utf8
+from typing import Optional, Union
 
+from ..._enums import (
+    BusinessDayConvention,
+    DateRollingConvention,
+    DayCountBasis,
+    Frequency,
+)
 from .._instrument_definition import InstrumentDefinition
-from ._enums import BusinessDayConvention
-from ._enums import DateRollingConvention
-from ._enums import DayCountBasis
 
 
 class TermDepositInstrumentDefinition(InstrumentDefinition):
     """
         API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
@@ -38,60 +42,71 @@
         May be retrieved from reference data.
     notional_amount : float, optional
         The notional amount of the term deposit at the start date.
         By default 1,000,000 is used.
     fixed_rate_percent : float, optional
         Fixed interest rate percent to be applied for notional by deal terms.
         Mandatory if instrument_code is None.
-    payment_business_day_convention : BusinessDayConvention, optional
+    payment_business_day_convention : BusinessDayConvention or str, optional
         The method to adjust dates to a working day.
         By default 'ModifiedFollowing'.
-    payment_roll_convention : DateRollingConvention, optional
+    payment_roll_convention : DateRollingConvention or str, optional
         Method to adjust payment dates when they fall at the end of the month.
         By default 'Last'.
-    year_basis : DayCountBasis, optional
+    year_basis : DayCountBasis or str, optional
         The Day Count Basis method used to calculate the interest payments.
         By default 'Dcb_Actual_365'.
     calendar : str, optional
         Calendar used to adjust deposit duration calculation.
         By default the calendar corresponding to notional currency is used.
+    interest_payment_frequency : Frequency or str, optional
+        The interest payment frequency.
+    interest_calculation_method : DayCountBasis or str, optional
+        The day count basis method used to calculate the interest payments.
     """
 
     def __init__(
         self,
         *,
-        instrument_tag=None,
-        instrument_code=None,
-        start_date=None,
-        end_date=None,
-        tenor,
-        notional_ccy,
-        notional_amount=None,
-        fixed_rate_percent=None,
-        payment_business_day_convention=None,
-        payment_roll_convention=None,
-        year_basis=None,
-        calendar=None,
+        instrument_tag: Optional[str] = None,
+        instrument_code: Optional[str] = None,
+        start_date: Optional[str] = None,
+        end_date: Optional[str] = None,
+        tenor: Optional[str] = None,
+        notional_ccy: Optional[str] = None,
+        notional_amount: Optional[float] = None,
+        fixed_rate_percent: Optional[float] = None,
+        payment_business_day_convention: Union[BusinessDayConvention, str] = None,
+        payment_roll_convention: Union[DateRollingConvention, str] = None,
+        year_basis: Union[DayCountBasis, str] = None,
+        calendar: Optional[str] = None,
+        interest_payment_frequency: Union[Frequency, str] = None,
+        interest_calculation_method: Union[DayCountBasis, str] = None,
+        payment_business_days: Optional[str] = None,
+        start_tenor: Optional[str] = None,
     ):
         super().__init__()
         self.instrument_tag = instrument_tag
         self.instrument_code = instrument_code
         self.start_date = start_date
         self.end_date = end_date
         self.tenor = tenor
         self.notional_ccy = notional_ccy
         self.notional_amount = notional_amount
         self.fixed_rate_percent = fixed_rate_percent
         self.payment_business_day_convention = payment_business_day_convention
         self.payment_roll_convention = payment_roll_convention
         self.year_basis = year_basis
         self.calendar = calendar
+        self.interest_payment_frequency = interest_payment_frequency
+        self.interest_calculation_method = interest_calculation_method
+        self.payment_business_days = payment_business_days
+        self.start_tenor = start_tenor
 
-    @classmethod
-    def get_instrument_type(cls):
+    def get_instrument_type(self):
         return "TermDeposit"
 
     @property
     def payment_business_day_convention(self):
         """
         The method to adjust dates to a working day.
         The possible values are:
@@ -102,23 +117,19 @@
          - PreviousBusinessDay (adjusts dates  according to the Preceeding convention - Previous Business Day),
          - NoMoving (does not adjust dates),
          - BbswModifiedFollowing (adjusts dates  according to the BBSW Modified Following convention).
         Optional. In case an instrument code/style has been defined, value comes from bond reference data. Otherwise
         'ModifiedFollowing' is used.
         :return: enum BusinessDayConvention
         """
-        return self._get_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention"
-        )
+        return self._get_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention")
 
     @payment_business_day_convention.setter
     def payment_business_day_convention(self, value):
-        self._set_enum_parameter(
-            BusinessDayConvention, "paymentBusinessDayConvention", value
-        )
+        self._set_enum_parameter(BusinessDayConvention, "paymentBusinessDayConvention", value)
 
     @property
     def payment_roll_convention(self):
         """
         Method to adjust payment dates when they fall at the end of the month (28th of February, 30th, 31st).
         The possible values are:
          - Last (For setting the calculated date to the last working day),
@@ -268,7 +279,39 @@
         :return: str
         """
         return self._get_parameter("tenor")
 
     @tenor.setter
     def tenor(self, value):
         self._set_parameter("tenor", value)
+
+    @property
+    def interest_payment_frequency(self):
+        return self._get_enum_parameter(Frequency, "interestPaymentFrequency")
+
+    @interest_payment_frequency.setter
+    def interest_payment_frequency(self, value):
+        self._set_enum_parameter(Frequency, "interestPaymentFrequency", value)
+
+    @property
+    def interest_calculation_method(self):
+        return self._get_enum_parameter(DayCountBasis, "interestCalculationMethod")
+
+    @interest_calculation_method.setter
+    def interest_calculation_method(self, value):
+        self._set_enum_parameter(DayCountBasis, "interestCalculationMethod", value)
+
+    @property
+    def payment_business_days(self):
+        return self._get_parameter("paymentBusinessDays")
+
+    @payment_business_days.setter
+    def payment_business_days(self, value):
+        self._set_parameter("paymentBusinessDays", value)
+
+    @property
+    def start_tenor(self):
+        return self._get_parameter("startTenor")
+
+    @start_tenor.setter
+    def start_tenor(self, value):
+        self._set_parameter("startTenor", value)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_pricing_parameters.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_pricing_parameters.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,28 +1,22 @@
-# coding: utf8
+from typing import Optional, Union
 
-from typing import Optional
-
-from ._enums import PriceSide
+from ..._enums import PriceSide
 from ..._object_definition import ObjectDefinition
 
 
 class PricingParameters(ObjectDefinition):
     """
     API endpoint for Financial Contract analytics,
     that returns calculations relevant to each contract type.
 
     Parameters
     ----------
-    price_side : PriceSide, optional
+    price_side : PriceSide or str, optional
         Price Side to consider when retrieving Market Data.
-    income_tax_percent : float, optional
-        Income tax percent is subtracted from applied interest rate percents at the end
-        of the deposit. Example: "5" means 5%
-        By default is 0.
     market_data_date : str, optional
         The market data date for pricing.
         By default, the market_data_date date is the valuation_date or Today.
     report_ccy : str, optional
         The reporting currency code, expressed in iso 4217 alphabetical format (e.g.,
         'usd'). It is set for the fields ending with 'xxxinreportccy'. Optional. The
         default value is the notional currency.
@@ -30,21 +24,23 @@
         The valuation date for pricing. If not set the valuation date is equal to
         market_data_date or Today. For assets that contains a settlementConvention, the
         default valuation date  is equal to the settlementdate of the Asset that is
         usually the TradeDate+SettlementConvention.
 
     Examples
     --------
-     >>> import refinitiv.data.content.ipa.financial_contracts as rdf
-     >>> rdf.term_deposit.PricingParameters(valuation_date="2020-04-24")
+    >>> import refinitiv.data.content.ipa.financial_contracts as rdf
+    >>> rdf.term_deposit.PricingParameters(valuation_date="2020-04-24")
     """
 
+    _income_tax_percent = None
+
     def __init__(
         self,
-        price_side: Optional[PriceSide] = None,
+        price_side: Union[PriceSide, str] = None,
         income_tax_percent: Optional[float] = None,
         market_data_date: Optional[str] = None,
         report_ccy: Optional[str] = None,
         valuation_date: Optional[str] = None,
     ):
         super().__init__()
         self.price_side = price_side
@@ -63,24 +59,19 @@
 
     @price_side.setter
     def price_side(self, value):
         self._set_enum_parameter(PriceSide, "priceSide", value)
 
     @property
     def income_tax_percent(self):
-        """
-        Income tax percent is substracted from applied interest rate percents in the end of deposit.
-        Example: "5" means 5%
-        :return: float
-        """
-        return self._get_parameter("incomeTaxPercent")
+        return self._income_tax_percent
 
     @income_tax_percent.setter
     def income_tax_percent(self, value):
-        self._set_parameter("incomeTaxPercent", value)
+        self._income_tax_percent = value
 
     @property
     def market_data_date(self):
         """
         The market data date for pricing.
         By default, the marketDataDate date is the ValuationDate or Today.
         :return: str
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/eti/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/eti/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -11,31 +11,33 @@
     "PriceSide",
     "StrikeFilter",
     "StrikeFilterRange",
     "SurfaceFilters",
     "SurfaceLayout",
     "TimeStamp",
     "VolatilityModel",
+    "VolatilitySurfacePoint",
 )
 
 from ._definition import Definition
-from ._enums import (
+from ..._surfaces._enums import (
     EtiInputVolatilityType,
     VolatilityModel,
     PriceSide,
     TimeStamp,
     MoneynessType,
     Axis,
     Format,
 )
-from ._models import (
+from ..._surfaces._models import (
     MoneynessWeight,
     SurfaceFilters,
     MaturityFilter,
     StrikeFilterRange,
     StrikeFilter,
     SurfaceLayout,
+    VolatilitySurfacePoint,
 )
 from ..._surfaces._eti_surface_definition import EtiSurfaceDefinition
 from ..._surfaces._eti_surface_parameters import (
     EtiSurfaceParameters as EtiCalculationParams,
 )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/fx/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/fx/__init__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,32 +1,39 @@
 __all__ = (
     "Axis",
     "BidAskMid",
     "DayWeight",
     "Definition",
     "Format",
     "FxCalculationParams",
+    "FxStatisticsParameters",
     "FxSurfaceDefinition",
     "FxSwapCalculationMethod",
     "FxVolatilityModel",
     "InterpolationWeight",
     "PriceSide",
     "SurfaceLayout",
     "TimeStamp",
 )
 
 from ._definition import Definition
-from ._enums import (
+from ..._surfaces._enums import (
     FxVolatilityModel,
     FxSwapCalculationMethod,
     PriceSide,
     TimeStamp,
     Axis,
     Format,
 )
-from ._models import BidAskMid, InterpolationWeight, DayWeight, SurfaceLayout
+from ..._surfaces._models import (
+    BidAskMid,
+    InterpolationWeight,
+    SurfaceLayout,
+)
+from ..._models import DayWeight
 from ..._surfaces._fx_surface_definition import (
     FxVolatilitySurfaceDefinition as FxSurfaceDefinition,
 )
 from ..._surfaces._fx_surface_parameters import (
     FxSurfaceParameters as FxCalculationParams,
 )
+from ..._surfaces._fx_statistics_parameters import FxStatisticsParameters
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/fx/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/fx/_definition.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,31 +1,32 @@
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Union
 
 from .._definition import BaseSurfaceDefinition
 from ..._surfaces._fx_surface_request_item import FxSurfaceRequestItem
 from ....._content_type import ContentType
 from ....._tools import create_repr
 
 if TYPE_CHECKING:
+    from . import FxSurfaceDefinition
     from ..._surfaces._surface_types import SurfaceParameters, SurfaceLayout
     from ....._types import ExtendedParams, OptStr
 
 
 class Definition(BaseSurfaceDefinition):
     """
     Create a Fx data Definition object.
 
     Parameters
     ----------
     surface_layout : SurfaceLayout
         See details in SurfaceLayout class
     surface_parameters : SurfaceParameters
         See details in SurfaceParameters class
-    underlying_definition : dict
-       Dict with instrument_code
+    underlying_definition : dict or FxSurfaceDefinition
+       Dict or FxSurfaceDefinition object. See details in FxSurfaceDefinition class
        Example:
             {"fxCrossCode": "EURUSD"}
     surface_tag : str, optional
         A user defined string to describe the volatility surface
     extended_params : dict, optional
         If necessary other parameters
 
@@ -54,15 +55,15 @@
     >>>
     """
 
     def __init__(
         self,
         surface_layout: "SurfaceLayout" = None,
         surface_parameters: "SurfaceParameters" = None,
-        underlying_definition: dict = None,
+        underlying_definition: Union[dict, "FxSurfaceDefinition"] = None,
         surface_tag: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ):
         request_item = FxSurfaceRequestItem(
             surface_layout=surface_layout,
             surface_parameters=surface_parameters,
             underlying_definition=underlying_definition,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ipa/surfaces/swaption/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ipa/surfaces/cap/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,27 +1,33 @@
 __all__ = (
     "Axis",
+    "CapCalculationParams",
+    "CapSurfaceDefinition",
     "Definition",
     "DiscountingType",
     "Format",
     "InputVolatilityType",
+    "SurfaceFilters",
     "SurfaceLayout",
-    "SwaptionCalculationParams",
-    "SwaptionSurfaceDefinition",
     "VolatilityAdjustmentType",
+    "PriceSide",
+    "TimeStamp",
 )
 
 from ._definition import Definition
-from ._enums import (
+from ..._surfaces._enums import (
     DiscountingType,
     VolatilityAdjustmentType,
     Axis,
     InputVolatilityType,
     Format,
+    PriceSide,
+    TimeStamp,
 )
-from ._models import SurfaceLayout
+from ..._surfaces._models import SurfaceLayout, SurfaceFilters
+
 from ..._surfaces._i_ir_vol_model_definition import (
-    IIrVolModelDefinition as SwaptionSurfaceDefinition,
+    IIrVolModelDefinition as CapSurfaceDefinition,
 )
 from ..._surfaces._i_ir_vol_model_pricing_parameters import (
-    IIrVolModelPricingParameters as SwaptionCalculationParams,
+    IIrVolModelPricingParameters as CapCalculationParams,
 )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/_headlines_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/headlines/_definition.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,17 @@
-from datetime import timedelta
-from typing import Callable, Optional, TYPE_CHECKING
+from typing import Callable, Optional, TYPE_CHECKING, Union
 
-from ._news_data_provider_layer import NewsDataProviderLayer
-from ._sort_order import SortOrder
-from ..._content_type import ContentType
-from ..._tools import create_repr
+from .._news_data_provider_layer import NewsDataProviderLayer
+from ..headlines._sort_order import SortOrder
+from ...._content_type import ContentType
+from ...._tools import create_repr
 
 if TYPE_CHECKING:
-    from ..._core.session import Session
-    from ..._types import ExtendedParams, OptDateTime
+    from ...._core.session import Session
+    from ...._types import ExtendedParams, OptDateTime
 
 
 class Definition(NewsDataProviderLayer):
     """
     This class describes parameters to retrieve data for news headlines.
 
     Parameters
@@ -27,22 +26,23 @@
         Beginning of date range.
         String format is: '%Y-%m-%dT%H:%M:%S'. e.g. '2016-01-20T15:04:05'.
 
     date_to: str or timedelta, optional
         End of date range.
         String format is: '%Y-%m-%dT%H:%M:%S'. e.g. '2016-01-20T15:04:05'.
 
-    sort_order: SortOrder
-        Value from SortOrder enum. Default: SortOrder.new_to_old
+    sort_order: str or SortOrder
+        Sort order for the response. Default: SortOrder.new_to_old
 
     extended_params: dict, optional
-        Other parameters can be provided if necessary
+        Additional parameters to provide to the API.
 
     Examples
     --------
+    >>> from datetime import timedelta
     >>> from refinitiv.data.content import news
     >>> definition = news.headlines.Definition(
     ...     "Refinitiv",
     ...     date_from="20.03.2021",
     ...     date_to=timedelta(days=-4),
     ...     count=3
     ... )
@@ -50,15 +50,15 @@
 
     def __init__(
         self,
         query: str,
         count: int = 10,
         date_from: "OptDateTime" = None,
         date_to: "OptDateTime" = None,
-        sort_order: SortOrder = SortOrder.new_to_old,
+        sort_order: Union[str, SortOrder] = SortOrder.new_to_old,
         extended_params: "ExtendedParams" = None,
     ):
         super().__init__(
             data_type=ContentType.NEWS_HEADLINES_RDP,
             query=query,
             count=count,
             date_from=date_from,
@@ -93,19 +93,22 @@
         Raises
         ------
         AttributeError
             If user didn't set default session.
 
         Examples
         --------
+        >>> from datetime import timedelta
         >>> from refinitiv.data.content import news
-        >>> definition = news.headlines.Definition("Refinitiv",
-        >>>                                        date_from="20.03.2021",
-        >>>                                        date_to=timedelta(days=-4),
-        >>>                                        count=3)
+        >>> definition = news.headlines.Definition(
+        ...     query="Refinitiv",
+        ...     date_from="20.03.2021",
+        ...     date_to=timedelta(days=-4),
+        ...     count=3
+        ... )
         >>> response = definition.get_data()
         """
         self._kwargs["on_page_response"] = on_page_response
         return super().get_data(session, on_response)
 
     async def get_data_async(
         self,
@@ -135,24 +138,25 @@
         Raises
         ------
         AttributeError
             If user didn't set default session.
 
         Examples
         --------
+        >>> from datetime import timedelta
         >>> from refinitiv.data.content import news
-        >>> definition = news.headlines.Definition("Refinitiv",
-        >>>                                        date_from="20.03.2021",
-        >>>                                        date_to=timedelta(days=-4),
-        >>>                                        count=3)
+        >>> definition = news.headlines.Definition(
+        ...     query="Refinitiv",
+        ...     date_from="20.03.2021",
+        ...     date_to=timedelta(days=-4),
+        ...     count=3
+        ... )
         >>> response = await definition.get_data_async()
         """
         self._kwargs["on_page_response"] = on_page_response
-        self._kwargs["closure"] = closure
-        return await super().get_data_async(session, on_response)
+        return await super().get_data_async(session, on_response, closure)
 
     def __repr__(self):
         return create_repr(
             self,
-            middle_path="headlines",
             content=f"{{query='{self._query}'}}",
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/_news_data_provider_layer.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/_news_data_provider_layer.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Optional, Callable, TYPE_CHECKING, Any
+from typing import Optional, Callable, TYPE_CHECKING
 
 from .._content_provider_layer import ContentUsageLoggerMixin
 from ..._content_type import ContentType
 from ..._core.session import get_valid_session
 from ..._core.session.tools import is_platform_session
 from ...delivery._data._data_provider import DataProviderLayer
 
@@ -20,18 +20,15 @@
 class NewsDataProviderLayer(ContentUsageLoggerMixin, DataProviderLayer):
     _USAGE_CLS_NAME = "NewsDataProviderLayer"
 
     def _check_underlying_platform(self, session):
         underlying_platform = session.config.get(NEWS_UNDERLYING_PLATFORM_KEY) or "rdp"
 
         if underlying_platform not in {"rdp", "udf"}:
-            message = (
-                f"Not correct value for '{NEWS_UNDERLYING_PLATFORM_KEY}'. "
-                "Possible values: 'udf', 'rdp'"
-            )
+            message = f"Not correct value for '{NEWS_UNDERLYING_PLATFORM_KEY}'. " "Possible values: 'udf', 'rdp'"
             session.error(message)
             raise ValueError(message)
 
         if underlying_platform == "udf":
             if is_platform_session(session):
                 session.debug(
                     "UDF News service cannot be used with platform sessions, RDP News will be used instead. "
@@ -54,14 +51,13 @@
         response = super().get_data(session, on_response)
         return response
 
     async def get_data_async(
         self,
         session: Optional["Session"] = None,
         on_response: Optional[Callable] = None,
-        closure: Optional[Any] = None,
+        closure: Optional[str] = None,
     ):
-        self._kwargs["closure"] = closure
         session = get_valid_session(session)
         self._check_underlying_platform(session)
-        response = await super().get_data_async(session, on_response)
+        response = await super().get_data_async(session, on_response, closure)
         return response
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/_story_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/story/_definition.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,37 +1,42 @@
-from ._news_data_provider_layer import NewsDataProviderLayer
-from ..._tools import create_repr
-
-from ..._content_type import ContentType
+from .._news_data_provider_layer import NewsDataProviderLayer
+from ...._content_type import ContentType
+from ...._tools import create_repr
+from ...._types import ExtendedParams
 
 
 class Definition(NewsDataProviderLayer):
     """
     This class describes parameters to retrieve data for news story.
 
     Parameters
     ----------
     story_id : str
-        News Story ID.
+        The ID of news story.
+
+    extended_params: dict, optional
+        Other parameters can be provided if necessary
+
 
     Examples
     --------
     >>> from refinitiv.data.content import news
     >>> definition = news.story.Definition("urn:newsml:reuters.com:20201026:nPt6BSyBh")
     """
 
     def __init__(
         self,
         story_id: str,
+        extended_params: "ExtendedParams" = None,
     ):
         super().__init__(
             data_type=ContentType.NEWS_STORY_RDP,
             story_id=story_id,
+            extended_params=extended_params,
         )
         self.story_id = story_id
 
     def __repr__(self):
         return create_repr(
             self,
-            middle_path="story",
             content=f"{{story_id='{self.story_id}'}}",
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/_tools.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/_tools.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,34 +1,28 @@
 from typing import TYPE_CHECKING, Callable, List, Union
 
 import pandas as pd
 
 from ..._tools._dataframe import (
-    convert_df_columns_to_datetime,
     convert_dtypes,
-    convert_str_to_datetime,
+    convert_str_to_timestamp,
 )
 from ...eikon._tools import tz_replacer
 
 if TYPE_CHECKING:
-    from ._data_classes import HeadlineRDP, HeadlineUDF
+    from .headlines._data import HeadlineRDP, HeadlineUDF
 
 
 def news_build_df_udf(content_data, **kwargs) -> pd.DataFrame:
     columns = ["text", "storyId", "sourceCode"]
 
     raw_headlines = content_data.get("headlines", [])
-    index = [
-        convert_str_to_datetime(tz_replacer(raw_headline["versionCreated"]))
-        for raw_headline in raw_headlines
-    ]
+    index = [convert_str_to_timestamp(tz_replacer(raw_headline["versionCreated"])) for raw_headline in raw_headlines]
 
-    data = [
-        [raw_headline[column] for column in columns] for raw_headline in raw_headlines
-    ]
+    data = [[raw_headline[column] for column in columns] for raw_headline in raw_headlines]
     if data:
         df = pd.DataFrame(
             data=data,
             index=index,
             columns=columns,
         )
         df = convert_dtypes(df)
@@ -49,32 +43,26 @@
         for i in raw:
             content_data.extend(i["data"])
 
     else:
         content_data = raw["data"]
 
     index = [
-        convert_str_to_datetime(
-            tz_replacer(headline["newsItem"]["itemMeta"]["versionCreated"]["$"])
-        )
+        convert_str_to_timestamp(tz_replacer(headline["newsItem"]["itemMeta"]["versionCreated"]["$"]))
         for headline in content_data
     ]
 
     data = []
 
     for headline_data in content_data:
         news_item = headline_data.get("newsItem", dict())
         item_meta = news_item.get("itemMeta", {})
         info_sources = news_item["contentMeta"]["infoSource"]
         info_source = next(
-            (
-                item["_qcode"]
-                for item in info_sources
-                if item["_role"] == "sRole:source"
-            ),
+            (item["_qcode"] for item in info_sources if item["_role"] == "sRole:source"),
             None,
         )
         data.append(
             [
                 item_meta["title"][0]["$"],
                 headline_data["storyId"],
                 info_source,
@@ -112,15 +100,14 @@
     raw: dict,
     build_headline: Callable[[dict], Union["HeadlineRDP", "HeadlineUDF"]],
     limit: int,
 ) -> List[Union["HeadlineRDP", "HeadlineUDF"]]:
     headlines = []
 
     if isinstance(raw, list):
-
         data = []
         for i in raw:
             data.extend(i.get("data", i.get("headlines", [])))
 
     else:
         data = raw.get("data", raw.get("headlines", []))
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/_udf_html_parser.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/story/_udf_html_parser.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/images/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/images/_data_provider.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,28 +1,26 @@
+from dataclasses import dataclass
+
 from ._image import Image, ResizedImage
 from ..._content_data_provider import ContentDataProvider
-from ...._tools import ParamItem
-from ...._tools import extend_params
-from ....delivery._data._data_provider import RequestFactory
-from ..._content_response_factory import ContentResponseFactory
-from ...._core.session import Session
-from ...._tools import ParamItem, get_correct_filename
+from ...._tools import ParamItem, extend_params, get_correct_filename
 from ....delivery._data._data_provider import (
     RequestFactory,
 )
 from ....delivery._data._endpoint_data import EndpointData
 from ....delivery._data._parsed_data import ParsedData
-from ....delivery._data._response_factory import ResponseFactory
+from ....delivery._data._response_factory import ResponseFactory, TypeResponse
 from ....delivery._data._validators import (
-    ValidatorContainer,
-    ContentValidator,
     ContentTypeValidator,
+    ContentValidator,
+    ValidatorContainer,
 )
 
 
+@dataclass
 class ImageData(EndpointData):
     @property
     def image(self) -> "Image":
         if "image" in self.raw:
             return ResizedImage(self.raw)
         return Image(self.raw)
 
@@ -37,17 +35,15 @@
     def extend_query_parameters(self, query_parameters, extended_params=None):
         return extend_params(query_parameters, extended_params)
 
     @property
     def query_params_config(self):
         return query_params
 
-    def get_header_parameters(
-        self, session=None, header_parameters=None, width=None, height=None, **kwargs
-    ):
+    def get_header_parameters(self, session=None, header_parameters=None, width=None, height=None, **kwargs):
         if width or height:
             return {"accept": "image/jpeg"}
         return {"accept": "application/json"}
 
     def get_path_parameters(self, session=None, *, image_id=None, **kwargs):
         return {"imageId": image_id}
 
@@ -74,29 +70,34 @@
     filename = f"{image_id}.{extension}"
     return get_correct_filename(filename, "_")
 
 
 class ImagesResponseFactory(ResponseFactory):
     def create_data_success(
         self,
-        parsed_data: "ParsedData",
+        raw: dict,
         image_id=None,
+        headers=None,
+        content=None,
         **kwargs,
     ):
-        headers = parsed_data.raw_response.headers
         content_type = dict(headers).get("content-type")
         if content_type.startswith("image/"):
-            image_data = {
-                "image": parsed_data.raw_response.content,
+            raw = {
+                "image": content,
                 "filename": get_image_filename(image_id, content_type),
             }
-        else:
-            image_data = self.get_raw(parsed_data)
 
-        return self.data_class(image_data, **kwargs)
+        return self.data_class(raw=raw, _kwargs=kwargs)
+
+    def create_success(self, parsed_data: "ParsedData", **kwargs) -> TypeResponse:
+        raw_response = parsed_data.raw_response
+        headers = raw_response.headers
+        content = raw_response.content
+        return super().create_success(parsed_data, headers=headers, content=content, **kwargs)
 
 
 class ImagesContentValidator(ContentValidator):
     @property
     def validators(self):
         return [self.content_data_is_not_none]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/images/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/images/_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -24,15 +24,15 @@
 
     height: int, optional
         New height of the image
 
     extended_params: dict, optional
         Other parameters can be provided if necessary
 
-     Examples
+    Examples
     --------
     >>> from refinitiv.data.content import news
     >>> definition = news.images.Definition("image_id")
     >>> response = definition.get_data()
     >>> image = response.data.image
     >>> image.save("images")
     """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/images/_image.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/images/_image.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_data_provider.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,30 +1,32 @@
+from dataclasses import dataclass
 from typing import List
 
 from ._top_news_headline import TopNewsHeadline
 from ..._content_data import Data
 from ..._content_data_provider import ContentDataProvider
 from ..._content_response_factory import ContentResponseFactory
-from ...._tools import cached_property, ParamItem
+from ...._tools import cached_property, ParamItem, extend_params
 from ....delivery._data._request_factory import RequestFactory
 
 
+@dataclass
 class TopNewsData(Data):
     @cached_property
     def headlines(self) -> "List[TopNewsHeadline]":
-        return [
-            TopNewsHeadline.from_dict(headline_data)
-            for headline_data in self.raw.get("data", [])
-        ]
+        return [TopNewsHeadline.from_dict(headline_data) for headline_data in self.raw.get("data", [])]
 
 
 query_params = [ParamItem("revision_id", "revisionId")]
 
 
 class TopNewsRequestFactory(RequestFactory):
+    def extend_query_parameters(self, query_parameters, extended_params=None):
+        return extend_params(query_parameters, extended_params)
+
     def get_path_parameters(self, session=None, *, top_news_id=None, **kwargs):
         return {"topNewsId": top_news_id}
 
     def get_url(self, *args, **kwargs):
         return f"{super().get_url(*args, **kwargs)}/{{topNewsId}}"
 
     @property
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_definition.py`

 * *Files 22% similar despite different names*

```diff
@@ -2,39 +2,44 @@
 
 from ._data_provider import TopNewsData
 from ...._content_type import ContentType
 from ....delivery._data._data_provider_layer import DataProviderLayer
 from ....delivery._data._response import BaseResponse
 
 if TYPE_CHECKING:
-    from ...._types import OptInt
+    from ...._types import OptInt, ExtendedParams
 
 
 class Definition(DataProviderLayer[BaseResponse[TopNewsData]]):
     """
     This class describes parameters to retrieve data for top news headlines.
 
     Parameters
     ----------
     top_news_id: str
         The id of the package
 
     revision_id: int, optional
         The package known version
 
-     Examples
+    extended_params: dict, optional
+        Other parameters can be provided if necessary
+
+    Examples
     --------
     >>> from refinitiv.data.content import news
     >>> definition = news.top_news.Definition("top_news_id")
     >>> response = definition.get_data()
     """
 
     def __init__(
         self,
         top_news_id: str,
         revision_id: "OptInt" = None,
+        extended_params: "ExtendedParams" = None,
     ):
         super().__init__(
-            data_type=ContentType.NEWS_TOP_NEWS_HEADLINES,
+            data_type=ContentType.NEWS_TOP_NEWS,
             top_news_id=top_news_id,
             revision_id=revision_id,
+            extended_params=extended_params,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_df_builder.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_df_builder.py`

 * *Files 16% similar despite different names*

```diff
@@ -14,21 +14,16 @@
 
 def news_top_build_df(raw: dict, **kwargs):
     raw_data = raw.get("data", [{}])
 
     # data
     columns = headline_data_key_by_column.keys()
     data = [
-        [
-            get_from_path(headline_data, path)
-            for path in headline_data_key_by_column.values()
-        ]
+        [get_from_path(headline_data, path) for path in headline_data_key_by_column.values()]
         for headline_data in raw_data
     ]
 
     # index
-    index_data = [
-        datetime64(headline_data.get("versionCreated")) for headline_data in raw_data
-    ]
+    index_data = [datetime64(headline_data.get("versionCreated")) for headline_data in raw_data]
     index = pd.Index(index_data, name="versionCreated")
     df = pd.DataFrame(data, columns=columns, index=index)
     return convert_dtypes(df)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/_top_news_headline.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/_top_news_headline.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,30 +1,28 @@
 from dataclasses import dataclass
 from typing import List, Optional
 
 import pandas as pd
+from numpy import datetime64
 
 
 @dataclass
 class TopNewsHeadline:
-    version_created: str
+    version_created: datetime64
     headline: str
     story_id: str
     date_line: str
-    first_created: str
+    first_created: datetime64
     snippet: str
     related_headlines: "Optional[List[TopNewsHeadline]]" = None
 
     @classmethod
     def from_dict(cls, datum: dict) -> "TopNewsHeadline":
         return cls(
             version_created=pd.to_datetime(datum.get("versionCreated")),
             headline=datum.get("text"),
             story_id=datum.get("storyId"),
             date_line=datum.get("dateLine"),
             first_created=pd.to_datetime(datum.get("firstCreated")),
             snippet=datum.get("snippet"),
-            related_headlines=[
-                cls.from_dict(data) for data in datum.get("relatedHeadlines", [])
-            ]
-            or None,
+            related_headlines=[cls.from_dict(data) for data in datum.get("relatedHeadlines", [])] or None,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/_data_provider.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,33 +1,42 @@
+from dataclasses import dataclass
+
 from ._subcategory import Subcategory
 from ._top_news_id import TopNewsId
 from ...._content_data import Data
 from ...._content_data_provider import ContentDataProvider
 from ...._content_response_factory import ContentResponseFactory
-from ....._tools import cached_property, ValueParamItem, make_enum_arg_parser
+from ....._tools import (
+    cached_property,
+    ValueParamItem,
+    make_enum_arg_parser,
+    extend_params,
+)
 from .....delivery._data._data_provider import RequestFactory
 
 
+@dataclass
 class HierarchyData(Data):
     @cached_property
     def hierarchy(self):
         return {
-            category["name"]: {
-                page["name"]: Subcategory.from_dict(page) for page in category["pages"]
-            }
+            category["name"]: {page["name"]: Subcategory.from_dict(page) for page in category["pages"]}
             for category in self.raw.get("data", [])
         }
 
 
 top_news_id_arg_parser = make_enum_arg_parser(TopNewsId)
 
 query_params = [ValueParamItem("id", function=top_news_id_arg_parser.get_str)]
 
 
 class HierarchyRequestFactory(RequestFactory):
+    def extend_query_parameters(self, query_parameters, extended_params=None):
+        return extend_params(query_parameters, extended_params)
+
     @property
     def query_params_config(self):
         return query_params
 
 
 news_top_news_hierarchy_data_provider = ContentDataProvider(
     request=HierarchyRequestFactory(),
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/online_reports/hierarchy/_definition.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,34 +1,31 @@
-from typing import TYPE_CHECKING
-
 from ._data_provider import HierarchyData
 from ....._content_type import ContentType
+from ....._types import ExtendedParams
 from .....delivery._data._data_provider_layer import DataProviderLayer
 from .....delivery._data._response import BaseResponse
 
-if TYPE_CHECKING:
-    from typing import Optional, Union
-    from ._top_news_id import TopNewsId
-
 
 class Definition(DataProviderLayer[BaseResponse[HierarchyData]]):
     """
-    This class describes parameters to retrieve data for top news hierarchy.
+    This class describes parameters to retrieve data for news online-reports.
 
     Parameters
     ----------
-    id: str or Enum, optional
-        The id of the package[current, test, next]
+    extended_params: dict, optional
+        Other parameters can be provided if necessary
 
-     Examples
+    Examples
     --------
     >>> from refinitiv.data.content import news
-    >>> definition = news.top_news.hierarchy.Definition()
+    >>> definition = news.online_reports.hierarchy.Definition()
     >>> response = definition.get_data()
-    >>> response.data.hierarchy['Commodities']
     """
 
     def __init__(
         self,
-        id: "Optional[Union[TopNewsId, str]]" = None,
+        extended_params: "ExtendedParams" = None,
     ):
-        super().__init__(data_type=ContentType.NEWS_TOP_NEWS, id=id)
+        super().__init__(
+            data_type=ContentType.NEWS_ONLINE_REPORTS_HIERARCHY,
+            extended_params=extended_params,
+        )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/news/top_news/hierarchy/_df_builder.py` & `refinitiv-data-1.2.0/refinitiv/data/content/news/top_news/hierarchy/_df_builder.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,29 +1,29 @@
 import pandas as pd
+from ....._tools._dataframe import convert_str_to_timestamp
+
 
 page_key_by_column = {
     "revisionId": "revisionId",
     "revisionDate": "revisionDate",
     "topNewsId": "topNewsId",
 }
 
 
 def news_top_hierarchy_build_df(raw: dict, **kwargs) -> pd.DataFrame:
     raw_data = raw.get("data", [{}])
 
     # data
     columns = page_key_by_column.keys()
     data = [
-        [page.get(key) for key in page_key_by_column.values()]
+        [
+            convert_str_to_timestamp(page.get(key)) if key == "revisionDate" else page.get(key)
+            for key in page_key_by_column.values()
+        ]
         for category in raw_data
         for page in category.get("pages", [{}])
     ]
-
     # index
-    index_data = [
-        (category["name"], page["name"])
-        for category in raw_data
-        for page in category["pages"]
-    ]
+    index_data = [(category["name"], page["name"]) for category in raw_data for page in category["pages"]]
     index = pd.MultiIndex.from_tuples(index_data, names=("Category", "Subcategory"))
 
     return pd.DataFrame(data, index=index, columns=columns)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/_org_info_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/_org_info_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_breakdown_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_breakdown_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_concentration_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_concentration_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_investors_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_investors_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_recent_activity_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_recent_activity_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_shareholders_history_report_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_shareholders_history_report_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_shareholders_report_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_shareholders_report_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/consolidated/_top_n_concentration_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/consolidated/_top_n_concentration_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_breakdown_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_breakdown_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_concentration_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_concentration_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_holdings_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_holdings_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_investors_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_investors_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_recent_activity_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_recent_activity_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_shareholders_history_report_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_shareholders_history_report_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_shareholders_report_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_shareholders_report_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/fund/_top_n_concentration_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/fund/_top_n_concentration_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/_shareholders_report_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/_shareholders_report_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/insider/_transaction_report_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/insider/_transaction_report_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/ownership/investor/_holdings_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/ownership/investor/_holdings_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 from typing import TYPE_CHECKING
 
+from ._pricing_content_provider import PricingData
 from ._stream_facade import Stream
-from .._content_data import Data
 from .._content_provider_layer import ContentUsageLoggerMixin
 from ..._content_type import ContentType
 from ..._core.session import Session
 from ..._tools import create_repr, try_copy_to_list
 from ..._tools._common import universe_arg_parser, fields_arg_parser
 from ...delivery._data._data_provider import DataProviderLayer, BaseResponse
 
 if TYPE_CHECKING:
     from ..._types import OptStr, ExtendedParams, StrStrings, OptStrStrs
 
 
 class Definition(
-    ContentUsageLoggerMixin[BaseResponse[Data]], DataProviderLayer[BaseResponse[Data]]
+    ContentUsageLoggerMixin[BaseResponse[PricingData]],
+    DataProviderLayer[BaseResponse[PricingData]],
 ):
     """
     This class defines parameters for requesting events from pricing
 
     Parameters
     ----------
     universe : str or list of str
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/_pricing_content_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/_pricing_content_provider.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,17 +1,19 @@
 # coding: utf8
-
+from dataclasses import dataclass
+from functools import partial
 from typing import TYPE_CHECKING, List, Callable
 
 import pandas as pd
 
+from .._content_data import Data
 from .._content_data_provider import ContentDataProvider
 from .._content_response_factory import ContentResponseFactory
 from .._error_parser import ErrorParser
-from ..._tools import PRICING_DATETIME_PATTERN
+from ..._tools import PRICING_DATETIME_PATTERN, ValueParamItem
 from ..._tools._common import fields_arg_parser, universe_arg_parser, cached_property
 from ..._tools._dataframe import convert_df_columns_to_datetime_re, convert_dtypes
 from ...delivery._data._data_provider import (
     ContentValidator,
     RequestFactory,
     ValidatorContainer,
 )
@@ -86,30 +88,26 @@
             )
     return PriceCache(cache)
 
 
 status_code_to_value = {"NotEntitled": "#N/P", "NotFound": "#N/F"}
 
 
-def pricing_build_df(
-    raw: List[dict], universe: list, fields: list, **kwargs
-) -> pd.DataFrame:
+def pricing_build_df(raw: List[dict], universe: list, fields: list, **kwargs) -> pd.DataFrame:
     """Pricing dataframe builder.
     Args:
         raw (List[dict]): list of raw data to build dataframe.
         universe (list): list of RICs.
         fields (list): list of fields used to build dataframe.
         **kwargs: additional keyword arguments.
     Returns:
         DataFrame: properly created dataframe.
     """
     if not fields:
-        fields = list(
-            dict.fromkeys(key for item in raw for key in item.get("Fields", {}).keys())
-        )
+        fields = list(dict.fromkeys(key for item in raw for key in item.get("Fields", {}).keys()))
 
     data = []
     num_fields = len(fields)
     for idx, item in enumerate(raw):
         inst_name = universe[idx]
         if item["Type"] == "Status":
             value = status_code_to_value.get(item["State"]["Code"])
@@ -125,49 +123,34 @@
 
     df = pd.DataFrame(data=data, columns=["Instrument", *fields])
     convert_df_columns_to_datetime_re(df, PRICING_DATETIME_PATTERN)
     df = convert_dtypes(df)
     return df
 
 
-class PricingResponseFactory(ContentResponseFactory):
-    def create_data_success(self, parsed_data: "ParsedData", **kwargs):
-        data = super().create_data_success(parsed_data, **kwargs)
-        data.prices = create_price_cache(
-            self.get_raw(parsed_data), kwargs.get("fields")
-        )
-        return data
+@dataclass
+class PricingData(Data):
+    @cached_property
+    def prices(self):
+        return create_price_cache(self.raw, self._kwargs)
 
 
 # ---------------------------------------------------------------------------
 #   Request factory
 # ---------------------------------------------------------------------------
+pricing_query_params = [
+    ValueParamItem("universe", function=partial(universe_arg_parser.get_str, delim=",")),
+    ValueParamItem("fields", function=partial(fields_arg_parser.get_str, delim=",")),
+]
 
 
 class PricingRequestFactory(RequestFactory):
-    def get_query_parameters(self, *args, **kwargs) -> list:
-        query_parameters = []
-
-        #
-        # universe
-        #
-        universe = kwargs.get("universe")
-        if universe:
-            universe = universe_arg_parser.get_str(universe, delim=",")
-            query_parameters.append(("universe", universe))
-
-        #
-        # fields
-        #
-        fields = kwargs.get("fields")
-        if fields:
-            fields = fields_arg_parser.get_str(fields, delim=",")
-            query_parameters.append(("fields", fields))
-
-        return query_parameters
+    @property
+    def query_params_config(self):
+        return pricing_query_params
 
 
 # ---------------------------------------------------------------------------
 #   Content data validator
 # ---------------------------------------------------------------------------
 
 
@@ -179,11 +162,11 @@
 
 # ---------------------------------------------------------------------------
 #   Data provider
 # ---------------------------------------------------------------------------
 
 pricing_data_provider = ContentDataProvider(
     request=PricingRequestFactory(),
-    response=PricingResponseFactory(),
+    response=ContentResponseFactory(data_class=PricingData),
     parser=ErrorParser(),
     validator=ValidatorContainer(content_validator=PricingContentValidator()),
 )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/_stream_facade.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/_stream_facade.py`

 * *Files 2% similar despite different names*

```diff
@@ -75,21 +75,16 @@
         self._fields = fields
         self._service = service
         self._api = api
         self._extended_params = extended_params
 
     @cached_property
     def _stream(self) -> _UniverseStreams:
-        if self._api:
-            content_type = ContentType.STREAMING_CUSTOM
-        else:
-            content_type = ContentType.STREAMING_PRICING
-
         return _UniverseStreams(
-            content_type=content_type,
+            content_type=ContentType.STREAMING_PRICING,
             item_facade_class=PricingStream,
             universe=self._universe,
             session=self._session,
             fields=self._fields,
             service=self._service,
             api=self._api,
             extended_params=self._extended_params,
@@ -167,19 +162,15 @@
         >>> from refinitiv.data.content import pricing
         >>> definition = pricing.Definition(["AIRP.PA", "AIR.PA", "ALSO.PA"])
         >>> stream = definition.get_stream()
         >>> fields
         ... {'AIRP.PA': {'BID': 147.14, 'ASK': 147.16}}
         """
         _fields = {
-            universe: {
-                key: value
-                for key, value in self._stream[universe].items()
-                if fields is None or key in fields
-            }
+            universe: {key: value for key, value in self._stream[universe].items() if fields is None or key in fields}
         }
         return _fields
 
     def get_snapshot(
         self,
         universe: Optional[Union[str, Strings]] = None,
         fields: Optional[Union[str, Strings]] = None,
@@ -227,17 +218,15 @@
         >>> stream = definition.get_stream()
         >>> data = stream.get_snapshot(["MSFT.O", "GOOG.O"], ["BID", "ASK"])
         >>> data
         ... "      Instrument   BID         ASK      "
         ... "0     MSFT.O        150.9000   150.9500 "
         ... "1     GOOG.O        1323.9000  1327.7900"
         """
-        df = self._stream.get_snapshot(
-            universe=universe, fields=fields, convert=convert
-        )
+        df = self._stream.get_snapshot(universe=universe, fields=fields, convert=convert)
         convert_df_columns_to_datetime_re(df, PRICING_DATETIME_PATTERN)
         return df
 
     def on_refresh(self, func: Callable[[Any, str, "Stream"], Any]) -> "Stream":
         """
         Called when a stream on instrument_name was opened successfully or when the
         stream is refreshed by the server
@@ -372,14 +361,15 @@
         -------
         Stream
             current instance
 
         Examples
         --------
         Prerequisite: The default session must be opened.
+
         >>> from refinitiv.data.content import pricing
         >>>
         >>> definition = pricing.Definition("EUR=")
         >>> stream = definition.get_stream()
         >>> def on_ack(ack_msg, ric, stream):
         ...     print(f"\tReceive ack [{ric}] : {ack_msg}")
         >>> stream.on_ack(on_ack)
@@ -473,17 +463,15 @@
         >>>
         >>> pricing_stream = rd.content.pricing.Definition(
         ...    ["MSFT.O", "GOOG.O", "IBM.N"],
         ...    fields=["BID", "ASK", "OPEN_PRC"]
         ...).get_stream()
         >>> response = await pricing_stream.contribute_async("MSFT.O", {"BID": 240.83})
         """
-        return await self._stream.contribute_async(
-            name, fields, contrib_type, post_user_info
-        )
+        return await self._stream.contribute_async(name, fields, contrib_type, post_user_info)
 
     def add_instruments(self, instruments) -> None:
         """
         Add instruments to the stream universe.
 
         Parameters
         ----------
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_chain_record.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_chain_record.py`

 * *Files 2% similar despite different names*

```diff
@@ -84,29 +84,26 @@
             f"prev_display={self.prev_display}, "
             f"pref_link={self.pref_link}, "
             f"display_name={self.display_name})"
         )
 
     @property
     def constituents(self) -> List[str]:
-        return [
-            constituent for _, constituent in sorted(self.link_field_by_index.items())
-        ]
+        return [constituent for _, constituent in sorted(self.link_field_by_index.items())]
 
     @property
     def display_template(self) -> int:
         if is_valid_disp_tmpl(self.pref_display):
             return self.pref_display
         elif is_valid_disp_tmpl(self.prev_display):
             return self.prev_display
         else:
             return self.rdn_display
 
     def update(self, fields) -> Dict[int, Tuple[str, str]]:
-
         get = fields.get
 
         old_num_constituents = None
         new_num_constituents = None
 
         if REF_COUNT_FIELD_NAME in fields:
             old_num_constituents = self.num_constituents
@@ -139,20 +136,16 @@
             if link_field_name in fields:
                 old_constituent = self.link_field_by_index.get(i, None)
                 self.link_field_by_index[i] = get(link_field_name)
                 new_constituent = self.link_field_by_index[i]
                 i = i - cfg.start_no_link_field_name
                 index_to_old_and_new_constituent[i] = (old_constituent, new_constituent)
 
-        self.prev_chain_record_name = get(
-            cfg.prev_field_name, self.prev_chain_record_name
-        )
-        self.next_chain_record_name = get(
-            cfg.next_field_name, self.next_chain_record_name
-        )
+        self.prev_chain_record_name = get(cfg.prev_field_name, self.prev_chain_record_name)
+        self.next_chain_record_name = get(cfg.next_field_name, self.next_chain_record_name)
 
         return index_to_old_and_new_constituent
 
 
 def _can_create(fields: dict, config: NCharsConfig) -> bool:
     if REF_COUNT_FIELD_NAME not in fields:
         return False
@@ -200,17 +193,15 @@
             start = config.start_no_link_field_name
             stop = chain_record.num_constituents + 1
             for i in range(start, stop):
                 link_field_name = config.link_field_name_template.format(i)
                 chain_record.link_field_by_index[i] = fields[link_field_name]
 
     except KeyError:
-        raise InvalidChainRecordException(
-            f"ERROR!!! Invalid chain record template of {config}"
-        )
+        raise InvalidChainRecordException(f"ERROR!!! Invalid chain record template of {config}")
 
     chain_record.prev_chain_record_name = get(config.prev_field_name)
     chain_record.next_chain_record_name = get(config.next_field_name)
     return chain_record
 
 
 def create_chain_record(fields: dict) -> ChainRecord:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_chain_records.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_chain_records.py`

 * *Files 2% similar despite different names*

```diff
@@ -84,17 +84,15 @@
 
     def _do_close(self):
         for refreshing in self.refreshing_by_name.values():
             if not refreshing.done():
                 refreshing.set_result(1)
 
         for stream in self.streams_by_name.values():
-            self._debug(
-                f"{self._classname} closes a stream with name {stream.name} [c]"
-            )
+            self._debug(f"{self._classname} closes a stream with name {stream.name} [c]")
             stream.close()
 
     def open_stream(self, name: str, with_updates: bool):
         stream = self.streams_by_name[name]
         self._debug(f"{self._classname} opens a stream with name {stream.name} [c]")
         stream.open(with_updates=with_updates)
 
@@ -120,18 +118,15 @@
 
             refreshing = self.refreshing_by_name[stream.name]
 
             if not refreshing.done():
                 refreshing.set_result(True)
 
         else:
-            self._error(
-                f"StreamingChain :: "
-                f"Cannot parse chain {stream.name} because it is an invalid chain."
-            )
+            self._error(f"StreamingChain :: Cannot parse chain {stream.name} because it is an invalid chain.")
 
         return message
 
     def _do_on_stream_status(self, originator, message, *args) -> Any:
         state = message.get("State", {})
         stream_state = state.get("Stream")
         if stream_state == "Closed":
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_chains_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_chains_data_provider.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_definition.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,17 +39,15 @@
         service: Optional[str] = None,
         # option for chain constituents
         skip_summary_links: bool = True,
         skip_empty: bool = True,
         override_summary_links: Optional[int] = None,
         extended_params: "ExtendedParams" = None,
     ):
-        validate_types(
-            override_summary_links, [int, type(None)], "override_summary_links"
-        )
+        validate_types(override_summary_links, [int, type(None)], "override_summary_links")
 
         self._name = name
         self._service = service
         self._skip_summary_links = skip_summary_links
         self._skip_empty = skip_empty
         self._override_summary_links = override_summary_links
         self._extended_params = extended_params
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_display_template.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_display_template.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/content/pricing/chain/_stream.py`

 * *Files 2% similar despite different names*

```diff
@@ -30,17 +30,15 @@
         skip_empty: "OptBool" = True,
         override_summary_links: "OptInt" = None,
         extended_params: "ExtendedParams" = None,
     ):
         self._session = session
 
         StreamStateManager.__init__(self, logger=self._session.logger())
-        OMMStreamListener.__init__(
-            self, logger=self._session.logger(), add_originator=False
-        )
+        OMMStreamListener.__init__(self, logger=self._session.logger(), add_originator=False)
 
         self.name: str = name
         self._service = service
         self._skip_summary_links = skip_summary_links
         self._skip_empty = skip_empty
         self._override_summary_links = override_summary_links
         self._chain_records: ChainRecords = ChainRecords(
@@ -72,19 +70,15 @@
 
     @property
     def summary_links(self) -> List[str]:
         if not self.is_chain:
             return []
 
         summary_links = self._constituents[: self.num_summary_links]
-        return [
-            summary_link
-            for summary_link in summary_links
-            if not self._skip_empty or summary_link is not None
-        ]
+        return [summary_link for summary_link in summary_links if not self._skip_empty or summary_link is not None]
 
     @property
     def display_name(self) -> str:
         if not self.is_chain:
             return ""
 
         return self._chain_records.get_display_name(self.name)
@@ -106,19 +100,15 @@
 
         num_summary_links = self.num_summary_links
         if self._skip_summary_links:
             constituents = self._constituents[num_summary_links:]
         else:
             constituents = self._constituents[:]
 
-        return [
-            constituent
-            for constituent in constituents
-            if not self._skip_empty or constituent is not None
-        ]
+        return [constituent for constituent in constituents if not self._skip_empty or constituent is not None]
 
     def _do_open(self, *args, with_updates=True):
         self._chain_records.open(with_updates=with_updates)
         self._constituents = []
         offset = 0
         name = self.name
         while name:
@@ -158,37 +148,30 @@
         self._constituents.append(constituent)
         self.dispatch_add(constituent, index)
 
     def _remove_constituent(self, index: int, constituent: str):
         self._constituents.pop(index)
         self.dispatch_remove(constituent, index)
 
-    def _update_constituent(
-        self, index: int, old_constituent: str, new_constituent: str
-    ):
+    def _update_constituent(self, index: int, old_constituent: str, new_constituent: str):
         self._constituents[index] = new_constituent
         self.dispatch_update(self, new_constituent, old_constituent, index)
 
     def _do_on_stream_status(self, originator: ChainRecords, message: dict, *_) -> Any:
         state = message.get("State", {})
         stream_state = state.get("Stream")
         if stream_state == "Closed":
             self._chain_records.close()
 
         return message
 
-    def _on_stream_update(
-        self, originator: ChainRecords, stream: "_OMMStream", message: dict, *args
-    ) -> Any:
+    def _on_stream_update(self, originator: ChainRecords, stream: "_OMMStream", message: dict, *args) -> Any:
         if not self._complete_future.done():
             self._update_messages.append((stream.name, message))
-            self._warning(
-                "StreamingChain :: "
-                "waiting to update because chain decode does not completed."
-            )
+            self._warning("StreamingChain :: waiting to update because chain decode does not completed.")
             return
 
         self._process_remaining_update_messages()
         self._update_chain_record(stream.name, message)
 
     def _process_remaining_update_messages(self):
         self._debug(f"{self._classname} starts process remaining update messages")
@@ -202,18 +185,15 @@
 
     def _update_chain_record(self, name: str, message: dict):
         self._debug(f"{self._classname} updates chain record, name={name}")
         fields = message.get("Fields", [])
         chain_record = self._chain_records.get_record(name)
 
         if not chain_record:
-            self._warning(
-                f"StreamingChain :: "
-                f"Skipping to update an invalid chain record = {name}."
-            )
+            self._warning(f"StreamingChain :: Skipping to update an invalid chain record = {name}.")
             return
 
         index_to_old_and_new_constituent = chain_record.update(fields)
 
         offset = self._chain_record_name_to_offset[name]
         for i, (old_c, new_c) in index_to_old_and_new_constituent.items():
             i = offset + i
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/pricing/chain/_stream_facade.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,160 +1,150 @@
-import copy
-from typing import Any, Callable, List, TYPE_CHECKING
-
-from ._stream import StreamingChain
-from ...._core.session import get_valid_session
-from ...._tools import cached_property, create_repr
-from ....delivery._stream.base_stream import StreamOpenWithUpdatesMixin
+# coding: utf8
+from threading import Event
+from typing import Optional, Any
+from typing import TYPE_CHECKING
+
+from ._protocol_type import ProtocolType
+from ._stream_cxn_cache import stream_cxn_cache
+from .event import StreamEvent, StreamCxnEvent
+from .stream_state import StreamState
+from .stream_state_manager import StreamStateManager
+from ..._core.session.tools import is_closed
+from ..._errors import ScopeError
+from ..._tools import lazy_dump
 
 if TYPE_CHECKING:
-    from ...._types import ExtendedParams, OptBool, OptInt, OptStr
-    from ...._core.session import Session
-
+    from ..._content_type import ContentType
+    from .._stream import StreamConnection
+    from ..._core.session import Session
+    from ._stream_factory import StreamDetails
 
-class Stream(StreamOpenWithUpdatesMixin):
-    """
-    Stream is designed to request streaming chains and decode it dynamically.
-    This class also act like a cache for each part of the chain record.
-
-    Parameters
-    ----------
-    name : str
-        Single instrument name
-    session : Session, optional
-        The Session defines the source where you want to retrieve your data
-    service : str, optional
-        Name service
-    skip_summary_links : bool, optional
-        Store skip summary links
-    skip_empty : bool, optional
-        Store skip empty
-    override_summary_links : int, optional
-        Store the override number of summary links
-    extended_params : dict, optional
-        If necessary other parameters
-
-    Methods
-    -------
-    open(**kwargs)
-        Open the Stream connection
-
-    close()
-        Closes the Stream connection, releases resources
-
-    is_chain
-        True - stream was decoded as a chain
-        False - stream wasn't identified as a chain
-
-    Attributes
-    __________
-    constituents: list
-        A list of constituents in the chain record or empty list
 
-    """
+class Stream(StreamStateManager):
+    _cxn: Optional["StreamConnection"] = None
+    _event: Optional[StreamEvent] = None
 
     def __init__(
         self,
-        name: str,
-        session: "Session" = None,
-        service: "OptStr" = None,
-        skip_summary_links: "OptBool" = True,
-        skip_empty: "OptBool" = True,
-        override_summary_links: "OptInt" = None,
-        extended_params: "ExtendedParams" = None,
+        stream_id: int,
+        session: "Session",
+        details: "StreamDetails",
     ) -> None:
-        self._session = get_valid_session(session)
-        self._always_use_default_session = session is None
-        self._name = name
-        self._service = service
-        self._skip_summary_links = skip_summary_links
-        self._skip_empty = skip_empty
-        self._override_summary_links = override_summary_links
-        self._extended_params = extended_params
-
-    @cached_property
-    def _stream(self) -> StreamingChain:
-        streaming_chain = StreamingChain(
-            name=self._name,
-            session=self._session,
-            service=self._service,
-            skip_summary_links=self._skip_summary_links,
-            skip_empty=self._skip_empty,
-            override_summary_links=self._override_summary_links,
-            extended_params=self._extended_params,
-        )
-        return streaming_chain
+        StreamStateManager.__init__(self, logger=session.logger())
+        self.details = details
+        self._id: int = stream_id
+        self._session: "Session" = session
+        self._opened: Optional[Event] = Event()
+        self._classname = f"[{self.__class__.__name__}_{self.id}]"
 
     @property
-    def name(self) -> str:
-        return self._stream.name
+    def classname(self):
+        return self._classname
 
     @property
-    def is_chain(self) -> bool:
-        return self._stream.is_chain
+    def id(self) -> int:
+        return self._id
 
     @property
-    def num_summary_links(self) -> int:
-        return self._stream.num_summary_links
+    def session(self) -> "Session":
+        return self._session
 
-    @property
-    def summary_links(self) -> List[str]:
-        return self._stream.summary_links
+    @session.setter
+    def session(self, session: "Session"):
+        if self._session != session and not self.is_open:
+            self._session = session
+            StreamStateManager.__init__(self, logger=self._session.logger())
 
     @property
-    def display_name(self) -> str:
-        return self._stream.display_name
+    def name(self) -> str:
+        return ""
 
     @property
-    def constituents(self) -> List[str]:
-        return copy.deepcopy(self._stream.get_constituents())
-
-    def on_add(self, func: Callable[[int, str, "Stream"], Any]) -> "Stream":
-        func = make_callback(self, func)
-        self._stream.on_add(func)
-        return self
-
-    def on_remove(self, func: Callable[[str, int, "Stream"], Any]) -> "Stream":
-        func = make_callback(self, func)
-        self._stream.on_remove(func)
-        return self
-
-    def on_update(self, func: Callable[[str, str, int, "Stream"], Any]) -> "Stream":
-        func = make_callback(self, func)
-        self._stream.on_update(func)
-        return self
-
-    def on_complete(self, func: Callable[[list, "Stream"], Any]) -> "Stream":
-        func = make_callback(self, func)
-        self._stream.on_complete(func)
-        return self
+    def content_type(self) -> "ContentType":
+        return self.details.content_type
 
-    def on_error(self, func: Callable[[tuple, str, "Stream"], Any]) -> "Stream":
-        func = make_error_callback(self, func)
-        self._stream.on_error(func)
-        return self
-
-    def __repr__(self):
-        return create_repr(
-            self,
-            content=f"{{name='{self._name}'}}",
-            class_name=self.__class__.__name__,
-        )
-
-
-def make_callback(stream: Stream, func: Callable) -> Callable:
-    """Return a callback functions with correct arguments order."""
-
-    def callback(*args):
-        func(*args, stream)
-
-    return callback
-
-
-def make_error_callback(stream: Stream, func: Callable) -> Callable:
-    """Return a callback function with correct arguments order for error handling."""
-
-    def callback(*args):
-        args = reversed(args)
-        func(*args, stream)
+    @property
+    def protocol_type(self) -> ProtocolType:
+        return ProtocolType.NONE
+
+    @property
+    def close_message(self) -> dict:
+        return {}
+
+    @property
+    def open_message(self) -> dict:
+        return {}
+
+    def open(self, *args, with_updates: bool = True) -> StreamState:
+        if is_closed(self._session):
+            raise AssertionError("Session must be open")
+
+        return super().open(with_updates=with_updates)
+
+    def send(self, message: dict) -> bool:
+        if self._cxn:
+            self._debug(f"{self._classname} send %s", lazy_dump(message))
+            return self._cxn.send_message(message)
+        else:
+            self._debug(
+                f"{self._classname} cannot send %s, cxn is {self._cxn}, state is {self.state}",
+                lazy_dump(message),
+            )
+            return False
+
+    def _initialize_cxn(self):
+        self._event = StreamEvent.get(self.id)
+        try:
+            self._cxn = stream_cxn_cache.get_cxn(self.session, self.details)
+        except ScopeError as e:
+            self.halt()
+            raise e
+        self._cxn.on(StreamCxnEvent.DISCONNECTING, self.close)
+        self._cxn.on(StreamCxnEvent.RECONNECTED, self._on_reconnected)
+        self._cxn.on(StreamCxnEvent.DISPOSED, self.halt)
+
+    def _release_cxn(self):
+        self._cxn.remove_listener(StreamCxnEvent.DISCONNECTING, self.close)
+        self._cxn.remove_listener(StreamCxnEvent.RECONNECTED, self._on_reconnected)
+        self._cxn.remove_listener(StreamCxnEvent.DISPOSED, self.halt)
+        self._debug(f"{self._classname} release cxn={self._cxn}")
+        if stream_cxn_cache.has_cxn(self.session, self.details):
+            stream_cxn_cache.release(self.session, self.details)
+        self._cxn = None
+
+    def _do_open(self, *args, **kwargs) -> None:
+        self._opened.clear()
+        self.send(self.open_message)
+        if not self._cxn.is_disposed:
+            self._opened.wait()
+
+    def _do_close(self, *args, **kwargs):
+        self.send(self.close_message)
+        self._dispose()
+
+    def _on_reconnected(self, *args, **kwargs):
+        if self.is_open:
+            self.send(self.open_message)
+
+    def _do_on_stream_error(self, originator, *args) -> Any:
+        if self.is_opening:
+            self._opened.set()
+
+        return args
+
+
+def update_message_with_extended_params(message: dict, extended_params: dict) -> dict:
+    return update_key_in_dict(message, extended_params)
+
+
+def update_key_in_dict(message: dict, extended_params: dict) -> dict:
+    for param, extended_val in extended_params.items():
+        if param in message:
+            prev_value = message[param]
+            if isinstance(prev_value, dict) and isinstance(extended_val, dict):
+                update_key_in_dict(prev_value, extended_val)
+            else:
+                message[param] = extended_val
+        else:
+            message[param] = extended_val
 
-    return callback
+    return message
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/search/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/content/search/_data_provider.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 import warnings
 from dataclasses import dataclass
-from typing import Dict, Iterable, TYPE_CHECKING, List
+from typing import Dict, Iterable, List, TYPE_CHECKING
 
 import pandas as pd
 
 from .._content_data import Data
 from .._content_data_provider import ContentDataProvider
 from .._content_response_factory import ContentResponseFactory
 from .._error_parser import ErrorParser
@@ -121,17 +121,15 @@
     attribute_name_type = ancestor_property_name_type[:] + (property_name,)
 
     #   check the attribute type is nested or not?
     property_attribute_type = property_attribute_dict[_ResponseTypeName]
     if property_attribute_type == "Nested":
         #   this property is a nested type, recursive convert this property attribute
         #   extract properties of this nested type
-        this_properties_of_nested_type = property_attribute_dict[
-            _ResponsePropertiesName
-        ]
+        this_properties_of_nested_type = property_attribute_dict[_ResponsePropertiesName]
 
         #   loop over all nested attributes and convert it
         for (
             nested_property_name,
             nested_property_attribute_dict,
         ) in this_properties_of_nested_type.items():
             #   call convert recusivly for nested type
@@ -146,20 +144,16 @@
         this_property_depth += 1
 
     #   convert the attribute flags of this property attributes
     #       loop over all possible attributes and convert it
     for property_attribute_name in _ResponseFlagNames:
         #   add properties of properties to dict of dict
         #       fill with None if the properties doesn't exist
-        property_dict = property_attribute_to_property_dict_dict.setdefault(
-            property_attribute_name, {}
-        )
-        property_dict[attribute_name_type] = property_attribute_dict.get(
-            property_attribute_name, False
-        )
+        property_dict = property_attribute_to_property_dict_dict.setdefault(property_attribute_name, {})
+        property_dict[attribute_name_type] = property_attribute_dict.get(property_attribute_name, False)
 
     return this_property_depth
 
 
 def discovery_metadata_build_df(raw: dict, **kwargs) -> pd.DataFrame:
     """parse the metadata response from dict of dict to be a dict of dict tuple"""
     if _ResponsePropertiesName not in raw:
@@ -198,17 +192,15 @@
         property_dict,
     ) in property_attribute_to_property_dict_dict.items():
         for property_key, property_value in property_dict.items():
             #   construct the property attribute for this property
             dataframe_property_dict = data.setdefault(property_attribute_name, {})
 
             #   construct the key of dataframe property
-            dataframe_property_key = _extend_tuple_with_last_element(
-                property_key, max_property_depth
-            )
+            dataframe_property_key = _extend_tuple_with_last_element(property_key, max_property_depth)
             dataframe_property_dict[dataframe_property_key] = property_value
 
     if len(data) == 0:
         return pd.DataFrame()
 
     df = pd.DataFrame(data)
     df = convert_dtypes(df)
@@ -225,17 +217,15 @@
         return self._data["Label"]
 
     @property
     def navigator(self):
         if not self._subnavigator:
             for name, value in self._data.items():
                 if isinstance(self._data[name], dict):
-                    buckets_objects = [
-                        Bucket(bucket) for bucket in self._data[name]["Buckets"]
-                    ]
+                    buckets_objects = [Bucket(bucket) for bucket in self._data[name]["Buckets"]]
                     self._subnavigator = Navigator(buckets_objects, name)
         return self._subnavigator
 
     @property
     def count(self):
         return self._data["Count"]
 
@@ -257,18 +247,19 @@
 
 @dataclass
 class Navigator:
     buckets: List
     name: str
 
 
+@dataclass
 class SearchData(Data):
-    _navigators = None
-    _hits = None
-    _total = None
+    _navigators: Dict[str, Navigator] = None
+    _hits: List[HitsItem] = None
+    _total: int = None
 
     @property
     def total(self):
         if not self._total:
             self._total = self.raw.get("Total")
         return self._total
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/search/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/search/_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/search/_lookup_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/search/_lookup_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -13,15 +13,15 @@
 class Definition(DataProviderLayer[BaseResponse[Data]]):
     """
     This class describe parameters to retrieve data for search lookup.
 
     Parameters
     ----------
 
-    view : Views
+    view : str or Views
         picks a subset of the data universe to search against. see Views
 
     terms : str
         lists the symbols to be solved
 
     scope : str
         identifies the symbology which 'terms' belong to
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/search/_metadata_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/search/_metadata_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -12,15 +12,15 @@
 class Definition(DataProviderLayer[BaseResponse[Data]]):
     """
     This class describe parameters to retrieve data for search metadata.
 
     Parameters
     ----------
 
-    view : Views
+    view : str or Views
         picks a subset of the data universe to search against. see Views
 
     Examples
     --------
     >>> from refinitiv.data.content import search
     >>> definition = search.metadata.Definition(view=search.Views.PEOPLE)
     """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/search/_views.py` & `refinitiv-data-1.2.0/refinitiv/data/content/search/_views.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,12 +1,14 @@
-import enum
+from enum import unique
 
+from ..._base_enum import StrEnum
 
-@enum.unique
-class Views(str, enum.Enum):
+
+@unique
+class Views(StrEnum):
     """Possible views values to request data from 'search' endpoint"""
 
     BOND_FUT_OPT_QUOTES = "BondFutOptQuotes"
     CDS_INSTRUMENTS = "CdsInstruments"
     CDS_QUOTES = "CdsQuotes"
     CMO_INSTRUMENTS = "CmoInstruments"
     CMO_QUOTES = "CmoQuotes"
@@ -45,13 +47,10 @@
     VESSEL_PHYSICAL_ASSETS = "VesselPhysicalAssets"
     YIELD_CURVE_CONT_QUOTES = "YieldCurveContQuotes"
     RCS = "RCS"
     INVESTORS = "Investors"
     CATALOG_ITEMS = "CatalogItems"
     ENTITIES = "Entities"
 
-    def __str__(self):
-        return str(self.value)
-
     @classmethod
     def possible_values(cls):
-        return [s.value for s in cls.__members__.values()]
+        return [s for s in cls.__members__.values()]
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_asset_class.py` & `refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_asset_class.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-from enum import Enum, unique
+from enum import unique
 from typing import Union, List, Optional, Iterable
 
 from ..._tools import make_convert_to_enum, EnumArgsParser, make_parse_enum
+from ..._base_enum import StrEnum
 
 
 @unique
-class AssetClass(Enum):
+class AssetClass(StrEnum):
     """
     Asset class values to build 'filter' parameter in request for SymbolConversion content object.
     """
 
     COMMODITIES = "Commodities"
     EQUITY_OR_INDEX_OPTIONS = "EquityOrIndexOptions"
     BOND_AND_STIR_FUTURES_AND_OPTIONS = "BondAndSTIRFuturesAndOptions"
@@ -52,34 +53,28 @@
 
 
 def _transform_to_string(values: Iterable, category: dict) -> str:
     return " ".join(f"'{category[value]}'" for value in values)
 
 
 def create_asset_class_request_strings(asset_class: list) -> tuple:
-    search_all_category_values = filter(
-        lambda x: x in search_all_category_by_asset_class, asset_class
-    )
-    rcs_asset_category_values = filter(
-        lambda x: x in rcsasset_category_genealogy_by_asset_class, asset_class
-    )
+    search_all_category_values = filter(lambda x: x in search_all_category_by_asset_class, asset_class)
+    rcs_asset_category_values = filter(lambda x: x in rcsasset_category_genealogy_by_asset_class, asset_class)
 
     search_all_category_string_values = _transform_to_string(
         search_all_category_values, search_all_category_by_asset_class
     )
 
     search_all_rcs_asset_category_string_values = _transform_to_string(
         rcs_asset_category_values, rcsasset_category_genealogy_by_asset_class
     )
 
     search_all_category_string = ""
     rcs_asset_category_string = ""
 
     if search_all_category_string_values:
-        search_all_category_string = (
-            f"SearchAllCategoryv3 in ({search_all_category_string_values})"
-        )
+        search_all_category_string = f"SearchAllCategoryv3 in ({search_all_category_string_values})"
 
     if search_all_rcs_asset_category_string_values:
         rcs_asset_category_string = f"RCSAssetCategoryGenealogy in ({search_all_rcs_asset_category_string_values})"
 
     return search_all_category_string, rcs_asset_category_string
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_asset_state.py` & `refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_asset_state.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-from enum import Enum, unique
+from enum import unique
 from typing import Optional, Union
 
 from ..._tools import make_convert_to_enum, EnumArgsParser, make_parse_enum
+from ..._base_enum import StrEnum
 
 
 @unique
-class AssetState(Enum):
+class AssetState(StrEnum):
     """
     Asset state values for 'filter' parameter in request for SymbolConversion content object.
     """
 
     ACTIVE = "Active"
     INACTIVE = "Inactive"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_country_code.py` & `refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_country_code.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-from enum import Enum, unique
+from enum import unique
 from typing import Optional, Union
 
 from ..._tools import make_enum_arg_parser_by_members
+from ..._base_enum import StrEnum
 
 
 @unique
-class CountryCode(Enum):
+class CountryCode(StrEnum):
     """
     List of ISO 3166 country codes.
 
     'G:7R' is unique country code for Afghanistan
     """
 
     AFG = "G:7R"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/symbol_conversion/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/symbol_conversion/_definition.py`

 * *Files 2% similar despite different names*

```diff
@@ -22,31 +22,27 @@
     from ._country_code import CountryCode
     from ._symbol_type import SymbolTypes
     from ..._types import ExtendedParams, StrStrings
 
 DEFAULT_SCOPE = "_AllUnique"
 
 
-def _prepare_filter(
-    asset_state: AssetState, asset_class: Union[list, AssetClass]
-) -> str:
+def _prepare_filter(asset_state: AssetState, asset_class: Union[list, AssetClass]) -> str:
     asset_state = asset_state or AssetState.ACTIVE
 
     if asset_state is AssetState.ACTIVE:
         ret_val = "AssetState eq 'AC'"
     else:
         ret_val = "(AssetState ne 'AC' and AssetState ne null)"
 
     if asset_class and not isinstance(asset_class, list):
         asset_class = [asset_class]
 
     if asset_class:
-        search_all_category, rcs_asset_category = create_asset_class_request_strings(
-            asset_class
-        )
+        search_all_category, rcs_asset_category = create_asset_class_request_strings(asset_class)
 
         if search_all_category and rcs_asset_category:
             ret_val = f"{ret_val} and ({search_all_category} or {rcs_asset_category})"
         else:
             ret_val = f"{ret_val} and ({search_all_category}{rcs_asset_category})"
 
     return ret_val
@@ -57,34 +53,34 @@
     This class describe parameters to retrieve data for symbol conversion.
 
     Parameters
     ----------
     symbols: str or list of str
         Single instrument or list of instruments to convert.
 
-    from_symbol_type: SymbolTypes, optional
+    from_symbol_type: str or SymbolTypes, optional
         Instrument code to convert from.
         Possible values: 'CUSIP', 'ISIN', 'SEDOL', 'RIC', 'ticker', 'lipperID', 'IMO'
         Default: '_AllUnique'
 
     to_symbol_types: SymbolTypes, str or list of str or SymbolTypes, optional
         Instrument code to convert to.
         Possible values: 'CUSIP', 'ISIN', 'SEDOL', 'RIC', 'ticker', 'lipperID', 'IMO', 'OAPermID'
         Default: all symbol types are requested
 
     extended_params: dict, optional
         Other parameters can be provided if necessary
 
-    preferred_country_code: CountryCode, optional
+    preferred_country_code: str or CountryCode, optional
         Unique ISO 3166 code for country
 
-    asset_class: AssetClass, optional
+    asset_class: str or AssetClass, optional
         AssetClass value to build filter parameter.
 
-    asset_state: AssetState, optional
+    asset_state: str or AssetState, optional
         AssetState value to build filter parameter.
 
     Examples
     --------
     >>> from refinitiv.data.content import symbol_conversion
     >>> definition = symbol_conversion.Definition(
     ...     symbols=["US5949181045", "US02079K1079"],
@@ -103,15 +99,15 @@
     ... )
     >>> response = definition.get_data()
     """
 
     def __init__(
         self,
         symbols: "StrStrings",
-        from_symbol_type: "SymbolTypes" = DEFAULT_SCOPE,
+        from_symbol_type: Union[str, "SymbolTypes"] = DEFAULT_SCOPE,
         to_symbol_types: "OptSymbolTypes" = SYMBOL_TYPE_VALUES,
         preferred_country_code: "OptCountryCode" = None,
         asset_class: "OptAssetClass" = None,
         asset_state: "OptAssetState" = None,
         extended_params: "ExtendedParams" = None,
     ):
         super().__init__(
@@ -134,19 +130,19 @@
     @symbols.setter
     def symbols(self, value: "StrStrings"):
         if value:
             value = Copier.get_list(value)
             self._kwargs["terms"] = ",".join(value)
 
     @property
-    def from_symbol_type(self) -> "SymbolTypes":
+    def from_symbol_type(self) -> Union[str, "SymbolTypes"]:
         return self._kwargs.get("scope")
 
     @from_symbol_type.setter
-    def from_symbol_type(self, value: "SymbolTypes"):
+    def from_symbol_type(self, value: Union[str, "SymbolTypes"]):
         scope = value or DEFAULT_SCOPE
         if value and value != DEFAULT_SCOPE:
             scope = symbol_types_arg_parser.get_str(value)
 
         self._kwargs["scope"] = scope
 
     @property
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/_definition.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Union
 
 from ._stream import (
     Events,
     FinalizedOrders,
     UniverseTypes,
     universe_type_arg_parser,
     finalized_orders_arg_parser,
@@ -22,23 +22,23 @@
 
     Parameters
     ----------
     universe : list, optional
         A list of RIC or symbol or user's id for retrieving trading analytics data.
     fields : list, optional
         A list of enumerate fields.
-    events : Events, optional
+    events : str or Events, optional
         Enable/Disable the detail of order event in the streaming.
         Default: False
-    finalized_orders : FinalizedOrders, optional
+    finalized_orders : str or FinalizedOrders, optional
         Enable/Disable the cached of finalized order of current day in the streaming.
         Default: False
     filters : list, optional
         Set the condition of subset of trading streaming data.
-    universe_type : UniverseTypes, optional
+    universe_type : str or UniverseTypes, optional
         A type of given universe can be RIC, Symbol or UserID.
         Default: UniverseTypes.RIC
     api: str, optional
         Specifies the data source. It can be updated/added using config file
     extended_params : dict, optional
         If necessary other parameters
 
@@ -52,18 +52,18 @@
     >>> from refinitiv.data.content import trade_data_service
     >>> definition = trade_data_service.Definition()
     """
 
     def __init__(
         self,
         universe: "OptStrStrs" = None,
-        universe_type: UniverseTypes = UniverseTypes.UserID,
+        universe_type: Union[str, UniverseTypes] = UniverseTypes.UserID,
         fields: "OptStrStrs" = None,
-        events: Events = Events.No,
-        finalized_orders: FinalizedOrders = FinalizedOrders.No,
+        events: Union[str, Events] = Events.No,
+        finalized_orders: Union[str, FinalizedOrders] = FinalizedOrders.No,
         filters: "OptStrStrs" = None,
         api: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ):
         self._universe = try_copy_to_list(universe)
         self._universe_type = universe_type_arg_parser.get_str(universe_type)
         self._fields = try_copy_to_list(fields)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/_stream.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,41 +1,41 @@
-import enum
 import re
 import traceback
 from collections.abc import Callable
-from typing import TYPE_CHECKING, Optional
+from typing import TYPE_CHECKING, Union
 
 from ..._content_type import ContentType
 from ..._tools import cached_property, make_enum_arg_parser
 from ...delivery._stream import StreamStateEvent, StreamStateManager
 from ...delivery._stream._stream_factory import create_rdp_stream
+from ..._base_enum import StrEnum
 
 if TYPE_CHECKING:
     from ..._types import ExtendedParams, Strings, OptStr, OptStrStrs, OptCall
     from ..._core.session import Session
     from ...delivery._stream import _RDPStream
 
 QUEUE_SIZE_PATTERN = re.compile(r"^queueSize=(?P<queue_size>[0-9]+)")
 
 
-class Events(enum.Enum):
+class Events(StrEnum):
     """Events"""
 
     No = "None"
     Full = "Full"
 
 
-class FinalizedOrders(enum.Enum):
+class FinalizedOrders(StrEnum):
     """Finalized order in cached"""
 
     No = "None"
     P1D = "P1D"
 
 
-class UniverseTypes(enum.Enum):
+class UniverseTypes(StrEnum):
     """Universe Types"""
 
     RIC = "RIC"
     Symbol = "Symbol"
     UserID = "UserID"
 
 
@@ -71,18 +71,18 @@
 
     """
 
     def __init__(
         self,
         session: "Session",
         universe: "OptStrStrs" = None,
-        universe_type: Optional["UniverseTypes"] = None,
+        universe_type: Union[str, "UniverseTypes"] = None,
         fields: "OptStrStrs" = None,
-        events: Optional["Events"] = None,
-        finalized_orders: Optional["FinalizedOrders"] = None,
+        events: Union[str, "Events"] = None,
+        finalized_orders: Union[str, "FinalizedOrders"] = None,
         filters: "OptStrStrs" = None,
         api: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ):
         self._session = session
 
         StreamStateManager.__init__(self, logger=self._session.logger())
@@ -128,20 +128,16 @@
         if self._filters is not None:
             parameters["filters"] = self._filters
 
         view = None
         if self._fields:
             view = self._fields.copy()
 
-        content_type = ContentType.STREAMING_TRADING
-        if self._api:
-            content_type = ContentType.STREAMING_CUSTOM
-
         stream = create_rdp_stream(
-            content_type=content_type,
+            content_type=ContentType.STREAMING_TRADING,
             session=self._session,
             universe=self._universe,
             view=view,
             parameters=parameters,
             api=self._api,
             extended_params=self._extended_params,
         )
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/content/trade_data_service/_stream_facade.py` & `refinitiv-data-1.2.0/refinitiv/data/content/trade_data_service/_stream_facade.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Callable as _Callable, Optional, TYPE_CHECKING
+from typing import Any, Callable as _Callable, Optional, TYPE_CHECKING, Union
 
 from ._stream import TradeDataStream
 from ..._core.session import get_valid_session
 from ..._tools import cached_property, create_repr, make_callback
 from ...delivery._stream.base_stream import StreamOpenMixin
 
 if TYPE_CHECKING:
@@ -12,18 +12,18 @@
 
 
 class Stream(StreamOpenMixin):
     def __init__(
         self,
         session: Optional["Session"] = None,
         universe: "OptStrStrs" = None,
-        universe_type: Optional["UniverseTypes"] = None,
+        universe_type: Union[str, "UniverseTypes"] = None,
         fields: "OptStrStrs" = None,
-        events: Optional["Events"] = None,
-        finalized_orders: Optional["FinalizedOrders"] = None,
+        events: Union[str, "Events"] = None,
+        finalized_orders: Union[str, "FinalizedOrders"] = None,
         filters: "OptStrStrs" = None,
         api: "OptStr" = None,
         extended_params: "ExtendedParams" = None,
     ):
         self._session = get_valid_session(session)
         self._always_use_default_session = session is None
         self._universe = universe
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_api_type.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_api_type.py`

 * *Files 15% similar despite different names*

```diff
@@ -14,14 +14,13 @@
     FILINGS = auto()
     FINANCIAL_CONTRACTS = auto()
     HISTORICAL_PRICING = auto()
     NEWS = auto()
     OWNERSHIP = auto()
     PRICING = auto()
     STREAMING_BENCHMARK = auto()
-    STREAMING_CONTRIB = auto()
     STREAMING_CUSTOM = auto()
     STREAMING_CUSTOM_INSTRUMENTS = auto()
     STREAMING_FINANCIAL_CONTRACTS = auto()
     STREAMING_PRICING = auto()
     STREAMING_TRADING = auto()
     DATES_AND_CALENDARS = auto()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_connection.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_connection.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,22 +4,18 @@
 
 if TYPE_CHECKING:
     import httpx
     from ..._core.session import Session
 
 
 class HttpSessionConnection:
-    def send(
-        self, request: Request, session: "Session", *args, auto_retry=False, **kwargs
-    ) -> "httpx.Response":
-
+    def send(self, request: Request, session: "Session", *args, auto_retry=False, **kwargs) -> "httpx.Response":
         request.auto_retry = auto_retry
 
         return session.http_request(request)
 
     async def send_async(
         self, request: Request, session: "Session", *args, auto_retry=False, **kwargs
     ) -> "httpx.Response":
-
         request.auto_retry = auto_retry
 
         return await session.http_request_async(request)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_provider.py`

 * *Files 6% similar despite different names*

```diff
@@ -35,14 +35,15 @@
     ContentTypeValidator,
     BaseValidator,
 )
 from ..._core.session import raise_if_closed
 
 if TYPE_CHECKING:
     import httpx
+    from ..._core.session import Session
 
 
 class DataProvider:
     def __init__(
         self,
         connection: HttpSessionConnection = None,
         request: RequestFactory = None,
@@ -53,32 +54,42 @@
         super().__init__()
         self.connection = connection or HttpSessionConnection()
         self.request = request or RequestFactory()
         self.response = response or ResponseFactory()
         self.parser = parser or Parser()
         self.validator = validator or ValidatorContainer()
 
+    def _handle_scope_error(self, request: Request, session: "Session", parsed_data):
+        if 403 in parsed_data.error_codes:
+            session._handle_insufficient_scope(
+                request.path,
+                request.method,
+                parsed_data.status.get("error", {}).get("message"),
+            )
+
     def _process_response(
-        self, raw_response: "httpx.Response", session, *args, **kwargs
+        self,
+        raw_response: "httpx.Response",
+        base_request: Request,
+        session: "Session",
+        *args,
+        **kwargs,
     ) -> BaseResponse:
         is_success, parsed_data = self.parser.parse_raw_response(raw_response)
+        self._handle_scope_error(base_request, session, parsed_data)
         is_success = is_success and self.validator.validate(parsed_data)
-        return self.response.create_response(
-            is_success, parsed_data=parsed_data, session=session, **kwargs
-        )
+        return self.response.create_response(is_success, parsed_data=parsed_data, session=session, **kwargs)
 
-    def get_data(self, session, *args, **kwargs) -> BaseResponse:
+    def get_data(self, session: "Session", *args, **kwargs) -> BaseResponse:
         raise_if_closed(session)
         request = self.request.create(session, *args, **kwargs)
         raw_response = self.connection.send(request, session, *args, **kwargs)
-        return self._process_response(raw_response, session, *args, **kwargs)
+        return self._process_response(raw_response, request, session, *args, **kwargs)
 
-    async def get_data_async(self, session, *args, **kwargs) -> BaseResponse:
+    async def get_data_async(self, session: "Session", *args, **kwargs) -> BaseResponse:
         raise_if_closed(session)
         request = self.request.create(session, *args, **kwargs)
-        raw_response = await self.connection.send_async(
-            request, session, *args, **kwargs
-        )
-        return self._process_response(raw_response, session, *args, **kwargs)
+        raw_response = await self.connection.send_async(request, session, *args, **kwargs)
+        return self._process_response(raw_response, request, session, *args, **kwargs)
 
 
 default_data_provider = DataProvider()
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_provider_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_provider_factory.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,15 +35,15 @@
 from ...content.ipa._curves._curves_data_provider import (
     curves_data_provider,
     cross_currency_curves_definitions_data_provider,
     cross_currency_curves_definitions_delete_data_provider,
     cross_currency_curves_triangulate_definitions_data_provider,
     curve_data_provider,
 )
-from ...content.ipa._surfaces._surfaces_data_provider import surfaces_data_provider
+from ...content.ipa._surfaces._surfaces_data_provider import surfaces_data_provider, swaption_surfaces_data_provider
 from ...content.ipa.dates_and_calendars.add_periods._add_periods_data_provider import (
     add_period_data_provider,
 )
 from ...content.ipa.dates_and_calendars.holidays._holidays_data_provider import (
     holidays_data_provider,
 )
 from ...content.ipa.dates_and_calendars.count_periods._count_periods_data_provider import (
@@ -54,20 +54,28 @@
 )
 from ...content.ipa.dates_and_calendars.is_working_day._is_working_day_data_provider import (
     is_working_day_data_provider,
 )
 from ...content.ipa.financial_contracts._contracts_data_provider import (
     contracts_data_provider,
 )
-from ...content.news._news_data_provider import (
-    news_headlines_data_provider_rdp,
-    news_headlines_data_provider_udf,
+from ...content.news.story._data_provider import (
     news_story_data_provider_rdp,
     news_story_data_provider_udf,
 )
+from ...content.news.headlines._data_provider import (
+    news_headlines_data_provider_udf,
+    news_headlines_data_provider_rdp,
+)
+from ...content.news.online_reports._data_provider import (
+    news_online_reports_data_provider,
+)
+from ...content.news.online_reports.hierarchy._data_provider import (
+    news_online_reports_hierarchy_data_provider,
+)
 from ...content.news.top_news._data_provider import (
     news_top_news_data_provider,
 )
 from ...content.news.top_news.hierarchy._data_provider import (
     news_top_news_hierarchy_data_provider,
 )
 from ...content.news.images._data_provider import news_images_data_provider
@@ -134,16 +142,18 @@
     ContentType.HISTORICAL_PRICING_INTERDAY_SUMMARIES: hp_summaries_data_provider,
     ContentType.HISTORICAL_PRICING_INTRADAY_SUMMARIES: hp_summaries_data_provider,
     ContentType.NEWS_HEADLINES_RDP: news_headlines_data_provider_rdp,
     ContentType.NEWS_HEADLINES_UDF: news_headlines_data_provider_udf,
     ContentType.NEWS_STORY_RDP: news_story_data_provider_rdp,
     ContentType.NEWS_STORY_UDF: news_story_data_provider_udf,
     ContentType.NEWS_IMAGES: news_images_data_provider,
-    ContentType.NEWS_TOP_NEWS: news_top_news_hierarchy_data_provider,
-    ContentType.NEWS_TOP_NEWS_HEADLINES: news_top_news_data_provider,
+    ContentType.NEWS_TOP_NEWS_HIERARCHY: news_top_news_hierarchy_data_provider,
+    ContentType.NEWS_TOP_NEWS: news_top_news_data_provider,
+    ContentType.NEWS_ONLINE_REPORTS: news_online_reports_data_provider,
+    ContentType.NEWS_ONLINE_REPORTS_HIERARCHY: news_online_reports_hierarchy_data_provider,
     ContentType.OWNERSHIP_CONSOLIDATED_BREAKDOWN: ownership_data_provider,
     ContentType.OWNERSHIP_CONSOLIDATED_CONCENTRATION: ownership_data_provider,
     ContentType.OWNERSHIP_CONSOLIDATED_INVESTORS: ownership_data_provider,
     ContentType.OWNERSHIP_CONSOLIDATED_RECENT_ACTIVITY: ownership_data_provider,
     ContentType.OWNERSHIP_CONSOLIDATED_SHAREHOLDERS_HISTORY_REPORT: ownership_data_provider,
     ContentType.OWNERSHIP_CONSOLIDATED_SHAREHOLDERS_REPORT: ownership_data_provider,
     ContentType.OWNERSHIP_CONSOLIDATED_TOP_N_CONCENTRATION: ownership_data_provider,
@@ -157,14 +167,15 @@
     ContentType.OWNERSHIP_FUND_TOP_N_CONCENTRATION: ownership_data_provider,
     ContentType.OWNERSHIP_INSIDER_SHAREHOLDERS_REPORT: ownership_data_provider,
     ContentType.OWNERSHIP_INSIDER_TRANSACTION_REPORT: ownership_data_provider,
     ContentType.OWNERSHIP_INVESTOR_HOLDINGS: ownership_data_provider,
     ContentType.OWNERSHIP_ORG_INFO: ownership_data_provider,
     ContentType.PRICING: pricing_data_provider,
     ContentType.SURFACES: surfaces_data_provider,
+    ContentType.SURFACES_SWAPTION: swaption_surfaces_data_provider,
     ContentType.DATES_AND_CALENDARS_ADD_PERIODS: add_period_data_provider,
     ContentType.DATES_AND_CALENDARS_HOLIDAYS: holidays_data_provider,
     ContentType.DATES_AND_CALENDARS_COUNT_PERIODS: count_periods_data_provider,
     ContentType.DATES_AND_CALENDARS_DATE_SCHEDULE: date_schedule_data_provider,
     ContentType.DATES_AND_CALENDARS_IS_WORKING_DAY: is_working_day_data_provider,
     ContentType.ZC_CURVE_DEFINITIONS: curves_data_provider,
     ContentType.ZC_CURVES: curves_data_provider,
@@ -209,14 +220,15 @@
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_GET: APIType.CURVES_AND_SURFACES,
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_UPDATE: APIType.CURVES_AND_SURFACES,
     ContentType.CROSS_CURRENCY_CURVES_DEFINITIONS_SEARCH: APIType.CURVES_AND_SURFACES,
     ContentType.CROSS_CURRENCY_CURVES_TRIANGULATE_DEFINITIONS_SEARCH: APIType.CURVES_AND_SURFACES,
     ContentType.ZC_CURVES: APIType.CURVES_AND_SURFACES,
     ContentType.ZC_CURVE_DEFINITIONS: APIType.CURVES_AND_SURFACES,
     ContentType.SURFACES: APIType.CURVES_AND_SURFACES,
+    ContentType.SURFACES_SWAPTION: APIType.CURVES_AND_SURFACES,
     ContentType.CONTRACTS: APIType.FINANCIAL_CONTRACTS,
     ContentType.DATES_AND_CALENDARS_ADD_PERIODS: APIType.DATES_AND_CALENDARS,
     ContentType.DATES_AND_CALENDARS_HOLIDAYS: APIType.DATES_AND_CALENDARS,
     ContentType.DATES_AND_CALENDARS_COUNT_PERIODS: APIType.DATES_AND_CALENDARS,
     ContentType.DATES_AND_CALENDARS_DATE_SCHEDULE: APIType.DATES_AND_CALENDARS,
     ContentType.DATES_AND_CALENDARS_IS_WORKING_DAY: APIType.DATES_AND_CALENDARS,
     ContentType.HISTORICAL_PRICING_EVENTS: APIType.HISTORICAL_PRICING,
@@ -251,16 +263,18 @@
     ContentType.CHAINS: APIType.CHAINS,
     ContentType.DATA_GRID_RDP: APIType.DATA_GRID,
     ContentType.DATA_GRID_UDF: APIType.DATA_GRID,
     ContentType.NEWS_HEADLINES_RDP: APIType.NEWS,
     ContentType.NEWS_HEADLINES_UDF: APIType.NEWS,
     ContentType.NEWS_STORY_RDP: APIType.NEWS,
     ContentType.NEWS_STORY_UDF: APIType.NEWS,
+    ContentType.NEWS_TOP_NEWS_HIERARCHY: APIType.NEWS,
     ContentType.NEWS_TOP_NEWS: APIType.NEWS,
-    ContentType.NEWS_TOP_NEWS_HEADLINES: APIType.NEWS,
+    ContentType.NEWS_ONLINE_REPORTS: APIType.NEWS,
+    ContentType.NEWS_ONLINE_REPORTS_HIERARCHY: APIType.NEWS,
     ContentType.NEWS_IMAGES: APIType.NEWS,
     ContentType.DISCOVERY_SEARCH: APIType.DISCOVERY,
     ContentType.DISCOVERY_LOOKUP: APIType.DISCOVERY,
     ContentType.DISCOVERY_METADATA: APIType.DISCOVERY,
     ContentType.ESTIMATES_VIEW_ACTUALS_ANNUAL: APIType.ESTIMATES,
     ContentType.ESTIMATES_VIEW_ACTUALS_INTERIM: APIType.ESTIMATES,
     ContentType.ESTIMATES_VIEW_ACTUALS_KPI_ANNUAL: APIType.ESTIMATES,
@@ -304,14 +318,15 @@
     ContentType.DATES_AND_CALENDARS_ADD_PERIODS: "endpoints.add-periods",
     ContentType.DATES_AND_CALENDARS_HOLIDAYS: "endpoints.holidays",
     ContentType.DATES_AND_CALENDARS_COUNT_PERIODS: "endpoints.count-periods",
     ContentType.DATES_AND_CALENDARS_DATE_SCHEDULE: "endpoints.date-schedule",
     ContentType.DATES_AND_CALENDARS_IS_WORKING_DAY: "endpoints.is-working-day",
     ContentType.ZC_CURVE_DEFINITIONS: "endpoints.zc-curve-definitions",
     ContentType.SURFACES: "endpoints.surfaces",
+    ContentType.SURFACES_SWAPTION: "endpoints.surfaces",
     ContentType.CONTRACTS: "endpoints.financial-contracts",
     ContentType.HISTORICAL_PRICING_EVENTS: "endpoints.events",
     ContentType.HISTORICAL_PRICING_INTERDAY_SUMMARIES: "endpoints.interday-summaries",
     ContentType.HISTORICAL_PRICING_INTRADAY_SUMMARIES: "endpoints.intraday-summaries",
     ContentType.ESG_STANDARD_SCORES: "endpoints.scores-standard",
     ContentType.ESG_STANDARD_MEASURES: "endpoints.measures-standard",
     ContentType.ESG_FULL_MEASURES: "endpoints.measures-full",
@@ -341,17 +356,19 @@
     ContentType.CHAINS: "endpoints.chains",
     ContentType.DATA_GRID_RDP: "endpoints.standard",
     ContentType.DATA_GRID_UDF: "endpoints.standard",
     ContentType.NEWS_HEADLINES_RDP: "endpoints.headlines",
     ContentType.NEWS_HEADLINES_UDF: "endpoints.headlines",
     ContentType.NEWS_STORY_RDP: "endpoints.stories",
     ContentType.NEWS_STORY_UDF: "endpoints.stories",
+    ContentType.NEWS_TOP_NEWS_HIERARCHY: "endpoints.top-news",
     ContentType.NEWS_TOP_NEWS: "endpoints.top-news",
-    ContentType.NEWS_TOP_NEWS_HEADLINES: "endpoints.top-news",
     ContentType.NEWS_IMAGES: "endpoints.images",
+    ContentType.NEWS_ONLINE_REPORTS: "endpoints.online-reports",
+    ContentType.NEWS_ONLINE_REPORTS_HIERARCHY: "endpoints.online-reports",
     ContentType.DISCOVERY_SEARCH: "endpoints.search",
     ContentType.DISCOVERY_LOOKUP: "endpoints.lookup",
     ContentType.DISCOVERY_METADATA: "endpoints.metadata",
     ContentType.ESTIMATES_VIEW_ACTUALS_ANNUAL: "endpoints.view-actuals.annual",
     ContentType.ESTIMATES_VIEW_ACTUALS_INTERIM: "endpoints.view-actuals.interim",
     ContentType.ESTIMATES_VIEW_ACTUALS_KPI_ANNUAL: "endpoints.view-actuals-kpi.annual",
     ContentType.ESTIMATES_VIEW_ACTUALS_KPI_INTERIM: "endpoints.view-actuals-kpi.interim",
@@ -382,24 +399,20 @@
     return api_config_key
 
 
 def _get_url_config_key(data_type: DataType) -> str:
     return url_config_key_by_data_type.get(data_type)
 
 
-def get_api_config(
-    data_type: Union[DataType, ContentType], config: "_RDPConfig"
-) -> Union[dict, Any]:
+def get_api_config(data_type: Union[DataType, ContentType], config: "_RDPConfig") -> Union[dict, Any]:
     api_config_key = _get_api_config_key(data_type)
     api_config = config.get(api_config_key)
 
     if api_config is None:
-        raise AttributeError(
-            f"Cannot find api_key, data_type={data_type} by key={api_config_key}"
-        )
+        raise AttributeError(f"Cannot find api_key, data_type={data_type} by key={api_config_key}")
 
     return api_config
 
 
 def get_base_url(data_type: DataType, config: "_RDPConfig") -> str:
     """
     Parameters
@@ -446,17 +459,15 @@
     content_url = api_config.get(url_config_key_with_request_mode)
 
     if content_url is None:
         # then test if content_url is a single endpoint
         content_url = api_config.get(url_config_key)
 
     if content_url is None:
-        raise AttributeError(
-            f"Cannot find content_url, data_type={data_type} by key={url_config_key}"
-        )
+        raise AttributeError(f"Cannot find content_url, data_type={data_type} by key={url_config_key}")
 
     base_url = api_config.get("url")
     url = urljoin(base_url, content_url)
 
     return url
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_data_provider_layer.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_data_provider_layer.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,26 +1,24 @@
 import traceback
 from typing import Generic, Optional, Callable, TYPE_CHECKING, Any
 
-from ._response import BaseResponse, Response, T
+from ._response import BaseResponse, Response, TypeData
 from ..._configure import _RDPConfig
 from ..._core.session._default_session_manager import get_valid_session
 from ..._tools import DEBUG
 from ...errors import RDError
 from ...usage_collection._filter_types import FilterType
 from ...usage_collection._logger import get_usage_logger
 from ...usage_collection._utils import ModuleName
 
 if TYPE_CHECKING:
     from ..._core.session import Session
 
 
-def _check_response(
-    response: BaseResponse, config: "_RDPConfig", response_class=Response
-) -> None:
+def _check_response(response: BaseResponse, config: "_RDPConfig", response_class=Response) -> None:
     if isinstance(response, response_class):
         is_raise_exception = config.get_param("raise_exception_on_error")
         if not response.is_success and is_raise_exception:
             error_code = response.errors[0].code
             error_message = response.errors[0].message
             exception_class = getattr(response, "exception_class", None)
 
@@ -64,17 +62,15 @@
     # Library usage logging
     get_usage_logger().log_func(
         name=f"{ModuleName.DELIVERY}.{DataProviderLayer._USAGE_CLS_NAME}.get_data_async",
         func_path=f"{DataProviderLayer.__module__}.{DataProviderLayer.__qualname__}.get_data_async",
         kwargs={"url": url, "auto_retry": auto_retry, **kwargs},
         desc={FilterType.ASYNC, FilterType.LAYER_DELIVERY},
     )
-    response = await provider.get_data_async(
-        session, url, auto_retry=auto_retry, **kwargs
-    )
+    response = await provider.get_data_async(session, url, auto_retry=auto_retry, **kwargs)
     return response
 
 
 def get_data_by_data_type(data_type, session, **kwargs):
     from refinitiv.data.delivery._data._data_provider_factory import make_provider
 
     provider = make_provider(data_type)
@@ -89,32 +85,30 @@
         session.error(f"{handler} callback raised exception: {e!r}")
         session.debug(traceback.format_exc())
 
         if DEBUG:
             raise e
 
 
-class DataProviderLayer(Generic[T]):
+class DataProviderLayer(Generic[TypeData]):
     # Should not change even if class name is changed
     _USAGE_CLS_NAME = "DataProviderLayer"
 
     def __init__(self, data_type, **kwargs):
         self._initialize(data_type, **kwargs)
         self._log__init__usage(
             data_type=data_type,
             **kwargs,
         )
 
     @staticmethod
     def _log__init__usage(*args, **kwargs):
         get_usage_logger().log_func(
             name=f"{ModuleName.DELIVERY}.{DataProviderLayer._USAGE_CLS_NAME}.__init__",
-            func_path=f"{DataProviderLayer.__module__}."
-            f"{DataProviderLayer.__qualname__}."
-            f"__init__",
+            func_path=f"{DataProviderLayer.__module__}.{DataProviderLayer.__qualname__}.__init__",
             args=args,
             kwargs=kwargs,
             desc={FilterType.INIT, FilterType.LAYER_DELIVERY},
         )
 
     def _initialize(self, data_type, **kwargs):
         from ._data_provider_factory import make_provider
@@ -130,33 +124,31 @@
     def _check_response(self, response: BaseResponse, config):
         _check_response(response, config)
 
     def get_data(
         self,
         session: Optional["Session"] = None,
         on_response: Optional[Callable] = None,
-    ) -> T:
+    ) -> TypeData:
         session = get_valid_session(session)
         response = get_data(self._data_type, self._provider, session, **self._kwargs)
         on_response and emit_event(on_response, response, self, session)
         self._check_response(response, session.config)
         return response
 
     async def get_data_async(
         self,
         session: Optional["Session"] = None,
         on_response: Optional[Callable] = None,
         closure: Optional[Any] = None,
-    ) -> T:
+    ) -> TypeData:
         if not self._kwargs.get("closure") and closure:
             self._kwargs["closure"] = closure
         session = get_valid_session(session)
-        response = await get_data_async(
-            self._data_type, self._provider, session, **self._kwargs
-        )
+        response = await get_data_async(self._data_type, self._provider, session, **self._kwargs)
         on_response and emit_event(on_response, response, self, session)
         return response
 
     def __repr__(self):
         s = super().__repr__()
         s = s.replace(">", f" {{name='{self._kwargs.get('universe')}'}}>")
         return s
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_endpoint_data.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_endpoint_data.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,15 +1,14 @@
-# coding: utf8
-import abc
 import collections
+from dataclasses import dataclass, field
 from enum import Enum, unique
-from typing import Any, TYPE_CHECKING
+from typing import Any, Dict, TYPE_CHECKING
 
 if TYPE_CHECKING:
-    from ._response import ABCResponse
+    from ._response import Response
 
 Error = collections.namedtuple("Error", ["code", "message"])
 
 
 @unique
 class RequestMethod(str, Enum):
     """
@@ -26,20 +25,12 @@
     DELETE = "DELETE"
     PUT = "PUT"
 
     def __str__(self) -> str:
         return str(self.value)
 
 
-class ABCData(abc.ABC):
-    pass
-
-
-class EndpointData(ABCData, object):
-    def __init__(self, raw: Any, owner_: "ABCResponse" = None, **kwargs):
-        self._raw = raw
-        self._owner = owner_
-        self._kwargs = kwargs
-
-    @property
-    def raw(self):
-        return self._raw
+@dataclass
+class EndpointData:
+    raw: Any
+    _owner: "Response" = None
+    _kwargs: Dict = field(default_factory=dict)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_endpoint_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_endpoint_data_provider.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_endpoint_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_endpoint_definition.py`

 * *Files 4% similar despite different names*

```diff
@@ -89,17 +89,15 @@
         >>> definition_endpoint = endpoint_request.Definition("/data/news/v1/analyze")
         >>> definition_endpoint.get_data()
         """
         validate_endpoint_request_url_parameters(self.url, self.path_parameters)
 
         session = get_valid_session(session)
         self._log_usage(
-            f"{self.__class__.__module__}."
-            f"{self.__class__.__qualname__}."
-            f"get_data",
+            f"{self.__class__.__module__}.{self.__class__.__qualname__}.get_data",
             {FilterType.SYNC, FilterType.LAYER_DELIVERY, FilterType.REST},
         )
         response = self._provider.get_data(
             session,
             self.url,
             method=self.method,
             path_parameters=self.path_parameters,
@@ -128,17 +126,15 @@
         >>> definition_endpoint = endpoint_request.Definition("/data/news/v1/analyze")
         >>> await definition_endpoint.get_data_async()
         """
         validate_endpoint_request_url_parameters(self.url, self.path_parameters)
 
         session = get_valid_session(session)
         self._log_usage(
-            f"{self.__class__.__module__}."
-            f"{self.__class__.__qualname__}."
-            f"get_data_async",
+            f"{self.__class__.__module__}.{self.__class__.__qualname__}.get_data_async",
             {FilterType.ASYNC, FilterType.LAYER_DELIVERY, FilterType.REST},
         )
         response = await self._provider.get_data_async(
             session,
             self.url,
             method=self.method,
             path_parameters=self.path_parameters,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_parsed_data.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_parsed_data.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_raw_data_parser.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_raw_data_parser.py`

 * *Files 0% similar despite different names*

```diff
@@ -101,17 +101,15 @@
             status,
             raw_response,
             error_codes=error_codes,
             error_messages=error_messages,
         )
         return parsed_data
 
-    def parse_raw_response(
-        self, raw_response: "httpx.Response"
-    ) -> Tuple[bool, ParsedData]:
+    def parse_raw_response(self, raw_response: "httpx.Response") -> Tuple[bool, ParsedData]:
         is_success = False
 
         if raw_response is None:
             return is_success, ParsedData({}, {})
 
         is_success = raw_response.status_code in success_http_codes
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_request_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_request_factory.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,41 +11,40 @@
 
 class RequestFactory:
     def get_url(self, session: "Session", url: str, *args, **kwargs) -> str:
         return url
 
     def create(self, session, url, *args, **kwargs) -> Request:
         method = self.get_request_method(**kwargs)
-        session.verify_scope(url, method)
-        url = self.get_url(session, url, *args, **kwargs)
+        path = self.get_url(session, url, *args, **kwargs)
+        session.verify_scope(path, method)
         url_root = session._get_rdp_url_root()
 
         header_parameters = self.get_header_parameters(session, **kwargs)
         path_parameters = self.get_path_parameters(session, **kwargs)
         query_parameters = self.get_query_parameters(*args, **kwargs)
-        query_parameters = self.extend_query_parameters(
-            query_parameters, extended_params=kwargs.get("extended_params")
-        )
+        query_parameters = self.extend_query_parameters(query_parameters, extended_params=kwargs.get("extended_params"))
         closure = kwargs.get("closure")
 
-        url = self.update_url(url_root, url, path_parameters, query_parameters)
+        url = self.update_url(url_root, path, path_parameters, query_parameters)
 
         body_parameters = None
         headers = header_parameters
         if method != RequestMethod.GET:
             headers["Content-Type"] = "application/json"
         if method in (RequestMethod.POST, RequestMethod.PUT):
             body_parameters = self.get_body_parameters(session, *args, **kwargs)
             body_parameters = self.extend_body_parameters(body_parameters, **kwargs)
 
         request = Request(
             url=url,
             method=method,
             headers=headers,
             json=body_parameters,
+            path=path,
         )
 
         if closure is not None:
             request.closure = closure
 
         return request
 
@@ -115,17 +114,15 @@
     @property
     def query_params_config(self):
         return []
 
     def get_query_parameters(self, *args, **kwargs) -> list:
         return get_params(self.query_params_config, *args, **kwargs)
 
-    def get_path_parameters(
-        self, session=None, *, path_parameters=None, **kwargs
-    ) -> dict:
+    def get_path_parameters(self, session=None, *, path_parameters=None, **kwargs) -> dict:
         return path_parameters or {}
 
     def add_multiple_query_parameters(self, url: str, query_parameters: dict) -> str:
         """Add multiple query parameters to query string.
 
         Args:
             url (str): url to construct query string.
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_data/_validators.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_data/_validators.py`

 * *Files 0% similar despite different names*

```diff
@@ -65,17 +65,15 @@
         if allowed_content_types is None:
             allowed_content_types = {"application/json"}
         self._allowed_content_types = allowed_content_types
 
     def validate(self, data: ParsedData) -> bool:
         # Checking only first part (type/subtype) of media_type
         # See https://httpwg.org/specs/rfc7231.html#media.type
-        content_type = (
-            data.raw_response.headers.get("content-type", "").split(";")[0].strip()
-        )
+        content_type = data.raw_response.headers.get("content-type", "").split(";")[0].strip()
         is_success = content_type in self._allowed_content_types
 
         if not is_success:
             data.error_codes = -1
             data.error_messages = (
                 f"Unexpected content-type in response,\n"
                 f"Expected: {self._allowed_content_types}\n"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/__init__.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_omm_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_omm_stream.py`

 * *Files 2% similar despite different names*

```diff
@@ -162,22 +162,18 @@
         self._cxn.on(self._event.error_by_id, self._on_stream_error)
         self._cxn.on(self._event.ack_by_id, self._on_stream_ack)
         super()._do_open(*args, **kwargs)
 
     def _dispose(self):
         self._debug(f"{self._classname} disposing [d]")
         if self._cxn is not None:
-            self._cxn.remove_listener(
-                self._event.refresh_by_id, self._on_stream_refresh
-            )
+            self._cxn.remove_listener(self._event.refresh_by_id, self._on_stream_refresh)
             self._cxn.remove_listener(self._event.update_by_id, self._on_stream_update)
             self._cxn.remove_listener(self._event.status_by_id, self._on_stream_status)
-            self._cxn.remove_listener(
-                self._event.complete_by_id, self._on_stream_complete
-            )
+            self._cxn.remove_listener(self._event.complete_by_id, self._on_stream_complete)
             self._cxn.remove_listener(self._event.error_by_id, self._on_stream_error)
             self._cxn.remove_listener(self._event.ack_by_id, self._on_stream_ack)
             self._release_cxn()
         self._contributed.set()
         self._debug(f"{self._classname} disposed [D]")
 
     def _do_on_stream_refresh(self, cxn: "StreamConnection", message: dict, *_) -> Any:
@@ -204,104 +200,89 @@
         message_state = message.get("State", {})
         stream_state = message_state.get("Stream", "")
         self._stream_state = stream_state
         self._message_state = message_state.get("Text", "")
 
         if stream_state == "Closed":
             self._debug(
-                f"{self._classname} received a closing message, "
-                f"message_state={message_state}, state={self.state}"
+                f"{self._classname} received a closing message, message_state={message_state}, state={self.state}"
             )
 
             if self.is_opening:
                 self._opened.set()
 
         return message
 
     def _on_stream_complete(self, originator, *args) -> None:
         self._propagate_event(StreamEvent.COMPLETE, originator, *args)
 
         if self.is_opening:
             self._opened.set()
 
-    def _do_on_stream_error(
-        self, originator: "StreamConnection", message: dict, *args
-    ) -> Any:
+    def _do_on_stream_error(self, originator: "StreamConnection", message: dict, *args) -> Any:
         self._error_message = message
 
         if self.is_contributing:
             if message.get("Type") == "Error":
                 debug_message = message.get("Debug", {}).get("Message")
                 if debug_message:
                     try:
                         debug_dict = json.loads(debug_message)
                         if debug_dict.get("PostID"):
                             self._contrib_response = ErrorContribResponse(message)
                     except json.decoder.JSONDecodeError:
-                        self._error(
-                            f"Cannot decode Debug message as JSON: {debug_message}"
-                        )
+                        self._error(f"Cannot decode Debug message as JSON: {debug_message}")
 
             self._error_event.set()
             return (message, *args)
 
         else:
             return super()._do_on_stream_error(originator, message, *args)
 
-    def _do_on_stream_ack(
-        self, originator: "StreamConnection", message: dict, *args
-    ) -> Any:
+    def _do_on_stream_ack(self, originator: "StreamConnection", message: dict, *args) -> Any:
         self._ack_message = message
 
         if self.is_contributing:
             self._contrib_response = AckContribResponse(message)
             ack_id = message.get("AckID")
             nack_code = message.get("NakCode")
 
             if ack_id != self.post_id:
                 # Received Ack message with wrong ack_id
                 self._error(
-                    f"{self._classname} Received Ack message "
-                    f"with wrong ack_id={ack_id} != post_id={self.post_id}"
+                    f"{self._classname} Received Ack message with wrong ack_id={ack_id} != post_id={self.post_id}"
                 )
             else:
                 if nack_code:
                     reason = message.get("Text")
                     self._error(
                         f"{self._classname} received Nack message, "
                         f"ackID={ack_id}, NackCode={nack_code}, reason={reason}"
                     )
 
                 else:
-                    self._debug(
-                        f"{self._classname} received Ack message, "
-                        f"ackID={ack_id}, state={self.state}"
-                    )
+                    self._debug(f"{self._classname} received Ack message, ackID={ack_id}, state={self.state}")
 
             self._ack_event.set()
             return (message, *args)
 
         else:
             return super()._do_on_stream_ack(originator, message, *args)
 
     def send_open_message(self):
         self.send(self.open_message)
 
-    def get_contrib_message(
-        self, fields: dict, contrib_type: Union[str, "ContribType", None]
-    ) -> dict:
+    def get_contrib_message(self, fields: dict, contrib_type: Union[str, "ContribType", None]) -> dict:
         message = {
             "Ack": True,
             "ID": self.id,
             "Message": {
                 "Fields": fields,
                 "ID": self.id,
-                "Type": contrib_type_enum_arg_parser.get_str(
-                    contrib_type if contrib_type else "Update"
-                ),
+                "Type": contrib_type_enum_arg_parser.get_str(contrib_type if contrib_type else "Update"),
                 "Domain": self.domain,
             },
             "PostID": self.post_id,
             "Type": "Post",
             "Domain": self.domain,
         }
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_rdp_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_rdp_stream.py`

 * *Files 1% similar despite different names*

```diff
@@ -109,17 +109,15 @@
         super()._do_open(*args, **kwargs)
 
     def _dispose(self):
         self._debug(f"{self._classname} disposing [d]")
         if self._cxn is not None:
             self._cxn.remove_listener(self._event.ack_by_id, self._on_stream_ack)
             self._cxn.remove_listener(self._event.update_by_id, self._on_stream_update)
-            self._cxn.remove_listener(
-                self._event.response_by_id, self._on_stream_response
-            )
+            self._cxn.remove_listener(self._event.response_by_id, self._on_stream_response)
             self._cxn.remove_listener(self._event.alarm_by_id, self._on_stream_alarm)
 
             self._release_cxn()
 
         self._debug(f"{self._classname} disposed [D]")
 
     def _do_on_stream_response(self, originator: "StreamConnection", *args) -> Any:
@@ -131,16 +129,15 @@
     def _do_on_stream_ack(self, originator: "StreamConnection", *args) -> Any:
         message = args[0]
         message_state = message.get("state", {})
         stream_state = message_state.get("stream")
 
         if stream_state == "Closed":
             self._debug(
-                f"{self._classname} received a closing message, "
-                f"message_state={message_state}, state={self.state}"
+                f"{self._classname} received a closing message, message_state={message_state}, state={self.state}"
             )
 
             if self.is_opening:
                 self._opened.set()
 
         return args
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_cxn_cache.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_cxn_cache.py`

 * *Files 7% similar despite different names*

```diff
@@ -49,18 +49,16 @@
     cxn.remove_listener(StreamCxnEvent.CONNECTING, on_event_connecting)
     cxn.remove_listener(StreamCxnEvent.CONNECTED, on_event_connected)
     cxn.remove_listener(StreamCxnEvent.DISCONNECTED, on_event_disconnected)
     cxn.remove_listener(StreamCxnEvent.RECONNECTING, on_event_reconnecting)
 
 
 class CacheItem:
-    def __init__(
-        self, cxn: "StreamConnection", details: "StreamDetails", owner: dict
-    ) -> None:
-        self.api_type_as_str = details.api_type_as_str
+    def __init__(self, cxn: "StreamConnection", details: "StreamDetails", owner: dict) -> None:
+        self.api_config_key = details.api_config_key
         self.owner = owner
         self.cxn: "StreamConnection" = cxn
         self.number_in_use = 0
 
         add_listeners(self.cxn)
 
     @property
@@ -68,29 +66,25 @@
         return self.number_in_use > 0
 
     def inc_use(self):
         self.number_in_use += 1
 
     def dec_use(self):
         if self.number_in_use == 0:
-            raise ValueError(
-                f"CacheItem: number_in_use cannot be less 0, cxn={self.cxn.state}"
-            )
+            raise ValueError(f"CacheItem: number_in_use cannot be less 0, cxn={self.cxn.state}")
 
         self.number_in_use -= 1
 
-        if self.number_in_use == 0 and (
-            self.cxn.is_disconnecting or self.cxn.is_disposed
-        ):
+        if self.number_in_use == 0 and (self.cxn.is_disconnecting or self.cxn.is_disposed):
             self.dispose()
 
     def dispose(self):
         self.number_in_use = -1
-        self.owner.pop(self.api_type_as_str)
-        self.api_type_as_str = None
+        self.owner.pop(self.api_config_key)
+        self.api_config_key = None
         self.owner = None
 
         remove_listeners(self.cxn)
         cxn = self.cxn
         cxn.dispose()
         try:
             cxn.join(5)
@@ -110,23 +104,19 @@
 class StreamCxnCache(object):
     def __init__(self) -> None:
         self._cache: Dict["Session", Dict[str, CacheItem]] = {}
         self._lock = threading.Lock()
         self.cxn_created = threading.Event()
 
     def has_cxn(self, session: "Session", details: "StreamDetails") -> bool:
-        item = self._cache.get(session, {}).get(details.api_type_as_str)
+        item = self._cache.get(session, {}).get(details.api_config_key)
         return bool(item)
 
-    def get_cxn(
-        self, session: "Session", details: "StreamDetails"
-    ) -> "StreamConnection":
-
+    def get_cxn(self, session: "Session", details: "StreamDetails") -> "StreamConnection":
         with self._lock:
-
             content_type = details.content_type
             protocol_type = details.protocol_type
 
             if not self.has_cxn(session, details):
                 from ._stream_factory import create_stream_cxn
 
                 self.cxn_created.clear()
@@ -150,20 +140,16 @@
                 f"StreamCxnCache wait for connection: "
                 f"id={cxn.id}, content_type={content_type}, "
                 f"protocol_type={protocol_type}"
             )
 
             cxn.wait_connection_result()
 
-            if (
-                cxn.is_disconnected or cxn.is_disposed
-            ):  # Connection failure for some reason
-                session.debug(
-                    "StreamCxnCache: Connection will be deleted, because failure"
-                )
+            if cxn.is_disconnected or cxn.is_disposed:  # Connection failure for some reason
+                session.debug("StreamCxnCache: Connection will be deleted, because failure")
                 self.del_cxn(cxn, session, details)
                 raise ConnectionError(f"Cannot prepare connection {cxn}")
 
             else:
                 item.inc_use()
                 session.debug(f" <=== StreamCxnCache connection id={cxn.id} is ready")
 
@@ -175,25 +161,23 @@
             raise ValueError(
                 f"Cannot release stream connection, "
                 f"because its not in the cache "
                 f"(content_type={content_type}, session={session})"
             )
 
         item_by_api_type = self._cache[session]
-        item = item_by_api_type[details.api_type_as_str]
+        item = item_by_api_type[details.api_config_key]
         item.dec_use()
         session.debug(
             f" ===> StreamCxnCache release (item={item},\n"
             f"\t\tcontent_type={content_type},\n"
             f"\t\tsession={session})"
         )
 
-    def del_cxn(
-        self, cxn: "StreamConnection", session: "Session", details: "StreamDetails"
-    ) -> None:
+    def del_cxn(self, cxn: "StreamConnection", session: "Session", details: "StreamDetails") -> None:
         content_type = details.content_type
 
         if not cxn:
             raise ValueError(
                 f"Cannot delete stream connection, "
                 f"because it is empty (content_type={content_type}, "
                 f"cxn={cxn}, session={session})"
@@ -203,15 +187,15 @@
             raise ValueError(
                 f"Cannot delete stream connection, "
                 f"because already deleted (content_type={content_type}, "
                 f"cxn={cxn}, session={session})"
             )
 
         item_by_api_type = self._cache[session]
-        item = item_by_api_type[details.api_type_as_str]
+        item = item_by_api_type[details.api_config_key]
         if item.is_using:
             raise AssertionError(
                 f"Cannot delete stream connection, "
                 f"because it is using (content_type={content_type}, "
                 f"cxn={cxn}, session={session})"
             )
 
@@ -254,17 +238,15 @@
                 item.dispose()
 
         with ThreadPoolExecutor(thread_name_prefix="CloseCxns-Thread") as pool:
             pool.map(_close_cxn, self._get_cache_items(session))
 
         self._cache.pop(session, None)
 
-    def _add_cxn(
-        self, cxn: "StreamConnection", session: "Session", details: "StreamDetails"
-    ) -> CacheItem:
+    def _add_cxn(self, cxn: "StreamConnection", session: "Session", details: "StreamDetails") -> CacheItem:
         content_type = details.content_type
 
         if not cxn:
             raise ValueError(
                 f"Cannot add stream connection, "
                 f"because it is empty: content_type={content_type}, "
                 f"cxn={cxn}, session={session}"
@@ -275,25 +257,23 @@
                 f"Cannot add stream connection, "
                 f"because already added: content_type={content_type}, "
                 f"cxn={cxn}, session={session}"
             )
 
         owner = self._cache.setdefault(session, {})
         item = CacheItem(cxn, details, owner)
-        owner[details.api_type_as_str] = item
+        owner[details.api_config_key] = item
         return item
 
     def _get_cache_items(self, session: "Session") -> List[CacheItem]:
         item_by_content_type = self._cache.get(session, {})
         return [item for item in item_by_content_type.values()]
 
-    def _get_cache_item(
-        self, session: "Session", details: "StreamDetails"
-    ) -> CacheItem:
-        cache_item = self._cache[session][details.api_type_as_str]
+    def _get_cache_item(self, session: "Session", details: "StreamDetails") -> CacheItem:
+        cache_item = self._cache[session][details.api_config_key]
         return cache_item
 
     def is_cxn_alive(self, session: "Session", content_type: "ContentType") -> bool:
         from ._stream_factory import content_type_to_details
 
         details = content_type_to_details(content_type)
         is_alive = False
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_cxn_config_data.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_cxn_config_data.py`

 * *Files 0% similar despite different names*

```diff
@@ -187,13 +187,11 @@
         else:
             path = info.path or "WebSocket"
             return f"{info.scheme}://{info.host}:{info.port}/{path}"
 
 
 class NullStreamCxnConfig(StreamCxnConfig):
     def __init__(self):
-        StreamCxnConfig.__init__(
-            self, [StreamServiceInfo("", "", 0, "", [""], "", "")], []
-        )
+        StreamCxnConfig.__init__(self, [StreamServiceInfo("", "", 0, "", [""], "", "")], [])
 
     def _get_url(self, info):
         return ""
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_cxn_config_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_cxn_config_provider.py`

 * *Files 3% similar despite different names*

```diff
@@ -22,25 +22,21 @@
 from ..._tools import parse_url, urljoin
 
 if TYPE_CHECKING:
     from ..._core.session import Session, PlatformSession
     from ..._configure import _RDPConfig
 
 
-def get_discovery_url(
-    root_url: str, base_config_name: str, full_config_name: str, config: "_RDPConfig"
-) -> str:
+def get_discovery_url(root_url: str, base_config_name: str, full_config_name: str, config: "_RDPConfig") -> str:
     base_path = config.get_str(f"{base_config_name}.url")
 
     try:
         endpoint_path = config.get_str(f"{full_config_name}.path")
     except KeyError:
-        raise KeyError(
-            f"Cannot find discovery endpoint '{full_config_name}' into config."
-        )
+        raise KeyError(f"Cannot find discovery endpoint '{full_config_name}' into config.")
 
     if base_path.startswith("http"):
         url = base_path
     else:
         url = urljoin(root_url, base_path)
 
     return urljoin(url, endpoint_path)
@@ -49,17 +45,15 @@
 def _filter_by_location(locations: List[str], infos: List[StreamServiceInfo]) -> list:
     if not locations:
         return infos
 
     filtered = []
     for location in locations:
         for info in infos:
-            has_location = any(
-                loc.strip().startswith(location) for loc in info.location
-            )
+            has_location = any(loc.strip().startswith(location) for loc in info.location)
             if has_location and info not in filtered:
                 filtered.append(info)
 
     return filtered
 
 
 def create_infos(data, transport, port_by_prefix, tier):
@@ -115,17 +109,15 @@
 
     def wait_start_connecting(self):
         self._start_connecting.clear()
 
     def start_connecting(self):
         self._start_connecting.set()
 
-    def get_cfg(
-        self, session: "Session", api_cfg_key: str
-    ) -> Union[PlatformStreamCxnConfig, DesktopStreamCxnConfig]:
+    def get_cfg(self, session: "Session", api_cfg_key: str) -> Union[PlatformStreamCxnConfig, DesktopStreamCxnConfig]:
         """
         Parameters
         ----------
         session: Session
         api_cfg_key: str
             Example - "apis.streaming.pricing.endpoints.main"
 
@@ -159,17 +151,15 @@
             if not isinstance(urls, list):
                 urls = [urls]
 
             infos = [self.info_from_url(transport, url) for url in urls]
 
         else:
             infos = self._request_infos(
-                discovery_url=get_discovery_url(
-                    session._get_rdp_url_root(), base_config_name, api_cfg_key, cfg
-                ),
+                discovery_url=get_discovery_url(session._get_rdp_url_root(), base_config_name, api_cfg_key, cfg),
                 api_cfg_key=api_cfg_key,
                 config=cfg,
                 session=session,
                 transport=transport,
             )
 
         protocols = cfg.get_list(f"{api_cfg_key}.protocols")
@@ -212,64 +202,58 @@
         discovery_url: str,
         api_cfg_key: str,
         config: "_RDPConfig",
         session: "Session",
         transport: str = "websocket",
     ) -> List[StreamServiceInfo]:
         method = RequestMethod.GET
+        path = urlsplit(discovery_url).path
         try:
-            session.verify_scope(urlsplit(discovery_url).path, method)
+            session.verify_scope(path, method)
         except ScopeError as e:
             session.error(
                 f"Insufficient Scope. Cannot load the list of associated URLs "
                 f"from {discovery_url} for {api_cfg_key} endpoint."
             )
             raise e
         tier: Optional[int] = config.get(f"{get_base_cfg_name(api_cfg_key)}.tier")
 
-        response = None
-        once = False
         server_mode = session.server_mode
-        while not once or server_mode is True:
-            once = True
+        request = Request(
+            url=discovery_url,
+            method=method,
+            # server won't accept tier: false
+            params={"tier": True} if tier else {},
+            auto_retry=True,
+            path=path,
+        )
+        response = None
+        while True:
             self._start_connecting.wait()
             try:
-                request = Request(
-                    url=discovery_url,
-                    method=method,
-                    # server won't accept tier: false
-                    params={"tier": True} if tier else {},
-                    auto_retry=True,
-                )
                 response = session.http_request(request)
             except httpx.HTTPError:
                 if server_mode is True:
                     delay = self._delays.next()
-                    session.debug(
-                        f"CxnConfigProvider waiting {delay} secs "
-                        f"until the next attempt."
-                    )
+                    session.debug(f"CxnConfigProvider waiting {delay} secs until the next attempt.")
                     self._timer.wait(delay)
-                else:
-                    break
-            else:
-                break
+                    continue
+            break
 
         try:
             data = response.json()
         except (AttributeError, json.decoder.JSONDecodeError):
-            message = (
-                f"Cannot load the list of associated URLs "
-                f"from {discovery_url} for {api_cfg_key} endpoint."
-            )
+            message = f"Cannot load the list of associated URLs from {discovery_url} for {api_cfg_key} endpoint."
             session.error(message)
             raise ConnectionError(message) from None
 
         err = data.get("error")
         if err:
+            if response.status_code == 403:
+                session._handle_insufficient_scope(request.path, request.method, err.get("message"))
             raise RDError(response.status_code, err.get("message"))
 
         infos = create_infos(data, transport, self._port_by_prefix, tier)
         return self._filter_infos(infos, api_cfg_key, config)
 
     def _filter_infos(
         self,
@@ -314,18 +298,15 @@
         cfg: "_RDPConfig",
     ) -> List[StreamServiceInfo]:
         locations = cfg.get_list(f"{api_cfg_key}.locations")
         return _filter_by_location(locations, infos)
 
 
 class DeployedCxnConfigProvider(CxnConfigProvider):
-    def get_cfg(
-        self, session: "PlatformSession", api_cfg_key: str
-    ) -> PlatformStreamCxnConfig:
-
+    def get_cfg(self, session: "PlatformSession", api_cfg_key: str) -> PlatformStreamCxnConfig:
         url: str = session._deployed_platform_host
         cfg: "_RDPConfig" = session.config
 
         if url is None:
             session_name: str = session.name
             key = keys.platform_realtime_distribution_system(session_name)
             url_key = f"{key}.url"
@@ -339,17 +320,15 @@
             data_formats = ["tr_json2"]
 
         info = self.info_from_url(transport, url, data_formats=data_formats)
 
         return PlatformStreamCxnConfig(info, "OMM", transport=transport)
 
 
-class PlatformAndDeployedCxnConfigProvider(
-    DeployedCxnConfigProvider, PlatformCxnConfigProvider
-):
+class PlatformAndDeployedCxnConfigProvider(DeployedCxnConfigProvider, PlatformCxnConfigProvider):
     def get_cfg(self, session: "PlatformSession", api_cfg_key: str) -> StreamCxnConfig:
         if api_cfg_key.startswith("apis.streaming.pricing.endpoints.main"):
             cxn_config = DeployedCxnConfigProvider.get_cfg(self, session, api_cfg_key)
 
         else:
             cxn_config = PlatformCxnConfigProvider.get_cfg(self, session, api_cfg_key)
 
@@ -363,25 +342,22 @@
     SessionCxnType.DESKTOP: DesktopCxnConfigProvider,
 }
 
 cache_provider_by_session = {}
 
 
 def get_cxn_config(api_config_key: str, session: "Session") -> StreamCxnConfig:
-    cxn_cfg_provider = get_cxn_cfg_provider(session)
-    return cxn_cfg_provider.get_cfg(session, api_config_key)
+    return get_cxn_cfg_provider(session).get_cfg(session, api_config_key)
 
 
 def get_cxn_cfg_provider(session: "Session") -> CxnConfigProvider:
     session_cxn_type = session._get_session_cxn_type()
     provider_class = provider_class_by_session_cxn_type.get(session_cxn_type)
 
     if not provider_class:
-        raise ValueError(
-            f"Can't find provider_class by session_cxn_type={session_cxn_type}"
-        )
+        raise ValueError(f"Can't find provider_class by session_cxn_type={session_cxn_type}")
 
     return cache_provider_by_session.setdefault(session, provider_class())
 
 
 def release_cxn_cfg_provider(session: "Session") -> None:
     cache_provider_by_session.pop(session, None)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_factory.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_factory.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 from ._protocol_type import ProtocolType
 from ._rdp_stream import _RDPStream
 from ._stream_cxn_config_provider import get_cxn_config, release_cxn_cfg_provider
 from .contrib._stream_connection import OffStreamContribConnection
 from .omm_stream_connection import OMMStreamConnection
 from .rdp_stream_connection import RDPStreamConnection
 from .._data._api_type import APIType
-from ... import _log as log, get_config
+from ... import _log as log
 from ..._content_type import ContentType
 from ..._types import OptDict, OptStr, Strings, ExtendedParams, OptCall
 
 if TYPE_CHECKING:
     from ._stream_cxn_config_data import StreamCxnConfig
     from . import StreamConnection
     from ..._core.session import Session
@@ -26,111 +26,86 @@
 
 
 protocol_type_by_name: Dict[str, ProtocolType] = {
     "OMM": ProtocolType.OMM,
     "RDP": ProtocolType.RDP,
 }
 
+default_api_config_key_by_api_type = {
+    APIType.STREAMING_FINANCIAL_CONTRACTS: "financial-contracts",
+    APIType.STREAMING_PRICING: "main",
+    APIType.STREAMING_TRADING: "redi",
+    APIType.STREAMING_BENCHMARK: "resource",
+    APIType.STREAMING_CUSTOM_INSTRUMENTS: "resource",
+}
+
 api_config_key_by_api_type: Dict[APIType, str] = {
-    APIType.STREAMING_FINANCIAL_CONTRACTS: "apis.streaming.quantitative-analytics.endpoints.financial-contracts",
-    APIType.STREAMING_PRICING: "apis.streaming.pricing.endpoints.main",
-    APIType.STREAMING_TRADING: "apis.streaming.trading-analytics.endpoints.redi",
-    APIType.STREAMING_BENCHMARK: "apis.streaming.benchmark.endpoints.resource",
-    APIType.STREAMING_CUSTOM_INSTRUMENTS: "apis.streaming.custom-instruments.endpoints.resource",
-    APIType.STREAMING_CONTRIB: "apis.streaming.contrib.endpoints.main",
+    APIType.STREAMING_FINANCIAL_CONTRACTS: "apis.streaming.quantitative-analytics.endpoints",
+    APIType.STREAMING_PRICING: "apis.streaming.pricing.endpoints",
+    APIType.STREAMING_TRADING: "apis.streaming.trading-analytics.endpoints",
+    APIType.STREAMING_BENCHMARK: "apis.streaming.benchmark.endpoints",
+    APIType.STREAMING_CUSTOM_INSTRUMENTS: "apis.streaming.custom-instruments.endpoints",
 }
+
 service_config_key_by_api_type = {
     APIType.STREAMING_FINANCIAL_CONTRACTS: "apis.streaming.quantitative-analytics.service",
     APIType.STREAMING_PRICING: "apis.streaming.pricing.service",
     APIType.STREAMING_TRADING: "apis.streaming.trading-analytics.service",
     APIType.STREAMING_BENCHMARK: "apis.streaming.benchmark.service",
     APIType.STREAMING_CUSTOM_INSTRUMENTS: "apis.streaming.custom-instruments.service",
-    APIType.STREAMING_CONTRIB: "apis.streaming.contrib.service",
 }
 
 api_type_by_content_type: Dict[ContentType, APIType] = {
     ContentType.STREAMING_CHAINS: APIType.STREAMING_PRICING,
     ContentType.STREAMING_PRICING: APIType.STREAMING_PRICING,
+    ContentType.STREAMING_DICTIONARY: APIType.STREAMING_PRICING,
     ContentType.STREAMING_TRADING: APIType.STREAMING_TRADING,
     ContentType.STREAMING_CONTRACTS: APIType.STREAMING_FINANCIAL_CONTRACTS,
     ContentType.STREAMING_CUSTOM_INSTRUMENTS: APIType.STREAMING_CUSTOM_INSTRUMENTS,
-    ContentType.STREAMING_CONTRIB: APIType.STREAMING_CONTRIB,
-    ContentType.STREAMING_OFF_CONTRIB: APIType.STREAMING_CONTRIB,
+    ContentType.STREAMING_CONTRIB: APIType.STREAMING_PRICING,
+    ContentType.STREAMING_OFF_CONTRIB: APIType.STREAMING_PRICING,
+    ContentType.STREAMING_OMM: APIType.STREAMING_PRICING,
+    ContentType.STREAMING_RDP: APIType.STREAMING_CUSTOM,
 }
 
 connection_id_iterator = itertools.count(0)
 
-stream_class_by_protocol_type: Dict[
-    ProtocolType, Type[Union[_OMMStream, _RDPStream]]
-] = {
+stream_class_by_protocol_type: Dict[ProtocolType, Type[Union[_OMMStream, _RDPStream]]] = {
     ProtocolType.OMM: _OMMStream,
     ProtocolType.RDP: _RDPStream,
 }
 
 
+def get_default_config_path(api_type):
+    return f"{api_config_key_by_api_type.get(api_type)}.{default_api_config_key_by_api_type.get(api_type)}"
+
+
 @dataclass
 class StreamDetails:
     content_type: ContentType
     protocol_type: ProtocolType
     api_type: APIType
-    api_config_key: str = ""
+    _api_config_key: str = ""
 
     @property
-    def api_type_as_str(self):
-        if self.api_type is ContentType.STREAMING_CUSTOM:
-            return f"{self.api_type}.{self.api_config_key}"
-        else:
-            return str(self.api_type)
+    def api_config_key(self):
+        if not self._api_config_key:
+            self._api_config_key = service_config_key_by_api_type.get(self.api_type)
+        return self._api_config_key
 
 
 def content_type_to_details(content_type: ContentType) -> StreamDetails:
     api_type = api_type_by_content_type.get(content_type, APIType.STREAMING_CUSTOM)
 
-    if content_type is ContentType.STREAMING_CUSTOM:
+    if content_type is ContentType.NONE:
         raise ValueError("Cannot create StreamDetails, without api.")
 
     return StreamDetails(content_type, ProtocolType.NONE, api_type)
 
 
-def convert_api_config_key_to_content_type(api_config_key: str) -> ContentType:
-    """
-    >>> api_type_by_api_config_key
-    {
-        'streaming/quantitative-analytics/financial-contracts': <APIType.STREAMING_FINANCIAL_CONTRACTS: 3>,
-        'streaming/pricing/main': <APIType.STREAMING_PRICING: 8>,
-        'streaming/trading-analytics/redi': <APIType.STREAMING_TRADING: 11>
-    }
-    >>> content_type_by_api_type
-    {
-        <APIType.STREAMING_PRICING: 8>: <ContentType.STREAMING_PRICING: 17>,
-        <APIType.STREAMING_TRADING: 11>: <ContentType.STREAMING_TRADING: 39>,
-        <APIType.STREAMING_FINANCIAL_CONTRACTS: 3>: <ContentType.STREAMING_CONTRACTS: 6>
-    }
-    """
-    api_type_by_api_config_key = {v: k for k, v in api_config_key_by_api_type.items()}
-    api_type = api_type_by_api_config_key.get(api_config_key)
-    content_type_by_api_type = {v: k for k, v in api_type_by_content_type.items()}
-    content_type = content_type_by_api_type.get(api_type)
-
-    if not content_type:
-        content_type = ContentType.STREAMING_CUSTOM
-
-    return content_type
-
-
-def get_valid_content_type(content_type: ContentType, api: str = "") -> ContentType:
-    if content_type in {ContentType.STREAMING_CUSTOM, ContentType.NONE}:
-        if not api:
-            raise ValueError("api cannot be None")
-
-        content_type = convert_api_config_key_to_content_type(api)
-
-    return content_type
-
-
 def create_omm_stream(
     content_type: ContentType,
     session: "Session",
     name: str,
     api: OptStr = None,
     domain: OptStr = None,
     service: OptStr = None,
@@ -140,42 +115,38 @@
     on_refresh: OptCall = None,
     on_status: OptCall = None,
     on_update: OptCall = None,
     on_complete: OptCall = None,
     on_error: OptCall = None,
     on_ack: OptCall = None,
 ) -> _OMMStream:
-    if content_type is ContentType.NONE and not api:
-        content_type = ContentType.STREAMING_PRICING
+    if content_type is ContentType.NONE:
+        raise ValueError(f"Cannot create omm stream with content_type: {ContentType.NONE}")
 
-    else:
-        content_type = get_valid_content_type(content_type, api)
+    api_type = APIType.STREAMING_CUSTOM if api else api_type_by_content_type.get(content_type, APIType.STREAMING_CUSTOM)
 
-    api_type = api_type_by_content_type.get(content_type, APIType.STREAMING_CUSTOM)
-    details = StreamDetails(content_type, ProtocolType.OMM, api_type, api)
     if not service:
         service = session.config.get(service_config_key_by_api_type.get(api_type))
 
-    stream_id = next(session._omm_stream_counter)
     stream = _OMMStream(
-        stream_id=stream_id,
+        stream_id=next(session._omm_stream_counter),
         session=session,
         name=name,
         domain=domain,
         service=service,
         fields=fields,
         key=key,
         extended_params=extended_params,
         on_refresh=on_refresh,
         on_status=on_status,
         on_update=on_update,
         on_complete=on_complete,
         on_error=on_error,
         on_ack=on_ack,
-        details=details,
+        details=StreamDetails(content_type, ProtocolType.OMM, api_type, api),
     )
     logger().debug(f" + Created stream: {stream.classname}")
     return stream
 
 
 def create_rdp_stream(
     content_type: ContentType,
@@ -187,92 +158,85 @@
     service: OptStr = None,
     api: str = "",
     on_ack: OptCall = None,
     on_response: OptCall = None,
     on_update: OptCall = None,
     on_alarm: OptCall = None,
 ) -> _RDPStream:
-    content_type = get_valid_content_type(content_type, api)
-    api_type = api_type_by_content_type.get(content_type, APIType.STREAMING_CUSTOM)
+    if content_type is ContentType.NONE:
+        raise ValueError(f"Cannot create rdp stream with content_type: {ContentType.NONE}")
+
+    api_type = APIType.STREAMING_CUSTOM if api else api_type_by_content_type.get(content_type, APIType.STREAMING_CUSTOM)
+
     if not service:
         service = session.config.get(service_config_key_by_api_type.get(api_type))
 
-    details = StreamDetails(content_type, ProtocolType.RDP, api_type, api)
-    stream_id = next(session._rdp_stream_counter)
     stream = _RDPStream(
-        stream_id=stream_id,
+        stream_id=next(session._rdp_stream_counter),
         session=session,
         service=service,
         universe=universe,
         view=view,
         parameters=parameters,
         extended_params=extended_params,
         on_ack=on_ack,
         on_response=on_response,
         on_update=on_update,
         on_alarm=on_alarm,
-        details=details,
+        details=StreamDetails(content_type, ProtocolType.RDP, api_type, api),
     )
     logger().debug(f" + Created stream: {stream.classname}")
     return stream
 
 
 def get_protocol_type_by_name(protocol_name: str) -> ProtocolType:
     protocol_type = protocol_type_by_name.get(protocol_name)
 
     if not protocol_type:
         raise ValueError(f"Can't find protocol type by name: {protocol_name}")
 
     return protocol_type
 
 
-cxn_class_by_protocol_type: Dict[
-    ProtocolType,
-    Type[Union[OMMStreamConnection, RDPStreamConnection]],
-] = {
+cxn_class_by_protocol_type: Dict[ProtocolType, Type[Union[OMMStreamConnection, RDPStreamConnection]]] = {
     ProtocolType.OMM: OMMStreamConnection,
     ProtocolType.OMM_OFF_CONTRIB: OffStreamContribConnection,
     ProtocolType.RDP: RDPStreamConnection,
 }
 
 
 def load_config(details: StreamDetails, session: "Session") -> "StreamCxnConfig":
-    content_type = details.content_type
+    api_type = details.api_type
 
-    if content_type is ContentType.STREAMING_CUSTOM:
+    if api_type is APIType.STREAMING_CUSTOM:
         api_config_key = details.api_config_key
 
         if not api_config_key:
-            raise ValueError(
-                "For ContentType.STREAMING_CUSTOM, api_config_key cannot be None"
-            )
+            raise ValueError("For APIType.STREAMING_CUSTOM, api_config_key cannot be None")
+
+        sess_config = session.config
 
-        file_config = get_config()
         if not api_config_key.startswith("apis."):
             api_config_key = f"apis.{api_config_key}"
+
         if api_config_key.endswith(".path"):
             api_config_key = api_config_key[:-5]
 
-        if not file_config.get(api_config_key):
-            raise ValueError(
-                f"Not an existing path {api_config_key} to url into config file"
-            )
+        if not sess_config.get(api_config_key):
+            raise ValueError(f"Path to url {api_config_key} does not exist in the config")
+
         end_word = api_config_key.rsplit(".", 1)[-1]
         if not end_word or end_word == "endpoints":
-            raise ValueError(
-                f"Not a valid format, use `apis.streaming.xxx.endpoints.xxx"
-            )
-        api_type = api_config_key
+            raise ValueError(f"Not a valid format, use `apis.streaming.xxx.endpoints.xxx")
 
     else:
-        api_type = api_type_by_content_type.get(content_type)
-        api_config_key = api_config_key_by_api_type.get(api_type)
+        api_config_key = get_default_config_path(api_type)
 
     config: "StreamCxnConfig" = get_cxn_config(api_config_key, session)
-    logger().debug(f"Loaded config for {api_type}, {config}")
+    logger().debug(f"Loaded config for {api_type}, config key is {api_config_key}, {config}")
     release_cxn_cfg_provider(session)
     return config
 
 
 def create_stream_cxn(details: StreamDetails, session: "Session") -> "StreamConnection":
     content_type = details.content_type
     protocol_type = details.protocol_type
@@ -287,32 +251,84 @@
         session=session,
         config=config,
     )
     logger().debug(f" + Created: \n\tcxn    : {cxn}\n\tconfig : {config}")
     return cxn
 
 
+CONTRIB_SECTION_KEY = "apis.streaming.contrib.endpoints.main"
+
+
 def create_offstream_contrib(
     session: "Session",
     name: str,
     api: OptStr = None,
     domain: OptStr = None,
     service: OptStr = None,
 ) -> "_OffStreamContrib":
     from .contrib._offstream import _OffStreamContrib
 
     content_type = ContentType.STREAMING_OFF_CONTRIB
+    contrib_section = session.config.get(CONTRIB_SECTION_KEY)
+
+    api_type = APIType.STREAMING_CUSTOM
+    if contrib_section and not api:
+        api = CONTRIB_SECTION_KEY
+
+    elif not contrib_section and not api:
+        api_type = api_type_by_content_type.get(content_type, APIType.STREAMING_CUSTOM)
+
     stream = _OffStreamContrib(
         post_id=next(session._contrib_post_id_counter),
         session=session,
         name=name,
-        details=StreamDetails(
-            content_type,
-            ProtocolType.OMM_OFF_CONTRIB,
-            api_type_by_content_type[content_type],
-            api,
-        ),
+        details=StreamDetails(content_type, ProtocolType.OMM_OFF_CONTRIB, api_type, api),
         service=service,
         domain=domain,
     )
     logger().debug(f" + Created offstream contrib={stream.classname}")
     return stream
+
+
+def create_dictionary_stream(
+    content_type: ContentType,
+    session: "Session",
+    name: str,
+    api: OptStr = None,
+    domain: OptStr = None,
+    service: OptStr = None,
+    key: OptDict = None,
+    extended_params: "ExtendedParams" = None,
+    on_refresh: OptCall = None,
+    on_status: OptCall = None,
+    on_update: OptCall = None,
+    on_complete: OptCall = None,
+    on_error: OptCall = None,
+) -> "PrvDictionaryStream":
+    from .metadata import PrvDictionaryStream
+
+    if content_type is ContentType.NONE and not api:
+        content_type = ContentType.STREAMING_DICTIONARY
+
+    api_type = api_type_by_content_type.get(content_type, APIType.STREAMING_CUSTOM)
+    details = StreamDetails(content_type, ProtocolType.OMM, api_type, api)
+    if not service:
+        service = session.config.get(service_config_key_by_api_type.get(api_type))
+
+    stream_id = next(session._omm_stream_counter)
+    stream = PrvDictionaryStream(
+        stream_id=stream_id,
+        session=session,
+        name=name,
+        details=details,
+        domain=domain,
+        service=service,
+        key=key,
+        extended_params=extended_params,
+        on_refresh=on_refresh,
+        on_status=on_status,
+        on_update=on_update,
+        on_complete=on_complete,
+        on_error=on_error,
+    )
+    logger().debug(f" + Created dictionary stream: {stream.classname}")
+    return stream
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/_stream_listener.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/_stream_listener.py`

 * *Files 2% similar despite different names*

```diff
@@ -24,23 +24,19 @@
     error(f"{self._classname} on_{event} {listener} raised exception: {exc!r}")
     debug(f"{traceback.format_exc()}")
 
     if DEBUG:
         raise exc
 
 
-def on_listener_error(
-    self, event: "StreamEvent", listener: Callable, exc: Exception
-) -> None:
+def on_listener_error(self, event: "StreamEvent", listener: Callable, exc: Exception) -> None:
     __on_listener_error(self.error, self.debug, self, event, listener, exc)
 
 
-def _on_listener_error(
-    self, event: "StreamEvent", listener: Callable, exc: Exception
-) -> None:
+def _on_listener_error(self, event: "StreamEvent", listener: Callable, exc: Exception) -> None:
     __on_listener_error(self._error, self._debug, self, event, listener, exc)
 
 
 def make_on_listener_error(self):
     if isinstance(self, _LogReporter):
         func = partial(_on_listener_error, self)
     elif isinstance(self, LogReporter):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/base_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/base_stream.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_funcs.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_funcs.py`

 * *Files 1% similar despite different names*

```diff
@@ -57,14 +57,15 @@
     Returns
     ----------
     ContribResponse
 
     Examples
     --------
     Prerequisite: The contrib_session must be opened
+
     >>> import refinitiv.data as rd
     >>> def on_ack_callback(ack_msg, stream):
     ...     print("Receive Ack response:", ack_msg)
     >>> def on_error_callback(error_msg, stream):
     ...     print("Receive Error:", error_msg)
     >>> update = {
     ...     "ASK": 1.23,
@@ -148,14 +149,15 @@
     Returns
     ----------
     ContribResponse
 
     Examples
     --------
     Prerequisite: The contrib_session must be opened.
+
     >>> import refinitiv.data as rd
     >>> def on_ack_callback(ack_msg, stream):
     ...     print("Receive Ack response:", ack_msg)
     >>> def on_error_callback(error_msg, stream):
     ...     print("Receive Error:", error_msg)
     >>> update = {
     ...     "ASK": 1.23,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_offstream.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_offstream.py`

 * *Files 1% similar despite different names*

```diff
@@ -39,26 +39,22 @@
     @cached_property
     def _contributed(self) -> OrEvent:
         return OrEvent(self._error_event, self._ack_event)
 
     def get_next_post_id(self) -> int:
         return self.post_id
 
-    def get_contrib_message(
-        self, fields: dict, contrib_type: Union[str, "ContribType", None]
-    ) -> dict:
+    def get_contrib_message(self, fields: dict, contrib_type: Union[str, "ContribType", None]) -> dict:
         return {
             "Ack": True,
             "ID": LOGIN_STREAM_ID,
             "Message": {
                 "Fields": fields,
                 "ID": 0,
-                "Type": contrib_type_enum_arg_parser.get_str(
-                    contrib_type if contrib_type else "Update"
-                ),
+                "Type": contrib_type_enum_arg_parser.get_str(contrib_type if contrib_type else "Update"),
                 "Domain": self.domain,
             },
             "PostID": self.post_id,
             "Type": "Post",
             "Key": {"Name": self.name, "Service": self.service},
             "Domain": self.domain,
         }
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/contrib/_response.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/contrib/_response.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/event.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/event.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/omm_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/omm_stream.py`

 * *Files 1% similar despite different names*

```diff
@@ -89,15 +89,15 @@
         self._service = service
         self._fields = fields
         self._extended_params = extended_params
 
     @cached_property
     def _stream(self) -> _OMMStream:
         return create_omm_stream(
-            ContentType.NONE,
+            ContentType.STREAMING_OMM,
             api=self._api,
             session=self._session,
             name=self._name,
             domain=self._domain,
             service=self._service,
             fields=self._fields,
             extended_params=self._extended_params,
@@ -441,17 +441,15 @@
         >>> definition = omm_stream.Definition("EUR")
         >>> stream = definition.get_stream()
         >>> stream.on_ack(lambda event, stream: display_response(stream, 'ack', event))
         >>>
         >>> stream.open()
         >>> response = stream.contribute({"ASK": 123, "BID": 125})
         """
-        return self._stream.contribute(
-            fields, contrib_type=contrib_type, post_user_info=post_user_info
-        )
+        return self._stream.contribute(fields, contrib_type=contrib_type, post_user_info=post_user_info)
 
     async def contribute_async(
         self,
         fields: dict,
         contrib_type: Union[str, "ContribType", None] = None,
         post_user_info: Optional[dict] = None,
     ) -> "ContribResponse":
@@ -491,10 +489,8 @@
         >>> definition = omm_stream.Definition("EUR")
         >>> stream = definition.get_stream()
         >>> stream.on_ack(lambda event, stream: display_response(stream, 'ack', event))
         >>>
         >>> stream.open()
         >>> response = await stream.contribute_async({"ASK": 123, "BID": 125})
         """
-        return await self._stream.contribute_async(
-            fields, contrib_type=contrib_type, post_user_info=post_user_info
-        )
+        return await self._stream.contribute_async(fields, contrib_type=contrib_type, post_user_info=post_user_info)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/omm_stream_connection.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/omm_stream_connection.py`

 * *Files 4% similar despite different names*

```diff
@@ -28,17 +28,15 @@
             key["NameType"] = "AuthnToken"
             if access_token:
                 key["Elements"]["AuthenticationToken"] = access_token
 
         ####
 
         key["Elements"]["ApplicationId"] = dacs_params.dacs_application_id
-        key["Elements"]["Position"] = dacs_params.dacs_position or "/".join(
-            self._get_socket_info()
-        )
+        key["Elements"]["Position"] = dacs_params.dacs_position or "/".join(self._get_socket_info())
 
         login_message = {
             "Domain": "Login",
             "ID": LOGIN_STREAM_ID,
             "Key": key,
         }
         return login_message
@@ -93,18 +91,15 @@
         stream_state = state.get("Stream")
 
         if stream_state == "Open":
             self._state = StreamCxnState.MessageProcessing
             self._connection_result_ready.set()
 
         elif stream_state == "Closed":
-            self.debug(
-                f"{self._classname} received a closing message: "
-                f"state={self.state}, message={message}"
-            )
+            self.debug(f"{self._classname} received a closing message: state={self.state}, message={message}")
             self._state = StreamCxnState.Disconnected
             not self.can_reconnect and self._connection_result_ready.set()
             self._config.info_not_available()
             self._listener.close()
 
         else:
             state_code = state.get("Code", "")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/omm_stream_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/omm_stream_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/proxy_info.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/proxy_info.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 from urllib.request import getproxies
 
 from ..._tools import parse_url
 
 
-class ProxyInfo(object):
-
+class ProxyInfo:
     __proxies_info = None
     __no_proxy = None
 
     @classmethod
     def get_proxies_info(cls):
         if cls.__proxies_info is None:
             cls.__proxies_info = {}
@@ -16,17 +15,15 @@
             for scheme, proxy_data in proxies.items():
                 if scheme == "no":
                     cls.__no_proxy = proxy_data.split(",")
                     if not cls.__no_proxy:
                         # if no_proxy is empty, rely on default value (websocket._url.DEFAULT_NO_PROXY_HOST)
                         cls.__no_proxy = None
                 else:
-                    cls.__proxies_info[scheme] = ProxyInfo.proxy_info_from_url(
-                        proxy_data
-                    )
+                    cls.__proxies_info[scheme] = ProxyInfo.proxy_info_from_url(proxy_data)
 
         return cls.__proxies_info
 
     @classmethod
     def get_no_proxy(cls):
         # call cls.get_proxies_info() to be sure class attr were initialized
         cls.get_proxies_info()
@@ -52,27 +49,20 @@
         proxy_user: The username used to authenticate with the proxy server.
         proxy_pass: The password used to authenticate with the proxy server.
         """
         if isinstance(proxy_user, bytes):
             proxy_user = proxy_user.decode()
         if isinstance(proxy_pass, bytes):
             proxy_pass = proxy_pass.decode()
-        (
-            self._proxy_type,
-            self._proxy_host,
-            self._proxy_port,
-            self._proxy_user,
-            self._proxy_pass,
-        ) = (
-            proxy_type,
-            proxy_host,
-            proxy_port,
-            proxy_user,
-            proxy_pass,
-        )
+
+        self._proxy_type = proxy_type
+        self._proxy_host = proxy_host
+        self._proxy_port = proxy_port
+        self._proxy_user = proxy_user
+        self._proxy_pass = proxy_pass
 
     @property
     def type(self):
         return self._proxy_type
 
     @property
     def host(self):
@@ -89,21 +79,18 @@
     @property
     def password(self):
         return self._proxy_pass
 
     @property
     def auth(self):
         if self._proxy_user and self._proxy_pass:
-            return tuple(self._proxy_user, self._proxy_pass)
+            return self._proxy_user, self._proxy_pass
 
     def __repr__(self):
-        return (
-            f"<ProxyInfo type={self.type} host={self.host}:{self.port} "
-            + f" user={self.user} pass={self.password}>"
-        )
+        return f"<ProxyInfo type={self.type} host={self.host}:{self.port} user={self.user} pass={self.password}>"
 
     def proxy_json_info(self):
         return {
             "type": self._proxy_type,
             "host": self._proxy_host,
             "port": self._proxy_port,
             "user": self._proxy_user,
@@ -139,11 +126,11 @@
             except KeyError:
                 pass
 
         pi = ProxyInfo(
             proxy_type=method,
             proxy_host=host,
             proxy_port=port,
-            proxy_user=username or None,
-            proxy_pass=password or None,
+            proxy_user=username,
+            proxy_pass=password,
         )
         return pi
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rdp_stream.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rdp_stream.py`

 * *Files 1% similar despite different names*

```diff
@@ -88,15 +88,15 @@
         self._parameters = parameters
         self._api = api
         self._extended_params = extended_params
 
     @cached_property
     def _stream(self) -> _RDPStream:
         return create_rdp_stream(
-            ContentType.STREAMING_CUSTOM,
+            ContentType.STREAMING_RDP,
             api=self._api,
             session=self._session,
             service=self._service,
             universe=self._universe,
             view=self._view,
             parameters=self._parameters,
             extended_params=self._extended_params,
@@ -213,17 +213,15 @@
         >>> stream.on_ack(display_response)
         >>>
         >>> stream.open()
         """
         self._stream.on_ack(make_callback(on_ack))
         return self
 
-    def on_response(
-        self, on_response: Callable[[dict, "RDPStream"], Any]
-    ) -> "RDPStream":
+    def on_response(self, on_response: Callable[[dict, "RDPStream"], Any]) -> "RDPStream":
         """
         This function called when the stream received an response message.
 
         Parameters
         ----------
         on_response : Callable, optional
              Callable object to process retrieved response data
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rdp_stream_connection.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rdp_stream_connection.py`

 * *Files 5% similar despite different names*

```diff
@@ -48,18 +48,15 @@
 
         # "OK" for qps and "Ok" for tds
         if status == "OK" or status == "Ok":
             self._state = StreamCxnState.MessageProcessing
             self._connection_result_ready.set()
 
         elif status == "Closed" or status == "Error" or code == requests.codes.bad:
-            self.debug(
-                f"{self._classname} received a bad message: "
-                f"state={self.state}, message={message}"
-            )
+            self.debug(f"{self._classname} received a bad message: state={self.state}, message={message}")
             self._state = StreamCxnState.Disconnected
             not self.can_reconnect and self._connection_result_ready.set()
             self._config.info_not_available()
             self._listener.close()
 
         else:
             raise ValueError(
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rdp_stream_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rdp_stream_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/conversion.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/conversion.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,22 +4,17 @@
 
 
 def json_marketprice_msg_to_ema(msg, rdm_dict):
     streaming = msg.get("Streaming", True)
     service_name = msg["Key"].get("Service", "ELEKTRON_DD")
     name = msg["Key"]["Name"]
 
-    ema_msg = (
-        ReqMsg().name(name).service_name(service_name).interest_after_refresh(streaming)
-    )
+    ema_msg = ReqMsg().name(name).service_name(service_name).interest_after_refresh(streaming)
 
     if msg.get("View"):
         # WARNING: Non-existent fields will be skipped
         field_ids = [rdm_dict[item] for item in msg["View"] if item in rdm_dict]
         view_data = IntArray(field_ids).complete()
         ema_msg.payload(
-            ElementList()
-            .add_uint(ENAME_VIEW_TYPE, VT_FIELD_ID_LIST)
-            .add_array(ENAME_VIEW_DATA, view_data)
-            .complete()
+            ElementList().add_uint(ENAME_VIEW_TYPE, VT_FIELD_ID_LIST).add_array(ENAME_VIEW_DATA, view_data).complete()
         )
     return ema_msg
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/ema.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/ema.py`

 * *Files 19% similar despite different names*

```diff
@@ -23,33 +23,22 @@
     login_msg_kwargs = dict(
         application_id=login_msg["Key"]["Elements"]["ApplicationId"],
         position=login_msg["Key"]["Elements"]["Position"],
     )
     if login_msg["Key"].get("NameType"):
         login_msg_kwargs["name_type"] = name_type_map.get(login_msg["Key"]["NameType"])
         if login_msg_kwargs["name_type"] == ema.USER_AUTH_TOKEN:
-            login_msg_kwargs["name"] = login_msg["Key"]["Elements"][
-                "AuthenticationToken"
-            ]
+            login_msg_kwargs["name"] = login_msg["Key"]["Elements"]["AuthenticationToken"]
     if "name" not in login_msg_kwargs:
         login_msg_kwargs["name"] = login_msg["Key"]["Name"]
     return login_msg_kwargs
 
 
-def ema_login_message(
-    application_id, position, name, name_type=ema.USER_NAME
-) -> ReqMsg:
-    return (
-        LoginReq()
-        .name(name)
-        .name_type(name_type)
-        .position(position)
-        .application_id(application_id)
-        .get_message()
-    )
+def ema_login_message(application_id, position, name, name_type=ema.USER_NAME) -> ReqMsg:
+    return LoginReq().name(name).name_type(name_type).position(position).application_id(application_id).get_message()
 
 
 severity_map = {
     "verbose": ema.LoggerSeverity.Verbose.value,
     "success": ema.LoggerSeverity.Success.value,
     "warning": ema.LoggerSeverity.Warning.value,
     "error": ema.LoggerSeverity.Error.value,
@@ -80,29 +69,29 @@
     config_map = Map()
     inner_map = Map()
     element_list = ElementList()
 
     element_list.add_ascii("DefaultConsumer", "Cons_Token")
     inner_map.add_key_ascii(
         "Cons_Token",
-        MapEntry.AddEnum,
+        MapEntry.Add,
         ElementList()
         .add_ascii("Channel", "Chan_Token")
         .add_ascii("Logger", "Logger_1")
         .add_ascii("Dictionary", "Dictionary_1")
         .add_uint("XmlTraceToStdout", 0)
         .complete(),
     ).complete()
 
     element_list.add_map("ConsumerList", inner_map)
 
     element_list.complete()
     inner_map.clear()
 
-    config_map.add_key_ascii("ConsumerGroup", MapEntry.AddEnum, element_list)
+    config_map.add_key_ascii("ConsumerGroup", MapEntry.Add, element_list)
     element_list.clear()
 
     if IP_PATTERN.match(host):
         channel_type = ema.RSSL_CONN_TYPE_SOCKET
     else:
         channel_type = ema.RSSL_CONN_TYPE_ENCRYPTED
     channel_element_list = (
@@ -115,41 +104,39 @@
         .add_uint("EncryptedProtocolType", ema.RSSL_CONN_TYPE_SOCKET.value)
     )
 
     if host is not None:
         channel_element_list.add_ascii("Host", host)
     if port is not None:
         channel_element_list.add_uint("Port", port)
-    inner_map.add_key_ascii(
-        "Chan_Token", MapEntry.AddEnum, channel_element_list.complete()
-    ).complete()
+    inner_map.add_key_ascii("Chan_Token", MapEntry.Add, channel_element_list.complete()).complete()
 
     element_list.add_map("ChannelList", inner_map)
 
     element_list.complete()
     inner_map.clear()
 
-    config_map.add_key_ascii("ChannelGroup", ema.MapEntry.AddEnum, element_list)
+    config_map.add_key_ascii("ChannelGroup", ema.MapEntry.Add, element_list)
     element_list.clear()
 
     inner_map.add_key_ascii(
         "Logger_1",
-        ema.MapEntry.AddEnum,
+        ema.MapEntry.Add,
         ema.ElementList()
         .add_enum("LoggerType", ema.LoggerType.Stdout.value)
         .add_enum("LoggerSeverity", logger_severity_from_env())
         .complete(),
     ).complete()
 
     element_list.add_map("LoggerList", inner_map)
 
     element_list.complete()
     inner_map.clear()
 
-    config_map.add_key_ascii("LoggerGroup", ema.MapEntry.AddEnum, element_list)
+    config_map.add_key_ascii("LoggerGroup", ema.MapEntry.Add, element_list)
     element_list.clear()
 
     if field_dict_path and enumtype_path:
         dict_elist = (
             ema.ElementList()
             .add_enum("DictionaryType", ema.DictionaryType.File.value)
             .add_ascii("RdmFieldDictionaryFileName", field_dict_path)
@@ -168,19 +155,19 @@
             .add_enum(
                 "DictionaryType",
                 ema.DictionaryType.Channel.value,
             )
             .complete()
         )
 
-    inner_map.add_key_ascii("Dictionary_1", ema.MapEntry.AddEnum, dict_elist).complete()
+    inner_map.add_key_ascii("Dictionary_1", ema.MapEntry.Add, dict_elist).complete()
 
     element_list.add_map("DictionaryList", inner_map)
 
     element_list.complete()
 
-    config_map.add_key_ascii("DictionaryGroup", ema.MapEntry.AddEnum, element_list)
+    config_map.add_key_ascii("DictionaryGroup", ema.MapEntry.Add, element_list)
     element_list.clear()
 
     config_map.complete()
 
     return config_map
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/rwf/socket.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/rwf/socket.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,26 +1,40 @@
 import datetime
 import socket
 import time
 import logging
 from collections import namedtuple
-from typing import Callable, Optional, Dict
+from typing import Callable, Optional, Dict, Tuple
 
 from refinitiv.data._core.log_reporter import LogReporter
 from refinitiv.data._core.session import Session
 from refinitiv.data._errors import SessionError
 
+
+def _get_version(version: str):
+    return tuple(map(int, version.split(".")[:3]))
+
+
+try:
+    from ema import __version__ as _ema_version
+
+    _ema_version = _get_version(_ema_version)
+except ImportError as e:
+    _ema_version = (0, 0, 0)
+
+_EMA_VERSION_MIN = (0, 5, 0)
+_EMA_VERSION_MAX = (0, 6, 0)
+
 try:
     from ema import (
         OmmConsumer,
         OmmConsumerConfig,
         AppClient,
         CustomLoggerClient,
         LoggerSeverity,
-        MapEntry,
         Msg,
         OmmConsumerEvent,
         RefreshMsg,
         LoginRefresh,
         EmaOmmInvalidHandleException,
         EmaOmmInvalidUsageException,
     )  # noqa
@@ -30,15 +44,14 @@
     EMA_INSTALLED = False
     _exc_info = e
 else:
     from .conversion import json_marketprice_msg_to_ema
     from .ema import (
         ema_login_message,
         create_programmatic_cfg,
-        name_type_map,
         generate_login_msg,
     )
 
 
 if EMA_INSTALLED:
 
     class EmaPythonLogger(CustomLoggerClient):
@@ -80,14 +93,22 @@
         python_logger: bool = True,
     ):
         LogReporter.__init__(self, logger=session.logger())
         if not EMA_INSTALLED:
             self.debug(f"EMA not installed, message: {_exc_info}")
             raise ImportError("You need to install refinitiv-ema to use RwfSocketApp")
 
+        if not (_EMA_VERSION_MIN <= _ema_version < _EMA_VERSION_MAX):
+            raise ImportError(
+                f"Incompatible refinitiv-ema version installed. "
+                f"Current: {_ema_version or 'unknown'}. "
+                f"Required: {_EMA_VERSION_MIN} .. {_EMA_VERSION_MAX}"
+            )
+        else:
+            self.debug(f"Found compatible refinitiv-ema=={_ema_version}")
         self.host = host
         self.port = port
 
         self.field_dict_path = field_dict_path
         self.enumtype_path = enumtype_path
         self.on_open = on_open
         self.on_message = on_message
@@ -177,29 +198,25 @@
                 self.login_client,
                 self.ema_logger,
             )
         except EmaOmmInvalidUsageException as e:
             self.close()
             raise SessionError(
                 -1,
-                "Error establishing connection to RSSL endpoint. "
-                "Check the logs for more details",
+                "Error establishing connection to RSSL endpoint. Check the logs for more details",
             ) from e
 
     def _handle_reissue(self, msg):
         reissue_success = False
         if self._reissue_handle is not None and self._reissue_timestamp >= time.time():
             try:
                 admin_msg = ema_login_message(**generate_login_msg(msg))
                 self.consumer.reissue(admin_msg, self._reissue_handle)
                 reissue_success = True
-                self.info(
-                    f"Sent reissue with handle {self._reissue_handle}"
-                    f" and timestamp {self._reissue_timestamp}"
-                )
+                self.info(f"Sent reissue with handle {self._reissue_handle} and timestamp {self._reissue_timestamp}")
             except EmaOmmInvalidHandleException:
                 self.warning(
                     f"Reissue failed: reissue was valid till "
                     f"{datetime.datetime.fromtimestamp(self._reissue_timestamp)}.\n"
                     f"This is likely because connection was broken "
                     f"for more than 10 minutes."
                 )
@@ -246,14 +263,15 @@
             else:
                 self.debug(f"Closing stream {msg['ID']}")
                 self.consumer.unregister(self.handles[msg["ID"]].handle)
                 del self.handles[msg["ID"]]
         else:
             raise ValueError(f"Unknown message type: {msg_type}")
 
-    def get_socket_info(self):
+    def get_socket_info(self) -> Tuple[str, str]:
+        ip, hostname = "127.0.0.1", "net"
         try:
             hostname = socket.gethostname()
-            ip_addr = f"{socket.gethostbyname(hostname)}/{hostname}"
+            ip = socket.gethostbyname(hostname)
         except socket.gaierror:
-            return "127.0.0.1/net"
-        return ip_addr
+            pass
+        return ip, hostname
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_cache.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_cache.py`

 * *Files 2% similar despite different names*

```diff
@@ -60,19 +60,15 @@
     #  Make StreamCache iterable                      #
     ###################################################
 
     def __iter__(self):
         return StreamCacheIterator(self)
 
     def __getitem__(self, field):
-        if (
-            self._record
-            and self._record.get("Fields")
-            and field in self._record["Fields"].keys()
-        ):
+        if self._record and self._record.get("Fields") and field in self._record["Fields"].keys():
             return self._record["Fields"][field]
         raise KeyError(f"Field '{field}' not in Stream cache")
 
     def __len__(self):
         return len(self._fields)
 
     def __repr__(self):
@@ -84,25 +80,19 @@
             }
         )
         return rep
 
     def __str__(self):
         if self.service:
             return str(
-                "|".join([self.service, self.name])
-                + "["
-                + ",".join([f"{f}:{v}" for f, v in self.items()])
-                + "]"
+                "|".join([self.service, self.name]) + "[" + ",".join([f"{f}:{v}" for f, v in self.items()]) + "]"
             )
         else:
             return str(
-                "|".join(["Unknown service", self.name])
-                + "["
-                + ",".join([f"{f}:{v}" for f, v in self.items()])
-                + "]"
+                "|".join(["Unknown service", self.name]) + "[" + ",".join([f"{f}:{v}" for f, v in self.items()]) + "]"
             )
 
     ###################################################
     #  StreamCache properties                         #
     ###################################################
 
     @property
@@ -128,19 +118,15 @@
         return True if self._status.get("Data") == "Ok" else False
 
     # ###################################################
     # #  StreamingCache data accessors                  #
     # ###################################################
 
     def get_field_value(self, field):
-        if (
-            self._record
-            and self._record.get("Fields")
-            and field in self._record["Fields"].keys()
-        ):
+        if self._record and self._record.get("Fields") and field in self._record["Fields"].keys():
             return self._record["Fields"][field]
 
     def get_fields(self, fields=None) -> dict:
         if not self._record:
             return dict.fromkeys(fields or self._fields)
 
         if not fields:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_connection.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_connection.py`

 * *Files 3% similar despite different names*

```diff
@@ -9,23 +9,22 @@
 
 from .event import StreamCxnEvent
 from .rwf.socket import RwfSocketClient
 from .stream_cxn_state import StreamCxnState
 from .ws.ws_client import WebSocketClient
 from ..._core.log_reporter import LogReporter
 from ..._core.session.tools import get_delays, is_desktop_session
-from ..._tools import DEBUG, CallbackHandler
+from ..._tools import DEBUG, CallbackHandler, lazy_dump, lazy_formatting
 from ...usage_collection import LogRecord, RecordData, FilterType, get_usage_logger
 
 if TYPE_CHECKING:
     from ._stream_cxn_config_data import StreamCxnConfig
     from ..._core.session import Session
 
 LOGIN_STREAM_ID = 2
-MAX_LISTENERS = 2000
 
 
 class StreamConnection(threading.Thread, LogReporter):
     _listener: Optional[Union[WebSocketClient, RwfSocketClient]] = None
     _num_attempt = 0
 
     def __init__(
@@ -48,21 +47,19 @@
         self._start_connecting = threading.Event()
         self._connection_result_ready = threading.Event()
         self._listener_created = threading.Event()
         self._timer = threading.Event()
 
         self.usage_counter = defaultdict(Counter)
         self._usage_close_event = threading.Event()
-        self._usage_report_thread = threading.Thread(
-            target=self._usage_callback, args=(10,)
-        )
+        self._usage_report_thread = threading.Thread(target=self._usage_callback, args=(10,))
         self._usage_lock = threading.Lock()
         self._dispose_lock = threading.Lock()
 
-        self._emitter = CallbackHandler(max_listeners=MAX_LISTENERS)
+        self._emitter = CallbackHandler()
         self._msg_queue: Optional[queue.Queue] = None
         self._msg_processor: Optional[threading.Thread] = None
         self._delays = get_delays()
 
         self._classname = f"[{name}]"
 
     @property
@@ -136,17 +133,15 @@
             self.debug(f"{self._classname} cannot wait connection result, {self.state}")
             return
 
         self._connection_result_ready.wait()
 
     def run(self) -> None:
         self._msg_queue = queue.Queue()
-        self._msg_processor = threading.Thread(
-            target=self._process_messages, name=f"Msg-Proc-{self.name}", daemon=True
-        )
+        self._msg_processor = threading.Thread(target=self._process_messages, name=f"Msg-Proc-{self.name}", daemon=True)
         self._usage_report_thread.start()
         self._msg_processor.start()
 
         once = False
         self._start_connecting.set()
         while not once or self.can_reconnect:
             once = True
@@ -179,18 +174,15 @@
                 self._start_connecting.wait()
                 self._is_emit_reconnected = True
 
                 try:
                     self._config.next_available_info()
                 except StopIteration:
                     delay = self._delays.next()
-                    self.debug(
-                        f"{self._classname} tried all infos, "
-                        f"waiting time {delay} secs until the next attempt."
-                    )
+                    self.debug(f"{self._classname} tried all infos, waiting time {delay} secs until the next attempt.")
                     self._timer.wait(delay)
                     self._num_attempt += 1
 
                 url = self._config.url
                 self.debug(f"{self._classname} try to reconnect over url {url}")
 
         self.dispose()
@@ -346,79 +338,71 @@
         pass
 
     def send_login_message(self) -> None:
         self.send_message(self.get_login_message())
 
     def send_message(self, message: dict) -> bool:
         if self.is_message_processing:
-            self.debug(f"{self._classname} send {message}")
+            self.debug(f"{self._classname} send %s", lazy_dump(message))
             self._listener.send(message)
             return True
         else:
-            self.debug(
-                f"{self._classname} cannot send message: "
-                f"state={self.state}, message={message}"
-            )
+            self.debug(f"{self._classname} cannot send message: state={self.state}, message=%s", lazy_dump(message))
             return False
 
     def on(self, event: str, listener: Callable) -> None:
         if self.is_disposed:
             self.debug(f"{self._classname} cannot on, {self.state}")
             return
 
         self._emitter.on(event, listener)
 
     def remove_listener(self, event: str, listener: Callable) -> None:
         if self.is_disposed:
-            self.debug(
-                f"{self._classname} {self.state}, "
-                f"can not remove listener {event}, {listener}"
-            )
+            self.debug(f"{self._classname} {self.state}, can not remove listener {event}, {listener}")
             return
 
         self._emitter.remove_listener(event, listener)
 
     def _on_ws_open(self, ws: websocket.WebSocketApp) -> None:
         self.debug(f"{self._classname} on_ws_open")
         self._delays.reset()
         self._emitter.emit(StreamCxnEvent.CONNECTED, self)
         message = self.get_login_message()
         self.debug(f"{self._classname} send login message {message}")
         self._listener.send(message)
 
     def _on_message(self, messages: List[Dict]) -> None:
-        self.debug(f"{self._classname} on_ws_message {messages}")
+        self.debug(f"{self._classname} on_ws_message %s", lazy_dump(messages))
 
         if self.is_connecting:
             if len(messages) > 1:
-                raise ValueError(
-                    f"Cannot process messages more then one, num={len(messages)}"
-                )
+                raise ValueError(f"Cannot process messages more then one, num={len(messages)}")
 
             message = messages[0]
             self._handle_login_message(message)
 
             if self._is_emit_reconnected:
-                self.debug(f"Reconnecting is over, emit event Reconnected.")
+                self.debug("Reconnecting is over, emit event Reconnected.")
                 self._is_emit_reconnected = False
                 self._emitter.emit(StreamCxnEvent.RECONNECTED, self)
 
         elif self.is_message_processing:
             self._msg_queue.put(messages)
 
         elif self.is_disconnecting:
             # do nothing and wait for all streams to close
             pass
 
         else:
-            debug_msg = (
-                f"{self._classname}._on_ws_message() | "
-                f"don't know what to do state={self.state}, "
-                f"message={messages}"
+            debug_msg = lazy_formatting(
+                f"{self._classname}._on_ws_message() | don't know what to do state={self.state}, message=%s",
+                lazy_dump(messages),
             )
+
             if DEBUG:
                 raise ValueError(debug_msg)
 
             else:
                 self.debug(debug_msg)
 
     def _handle_login_message(self, message: dict):
@@ -437,17 +421,15 @@
 
             self._msg_queue.task_done()
 
     def _process_message(self, message: dict) -> None:
         # for override
         pass
 
-    def _on_ws_close(
-        self, ws: websocket.WebSocketApp, close_status_code: str, close_msg: str
-    ) -> None:
+    def _on_ws_close(self, ws: websocket.WebSocketApp, close_status_code: str, close_msg: str) -> None:
         self.debug(
             f"{self._classname} on_ws_close: "
             f"close_status_code={close_status_code}, close_msg={close_msg}, "
             f"state={self.state}"
         )
         self._emitter.emit(StreamCxnEvent.DISCONNECTED, self)
         if not self.is_disposed:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/stream_state_manager.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/stream_state_manager.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/_stream/ws/ws_client.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/_stream/ws/ws_client.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,26 +1,22 @@
-import json
+import simplejson as json
 import socket
 from typing import Callable, List, Dict, Tuple
 
 import websocket
 
 
 class WebSocketClient:
-    def __init__(
-        self, on_message: Callable[[List[Dict]], None], *args, **kwargs
-    ) -> None:
+    def __init__(self, on_message: Callable[[List[Dict]], None], *args, **kwargs) -> None:
         # set global timeout for WebSocketApp according documentation
         # https://websocket-client.readthedocs.io/en/latest/examples.html#setting-timeout-value
         websocket.setdefaulttimeout(5)
 
         self._on_message = on_message
-        self.ws = websocket.WebSocketApp(
-            on_message=self._on_ws_message, *args, **kwargs
-        )
+        self.ws = websocket.WebSocketApp(on_message=self._on_ws_message, *args, **kwargs)
 
     def _on_ws_message(self, ws: websocket.WebSocketApp, s: str) -> None:
         try:
             messages = json.loads(s)
         except UnicodeDecodeError:
             messages = "".join(map(chr, bytearray(s)))
             messages = json.loads(messages)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_buckets_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_buckets_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -45,22 +45,23 @@
     get_data(session=session)
         Returns a response to the data platform
     get_data_async(session=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> from refinitiv.data.delivery import cfs
-     >>> definition = cfs.buckets.Definition()
-     >>> buckets = definition.get_data()
+    >>> from refinitiv.data.delivery import cfs
+    >>> definition = cfs.buckets.Definition()
+    >>> buckets = definition.get_data()
 
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         name: Optional[str] = None,
         created_since: Optional[str] = None,
         modified_since: Optional[str] = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_cfs_data_provider.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_cfs_data_provider.py`

 * *Files 7% similar despite different names*

```diff
@@ -8,15 +8,15 @@
 from .._data._data_provider import (
     DataProvider,
     RequestFactory,
 )
 from .._data._response_factory import ResponseFactory
 
 if TYPE_CHECKING:
-    from .._data._data_provider import ParsedData
+    from .._data._response import Response
     from ..._core.session import Session
 
 
 # --------------------------------------------------------------------------------------
 #   Request factory
 # --------------------------------------------------------------------------------------
 
@@ -45,17 +45,15 @@
         return url
 
 
 class CFSStreamRequestFactory(RequestFactory):
     def get_url(self, *args, **kwargs):
         return super().get_url(*args, **kwargs) + "/{id}/stream"
 
-    def get_path_parameters(
-        self, session=None, *, path_parameters=None, id=None, **kwargs
-    ) -> dict:
+    def get_path_parameters(self, session=None, *, path_parameters=None, id=None, **kwargs) -> dict:
         path_parameters = path_parameters or {}
         if id:
             path_parameters["id"] = id
         return path_parameters
 
     def get_query_parameters(self, *_, **kwargs) -> list:
         query_parameters = kwargs.get("query_parameters") or []
@@ -65,37 +63,36 @@
 
 # --------------------------------------------------------------------------------------
 #   Response factory
 # --------------------------------------------------------------------------------------
 
 
 class CFSResponseFactory(ResponseFactory):
-    def create_data_success(
-        self, parsed_data: "ParsedData", session: "Session" = None, **kwargs
-    ):
-        raw = self.get_raw(parsed_data)
+    def create_data_success(self, raw: dict, owner_: "Response", session: "Session" = None, **kwargs):
         content_value = raw.get("value") or [raw]
-        return self.data_class(raw, IterObj(content_value, session, self.data_class))
+        return self.data_class(
+            raw=raw,
+            _owner=owner_,
+            _iter_object=IterObj(content_value, session, self.data_class),
+        )
 
 
 # --------------------------------------------------------------------------------------
 #   Data provider
 # --------------------------------------------------------------------------------------
 
 
 cfs_request_factory = CFSRequestFactory()
 cfs_buckets_data_provider = DataProvider(
     request=cfs_request_factory, response=CFSResponseFactory(data_class=BucketData)
 )
 cfs_file_sets_data_provider = DataProvider(
     request=cfs_request_factory, response=CFSResponseFactory(data_class=FileSetData)
 )
-cfs_files_data_provider = DataProvider(
-    request=cfs_request_factory, response=CFSResponseFactory(data_class=FileData)
-)
+cfs_files_data_provider = DataProvider(request=cfs_request_factory, response=CFSResponseFactory(data_class=FileData))
 cfs_packages_data_provider = DataProvider(
     request=CFSPackageRequestFactory(),
     response=CFSResponseFactory(data_class=PackageData),
 )
 cfs_stream_data_provider = DataProvider(request=CFSStreamRequestFactory())
 
 del cfs_request_factory
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_data_class.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_data_class.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_data_types.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_data_types.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,45 +1,45 @@
+from dataclasses import dataclass
+
 from pandas import DataFrame
 
 from ._tools import _get_query_parameter
 from .._data._endpoint_data import EndpointData
 
 
+@dataclass
 class BaseData(EndpointData):
-    def __init__(self, raw, iter_object=None, **kwargs):
-        EndpointData.__init__(self, raw, **kwargs)
-        self._raw["skip_token"] = _get_query_parameter(
-            "skipToken", self._raw.get("@nextLink", None)
-        )
-        self._iter_object = iter_object
-        self._dataframe = None
+    _iter_object: "IterObj" = None
+    _dataframe: "DataFrame" = None
+
+    def __post_init__(self):
+        self.raw["skip_token"] = _get_query_parameter("skipToken", self.raw.get("@nextLink", None))
 
     @property
     def df(self):
         if self._dataframe is None:
             value = self.raw.get("value") or [self.raw]
             columns = set()
             for i in value:
                 columns = columns | i.keys()
             columns = tuple(columns)
-            data = [
-                [value[key] if key in value else None for key in columns]
-                for value in value
-            ]
+            data = [[value[key] if key in value else None for key in columns] for value in value]
             self._dataframe = DataFrame(data, columns=columns)
 
         return self._dataframe
 
 
+@dataclass
 class BucketData(BaseData):
     @property
     def buckets(self):
         return self._iter_object
 
 
+@dataclass
 class FileSetData(BaseData):
     @property
     def file_sets(self):
         return self._iter_object
 
 
 class PackageData(BaseData):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_downloader.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_downloader.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_downloader_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_downloader_definition.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_downloader_facade.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_downloader_facade.py`

 * *Files 6% similar despite different names*

```diff
@@ -19,16 +19,14 @@
         return _FileDownloader(logger())
 
     @cached_property
     def _unpacker(self) -> _Unpacker:
         return _Unpacker(logger())
 
     def download(self, path="") -> "FileDownloader":
-        self._downloaded_filepath = self._downloader.download(
-            self._url, self._filename_ext, path
-        )
+        self._downloaded_filepath = self._downloader.download(self._url, self._filename_ext, path)
         return self
 
     def extract(self, path="") -> str:
         filepath = self._downloaded_filepath or self._filename_ext
         extracted_filepath = self._unpacker.unpack(filepath, path)
         return extracted_filepath
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_file_sets_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_file_sets_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -54,22 +54,23 @@
     get_data(session=session)
         Returns a response to the data platform
     get_data_async(session=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> from refinitiv.data.delivery import cfs
-     >>> definition = cfs.file_sets.Definition()
-     >>> file_sets = definition.get_data()
+    >>> from refinitiv.data.delivery import cfs
+    >>> definition = cfs.file_sets.Definition()
+    >>> file_sets = definition.get_data()
 
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         bucket: str,
         name: Optional[str] = None,
         attributes: Optional[dict] = None,
@@ -88,17 +89,15 @@
         created_since = _convert_date_time(created_since)
         modified_since = _convert_date_time(modified_since)
         available_from = _convert_date_time(available_from)
         available_to = _convert_date_time(available_to)
         content_from = _convert_date_time(content_from)
         content_to = _convert_date_time(content_to)
         if attributes is not None:
-            attributes = ",".join(
-                [f"{key}:{value}" for key, value in attributes.items()]
-            )
+            attributes = ",".join([f"{key}:{value}" for key, value in attributes.items()])
         super().__init__(
             data_type=DataType.CFS_FILE_SETS,
             bucket=bucket,
             name=name,
             attributes=attributes,
             package_id=package_id,
             status=status,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_files_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_files_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -33,22 +33,23 @@
     get_data(session=session)
         Returns a response to the data platform
     get_data_async(session=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> from refinitiv.data.delivery import cfs
-     >>> definition = cfs.files.Definition()
-     >>> files = definition.get_data()
+    >>> from refinitiv.data.delivery import cfs
+    >>> definition = cfs.files.Definition()
+    >>> files = definition.get_data()
 
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         fileset_id: str,
         file_name: Optional[str] = None,
         created_since: Union[str, datetime, timedelta] = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_iter_object.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_iter_object.py`

 * *Files 1% similar despite different names*

```diff
@@ -18,17 +18,15 @@
 
     def __iter__(self):
         if self._child_type is None:
             raise TypeError(f"object {self._provider.name, type(self)} is not iterable")
 
         args = self._params
         kwargs = {args[0]: self._data[args[1]]}
-        self._child_objects = get_data_by_data_type(
-            data_type=self._child_type, session=self._session, **kwargs
-        )
+        self._child_objects = get_data_by_data_type(data_type=self._child_type, session=self._session, **kwargs)
         self._n = 0
         return self
 
     def __next__(self):
         _iter_obj = self._child_objects.data._iter_object
         if not _iter_obj:
             raise StopIteration
@@ -94,17 +92,15 @@
             result = self._child_objects.data._iter_object[self._n]
             self._n += 1
             return result
         while self._bucket_names:
             bucket = self._bucket_names.pop(0)
             args = self._params
             kwargs = {args[0]: self._data[args[1]], "bucket": bucket}
-            self._child_objects = get_data_by_data_type(
-                self._child_type, self._session, **kwargs
-            )
+            self._child_objects = get_data_by_data_type(self._child_type, self._session, **kwargs)
             if not self._child_objects.errors and self._child_objects.data.raw["value"]:
                 self._n = 0
                 result = self._child_objects.data._iter_object[self._n]
                 self._n += 1
                 return result
         raise StopIteration
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_packages_definition.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_packages_definition.py`

 * *Files 0% similar despite different names*

```diff
@@ -43,22 +43,23 @@
     get_data(session=session)
         Returns a response to the data platform
     get_data_async(session=None)
         Returns a response asynchronously to the data platform
 
     Examples
     --------
-     >>> from refinitiv.data.delivery import cfs
-     >>> definition = cfs.packages.Definition()
-     >>> packages = definition.get_data()
+    >>> from refinitiv.data.delivery import cfs
+    >>> definition = cfs.packages.Definition()
+    >>> packages = definition.get_data()
 
-     Using get_data_async
-     >>> import asyncio
-     >>> task = definition.get_data_async()
-     >>> response = asyncio.run(task)
+    Using get_data_async
+
+    >>> import asyncio
+    >>> task = definition.get_data_async()
+    >>> response = asyncio.run(task)
     """
 
     def __init__(
         self,
         package_name: Optional[str] = None,
         package_id: Optional[str] = None,
         package_type: Optional[str] = None,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_tools.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_tools.py`

 * *Files 11% similar despite different names*

```diff
@@ -17,22 +17,20 @@
     --------
     >>> _convert_name("fileset_id")
     ... 'filesetId'
     """
     if "_" not in name:
         return name
 
-    return "".join(
-        word if i == 0 else word.title() for i, word in enumerate(name.split("_"))
-    )
+    return "".join(word if i == 0 else word.title() for i, word in enumerate(name.split("_")))
 
 
 def _get_query_params(**kwargs):
     _query_parameters = []
-    for (key, value) in kwargs.items():
+    for key, value in kwargs.items():
         if value is not None and not key.startswith("_"):
             _query_parameters.append((_convert_name(key), value))
     return _query_parameters
 
 
 def _get_url(config, endpoint):
     base_url = config.get_str("url")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/delivery/cfs/_unpacker.py` & `refinitiv-data-1.2.0/refinitiv/data/delivery/cfs/_unpacker.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/__init__.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -6,15 +6,18 @@
     "Peers",
     "Chain",
     "Screener",
     "AssetClass",
     "AssetState",
     "CountryCode",
     "SymbolTypes",
+    "_Customers",
+    "_Suppliers",
 )
 
+from ._convert_symbols import convert_symbols
 from ._search import search
-from ..content.search import Views
-from ..content.symbol_conversion import AssetClass, AssetState, CountryCode, SymbolTypes
 from ._search_templates import templates as search_templates
+from ._stakeholders import Customers as _Customers, Suppliers as _Suppliers
 from ._universe_expanders import Peers, Chain, Screener
-from ._convert_symbols import convert_symbols
+from ..content.search import Views
+from ..content.symbol_conversion import AssetClass, AssetState, CountryCode, SymbolTypes
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_convert_symbols.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_convert_symbols.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,56 +1,55 @@
-from typing import TYPE_CHECKING
+from typing import TYPE_CHECKING, Union
 
 from .. import content
-
 from ..content.symbol_conversion._definition import DEFAULT_SCOPE
 from ..content.symbol_conversion._symbol_type import SYMBOL_TYPE_VALUES
 
 if TYPE_CHECKING:
     from ..content.symbol_conversion._symbol_type import OptSymbolTypes
     from ..content.symbol_conversion._symbol_type import SymbolTypes
     from .._types import StrStrings
-    from ..content.symbol_conversion._symbol_type import OptAssetState
-    from ..content.symbol_conversion._symbol_type import OptCountryCode
-    from ..content.symbol_conversion._symbol_type import OptAssetClass
+    from ..content.symbol_conversion._asset_class import OptAssetClass
+    from ..content.symbol_conversion._asset_state import OptAssetState
+    from ..content.symbol_conversion._country_code import OptCountryCode
 
 
 def convert_symbols(
     symbols: "StrStrings",
-    from_symbol_type: "SymbolTypes" = DEFAULT_SCOPE,
+    from_symbol_type: Union[str, "SymbolTypes"] = DEFAULT_SCOPE,
     to_symbol_types: "OptSymbolTypes" = SYMBOL_TYPE_VALUES,
     preferred_country_code: "OptCountryCode" = None,
     asset_class: "OptAssetClass" = None,
     asset_state: "OptAssetState" = None,
 ):
     """
     This function describes parameters to convert symbols
 
     Parameters
     ----------
     symbols: str or list of str
         Single instrument or list of instruments to convert.
 
-    from_symbol_type: SymbolTypes, optional
+    from_symbol_type: str or SymbolTypes, optional
         Instrument code to convert from.
         Possible values: 'CUSIP', 'ISIN', 'SEDOL', 'RIC', 'ticker', 'lipperID', 'IMO'
         Default: '_AllUnique'
 
     to_symbol_types: SymbolTypes, str or list of str or SymbolTypes, optional
         Instrument code to convert to.
         Possible values: 'CUSIP', 'ISIN', 'SEDOL', 'RIC', 'ticker', 'lipperID', 'IMO', 'OAPermID'
         Default: all symbol types are requested
 
-    preferred_country_code: CountryCode, optional
+    preferred_country_code: str or CountryCode, optional
         Unique ISO 3166 code for country
 
-    asset_class: AssetClass, optional
+    asset_class: str or AssetClass, optional
         AssetClass value to build filter parameter.
 
-    asset_state: AssetState, optional
+    asset_state: str or AssetState, optional
         AssetState value to build filter parameter.
 
     Returns
     -------
     pd.DataFrame
 
     Examples
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_search.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_search.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/base.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/base.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,20 +1,28 @@
 """Define basic abstract classes for templates functionality
 
 That is abstracted from the original goal - discovery search templates.
 And can be used for any other Definition or any other callable.
 """
-
+import logging
+from functools import lru_cache
+from threading import Lock
 from typing import Optional, Dict, Any, Set, Iterable
 
+from pandas import DataFrame
+
 from refinitiv.data._tools.templates import StringTemplate
 
 from .namespaces import Namespace
 
 
+class SearchInterrupt(Exception):
+    pass
+
+
 class Target:
     """Abstract callable target with accessible list of keyword arguments names
 
     Base for building templates in TargetTemplate.
     You need to define your own targets that do some work. Like DiscoverySearchTarget.
     """
 
@@ -23,37 +31,37 @@
 
     def __call__(self, **kwargs):
         pass
 
 
 def extract_used_templates_from_placeholders_and_namespace(
     names: Set[str], namespace: Namespace
-) -> Set[str]:
+) -> Dict[str, Set[str]]:
     """Get a full path to namespaces that was used in the set of template placeholders
 
     In the string template we can have placeholders like
     "builtins.utils.geo.Mines.location.lat". And it's impossible to decide just by
     looking on it what part of this string is namespaces chain (prefix), what is
     Template name, and what is chain of attributes of the Template call result (suffix).
-    The goal of this functions is to find maximum namespaces + Template name chain
+    The goal of these functions is to find maximum namespaces + Template name chain
     based on given namespace.
 
     For example, if in namespace we have template with the namespace chain
-    "builtings.utils.geo" and name "Mines", we will return "builtins.utils.geo.Mines".
+    "builtins.utils.geo" and name "Mines", we will return "builtins.utils.geo.Mines".
     And what left of original string must be processed as a result attributes later.
 
     You must be sure that you are passing as a names arguments all names
-    that MUST BE treated as a subtemplate usage. In other case, exception will be raised.
+    that MUST BE treated as a sub-template usage. In other case, exception will be raised.
 
     Parameters
     ----------
 
     names: Set[str]
         set of placeholder names, without suffixes like jinja filters, but with
-        attributes of the expected subtemplate output
+        attributes of the expected sub-template output
     namespace: Namespace
         namespace object of your template
 
     Returns
     -------
     Set[str]
         Templates used, full paths, without attributes
@@ -70,39 +78,41 @@
     def match_name(orig_name):
         """Check if orig_name or its prefix is presented in the namespace
 
         Function expects that orig_name already  starts with one of the root namespace
         prefixes. So it always matches at least to this prefix.
         """
         cur_name = orig_name
-
+        attr_list = []
         while True:
             try:
                 found = namespace.get(cur_name)
             except KeyError:
-                raise ValueError(f"Unknown subtemplate usage: {orig_name}")
+                raise ValueError(f"Unknown sub-template usage: {orig_name}")
 
             if found:
                 break
             else:
                 # cut the tail
-                parts = cur_name.rsplit(".", maxsplit=1)
-                cur_name = parts[0]
+                cur_name, suffix = cur_name.rsplit(".", maxsplit=1)
+                attr_list.append(suffix)
 
-        return cur_name
+        return cur_name, ".".join(reversed(attr_list))
 
-    result = set()
+    result = {}
 
     for name in names:
         prefix = name.split(".", maxsplit=1)[0]
 
         if prefix in ns_prefixes:
-            matched = match_name(name)
+            matched, rest = match_name(name)
             if matched and matched not in ns_prefixes:
-                result.add(matched)
+                if matched not in result:
+                    result[matched] = set()
+                result[matched].add(rest)
 
     return result
 
 
 class TargetTemplate:
     """Abstract target search preset
 
@@ -121,14 +131,15 @@
 
     name: str
         name of the template
 
     """
 
     _target_class = Target
+    _cache_lock = Lock()
 
     def __init__(
         self,
         name=None,
         *,
         placeholders_defaults: Optional[Dict[str, Any]] = None,
         pass_through_defaults: Optional[Dict[str, Any]] = None,
@@ -144,29 +155,25 @@
         placeholders_defaults: dict, optional
             Dict of string template placeholders default values.
         pass_through_defaults: dict, optional
             default values for the Target parameters
         optional_placeholders: Iterable[str], optional
             names of placeholders that are optional without default values
         ns: Namespace
-            Namespace in which template will operate. Used for subtemplates.
+            Namespace in which template will operate. Used for sub-templates.
         """
 
         self._ns = ns if ns is not None else Namespace()
         self._target: Target = self._target_class()
         # Names of the templates used, with full path from self.ns root
-        self._subtemplates_used = set()
+        self._subtemplates_used = {}
 
         """ List search keyword arguments we can use in this template """
-        self._placeholders_defaults = (
-            {} if placeholders_defaults is None else placeholders_defaults
-        )
-        self._optional_placeholders = set(
-            [] if optional_placeholders is None else optional_placeholders
-        )
+        self._placeholders_defaults = {} if placeholders_defaults is None else placeholders_defaults
+        self._optional_placeholders = set([] if optional_placeholders is None else optional_placeholders)
         """ Default template variables values for a templated defaults """
         if pass_through_defaults is None:
             pass_through_defaults = {}
 
         bad_pass_through_params = set(pass_through_defaults) - self._target.args_names
         if bad_pass_through_params:
             raise ValueError(
@@ -177,68 +184,65 @@
             )
 
         self.name = name
 
         unknown_defaults = set(search_defaults) - self._target.args_names
         if unknown_defaults:
             raise ValueError(
-                "These arguments are defined in template, but not in search Definition: "
-                + ", ".join(unknown_defaults)
+                "These arguments are defined in template, but not in search Definition: " + ", ".join(unknown_defaults)
             )
         # Names of all placeholders inside string templates
         """Set of names for all placeholders in string templates"""
         self._placeholder_names: Set[str] = set()
         # Arguments to be passed to Definition as templates
         self._templated_defaults: Dict[str, StringTemplate] = {}
         # Arguments to be directly passed to Definition without any preprocessing
         self._pass_through_defaults: Dict[str, Any] = {}
 
         def is_subtemplate(placeholder_name: str):
-            return any(
-                placeholder_name.startswith(prefix) for prefix in self._ns.keys()
-            )
+            return any(placeholder_name.startswith(prefix) for prefix in self._ns.keys())
 
         for name, value in search_defaults.items():
-
             if not isinstance(value, str):
                 self._pass_through_defaults[name] = value
                 continue
 
             template = StringTemplate(value)
             template.validate()
             if template.placeholders():
                 self._templated_defaults[name] = template
                 for name in template.placeholders():
                     if not is_subtemplate(name):
+                        if "." in name:
+                            logging.info(
+                                f"Placeholder {name} for template {self.name} contains "
+                                f"dots, but has an invalid prefix."
+                            )
                         self._placeholder_names.add(name)
             else:
                 self._pass_through_defaults[name] = value
 
-            self._subtemplates_used |= (
-                extract_used_templates_from_placeholders_and_namespace(
-                    template.placeholders(), self._ns
-                )
+            self._subtemplates_used.update(
+                extract_used_templates_from_placeholders_and_namespace(template.placeholders(), self._ns)
             )
 
         self._pass_through_defaults.update(pass_through_defaults)
 
         bad_tpl_var_names = self._get_full_placeholder_names() & self._target.args_names
         if bad_tpl_var_names:
             raise ValueError(
                 "You can't use template arguments with the same name"
                 " as search arguments. You have used: " + ", ".join(bad_tpl_var_names)
             )
 
     def _get_full_placeholder_names(self):
-        placeholder_names = set()
-        for st_path in self._subtemplates_used:
-            placeholder_names.update(
-                self._ns.get(st_path)._get_full_placeholder_names()
-            )
-        return self._placeholder_names | placeholder_names
+        placeholder_names = self._placeholder_names.copy()
+        for st_path in self._subtemplates_used.keys():
+            placeholder_names.update(self._ns.get(st_path)._get_full_placeholder_names())
+        return placeholder_names
 
     def __repr__(self):
         return f"<TargetTemplate for {self._target_class.__name__} name='{self.name}'>"
 
     def _search_kwargs(self, **kwargs) -> dict:
         """Get dictionary of arguments for the Target"""
 
@@ -249,27 +253,30 @@
             - self._optional_placeholders
         )
 
         assert all("." not in ph for ph in undefined_placeholders)
 
         if undefined_placeholders:
             raise KeyError(
-                "Those keyword arguments must be defined, but they are not: "
-                + ", ".join(undefined_placeholders)
+                "Following keyword arguments must be defined, but they are not: " + ", ".join(undefined_placeholders)
             )
 
         unexpected_arguments = (
             set(kwargs)
             - self._get_full_placeholder_names()
             # templated defaults can't be redefined
             - (self._target.args_names - self._templated_defaults.keys())
         )
 
         if unexpected_arguments:
-            raise KeyError(f"Unexpected arguments: {', '.join(unexpected_arguments)}")
+            raise KeyError(
+                f"Unexpected arguments: {', '.join(unexpected_arguments)}."
+                f"Possible arguments for template '{self.name}': "
+                f"{', '.join(self._target.args_names)}"
+            )
 
         old_kwargs = kwargs
         kwargs = kwargs.copy()
         # Applying template variables defaults
         for name, value in self._placeholders_defaults.items():
             if name not in kwargs:
                 kwargs[name] = value
@@ -285,35 +292,50 @@
             current = kwargs
             for part in parts[:-1]:
                 if part not in current:
                     current[part] = {}
                 current = current[part]
             current[parts[-1]] = value
 
-        for subname in self._subtemplates_used:
+        def check_result(value, required_attributes):
+            for attr in required_attributes:
+                if attr not in value:
+                    raise ValueError(f"{attr} is not in the result of search.")
+            return value
+
+        for subname, required_items in self._subtemplates_used.items():
             st = self._ns.get(subname)
-            filtered_kwargs = {
-                k: v
-                for k, v in old_kwargs.items()
-                if k in st._get_full_placeholder_names()
-            }
-            set_path_value(subname, st.search(**filtered_kwargs))
+            filtered_kwargs = {k: v for k, v in old_kwargs.items() if k in st._get_full_placeholder_names()}
+            try:
+                set_path_value(subname, check_result(st._search(**filtered_kwargs), required_items))
+            except ValueError as e:
+                raise SearchInterrupt(
+                    f"Result of {st} search with parameters {filtered_kwargs} did not contain all required values"
+                ) from e
 
         result = self._pass_through_defaults.copy()
 
         # Apply variables to templated defaults
         for name, template in self._templated_defaults.items():
             result[name] = template.substitute(**kwargs)
 
         # Apply other variables from kwargs
         for name, value in kwargs.items():
-            if (
-                name not in self._get_full_placeholder_names()
-                and name not in self._ns.keys()
-            ):
+            if name not in self._get_full_placeholder_names() and name not in self._ns.keys():
                 result[name] = value
 
         return result
 
-    def search(self, **kwargs):
+    @lru_cache(None)
+    def _search(self, **kwargs) -> Any:
         """Target call with given parameters"""
-        return self._target(**self._search_kwargs(**kwargs))
+        try:
+            return self._target(**self._search_kwargs(**kwargs))
+        except SearchInterrupt:
+            return DataFrame()
+
+    def search(self, **kwargs) -> Any:
+        """Public uncached call to search"""
+        with self._cache_lock:
+            result = self._search(**kwargs)
+            self._search.cache_clear()
+        return result
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/embedded.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/embedded.py`

 * *Files 2% similar despite different names*

```diff
@@ -186,20 +186,20 @@
         -------
         DataFrame
         Default columns: DTSubjectName, RCSIssuerCountryLeaf, IssuerOAPermID, PrimaryRIC
         """
         return super().search(ric=ric, **kwargs)
 
 
-class FutureRicToFutureTemplate(DiscoverySearchTemplate):
+class FutureRICToFutureTemplate(DiscoverySearchTemplate):
     """From one future RIC, find all other futures on its underlying"""
 
     def __init__(self):
         super().__init__(
-            name="FutureRicToFuture",
+            name="FutureRICToFuture",
             pass_through_defaults={
                 "view": "SearchAll",
                 "select": "RicRoot,RIC,DTSubjectName,ExpiryDateString,ContractType,Currency",
                 "order_by": "GrossTonnage desc",
                 "top": 1000,
             },
             filter="RicRoot eq '#{_.GetRoot.RicRoot.0}' and RCSAssetCategoryLeaf xeq 'Equity Future' and IsChain eq false and AssetStateName eq 'Active' and DisplayType ne 'CONTN'",
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/manage.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/manage.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,31 @@
 """Functionality specific to special SearchTemplates template access class
 
 Takes care of public API parts and loading user templates from the config
 """
-from typing import Union, List
+from typing import Union, List, TYPE_CHECKING
 
 from humps import depascalize
 
 from refinitiv.data import get_config
 from .embedded import (
     RICCategoryTemplate,
     UnderlyingRICToOptionTemplate,
     UnderlyingRICToFutureTemplate,
     RICToIssuerTemplate,
     OrganisationPermIDToUPTemplate,
+    FutureRICToFutureTemplate,
 )
 from .search import DiscoverySearchTemplate
 from .namespaces import Namespace
 from .utils import generate_docstring
 
+if TYPE_CHECKING:
+    from .base import TargetTemplate
+
 
 def depascalize_view(value: str) -> str:
     """Convert search View value to upper snakecase
 
     We need to have separate function for that because some enum View values
     does not follow the rules of pascal case
     """
@@ -41,22 +45,18 @@
                 for name, sub in data.get("locals", {}).items()
             }
         ),
     )
     template = DiscoverySearchTemplate(
         name,
         placeholders_defaults={
-            name: attrs["default"]
-            for name, attrs in data.get("parameters", {}).items()
-            if "default" in attrs
+            name: attrs["default"] for name, attrs in data.get("parameters", {}).items() if "default" in attrs
         },
         optional_placeholders={
-            name
-            for name, attrs in data.get("parameters", {}).items()
-            if attrs.get("optional", False)
+            name for name, attrs in data.get("parameters", {}).items() if attrs.get("optional", False)
         },
         ns=namespace,
         **depascalize(data.get("request_body", {})),
     )
 
     method_args = {}
     # Some placeholders may be only in string, but not in "parameters"
@@ -74,14 +74,17 @@
         methods={"search": {"description": "", "args": method_args}},
     )
 
     return template
 
 
 class BuiltinNamespace(Namespace):
+    def __setitem__(self, key: str, value: Union["Namespace", "TargetTemplate"]):
+        raise NotImplementedError("Setting namespace key-value not supported for built-in namespace")
+
     def __getitem__(self, name: str):
         attr = getattr(SearchTemplates, name)
         if not isinstance(attr, DiscoverySearchTemplate):
             raise AttributeError(f"Embedded search template named {name} is not found")
         return attr
 
 
@@ -92,14 +95,17 @@
         super().__init__()
 
         self._CONFIG_PREFIX = config_prefix
 
     def __iter__(self):
         return get_config().get(self._CONFIG_PREFIX, {}).keys().__iter__()
 
+    def __setitem__(self, key: str, value: Union["Namespace", "TargetTemplate"]):
+        raise NotImplementedError("Setting namespace key-value not supported for user namespace")
+
     def __getitem__(self, name: str) -> Union[DiscoverySearchTemplate, "UserNamespace"]:
         if name in self._blacklisted_keys:
             raise KeyError(f"'{name}' is a reserved key")
         config = get_config()
         key = f"{self._CONFIG_PREFIX}.{name}"
         if key not in config:
             raise KeyError(f"Template or Namespace '{name}' is not found in the config")
@@ -130,11 +136,15 @@
     ["Equity"]
     """
 
     RICCategory = RICCategoryTemplate()
     UnderlyingRICToOption = UnderlyingRICToOptionTemplate()
     UnderlyingRICToFuture = UnderlyingRICToFutureTemplate()
     RICToIssuer = RICToIssuerTemplate()
+    FutureRICToFuture = FutureRICToFutureTemplate()
     OrganisationPermIDToUP = OrganisationPermIDToUPTemplate()
 
     def __init__(self):
         super().__init__(config_prefix="search.templates")
+
+    def __setitem__(self, key: str, value: Union["Namespace", "TargetTemplate"]):
+        raise NotImplementedError("Setting namespace key-value not supported for global namespace")
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/namespaces.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/namespaces.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,38 +1,42 @@
 """Base for templates namespaces"""
-
 from collections import deque
+from typing import Union, TYPE_CHECKING
+
+if TYPE_CHECKING:
+    from .base import TargetTemplate
 
 
 class Namespace:
     def __init__(self, target=None, **kwargs):
         # Don't have an idea yet how to it better without need to pass TargetTemplate
         # to Namespace each time or creating circular dependencies
         if target is None:
             from .base import TargetTemplate
 
             self.target = TargetTemplate
         for key, value in kwargs.items():
             if "." in key:
-                raise ValueError(
-                    f"Name in namespace can't contain dot. Value with dot encountered: {value}"
-                )
+                raise ValueError(f"Name in namespace can't contain dot. Value with dot encountered: {value}")
             if not isinstance(value, (Namespace, self.target)):
                 raise TypeError(
                     f"Value in namespace must be either SearchTemplate or other Namespace. {type(value)} encountered"
                 )
 
         self._data = kwargs
 
     def __iter__(self):
         return self._data.__iter__()
 
     def __getitem__(self, name: str):
         return self._data.__getitem__(name)
 
+    def __setitem__(self, key: str, value: Union["Namespace", "TargetTemplate"]):
+        self._data[key] = value
+
     def get(self, path: str):
         """Get namespace or key by path"""
         keys = deque(path.split("."))
         result = self
         while keys:
             result = result[keys.popleft()]
             if isinstance(result, self.target) and keys:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/search.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/search.py`

 * *Files 5% similar despite different names*

```diff
@@ -47,23 +47,18 @@
 
         Or use it as a base for another search template.
         Exported without documentation.
 
         Experimental, does not work with all subfeatures and may never be
         """
         request_body = pascalize(self._pass_through_defaults)
-        request_body.update(
-            {pascalize(k): v.template for k, v in self._templated_defaults.items()}
-        )
+        request_body.update({pascalize(k): v.template for k, v in self._templated_defaults.items()})
 
         if "View" in request_body and isinstance(request_body["View"], search.Views):
             request_body["View"] = request_body["View"].value
 
         result = {"request_body": request_body}
 
         if self._placeholders_defaults:
-            result["parameters"] = {
-                name: {"default": value}
-                for name, value in self._placeholders_defaults.items()
-            }
+            result["parameters"] = {name: {"default": value} for name, value in self._placeholders_defaults.items()}
 
         return result
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_search_templates/utils.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_search_templates/utils.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/_chain.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/_chain.py`

 * *Files 1% similar despite different names*

```diff
@@ -44,32 +44,28 @@
     is_stream_closed.wait()
     if not constituents:
         raise RDError(-1, default_error_message)
     return ChainData(constituents, summary_links)
 
 
 def get_chain_data_from_adc(name):
-    adc_response = fundamental_and_reference.Definition(
-        universe=name, fields=["TR.RIC"]
-    ).get_data()
+    adc_response = fundamental_and_reference.Definition(universe=name, fields=["TR.RIC"]).get_data()
     constituents = [item[0] for item in adc_response.data.raw.get("data", {})]
     return ChainData(constituents)
 
 
 def _get_constituents(response: "Response") -> list:
     return response.data.raw.get("data", {}).get("constituents", [])
 
 
 def get_chain_data_from_chain_endpoint(name):
     url = "/data/pricing/chains/v1/"
     session = get_default()
     session.verify_scope(url, "get")
-    chain_response = endpoint_request.Definition(
-        url=url, query_parameters={"universe": name}
-    ).get_data()
+    chain_response = endpoint_request.Definition(url=url, query_parameters={"universe": name}).get_data()
     _check_response(chain_response, session.config)
 
     summary_links = []
     constituents = []
 
     for item in _get_constituents(chain_response):
         if item.startswith(".") or item.startswith("/") or item.endswith("="):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/discovery/_universe_expanders/_screener.py` & `refinitiv-data-1.2.0/refinitiv/data/discovery/_universe_expanders/_screener.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/eikon/_data_grid.py` & `refinitiv-data-1.2.0/refinitiv/data/eikon/_data_grid.py`

 * *Files 2% similar despite different names*

```diff
@@ -176,17 +176,15 @@
         if len(keys) != 1:
             with "get_data error: The field dictionary should contain a single key which is the field name" as msg:
                 logger.error(msg)
                 raise ValueError(msg)
         name = list(f.keys())[0]
         field_info = f[name]
         if type(field_info) != dict:
-            with "get_data error: The parameters for the file {} should be passed in a dict".format(
-                name
-            ) as error_msg:
+            with "get_data error: The parameters for the file {} should be passed in a dict".format(name) as error_msg:
                 logger.error(error_msg)
                 raise ValueError(error_msg)
 
         field = {"name": name}
         if "sort_dir" in list(field_info.keys()):
             field["sort"] = field_info["sort_dir"]
         if "sort_priority" in list(field_info.keys()):
@@ -200,17 +198,15 @@
         payload.update({"parameters": parameters})
 
     _endpoint = DataGridAsync_UDF_endpoint
 
     if _endpoint == DataGridAsync_UDF_endpoint:
         payload = {"requests": [payload]}
 
-    response = json_requests.send_json_request(
-        _endpoint, payload, debug=debug, raw_response=raw_response
-    )
+    response = json_requests.send_json_request(_endpoint, payload, debug=debug, raw_response=raw_response)
 
     if raw_response:
         return response
 
     result = response.json()
     if result.get("responses"):
         result = result["responses"][0]
@@ -240,22 +236,22 @@
                 raise ValueError(error_msg)
         for f in fields:
             if is_string_type(f):
                 field_list.append({f: {}})
             elif type(f) == dict:
                 field_list.append(f)
             else:
-                error_msg = (
-                    "get_data error: the fields should be of type string or dictionary"
-                )
+                error_msg = "get_data error: the fields should be of type string or dictionary"
                 get_default_session().logger().error(error_msg)
                 raise ValueError(error_msg)
         return field_list
 
-    error_msg = "get_data error: the field parameter should be a string, a dictionary , or a list of strings|dictionaries"
+    error_msg = (
+        "get_data error: the field parameter should be a string, a dictionary , or a list of strings|dictionaries"
+    )
     get_default_session().logger().error(error_msg)
     raise ValueError(error_msg)
 
 
 def get_data_value(value):
     if is_string_type(value):
         return value
@@ -268,22 +264,18 @@
 def get_data_frame(data_dict, field_name=False):
     if "headers" not in data_dict:
         return None
 
     raw_headers = data_dict["headers"][0]
 
     if field_name:
-        headers = [
-            header.get("field", header.get("displayName")) for header in raw_headers
-        ]
+        headers = [header.get("field", header.get("displayName")) for header in raw_headers]
     else:
         headers = [header["displayName"] for header in raw_headers]
-    data = numpy.array(
-        [[get_data_value(value) for value in row] for row in data_dict["data"]]
-    )
+    data = numpy.array([[get_data_value(value) for value in row] for row in data_dict["data"]])
     if len(data):
         df = pd.DataFrame(data, columns=headers)
         df = df.apply(pd.to_numeric, errors="ignore")
         if not df.empty:
             df = df.convert_dtypes()
     else:
         df = pd.DataFrame([], columns=headers)
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/eikon/_json_requests.py` & `refinitiv-data-1.2.0/refinitiv/data/eikon/_json_requests.py`

 * *Files 3% similar despite different names*

```diff
@@ -76,17 +76,15 @@
             error_msg += str(e)
             logger.error(error_msg)
             raise e
 
         try:
             # build the request
             udf_request = {"Entity": {"E": entity, "W": data}, "ID": ID}
-            logger.debug(
-                "Request to {} :{}".format(_session._get_udf_url(), udf_request)
-            )
+            logger.debug("Request to {} :{}".format(_session._get_udf_url(), udf_request))
             request = Request(
                 method="POST",
                 url=_session._get_udf_url(),
                 headers={"Content-Type": "application/json"},
                 # "x-tr-applicationid": _session.app_key},
                 json=udf_request,
             )
@@ -98,17 +96,15 @@
             except UnicodeEncodeError:
                 logger.error("HTTP Response: cannot decode error message")
 
             if response.status_code == 200:
                 result = {}
                 try:
                     result = response.json()
-                    logger.debug(
-                        "Response size: {}".format(sys.getsizeof(json.dumps(result)))
-                    )
+                    logger.debug("Response size: {}".format(sys.getsizeof(json.dumps(result))))
                 except JSONDecodeError:
                     logger.error(f"Failed to decode response to json: {response.text}")
 
                 # Manage specifically DataGrid async mode
                 if entity.startswith("DataGrid") and entity.endswith("Async"):
                     ticket = _check_ticket_async(result)
                     while ticket:
@@ -124,19 +120,15 @@
                             url=_session._get_udf_url(),
                             headers={"Content-Type": "application/json"},
                             json=ticket_request,
                         )
                         response = _session.http_request(request)
 
                         result = response.json()
-                        logger.debug(
-                            "Response size: {}".format(
-                                sys.getsizeof(json.dumps(result))
-                            )
-                        )
+                        logger.debug("Response size: {}".format(sys.getsizeof(json.dumps(result))))
                         ticket = _check_ticket_async(result)
 
                 if not raw_response:
                     check_server_error(result)
                 return response
             else:
                 raise_for_status(response)
@@ -162,17 +154,15 @@
     if len(server_response) == 1:
         for key, value in server_response.items():
             ticket = value[0]
             if ticket and ticket.get("estimatedDuration"):
                 ticket_duration = int(ticket["estimatedDuration"])
                 ticket_duration = min(ticket_duration, 15000)
                 ticket_value = ticket["ticket"]
-                message = "Receive ticket from {}, wait for {} second".format(
-                    key, ticket_duration / 1000.0
-                )
+                message = "Receive ticket from {}, wait for {} second".format(key, ticket_duration / 1000.0)
                 if ticket_duration > 1000:
                     message = message + "s"
                 logger.info(message)
                 time.sleep(ticket_duration / 1000.0)
                 return ticket_value
     return None
 
@@ -197,24 +187,18 @@
         logger = session.logger()
     else:
         logger = get_default_session().logger()
 
     # check HTTP response (server response is an object that can contain ErrorCode attribute)
     if hasattr(server_response, "ErrorCode"):
         logger.error(getattr(server_response, "ErrorMessage"))
-        raise RDError(
-            server_response["ErrorCode"], getattr(server_response, "ErrorMessage")
-        )
+        raise RDError(server_response["ErrorCode"], getattr(server_response, "ErrorMessage"))
 
     # check UDF response (server response is JSON and it can contain ErrorCode + ErrorMessage keys)
-    if (
-        isinstance(server_response, dict)
-        and "ErrorCode" in server_response
-        and "ErrorMessage" in server_response
-    ):
+    if isinstance(server_response, dict) and "ErrorCode" in server_response and "ErrorMessage" in server_response:
         error_message = server_response["ErrorMessage"]
         logger.error(error_message)
         raise RDError(server_response["ErrorCode"], error_message)
 
     # check DataGrid response (server response is JSON or text and it can contain error + optionally transactionId keys)
     if "error" in server_response:
         if "transactionId" in server_response:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/eikon/_news_request.py` & `refinitiv-data-1.2.0/refinitiv/data/eikon/_news_request.py`

 * *Files 1% similar despite different names*

```diff
@@ -146,17 +146,15 @@
 
     if date_from is not None:
         payload.update({"dateFrom": to_datetime(date_from).isoformat()})
 
     if date_to is not None:
         payload.update({"dateTo": to_datetime(date_to).isoformat()})
 
-    response = json_requests.send_json_request(
-        News_Headlines_UDF_endpoint, payload, debug=debug
-    )
+    response = json_requests.send_json_request(News_Headlines_UDF_endpoint, payload, debug=debug)
     result = response.json()
 
     if raw_output:
         return result
     else:
         return convert_content_data_to_df_udf(result)
 
@@ -200,17 +198,15 @@
     if not is_string_type(story_id):
         error_msg = "story_id must be a string"
         logger.error(error_msg)
         raise ValueError(error_msg)
 
     app_key = get_default_session().app_key
     payload = {"attributionCode": "", "productName": app_key, "storyId": story_id}
-    response = json_requests.send_json_request(
-        News_Story_UDF_endpoint, payload, debug=debug
-    )
+    response = json_requests.send_json_request(News_Story_UDF_endpoint, payload, debug=debug)
     json_data = response.json()
     if raw_output:
         return json_data
 
     if json_data:
         if json_data.get("story"):
             if json_data.get("story").get("storyHtml"):
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/eikon/_streaming_prices.py` & `refinitiv-data-1.2.0/refinitiv/data/eikon/_streaming_prices.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/eikon/_symbology.py` & `refinitiv-data-1.2.0/refinitiv/data/eikon/_symbology.py`

 * *Files 3% similar despite different names*

```diff
@@ -135,25 +135,21 @@
 
     payload = {
         "symbols": symbol,
         "from": from_symbol_type,
         "to": to_symbol_type,
         "bestMatchOnly": best_match,
     }
-    response = json_requests.send_json_request(
-        Symbology_UDF_endpoint, payload, debug=debug
-    )
+    response = json_requests.send_json_request(Symbology_UDF_endpoint, payload, debug=debug)
     result = response.json()
     if raw_output:
         return result
     else:
         if best_match:
-            results_dict = dict(
-                [(_["symbol"], _["bestMatch"]) for _ in result["mappedSymbols"]]
-            )
+            results_dict = dict([(_["symbol"], _["bestMatch"]) for _ in result["mappedSymbols"]])
         else:
             results_dict = dict([(_["symbol"], _) for _ in result["mappedSymbols"]])
         if len(results_dict):
             df_result = pd.DataFrame(results_dict).transpose()
             if not df_result.empty:
                 df_result = df_result.convert_dtypes()
         else:
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/eikon/_time_series.py` & `refinitiv-data-1.2.0/refinitiv/data/eikon/_time_series.py`

 * *Files 4% similar despite different names*

```diff
@@ -151,17 +151,15 @@
     if end_date is None:
         end_date = get_date_from_today(0)
 
     start_date = to_datetime(start_date).isoformat()
     end_date = to_datetime(end_date).isoformat()
 
     if start_date > end_date:
-        error_msg = (
-            f"end date ({end_date}) should be after than start date ({start_date})"
-        )
+        error_msg = f"end date ({end_date}) should be after than start date ({start_date})"
         logger.error(error_msg)
         raise ValueError(error_msg)
 
     payload = {
         "rics": rics,
         "fields": fields,
         "interval": interval,
@@ -190,26 +188,20 @@
         if is_string_type(corax):
             payload.update({"corax": corax})
         else:
             error_msg = "corax must be a string"
             logger.error(error_msg)
             raise ValueError(error_msg)
 
-    response = json_requests.send_json_request(
-        TimeSeries_UDF_endpoint, payload, debug=debug
-    )
+    response = json_requests.send_json_request(TimeSeries_UDF_endpoint, payload, debug=debug)
     ts_result = response.json()
 
     # Catch all errors to raise a warning
     ts_timeserie_data = ts_result["timeseriesData"]
-    ts_status_errors = [
-        ts_data
-        for ts_data in ts_timeserie_data
-        if get_json_value(ts_data, "statusCode") == "Error"
-    ]
+    ts_status_errors = [ts_data for ts_data in ts_timeserie_data if get_json_value(ts_data, "statusCode") == "Error"]
 
     ts_error_messages = ""
     for ts_status in ts_status_errors:
         ts_error_message = get_json_value(ts_status, "errorMessage")
         ts_error_message = ts_error_message[ts_error_message.find("Description") :]
         ts_instrument = get_json_value(ts_status, "ric")
         ts_error_message = ts_error_message.replace("Description", ts_instrument)
@@ -248,38 +240,30 @@
             ric = timeseries["ric"]
             error_code = timeseries["statusCode"]
             if error_code.lower() == "error":
                 continue
 
             fields = [f["name"] for f in timeseries["fields"]]
             timestamp_index = fields.index("TIMESTAMP")
-            fields.pop(
-                timestamp_index
-            )  # remove timestamp from fields (timestamp is used as index for dataframe)
+            fields.pop(timestamp_index)  # remove timestamp from fields (timestamp is used as index for dataframe)
             datapoints = numpy.array(timeseries["dataPoints"])
 
             if len(datapoints):
-                timestamps = [
-                    tz_replacer(value) for value in datapoints[:, timestamp_index]
-                ]
-                timestamps = numpy.array(
-                    timestamps, dtype="datetime64"
-                )  # index for dataframe
+                timestamps = [tz_replacer(value) for value in datapoints[:, timestamp_index]]
+                timestamps = numpy.array(timestamps, dtype="datetime64")  # index for dataframe
                 # remove timestamp column from numpy array
                 datapoints = numpy.delete(datapoints, numpy.s_[timestamp_index], 1)
                 fields_count = len(fields)
                 column_size = len(datapoints)
                 symbol_column = numpy.array([ric] * fields_count * column_size)
                 fields_column = numpy.array(fields * column_size)
                 values_column = numpy.concatenate(datapoints, axis=0)
                 values_column = values_column.astype("float")
 
-                timestamp_column = [
-                    [timestamps[i]] * fields_count for i in range(timestamps.size)
-                ]
+                timestamp_column = [[timestamps[i]] * fields_count for i in range(timestamps.size)]
                 timestamp_column = numpy.concatenate(timestamp_column, axis=0)
                 df = pd.DataFrame(
                     dict(
                         Date=timestamp_column,
                         Security=symbol_column,
                         Field=fields_column,
                         Value=values_column,
@@ -320,30 +304,26 @@
             error_code = timeseries["statusCode"]
             if error_code.lower() == "error":
                 continue
 
             rics.append(ric)
             fields = [f["name"] for f in timeseries["fields"]]
             timestamp_index = fields.index("TIMESTAMP")
-            fields.pop(
-                timestamp_index
-            )  # remove timestamp from fields (timestamp is used as index for dataframe)
+            fields.pop(timestamp_index)  # remove timestamp from fields (timestamp is used as index for dataframe)
             unique_fields = fields
             datapoints = numpy.array(timeseries["dataPoints"])
             if len(datapoints):
                 timestamps = numpy.array(
                     [tz_replacer(value) for value in datapoints[:, timestamp_index]],
                     dtype="datetime64",
                 )  # index for dataframe
                 datapoints = numpy.delete(
                     datapoints, numpy.s_[timestamp_index], 1
                 )  # remove timestamp column from numpy array
-                df = pd.DataFrame(
-                    datapoints, columns=fields, index=timestamps, dtype="float"
-                )
+                df = pd.DataFrame(datapoints, columns=fields, index=timestamps, dtype="float")
                 if not df.empty:
                     df = df.convert_dtypes()
             else:
                 df = pd.DataFrame([], columns=fields)
             if not df.empty:
                 df = df.convert_dtypes()
             df.index.name = "Date"
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/eikon/_tools.py` & `refinitiv-data-1.2.0/refinitiv/data/eikon/_tools.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,68 +27,53 @@
 
 def is_list_of_string(values):
     return all(is_string_type(value) for value in values)
 
 
 def check_for_string(parameter, name):
     if not is_string_type(parameter):
-        raise ValueError(
-            "The parameter {} should be a string, found {}".format(name, str(parameter))
-        )
+        raise ValueError("The parameter {} should be a string, found {}".format(name, str(parameter)))
 
 
 def check_for_string_or_list_of_strings(parameter, name):
     if type(parameter) != list and (not parameter or not is_string_type(parameter)):
         raise ValueError(
-            "The parameter {} should be a string or a list of string, found {}".format(
-                name, type(parameter)
-            )
+            "The parameter {} should be a string or a list of string, found {}".format(name, type(parameter))
         )
     if type(parameter) == list and not is_list_of_string(parameter):
         raise ValueError(
             "All items in the parameter {} should be of data type string, found {}".format(
                 name, [type(v) for v in parameter]
             )
         )
 
 
 def check_for_int(parameter, name):
     if type(parameter) is not int:
         raise ValueError(
-            "The parameter {} should be an int, found {} type value ({})".format(
-                name, type(parameter), str(parameter)
-            )
+            "The parameter {} should be an int, found {} type value ({})".format(name, type(parameter), str(parameter))
         )
 
 
 def build_list_with_params(values, name):
     if values is None:
         raise ValueError(name + " is None, it must be a string or a list of strings")
 
     if is_string_type(values):
         return [(v, None) for v in values.split()]
     elif type(values) is list:
         try:
-            return [
-                (value, None) if is_string_type(value) else (value[0], value[1])
-                for value in values
-            ]
+            return [(value, None) if is_string_type(value) else (value[0], value[1]) for value in values]
         except Exception:
-            raise ValueError(
-                name
-                + " must be a string or a list of strings or a tuple or a list of tuple"
-            )
+            raise ValueError(name + " must be a string or a list of strings or a tuple or a list of tuple")
     else:
         try:
             return values[0], values[1]
         except Exception:
-            raise ValueError(
-                name
-                + " must be a string or a list of strings or a tuple or a list of tuple"
-            )
+            raise ValueError(name + " must be a string or a list of strings or a tuple or a list of tuple")
 
 
 def build_list(values, name):
     if values is None:
         raise ValueError(name + " is None, it must be a string or a list of strings")
 
     if is_string_type(values):
@@ -100,17 +85,15 @@
             raise ValueError(name + " must be a string or a list of strings")
     else:
         raise ValueError(name + " must be a string or a list of strings")
 
 
 def build_dictionary(dic, name):
     if dic is None:
-        raise ValueError(
-            name + " is None, it must be a string or a dictionary of strings"
-        )
+        raise ValueError(name + " is None, it must be a string or a dictionary of strings")
 
     if is_string_type(dic):
         return json.loads(dic)
     elif type(dic) is dict:
         return dic
     else:
         raise ValueError(name + " must be a string or a dictionary")
@@ -150,32 +133,26 @@
     default_session.set_log_level(log_level)
 
 
 def convert_content_data_to_df_udf(raw: dict) -> pd.DataFrame:
     selected_fields = ["versionCreated", "text", "storyId", "sourceCode"]
 
     raw_headlines = raw.get("headlines", [])
-    first_created = [
-        tz_replacer(headline["firstCreated"]) for headline in raw_headlines
-    ]
-    headlines = [
-        [headline[field] for field in selected_fields] for headline in raw_headlines
-    ]
+    first_created = [tz_replacer(headline["firstCreated"]) for headline in raw_headlines]
+    headlines = [[headline[field] for field in selected_fields] for headline in raw_headlines]
     if len(headlines):
         df = pd.DataFrame(
             headlines,
             numpy.array(first_created, dtype="datetime64[ns]"),
             selected_fields,
         )
 
         if not df.empty:
             df = df.convert_dtypes()
 
     else:
-        df = pd.DataFrame(
-            [], numpy.array(first_created, dtype="datetime64[ns]"), selected_fields
-        )
+        df = pd.DataFrame([], numpy.array(first_created, dtype="datetime64[ns]"), selected_fields)
 
     df["versionCreated"] = df.versionCreated.apply(pd.to_datetime)
     df.fillna(pd.NA, inplace=True)
 
     return df
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/errors.py` & `refinitiv-data-1.2.0/refinitiv/data/errors.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv/data/session/desktop.py` & `refinitiv-data-1.2.0/refinitiv/data/session/desktop.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,22 +1,28 @@
+"""
+This type of session is used to connect to the Refinitiv Data Platform through Refinitiv Eikon or Refinitiv Workspace.
+It requires Refinitiv Eikon or Refinitiv Workspace to be running alongside your application. This type of session does
+not work with Refinitiv Eikon Web or Refinitiv Workspace for Web.
+"""
+
 __all__ = ("Definition",)
 
 
 class Definition(object):
     """
     Desktop session.
     Can be defined indirectly using the name of a session defined in
     the configuration file or directly by specifying the app_key parameter.
 
     Parameters
     ----------
     name: str, default "workspace"
         Name of the session in the config file.
     app_key: str, optional
-        Application key of the desktop session to create.
+        Application key.
 
     Raises
     ---------
     Exception
         If app-key is not found in the config file and in arguments.
 
     Examples
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/session/platform.py` & `refinitiv-data-1.2.0/refinitiv/data/session/platform.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,7 +1,15 @@
+"""
+This type of session is used to connect directly to the Refinitiv Data Platform or through a Real-Time Distribution
+System. If you would like to connect directly to RDP, you require a Refinitiv Data account (either a user account or
+a machine account). In both instances, you need to provide Refinitiv data credentials to create the session. If you
+would like to work through a Real-Time Distribution System, you need the IP of the local platform and a username (in
+other words, a DACS user name).
+"""
+
 __all__ = ("Definition", "GrantPassword")
 
 from .._core.session.grant_password import GrantPassword
 
 
 class Definition(object):
     """
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/usage_collection/_logger.py` & `refinitiv-data-1.2.0/refinitiv/data/usage_collection/_logger.py`

 * *Files 2% similar despite different names*

```diff
@@ -63,33 +63,27 @@
         return self._logging_enabled
 
     @property
     def queue(self):
         return self._queue
 
     @staticmethod
-    def _filter_batch(
-        batch: List[LogRecord], logger_filter: Optional[Set[FilterType]]
-    ) -> List[LogRecord]:
+    def _filter_batch(batch: List[LogRecord], logger_filter: Optional[Set[FilterType]]) -> List[LogRecord]:
         """
         Filter the batch by the given filter.
         Parameters
         ----------
         batch : List[LogRecord]
         logger_filter : Optional[Set[FilterType]]
 
         Returns
         -------
 
         """
-        return [
-            record
-            for record in batch
-            if not logger_filter or (logger_filter & record.filter)
-        ]
+        return [record for record in batch if not logger_filter or (logger_filter & record.filter)]
 
     def flush(self) -> None:
         """
         Flush the batch buffer to the loggers.
         Returns
         -------
 
@@ -108,34 +102,28 @@
         -------
 
         """
         with self._lock:
             for logger_config in self._loggers:
                 self._logger_instances.append(
                     (
-                        logger_config.logger_type(
-                            *logger_config.args, **logger_config.kwargs
-                        ),
+                        logger_config.logger_type(*logger_config.args, **logger_config.kwargs),
                         logger_config.filters,
                     )
                 )
             self._loggers = []
 
     def run(self) -> None:
         self._last_flush = time.monotonic()
 
         while True:
             if len(self._loggers) > 0:
                 self._update_loggers()
             try:
-                record = self._queue.get(
-                    timeout=max(
-                        0.1, self._flush_timeout - (time.monotonic() - self._last_flush)
-                    )
-                )
+                record = self._queue.get(timeout=max(0.1, self._flush_timeout - (time.monotonic() - self._last_flush)))
             except Empty:
                 self.flush()
                 continue
             if record is None:
                 break
             self._batch_buffer.append(record)
             if len(self._batch_buffer) >= self._batch_size:
@@ -160,18 +148,15 @@
         """
         if self._logging_enabled:
             if not issubclass(logger, AbstractUsageLogger):
                 raise ValueError("Logger must be a subclass of UsageLogger")
             with self._lock:
                 self._loggers.append(LoggerConfig(logger, args, kwargs, _filter))
         else:
-            raise RuntimeError(
-                "Tried to add a logger to a disabled logger thread. "
-                "Check session config"
-            )
+            raise RuntimeError("Tried to add a logger to a disabled logger thread. Check session config")
 
     def log(self, record: LogRecord) -> None:
         if self._logging_enabled:
             self.queue.put(record)
 
     def log_func(
         self,
```

### Comparing `refinitiv-data-1.1.1/refinitiv/data/usage_collection/_utils.py` & `refinitiv-data-1.2.0/refinitiv/data/usage_collection/_utils.py`

 * *Files identical despite different names*

### Comparing `refinitiv-data-1.1.1/refinitiv_data.egg-info/PKG-INFO` & `refinitiv-data-1.2.0/refinitiv_data.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: refinitiv-data
-Version: 1.1.1
+Version: 1.2.0
 Summary: Client for Refinitiv Data Platform API's
 Author: REFINITIV
 License: Apache 2.0
 Project-URL: Homepage, https://developers.refinitiv.com/en/api-catalog/refinitiv-data-platform/refinitiv-data-library-for-python
 Project-URL: Documentation, https://developers.refinitiv.com/en/api-catalog/refinitiv-data-platform/refinitiv-data-library-for-python/documentation
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
```

### Comparing `refinitiv-data-1.1.1/refinitiv_data.egg-info/SOURCES.txt` & `refinitiv-data-1.2.0/refinitiv_data.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -1,31 +1,35 @@
+CHANGELOG.md
 LICENSE
 README.md
 pyproject.toml
+python
+changes/template.jinja
 refinitiv/__init__.py
 refinitiv/data/__init__.py
+refinitiv/data/_base_enum.py
 refinitiv/data/_config_defaults.py
 refinitiv/data/_config_functions.py
 refinitiv/data/_configure.py
 refinitiv/data/_content_type.py
 refinitiv/data/_errors.py
 refinitiv/data/_log.py
 refinitiv/data/_open_state.py
 refinitiv/data/_types.py
 refinitiv/data/_version.py
 refinitiv/data/errors.py
+refinitiv/data/warnings.py
 refinitiv/data/_core/__init__.py
 refinitiv/data/_core/log_reporter.py
 refinitiv/data/_core/session/__init__.py
 refinitiv/data/_core/session/_dacs_params.py
 refinitiv/data/_core/session/_default_session_manager.py
 refinitiv/data/_core/session/_desktop_session.py
 refinitiv/data/_core/session/_platform_session.py
 refinitiv/data/_core/session/_retry_transport.py
-refinitiv/data/_core/session/_scope_map.py
 refinitiv/data/_core/session/_session.py
 refinitiv/data/_core/session/_session_cxn_factory.py
 refinitiv/data/_core/session/_session_cxn_type.py
 refinitiv/data/_core/session/_session_definition.py
 refinitiv/data/_core/session/_session_provider.py
 refinitiv/data/_core/session/_session_type.py
 refinitiv/data/_core/session/access_token_updater.py
@@ -44,40 +48,39 @@
 refinitiv/data/_external_libraries/python_configuration/LICENSE
 refinitiv/data/_external_libraries/python_configuration/__init__.py
 refinitiv/data/_external_libraries/python_configuration/configuration.py
 refinitiv/data/_external_libraries/python_configuration/configuration_set.py
 refinitiv/data/_external_libraries/python_configuration/helpers.py
 refinitiv/data/_fin_coder_layer/__init__.py
 refinitiv/data/_fin_coder_layer/_containers.py
+refinitiv/data/_fin_coder_layer/_data_df_builder.py
 refinitiv/data/_fin_coder_layer/_data_provider.py
-refinitiv/data/_fin_coder_layer/_history_provider.py
-refinitiv/data/_fin_coder_layer/_history_provider_factory.py
+refinitiv/data/_fin_coder_layer/_history_df_builder.py
+refinitiv/data/_fin_coder_layer/_history_df_builder_factory.py
 refinitiv/data/_fin_coder_layer/_intervals_consts.py
 refinitiv/data/_fin_coder_layer/_mixed_streams.py
 refinitiv/data/_fin_coder_layer/_ohlc_builder.py
 refinitiv/data/_fin_coder_layer/_pricing_recorder.py
 refinitiv/data/_fin_coder_layer/_recording_control.py
 refinitiv/data/_fin_coder_layer/_stream_update_handler.py
-refinitiv/data/_fin_coder_layer/get_data.py
-refinitiv/data/_fin_coder_layer/get_history.py
+refinitiv/data/_fin_coder_layer/get_data_func.py
+refinitiv/data/_fin_coder_layer/get_history_func.py
 refinitiv/data/_fin_coder_layer/get_stream.py
 refinitiv/data/_fin_coder_layer/session.py
 refinitiv/data/_fin_coder_layer/context_collection/__init__.py
 refinitiv/data/_fin_coder_layer/context_collection/_adc_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_adc_rdp_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_adc_udf_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_context_factory.py
 refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_rdp_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_cust_inst_udf_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_hp_and_cust_inst_context.py
 refinitiv/data/_fin_coder_layer/context_collection/_hp_context.py
-refinitiv/data/_fin_coder_layer/context_collection/_hp_rdp_context.py
-refinitiv/data/_fin_coder_layer/context_collection/_hp_udf_context.py
 refinitiv/data/_fin_coder_layer/dates_and_calendars/__init__.py
 refinitiv/data/_fin_coder_layer/dates_and_calendars/_add_periods.py
 refinitiv/data/_fin_coder_layer/dates_and_calendars/_count_periods.py
 refinitiv/data/_fin_coder_layer/dates_and_calendars/_date_schedule.py
 refinitiv/data/_fin_coder_layer/dates_and_calendars/_holidays.py
 refinitiv/data/_fin_coder_layer/dates_and_calendars/_is_working_day.py
 refinitiv/data/_fin_coder_layer/news/__init__.py
@@ -101,29 +104,29 @@
 refinitiv/data/_tools/_patterns.py
 refinitiv/data/_tools/_repr.py
 refinitiv/data/_tools/_specification.py
 refinitiv/data/_tools/_utils.py
 refinitiv/data/_tools/templates.py
 refinitiv/data/content/__init__.py
 refinitiv/data/content/_content_data.py
+refinitiv/data/content/_content_data_factory.py
 refinitiv/data/content/_content_data_provider.py
 refinitiv/data/content/_content_provider_layer.py
 refinitiv/data/content/_content_response_factory.py
 refinitiv/data/content/_df_build_type.py
 refinitiv/data/content/_df_builder.py
 refinitiv/data/content/_df_builder_factory.py
 refinitiv/data/content/_entire_data_provider.py
 refinitiv/data/content/_error_parser.py
 refinitiv/data/content/_historical_content_validator.py
 refinitiv/data/content/_historical_data_provider.py
 refinitiv/data/content/_historical_df_builder.py
-refinitiv/data/content/_historical_join_responses.py
+refinitiv/data/content/_historical_raw_transf.py
 refinitiv/data/content/_historical_response_factory.py
 refinitiv/data/content/_intervals.py
-refinitiv/data/content/_join_responses.py
 refinitiv/data/content/_universe_content_validator.py
 refinitiv/data/content/_universe_stream.py
 refinitiv/data/content/_universe_streams.py
 refinitiv/data/content/custom_instruments/__init__.py
 refinitiv/data/content/custom_instruments/_custom_instruments_data_provider.py
 refinitiv/data/content/custom_instruments/_definition.py
 refinitiv/data/content/custom_instruments/_enums.py
@@ -198,24 +201,29 @@
 refinitiv/data/content/estimates/view_summary_kpi/_historical_snapshots_kpi_definition.py
 refinitiv/data/content/estimates/view_summary_kpi/_interim_definition.py
 refinitiv/data/content/estimates/view_summary_kpi/annual.py
 refinitiv/data/content/estimates/view_summary_kpi/historical_snapshots_kpi.py
 refinitiv/data/content/estimates/view_summary_kpi/interim.py
 refinitiv/data/content/filings/__init__.py
 refinitiv/data/content/filings/_errors.py
+refinitiv/data/content/filings/_feed_name.py
+refinitiv/data/content/filings/_filing_query.py
 refinitiv/data/content/filings/_retrieval_data_provider.py
 refinitiv/data/content/filings/_retrieval_definition.py
 refinitiv/data/content/filings/_search_data_provider.py
 refinitiv/data/content/filings/_search_definition.py
 refinitiv/data/content/filings/retrieval.py
 refinitiv/data/content/filings/search.py
 refinitiv/data/content/fundamental_and_reference/__init__.py
+refinitiv/data/content/fundamental_and_reference/_content_validator.py
 refinitiv/data/content/fundamental_and_reference/_data_grid_type.py
 refinitiv/data/content/fundamental_and_reference/_data_provider.py
 refinitiv/data/content/fundamental_and_reference/_definition.py
+refinitiv/data/content/fundamental_and_reference/_request_factory.py
+refinitiv/data/content/fundamental_and_reference/_response_factory.py
 refinitiv/data/content/historical_pricing/__init__.py
 refinitiv/data/content/historical_pricing/_enums.py
 refinitiv/data/content/historical_pricing/_events_definition.py
 refinitiv/data/content/historical_pricing/_historical_pricing_data_provider.py
 refinitiv/data/content/historical_pricing/_historical_pricing_request_factory.py
 refinitiv/data/content/historical_pricing/_summaries_definition.py
 refinitiv/data/content/historical_pricing/events.py
@@ -227,25 +235,27 @@
 refinitiv/data/content/ipa/_object_definition.py
 refinitiv/data/content/ipa/_curves/__init__.py
 refinitiv/data/content/ipa/_curves/_curves_builder_df.py
 refinitiv/data/content/ipa/_curves/_curves_data_provider.py
 refinitiv/data/content/ipa/_curves/_forward_curve_definition.py
 refinitiv/data/content/ipa/_curves/_forward_curve_request_item.py
 refinitiv/data/content/ipa/_curves/_forward_curve_types.py
-refinitiv/data/content/ipa/_curves/_interest_rate_curve_get_definition.py
+refinitiv/data/content/ipa/_curves/_shift_scenario.py
 refinitiv/data/content/ipa/_curves/_swap_zc_curve_definition.py
 refinitiv/data/content/ipa/_curves/_swap_zc_curve_parameters.py
 refinitiv/data/content/ipa/_curves/_zc_curve_definition.py
+refinitiv/data/content/ipa/_curves/_zc_curve_definition_request.py
 refinitiv/data/content/ipa/_curves/_zc_curve_definitions.py
 refinitiv/data/content/ipa/_curves/_zc_curve_parameters.py
 refinitiv/data/content/ipa/_curves/_zc_curve_request_item.py
 refinitiv/data/content/ipa/_curves/_zc_curve_types.py
 refinitiv/data/content/ipa/_curves/_bond_curves/__init__.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_types.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_curves/__init__.py
+refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_constituents.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_definition.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_credit_curve_parameters.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_curves/_request.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_enums/__init__.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_basis_spline_smooth_model.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_business_sector.py
 refinitiv/data/content/ipa/_curves/_bond_curves/_enums/_calendar_adjustment.py
@@ -266,26 +276,26 @@
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/__init__.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_types.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/__init__.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_butterfly_shift.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_combined_shift.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_curve_definition_pricing.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_flattening_shift.py
+refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_constituents.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_definition.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_forward_curve_parameters.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_fx_shift_scenario.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_long_end_shift.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_parallel_shift.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_request.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_shift_definition.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_short_end_shift.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_time_bucket_shift.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_twist_shift.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_types.py
-refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_valuation_time.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/__init__.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_array_main_constituent_asset_class.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_array_risk_type.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_constituent_override_mode.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_shift_type.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_curves/_enums/_shift_unit.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_definitions/__init__.py
@@ -332,30 +342,33 @@
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_main_constituent_asset_class.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_enums/_risk_type.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/__init__.py
 refinitiv/data/content/ipa/_curves/_cross_currency_curves/_triangulate_definitions/_request.py
 refinitiv/data/content/ipa/_curves/_enums/__init__.py
 refinitiv/data/content/ipa/_curves/_enums/_calendar_adjustment.py
 refinitiv/data/content/ipa/_curves/_enums/_compounding_type.py
+refinitiv/data/content/ipa/_curves/_enums/_constituent_override_mode.py
 refinitiv/data/content/ipa/_curves/_enums/_forward_curves_outputs.py
 refinitiv/data/content/ipa/_curves/_enums/_instrument_type.py
 refinitiv/data/content/ipa/_curves/_enums/_main_constituent_asset_class.py
 refinitiv/data/content/ipa/_curves/_enums/_market_data_access_denied_fallback.py
 refinitiv/data/content/ipa/_curves/_enums/_swap_price_side.py
 refinitiv/data/content/ipa/_curves/_enums/_zc_curves_outputs.py
 refinitiv/data/content/ipa/_curves/_enums/_zc_interpolation_mode.py
 refinitiv/data/content/ipa/_curves/_models/__init__.py
 refinitiv/data/content/ipa/_curves/_models/_constituents.py
 refinitiv/data/content/ipa/_curves/_models/_convexity.py
 refinitiv/data/content/ipa/_curves/_models/_curve.py
 refinitiv/data/content/ipa/_curves/_models/_curve_point.py
 refinitiv/data/content/ipa/_curves/_models/_instrument.py
 refinitiv/data/content/ipa/_curves/_models/_interest_rate_curve_parameters.py
+refinitiv/data/content/ipa/_curves/_models/_par_rate_shift.py
 refinitiv/data/content/ipa/_curves/_models/_step.py
 refinitiv/data/content/ipa/_curves/_models/_turn.py
+refinitiv/data/content/ipa/_curves/_models/_valuation_time.py
 refinitiv/data/content/ipa/_enums/__init__.py
 refinitiv/data/content/ipa/_enums/_adjust_interest_to_payment_date.py
 refinitiv/data/content/ipa/_enums/_american_monte_carlo_method.py
 refinitiv/data/content/ipa/_enums/_amortization_frequency.py
 refinitiv/data/content/ipa/_enums/_amortization_type.py
 refinitiv/data/content/ipa/_enums/_asset_class.py
 refinitiv/data/content/ipa/_enums/_average_type.py
@@ -464,69 +477,67 @@
 refinitiv/data/content/ipa/_models/_payout_scaling.py
 refinitiv/data/content/ipa/_models/_pde_parameters.py
 refinitiv/data/content/ipa/_surfaces/__init__.py
 refinitiv/data/content/ipa/_surfaces/_cap_surface_request_item.py
 refinitiv/data/content/ipa/_surfaces/_eti_surface_definition.py
 refinitiv/data/content/ipa/_surfaces/_eti_surface_parameters.py
 refinitiv/data/content/ipa/_surfaces/_eti_surface_request_item.py
+refinitiv/data/content/ipa/_surfaces/_fx_statistics_parameters.py
 refinitiv/data/content/ipa/_surfaces/_fx_surface_definition.py
 refinitiv/data/content/ipa/_surfaces/_fx_surface_parameters.py
 refinitiv/data/content/ipa/_surfaces/_fx_surface_request_item.py
 refinitiv/data/content/ipa/_surfaces/_i_ir_vol_model_definition.py
 refinitiv/data/content/ipa/_surfaces/_i_ir_vol_model_pricing_parameters.py
 refinitiv/data/content/ipa/_surfaces/_surface_request_item.py
 refinitiv/data/content/ipa/_surfaces/_surface_types.py
 refinitiv/data/content/ipa/_surfaces/_surfaces_data_provider.py
+refinitiv/data/content/ipa/_surfaces/_swaption_calculation_params.py
+refinitiv/data/content/ipa/_surfaces/_swaption_surface_definition.py
 refinitiv/data/content/ipa/_surfaces/_swaption_surface_request_item.py
 refinitiv/data/content/ipa/_surfaces/_enums/__init__.py
+refinitiv/data/content/ipa/_surfaces/_enums/_calibration_type.py
 refinitiv/data/content/ipa/_surfaces/_enums/_moneyness_type.py
 refinitiv/data/content/ipa/_surfaces/_enums/_outputs.py
+refinitiv/data/content/ipa/_surfaces/_enums/_strike_type.py
+refinitiv/data/content/ipa/_surfaces/_enums/_volatility_type.py
 refinitiv/data/content/ipa/_surfaces/_models/__init__.py
 refinitiv/data/content/ipa/_surfaces/_models/_maturity_filter.py
 refinitiv/data/content/ipa/_surfaces/_models/_moneyness_weight.py
 refinitiv/data/content/ipa/_surfaces/_models/_strike_filter.py
 refinitiv/data/content/ipa/_surfaces/_models/_strike_filter_range.py
 refinitiv/data/content/ipa/_surfaces/_models/_surface.py
 refinitiv/data/content/ipa/_surfaces/_models/_surface_filters.py
 refinitiv/data/content/ipa/_surfaces/_models/_surface_output.py
 refinitiv/data/content/ipa/_surfaces/_models/_surface_point.py
 refinitiv/data/content/ipa/_surfaces/_models/_volatility_surface_point.py
 refinitiv/data/content/ipa/curves/__init__.py
 refinitiv/data/content/ipa/curves/_bond_curves/__init__.py
 refinitiv/data/content/ipa/curves/_bond_curves/curves/__init__.py
 refinitiv/data/content/ipa/curves/_bond_curves/curves/_definition.py
-refinitiv/data/content/ipa/curves/_bond_curves/curves/_enums.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/__init__.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/_arg_enums.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/_base_data_class.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/__init__.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/_definition.py
-refinitiv/data/content/ipa/curves/_cross_currency_curves/curves/_enums.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/__init__.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_data_classes.py
-refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_enums.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_manage.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/_search.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/manage.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/definitions/search.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/__init__.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_data_provider.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/_search.py
 refinitiv/data/content/ipa/curves/_cross_currency_curves/triangulate_definitions/search.py
 refinitiv/data/content/ipa/curves/forward_curves/__init__.py
 refinitiv/data/content/ipa/curves/forward_curves/_definition.py
-refinitiv/data/content/ipa/curves/forward_curves/_enums.py
-refinitiv/data/content/ipa/curves/forward_curves/_models.py
 refinitiv/data/content/ipa/curves/zc_curve_definitions/__init__.py
 refinitiv/data/content/ipa/curves/zc_curve_definitions/_definition.py
-refinitiv/data/content/ipa/curves/zc_curve_definitions/_enums.py
 refinitiv/data/content/ipa/curves/zc_curves/__init__.py
 refinitiv/data/content/ipa/curves/zc_curves/_definition.py
-refinitiv/data/content/ipa/curves/zc_curves/_enums.py
-refinitiv/data/content/ipa/curves/zc_curves/_models.py
 refinitiv/data/content/ipa/dates_and_calendars/__init__.py
 refinitiv/data/content/ipa/dates_and_calendars/_base_request_items.py
 refinitiv/data/content/ipa/dates_and_calendars/_content_data_validator.py
 refinitiv/data/content/ipa/dates_and_calendars/_request_factory.py
 refinitiv/data/content/ipa/dates_and_calendars/add_periods/__init__.py
 refinitiv/data/content/ipa/dates_and_calendars/add_periods/_add_periods_data_provider.py
 refinitiv/data/content/ipa/dates_and_calendars/add_periods/_definition.py
@@ -549,40 +560,31 @@
 refinitiv/data/content/ipa/financial_contracts/_instrument_definition.py
 refinitiv/data/content/ipa/financial_contracts/_quantitative_data_stream.py
 refinitiv/data/content/ipa/financial_contracts/_stream_facade.py
 refinitiv/data/content/ipa/financial_contracts/bond/__init__.py
 refinitiv/data/content/ipa/financial_contracts/bond/_bond_definition.py
 refinitiv/data/content/ipa/financial_contracts/bond/_bond_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/bond/_definition.py
-refinitiv/data/content/ipa/financial_contracts/bond/_enums.py
-refinitiv/data/content/ipa/financial_contracts/bond/_models.py
 refinitiv/data/content/ipa/financial_contracts/cap_floor/__init__.py
 refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_definition.py
 refinitiv/data/content/ipa/financial_contracts/cap_floor/_cap_floor_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/cap_floor/_definition.py
-refinitiv/data/content/ipa/financial_contracts/cap_floor/_enums.py
-refinitiv/data/content/ipa/financial_contracts/cap_floor/_models.py
 refinitiv/data/content/ipa/financial_contracts/cds/__init__.py
 refinitiv/data/content/ipa/financial_contracts/cds/_cds_definition.py
 refinitiv/data/content/ipa/financial_contracts/cds/_cds_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/cds/_definition.py
-refinitiv/data/content/ipa/financial_contracts/cds/_enums.py
-refinitiv/data/content/ipa/financial_contracts/cds/_models.py
 refinitiv/data/content/ipa/financial_contracts/cds/_premium_leg_definition.py
 refinitiv/data/content/ipa/financial_contracts/cds/_protection_leg_definition.py
 refinitiv/data/content/ipa/financial_contracts/cross/__init__.py
 refinitiv/data/content/ipa/financial_contracts/cross/_definition.py
-refinitiv/data/content/ipa/financial_contracts/cross/_enums.py
 refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_definition.py
 refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_leg_definition.py
 refinitiv/data/content/ipa/financial_contracts/cross/_fx_cross_pricing_parameters.py
-refinitiv/data/content/ipa/financial_contracts/cross/_models.py
 refinitiv/data/content/ipa/financial_contracts/option/__init__.py
 refinitiv/data/content/ipa/financial_contracts/option/_definition.py
-refinitiv/data/content/ipa/financial_contracts/option/_models.py
 refinitiv/data/content/ipa/financial_contracts/option/_option_definition.py
 refinitiv/data/content/ipa/financial_contracts/option/_option_instrument_definition.py
 refinitiv/data/content/ipa/financial_contracts/option/_option_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/option/_base/__init__.py
 refinitiv/data/content/ipa/financial_contracts/option/_base/_barrier_definition.py
 refinitiv/data/content/ipa/financial_contracts/option/_base/_binary_definition.py
 refinitiv/data/content/ipa/financial_contracts/option/_base/_info.py
@@ -609,86 +611,94 @@
 refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_barrier_info.py
 refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_double_binary_definition.py
 refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_dual_currency_definition.py
 refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_forward_start.py
 refinitiv/data/content/ipa/financial_contracts/option/_fx/_fx_underlying_definition.py
 refinitiv/data/content/ipa/financial_contracts/repo/__init__.py
 refinitiv/data/content/ipa/financial_contracts/repo/_definition.py
-refinitiv/data/content/ipa/financial_contracts/repo/_enums.py
 refinitiv/data/content/ipa/financial_contracts/repo/_repo_definition.py
 refinitiv/data/content/ipa/financial_contracts/repo/_repo_parameters.py
 refinitiv/data/content/ipa/financial_contracts/repo/_repo_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_contract.py
 refinitiv/data/content/ipa/financial_contracts/repo/_repo_underlying_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/swap/__init__.py
 refinitiv/data/content/ipa/financial_contracts/swap/_definition.py
-refinitiv/data/content/ipa/financial_contracts/swap/_enums.py
-refinitiv/data/content/ipa/financial_contracts/swap/_models.py
 refinitiv/data/content/ipa/financial_contracts/swap/_swap_definition.py
 refinitiv/data/content/ipa/financial_contracts/swap/_swap_leg_definition.py
 refinitiv/data/content/ipa/financial_contracts/swap/_swap_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/swaption/__init__.py
 refinitiv/data/content/ipa/financial_contracts/swaption/_bermudan_swaption_definition.py
 refinitiv/data/content/ipa/financial_contracts/swaption/_definition.py
-refinitiv/data/content/ipa/financial_contracts/swaption/_enums.py
-refinitiv/data/content/ipa/financial_contracts/swaption/_models.py
 refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_definition.py
 refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_market_data_rule.py
 refinitiv/data/content/ipa/financial_contracts/swaption/_swaption_pricing_parameters.py
 refinitiv/data/content/ipa/financial_contracts/term_deposit/__init__.py
 refinitiv/data/content/ipa/financial_contracts/term_deposit/_definition.py
-refinitiv/data/content/ipa/financial_contracts/term_deposit/_enums.py
 refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_definition.py
 refinitiv/data/content/ipa/financial_contracts/term_deposit/_term_deposit_pricing_parameters.py
 refinitiv/data/content/ipa/surfaces/__init__.py
 refinitiv/data/content/ipa/surfaces/_definition.py
 refinitiv/data/content/ipa/surfaces/cap/__init__.py
 refinitiv/data/content/ipa/surfaces/cap/_definition.py
-refinitiv/data/content/ipa/surfaces/cap/_enums.py
-refinitiv/data/content/ipa/surfaces/cap/_models.py
 refinitiv/data/content/ipa/surfaces/eti/__init__.py
 refinitiv/data/content/ipa/surfaces/eti/_definition.py
-refinitiv/data/content/ipa/surfaces/eti/_enums.py
-refinitiv/data/content/ipa/surfaces/eti/_models.py
 refinitiv/data/content/ipa/surfaces/fx/__init__.py
 refinitiv/data/content/ipa/surfaces/fx/_definition.py
-refinitiv/data/content/ipa/surfaces/fx/_enums.py
-refinitiv/data/content/ipa/surfaces/fx/_models.py
 refinitiv/data/content/ipa/surfaces/swaption/__init__.py
 refinitiv/data/content/ipa/surfaces/swaption/_definition.py
-refinitiv/data/content/ipa/surfaces/swaption/_enums.py
-refinitiv/data/content/ipa/surfaces/swaption/_models.py
 refinitiv/data/content/news/__init__.py
-refinitiv/data/content/news/_data_classes.py
-refinitiv/data/content/news/_headlines_definition.py
-refinitiv/data/content/news/_news_data_provider.py
+refinitiv/data/content/news/_content_validator_udf.py
+refinitiv/data/content/news/_df_builder.py
+refinitiv/data/content/news/_news_data.py
 refinitiv/data/content/news/_news_data_provider_layer.py
-refinitiv/data/content/news/_sort_order.py
-refinitiv/data/content/news/_story_definition.py
 refinitiv/data/content/news/_tools.py
-refinitiv/data/content/news/_udf_html_parser.py
 refinitiv/data/content/news/_urgency.py
-refinitiv/data/content/news/headlines.py
-refinitiv/data/content/news/story.py
+refinitiv/data/content/news/headlines/__init__.py
+refinitiv/data/content/news/headlines/_data.py
+refinitiv/data/content/news/headlines/_data_provider.py
+refinitiv/data/content/news/headlines/_definition.py
+refinitiv/data/content/news/headlines/_request_factory.py
+refinitiv/data/content/news/headlines/_sort_order.py
 refinitiv/data/content/news/images/__init__.py
 refinitiv/data/content/news/images/_data_provider.py
 refinitiv/data/content/news/images/_definition.py
 refinitiv/data/content/news/images/_image.py
+refinitiv/data/content/news/online_reports/__init__.py
+refinitiv/data/content/news/online_reports/_data_provider.py
+refinitiv/data/content/news/online_reports/_definition.py
+refinitiv/data/content/news/online_reports/_df_builder.py
+refinitiv/data/content/news/online_reports/_image_data.py
+refinitiv/data/content/news/online_reports/_news_item.py
+refinitiv/data/content/news/online_reports/_report.py
+refinitiv/data/content/news/online_reports/hierarchy/__init__.py
+refinitiv/data/content/news/online_reports/hierarchy/_data_provider.py
+refinitiv/data/content/news/online_reports/hierarchy/_definition.py
+refinitiv/data/content/news/online_reports/hierarchy/_df_builder.py
+refinitiv/data/content/news/online_reports/hierarchy/_report.py
+refinitiv/data/content/news/story/__init__.py
+refinitiv/data/content/news/story/_data.py
+refinitiv/data/content/news/story/_data_provider.py
+refinitiv/data/content/news/story/_definition.py
+refinitiv/data/content/news/story/_request_factory.py
+refinitiv/data/content/news/story/_response.py
+refinitiv/data/content/news/story/_response_factory.py
+refinitiv/data/content/news/story/_udf_html_parser.py
 refinitiv/data/content/news/top_news/__init__.py
 refinitiv/data/content/news/top_news/_data_provider.py
 refinitiv/data/content/news/top_news/_definition.py
 refinitiv/data/content/news/top_news/_df_builder.py
 refinitiv/data/content/news/top_news/_top_news_headline.py
 refinitiv/data/content/news/top_news/hierarchy/__init__.py
 refinitiv/data/content/news/top_news/hierarchy/_data_provider.py
 refinitiv/data/content/news/top_news/hierarchy/_definition.py
 refinitiv/data/content/news/top_news/hierarchy/_df_builder.py
 refinitiv/data/content/news/top_news/hierarchy/_subcategory.py
 refinitiv/data/content/news/top_news/hierarchy/_top_news_id.py
 refinitiv/data/content/ownership/__init__.py
+refinitiv/data/content/ownership/_df_builder.py
 refinitiv/data/content/ownership/_enums.py
 refinitiv/data/content/ownership/_org_info_definition.py
 refinitiv/data/content/ownership/_ownership_data_provider.py
 refinitiv/data/content/ownership/org_info.py
 refinitiv/data/content/ownership/consolidated/__init__.py
 refinitiv/data/content/ownership/consolidated/_breakdown_definition.py
 refinitiv/data/content/ownership/consolidated/_concentration_definition.py
@@ -756,14 +766,15 @@
 refinitiv/data/content/symbol_conversion/_definition.py
 refinitiv/data/content/symbol_conversion/_symbol_type.py
 refinitiv/data/content/trade_data_service/__init__.py
 refinitiv/data/content/trade_data_service/_definition.py
 refinitiv/data/content/trade_data_service/_stream.py
 refinitiv/data/content/trade_data_service/_stream_facade.py
 refinitiv/data/delivery/__init__.py
+refinitiv/data/delivery/_dictionary.py
 refinitiv/data/delivery/endpoint_request.py
 refinitiv/data/delivery/omm_stream.py
 refinitiv/data/delivery/rdp_stream.py
 refinitiv/data/delivery/_data/__init__.py
 refinitiv/data/delivery/_data/_api_type.py
 refinitiv/data/delivery/_data/_connection.py
 refinitiv/data/delivery/_data/_data_factory.py
@@ -808,14 +819,22 @@
 refinitiv/data/delivery/_stream/stream_state_manager.py
 refinitiv/data/delivery/_stream/contrib/__init__.py
 refinitiv/data/delivery/_stream/contrib/_funcs.py
 refinitiv/data/delivery/_stream/contrib/_offstream.py
 refinitiv/data/delivery/_stream/contrib/_response.py
 refinitiv/data/delivery/_stream/contrib/_stream_connection.py
 refinitiv/data/delivery/_stream/contrib/_type.py
+refinitiv/data/delivery/_stream/metadata/__init__.py
+refinitiv/data/delivery/_stream/metadata/_dictionary.py
+refinitiv/data/delivery/_stream/metadata/_dictionary_type.py
+refinitiv/data/delivery/_stream/metadata/_enum_type_entry.py
+refinitiv/data/delivery/_stream/metadata/_field_description.py
+refinitiv/data/delivery/_stream/metadata/_stream.py
+refinitiv/data/delivery/_stream/metadata/_types.py
+refinitiv/data/delivery/_stream/metadata/_validator.py
 refinitiv/data/delivery/_stream/rwf/__init__.py
 refinitiv/data/delivery/_stream/rwf/conversion.py
 refinitiv/data/delivery/_stream/rwf/ema.py
 refinitiv/data/delivery/_stream/rwf/socket.py
 refinitiv/data/delivery/_stream/ws/__init__.py
 refinitiv/data/delivery/_stream/ws/ws_client.py
 refinitiv/data/delivery/cfs/__init__.py
@@ -837,27 +856,39 @@
 refinitiv/data/delivery/cfs/file_downloader.py
 refinitiv/data/delivery/cfs/file_sets.py
 refinitiv/data/delivery/cfs/files.py
 refinitiv/data/delivery/cfs/packages.py
 refinitiv/data/discovery/__init__.py
 refinitiv/data/discovery/_convert_symbols.py
 refinitiv/data/discovery/_search.py
+refinitiv/data/discovery/_search_explorer/__init__.py
+refinitiv/data/discovery/_search_explorer/_dataclasses.py
+refinitiv/data/discovery/_search_explorer/_search_explorer.py
 refinitiv/data/discovery/_search_templates/__init__.py
 refinitiv/data/discovery/_search_templates/base.py
 refinitiv/data/discovery/_search_templates/embedded.py
 refinitiv/data/discovery/_search_templates/manage.py
 refinitiv/data/discovery/_search_templates/namespaces.py
 refinitiv/data/discovery/_search_templates/search.py
 refinitiv/data/discovery/_search_templates/utils.py
+refinitiv/data/discovery/_stakeholders/__init__.py
+refinitiv/data/discovery/_stakeholders/_customers.py
+refinitiv/data/discovery/_stakeholders/_fetch_data.py
+refinitiv/data/discovery/_stakeholders/_relationship_type.py
+refinitiv/data/discovery/_stakeholders/_stakeholder_data.py
+refinitiv/data/discovery/_stakeholders/_stakeholders.py
+refinitiv/data/discovery/_stakeholders/_suppliers.py
 refinitiv/data/discovery/_universe_expanders/__init__.py
 refinitiv/data/discovery/_universe_expanders/_chain.py
 refinitiv/data/discovery/_universe_expanders/_discovery_universe.py
 refinitiv/data/discovery/_universe_expanders/_peers.py
 refinitiv/data/discovery/_universe_expanders/_screener.py
 refinitiv/data/discovery/_universe_expanders/_universe_expander.py
+refinitiv/data/early_access/__init__.py
+refinitiv/data/early_access/discovery/__init__.py
 refinitiv/data/eikon/__init__.py
 refinitiv/data/eikon/_data_grid.py
 refinitiv/data/eikon/_errors.py
 refinitiv/data/eikon/_json_requests.py
 refinitiv/data/eikon/_news_request.py
 refinitiv/data/eikon/_streaming_prices.py
 refinitiv/data/eikon/_symbology.py
```

