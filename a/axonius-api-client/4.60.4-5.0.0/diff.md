# Comparing `tmp/axonius_api_client-4.60.4-py2.py3-none-any.whl.zip` & `tmp/axonius_api_client-5.0.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,137 +1,161 @@
-Zip file size: 805895 bytes, number of entries: 527
--rw-r--r--  2.0 unx     2409 b- defN 23-May-02 00:40 axonius_api_client/__init__.py
--rw-r--r--  2.0 unx    23367 b- defN 23-May-02 00:40 axonius_api_client/connect.py
+Zip file size: 905887 bytes, number of entries: 605
+-rw-r--r--  2.0 unx     3139 b- defN 23-May-02 15:09 axonius_api_client/__init__.py
+-rw-r--r--  2.0 unx    34483 b- defN 23-May-02 15:09 axonius_api_client/connect.py
 -rw-r--r--  2.0 unx     4025 b- defN 23-Apr-27 20:53 axonius_api_client/data.py
--rw-r--r--  2.0 unx    15376 b- defN 23-May-02 00:40 axonius_api_client/exceptions.py
+-rw-r--r--  2.0 unx    15429 b- defN 23-May-02 15:09 axonius_api_client/exceptions.py
 -rw-r--r--  2.0 unx     5153 b- defN 23-Mar-09 15:18 axonius_api_client/features.py
--rw-r--r--  2.0 unx    19884 b- defN 23-May-02 00:40 axonius_api_client/http.py
+-rw-r--r--  2.0 unx    31439 b- defN 23-May-02 15:09 axonius_api_client/http.py
 -rw-r--r--  2.0 unx    12568 b- defN 23-Apr-27 20:55 axonius_api_client/logs.py
--rw-r--r--  2.0 unx     9825 b- defN 23-May-02 00:40 axonius_api_client/setup_env.py
--rw-r--r--  2.0 unx    69564 b- defN 23-May-02 00:40 axonius_api_client/tools.py
--rw-r--r--  2.0 unx    20485 b- defN 23-May-02 00:40 axonius_api_client/utils.py
--rw-r--r--  2.0 unx     1022 b- defN 23-May-02 00:40 axonius_api_client/version.py
--rw-r--r--  2.0 unx     1255 b- defN 23-May-02 00:40 axonius_api_client/api/__init__.py
--rw-r--r--  2.0 unx    18430 b- defN 23-May-02 00:40 axonius_api_client/api/api_endpoint.py
--rw-r--r--  2.0 unx    50479 b- defN 23-May-02 00:40 axonius_api_client/api/api_endpoints.py
--rw-r--r--  2.0 unx     2304 b- defN 23-May-02 00:40 axonius_api_client/api/mixins.py
+-rw-r--r--  2.0 unx    23869 b- defN 23-May-02 15:09 axonius_api_client/setup_env.py
+-rw-r--r--  2.0 unx    76664 b- defN 23-May-02 15:09 axonius_api_client/tools.py
+-rw-r--r--  2.0 unx     1021 b- defN 23-May-02 15:09 axonius_api_client/version.py
+-rw-r--r--  2.0 unx     1368 b- defN 23-May-02 15:09 axonius_api_client/api/__init__.py
+-rw-r--r--  2.0 unx    19821 b- defN 23-May-02 15:09 axonius_api_client/api/api_endpoint.py
+-rw-r--r--  2.0 unx    54346 b- defN 23-May-02 15:09 axonius_api_client/api/api_endpoints.py
+-rw-r--r--  2.0 unx     3284 b- defN 23-May-02 15:09 axonius_api_client/api/mixins.py
 -rw-r--r--  2.0 unx      161 b- defN 21-Dec-14 21:37 axonius_api_client/api/adapters/__init__.py
--rw-r--r--  2.0 unx    25243 b- defN 23-May-02 00:40 axonius_api_client/api/adapters/adapters.py
--rw-r--r--  2.0 unx    46603 b- defN 23-May-02 00:40 axonius_api_client/api/adapters/cnx.py
--rw-r--r--  2.0 unx      488 b- defN 23-May-02 00:40 axonius_api_client/api/asset_callbacks/__init__.py
--rw-r--r--  2.0 unx    49862 b- defN 23-May-02 00:40 axonius_api_client/api/asset_callbacks/base.py
--rw-r--r--  2.0 unx     7510 b- defN 23-May-02 00:40 axonius_api_client/api/asset_callbacks/base_csv.py
+-rw-r--r--  2.0 unx    25244 b- defN 23-May-02 15:09 axonius_api_client/api/adapters/adapters.py
+-rw-r--r--  2.0 unx    46857 b- defN 23-May-02 15:09 axonius_api_client/api/adapters/cnx.py
+-rw-r--r--  2.0 unx      522 b- defN 23-May-02 15:09 axonius_api_client/api/asset_callbacks/__init__.py
+-rw-r--r--  2.0 unx    50174 b- defN 23-May-02 15:09 axonius_api_client/api/asset_callbacks/base.py
+-rw-r--r--  2.0 unx     7609 b- defN 23-May-02 15:09 axonius_api_client/api/asset_callbacks/base_csv.py
 -rw-r--r--  2.0 unx     4497 b- defN 23-Apr-27 20:19 axonius_api_client/api/asset_callbacks/base_json.py
 -rw-r--r--  2.0 unx     5817 b- defN 23-Apr-27 20:19 axonius_api_client/api/asset_callbacks/base_json_to_csv.py
 -rw-r--r--  2.0 unx     6435 b- defN 23-Apr-27 20:19 axonius_api_client/api/asset_callbacks/base_table.py
--rw-r--r--  2.0 unx     6043 b- defN 23-May-02 00:40 axonius_api_client/api/asset_callbacks/base_xlsx.py
+-rw-r--r--  2.0 unx     6142 b- defN 23-May-02 15:09 axonius_api_client/api/asset_callbacks/base_xlsx.py
 -rw-r--r--  2.0 unx     3823 b- defN 23-Apr-27 20:18 axonius_api_client/api/asset_callbacks/base_xml.py
--rw-r--r--  2.0 unx      744 b- defN 23-May-02 00:40 axonius_api_client/api/asset_callbacks/tools.py
+-rw-r--r--  2.0 unx      757 b- defN 23-May-02 15:09 axonius_api_client/api/asset_callbacks/tools.py
 -rw-r--r--  2.0 unx      489 b- defN 23-Mar-09 15:18 axonius_api_client/api/assets/__init__.py
--rw-r--r--  2.0 unx    69493 b- defN 23-May-02 00:40 axonius_api_client/api/assets/asset_mixin.py
--rw-r--r--  2.0 unx     7767 b- defN 23-May-02 00:40 axonius_api_client/api/assets/devices.py
--rw-r--r--  2.0 unx    22191 b- defN 23-May-02 00:40 axonius_api_client/api/assets/fields.py
+-rw-r--r--  2.0 unx    86029 b- defN 23-May-02 15:09 axonius_api_client/api/assets/asset_mixin.py
+-rw-r--r--  2.0 unx     7758 b- defN 23-May-02 15:09 axonius_api_client/api/assets/devices.py
+-rw-r--r--  2.0 unx    23388 b- defN 23-May-02 15:09 axonius_api_client/api/assets/fields.py
 -rw-r--r--  2.0 unx     5207 b- defN 23-Apr-27 20:30 axonius_api_client/api/assets/labels.py
--rw-r--r--  2.0 unx    20162 b- defN 23-May-02 00:40 axonius_api_client/api/assets/runner.py
--rw-r--r--  2.0 unx    48521 b- defN 23-May-02 00:40 axonius_api_client/api/assets/saved_query.py
+-rw-r--r--  2.0 unx    20161 b- defN 23-May-02 15:09 axonius_api_client/api/assets/runner.py
+-rw-r--r--  2.0 unx    49441 b- defN 23-May-02 15:09 axonius_api_client/api/assets/saved_query.py
 -rw-r--r--  2.0 unx     5226 b- defN 23-Apr-27 20:13 axonius_api_client/api/assets/users.py
 -rw-r--r--  2.0 unx     1726 b- defN 22-Jul-05 03:10 axonius_api_client/api/assets/vulnerabilities.py
 -rw-r--r--  2.0 unx      179 b- defN 23-Apr-08 13:43 axonius_api_client/api/enforcements/__init__.py
--rw-r--r--  2.0 unx    41652 b- defN 23-May-02 00:40 axonius_api_client/api/enforcements/enforcements.py
--rw-r--r--  2.0 unx     4288 b- defN 23-May-02 00:40 axonius_api_client/api/enforcements/tasks.py
+-rw-r--r--  2.0 unx    40371 b- defN 23-May-02 15:09 axonius_api_client/api/enforcements/enforcements.py
+-rw-r--r--  2.0 unx    16252 b- defN 23-May-02 15:09 axonius_api_client/api/enforcements/tasks.py
 -rw-r--r--  2.0 unx      208 b- defN 23-Mar-09 15:18 axonius_api_client/api/folders/__init__.py
--rw-r--r--  2.0 unx    20842 b- defN 23-May-02 00:40 axonius_api_client/api/folders/folders.py
--rw-r--r--  2.0 unx     1137 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/__init__.py
--rw-r--r--  2.0 unx     3644 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/account.py
--rw-r--r--  2.0 unx    58615 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/adapters.py
--rw-r--r--  2.0 unx    22751 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/assets.py
--rw-r--r--  2.0 unx     4791 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/audit_logs.py
--rw-r--r--  2.0 unx    22598 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/base.py
--rw-r--r--  2.0 unx     6705 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/base2.py
+-rw-r--r--  2.0 unx    20856 b- defN 23-May-02 15:09 axonius_api_client/api/folders/folders.py
+-rw-r--r--  2.0 unx     1219 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/__init__.py
+-rw-r--r--  2.0 unx    24980 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/base.py
+-rw-r--r--  2.0 unx     6649 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/base2.py
 -rw-r--r--  2.0 unx     3498 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/central_core.py
--rw-r--r--  2.0 unx     1375 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/count_operator.py
--rw-r--r--  2.0 unx     6277 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/custom_fields.py
+-rw-r--r--  2.0 unx     2169 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/count_operator.py
+-rw-r--r--  2.0 unx     6495 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/custom_fields.py
 -rw-r--r--  2.0 unx    25113 b- defN 23-Mar-31 22:23 axonius_api_client/api/json_api/dashboard_spaces.py
--rw-r--r--  2.0 unx     9715 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/data_scopes.py
--rw-r--r--  2.0 unx     1717 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/duration_operator.py
--rw-r--r--  2.0 unx    57753 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/enforcements.py
--rw-r--r--  2.0 unx     6931 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/generic.py
+-rw-r--r--  2.0 unx     9714 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/data_scopes.py
+-rw-r--r--  2.0 unx     1943 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/duration_operator.py
+-rw-r--r--  2.0 unx    57755 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/enforcements.py
+-rw-r--r--  2.0 unx     6802 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/generic.py
 -rw-r--r--  2.0 unx    11745 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/instances.py
 -rw-r--r--  2.0 unx     1338 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/lifecycle.py
 -rw-r--r--  2.0 unx     2668 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/nested_access.py
--rw-r--r--  2.0 unx     7743 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/paging_state.py
+-rw-r--r--  2.0 unx     8156 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/paging_state.py
 -rw-r--r--  2.0 unx     2056 b- defN 23-Apr-27 20:35 axonius_api_client/api/json_api/password_reset.py
 -rw-r--r--  2.0 unx     4807 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/remote_support.py
--rw-r--r--  2.0 unx     4259 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/resources.py
--rw-r--r--  2.0 unx    40498 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/saved_queries.py
+-rw-r--r--  2.0 unx     4929 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/resources.py
+-rw-r--r--  2.0 unx    40502 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/saved_queries.py
 -rw-r--r--  2.0 unx      879 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/selection.py
--rw-r--r--  2.0 unx     2729 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/signup.py
 -rw-r--r--  2.0 unx    19576 b- defN 23-Apr-27 20:36 axonius_api_client/api/json_api/spaces_export.py
 -rw-r--r--  2.0 unx     1281 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/system_meta.py
--rw-r--r--  2.0 unx     9819 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/system_roles.py
+-rw-r--r--  2.0 unx     9813 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/system_roles.py
 -rw-r--r--  2.0 unx     7413 b- defN 23-Apr-11 18:58 axonius_api_client/api/json_api/system_settings.py
--rw-r--r--  2.0 unx     7403 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/system_users.py
+-rw-r--r--  2.0 unx     7644 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/system_users.py
 -rw-r--r--  2.0 unx     4290 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/time_range.py
+-rw-r--r--  2.0 unx      400 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/account/__init__.py
+-rw-r--r--  2.0 unx     4032 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/account/current_user.py
+-rw-r--r--  2.0 unx     2892 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/account/login_request.py
+-rw-r--r--  2.0 unx     3252 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/account/login_response.py
+-rw-r--r--  2.0 unx     2356 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/__init__.py
+-rw-r--r--  2.0 unx    10010 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/adapter_node.py
+-rw-r--r--  2.0 unx     4702 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/adapter_settings_response.py
+-rw-r--r--  2.0 unx     1335 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/adapter_settings_update_request.py
+-rw-r--r--  2.0 unx     2687 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/adapters_list_response.py
+-rw-r--r--  2.0 unx     1192 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/adapters_request.py
+-rw-r--r--  2.0 unx     3229 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/adapters_response.py
+-rw-r--r--  2.0 unx      792 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/clients_count.py
+-rw-r--r--  2.0 unx     3633 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_create_request.py
+-rw-r--r--  2.0 unx     1994 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_create_response.py
+-rw-r--r--  2.0 unx     1338 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_delete_request.py
+-rw-r--r--  2.0 unx     1264 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_delete_response.py
+-rw-r--r--  2.0 unx     1598 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_labels_response.py
+-rw-r--r--  2.0 unx     1464 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_test_request.py
+-rw-r--r--  2.0 unx     3017 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_update_request.py
+-rw-r--r--  2.0 unx      834 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnx_update_response.py
+-rw-r--r--  2.0 unx     5244 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/cnxs_response.py
+-rw-r--r--  2.0 unx     6282 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/fetch_history_filters_response.py
+-rw-r--r--  2.0 unx     6246 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/fetch_history_request.py
+-rw-r--r--  2.0 unx     8950 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/adapters/fetch_history_response.py
+-rw-r--r--  2.0 unx     1581 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/__init__.py
+-rw-r--r--  2.0 unx     1434 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/asset_id_request.py
+-rw-r--r--  2.0 unx     4138 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/asset_id_response.py
+-rw-r--r--  2.0 unx    14623 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/asset_request.py
+-rw-r--r--  2.0 unx    11019 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/asset_response.py
+-rw-r--r--  2.0 unx     3296 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/count_request.py
+-rw-r--r--  2.0 unx     1059 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/count_response.py
+-rw-r--r--  2.0 unx     1206 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/destroy_request.py
+-rw-r--r--  2.0 unx      745 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/destroy_response.py
+-rw-r--r--  2.0 unx      784 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/fields_response.py
+-rw-r--r--  2.0 unx     7266 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/history_dates_human.py
+-rw-r--r--  2.0 unx     1198 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/history_dates_response.py
+-rw-r--r--  2.0 unx     1506 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/modify_tags_request.py
+-rw-r--r--  2.0 unx      687 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/modify_tags_response.py
+-rw-r--r--  2.0 unx     1713 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/assets/run_enforcement_request.py
+-rw-r--r--  2.0 unx      299 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/audit_logs/__init__.py
+-rw-r--r--  2.0 unx     1158 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/audit_logs/audit_log_request.py
+-rw-r--r--  2.0 unx     4440 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/audit_logs/audit_log_response.py
 -rw-r--r--  2.0 unx      192 b- defN 23-Mar-09 15:18 axonius_api_client/api/json_api/folders/__init__.py
--rw-r--r--  2.0 unx    67702 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/folders/base.py
+-rw-r--r--  2.0 unx    67993 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/folders/base.py
 -rw-r--r--  2.0 unx     7283 b- defN 23-Mar-13 01:19 axonius_api_client/api/json_api/folders/enforcements.py
--rw-r--r--  2.0 unx     9912 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/folders/queries.py
--rw-r--r--  2.0 unx      700 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/tasks/__init__.py
--rw-r--r--  2.0 unx     7104 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/tasks/get_tasks.py
--rw-r--r--  2.0 unx     6266 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/tasks/result.py
--rw-r--r--  2.0 unx    20487 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/tasks/task.py
--rw-r--r--  2.0 unx     8098 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/tasks/task_basic.py
--rw-r--r--  2.0 unx     3590 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/tasks/task_filters.py
--rw-r--r--  2.0 unx     3904 b- defN 23-May-02 00:40 axonius_api_client/api/json_api/tasks/task_full.py
+-rw-r--r--  2.0 unx    10747 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/folders/queries.py
+-rw-r--r--  2.0 unx      415 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/signup/__init__.py
+-rw-r--r--  2.0 unx     1915 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/signup/signup_request.py
+-rw-r--r--  2.0 unx     1301 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/signup/signup_response.py
+-rw-r--r--  2.0 unx     1405 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/signup/system_status.py
+-rw-r--r--  2.0 unx      701 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/tasks/__init__.py
+-rw-r--r--  2.0 unx    13475 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/tasks/get_tasks.py
+-rw-r--r--  2.0 unx     6170 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/tasks/result.py
+-rw-r--r--  2.0 unx    21942 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/tasks/task.py
+-rw-r--r--  2.0 unx     7988 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/tasks/task_basic.py
+-rw-r--r--  2.0 unx    12876 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/tasks/task_filters.py
+-rw-r--r--  2.0 unx     3683 b- defN 23-May-02 15:09 axonius_api_client/api/json_api/tasks/task_full.py
 -rw-r--r--  2.0 unx      145 b- defN 22-Mar-30 12:19 axonius_api_client/api/openapi/__init__.py
 -rw-r--r--  2.0 unx      639 b- defN 22-Mar-30 12:19 axonius_api_client/api/openapi/openapi_spec.py
 -rw-r--r--  2.0 unx      817 b- defN 23-Apr-21 17:55 axonius_api_client/api/system/__init__.py
 -rw-r--r--  2.0 unx     3987 b- defN 23-Apr-27 20:24 axonius_api_client/api/system/activity_logs.py
--rw-r--r--  2.0 unx    11592 b- defN 23-May-02 00:40 axonius_api_client/api/system/dashboard.py
+-rw-r--r--  2.0 unx    11606 b- defN 23-May-02 15:09 axonius_api_client/api/system/dashboard.py
 -rw-r--r--  2.0 unx    14794 b- defN 23-Apr-21 17:55 axonius_api_client/api/system/dashboard_spaces.py
 -rw-r--r--  2.0 unx    12622 b- defN 23-Apr-27 20:24 axonius_api_client/api/system/data_scopes.py
--rw-r--r--  2.0 unx    28156 b- defN 23-May-02 00:40 axonius_api_client/api/system/instances.py
--rw-r--r--  2.0 unx     3718 b- defN 23-May-02 00:40 axonius_api_client/api/system/meta.py
+-rw-r--r--  2.0 unx    28173 b- defN 23-May-02 15:09 axonius_api_client/api/system/instances.py
+-rw-r--r--  2.0 unx     3778 b- defN 23-May-02 15:09 axonius_api_client/api/system/meta.py
 -rw-r--r--  2.0 unx     3966 b- defN 23-Apr-27 20:25 axonius_api_client/api/system/remote_support.py
--rw-r--r--  2.0 unx    21545 b- defN 23-May-02 00:40 axonius_api_client/api/system/settings.py
--rw-r--r--  2.0 unx     5292 b- defN 23-May-02 00:40 axonius_api_client/api/system/signup.py
+-rw-r--r--  2.0 unx    21566 b- defN 23-May-02 15:09 axonius_api_client/api/system/settings.py
+-rw-r--r--  2.0 unx     6590 b- defN 23-May-02 15:09 axonius_api_client/api/system/signup.py
 -rw-r--r--  2.0 unx    19278 b- defN 23-Apr-27 20:25 axonius_api_client/api/system/system_roles.py
--rw-r--r--  2.0 unx    19330 b- defN 23-May-02 00:40 axonius_api_client/api/system/system_users.py
+-rw-r--r--  2.0 unx    19313 b- defN 23-May-02 15:09 axonius_api_client/api/system/system_users.py
 -rw-r--r--  2.0 unx      236 b- defN 21-Dec-14 21:37 axonius_api_client/api/wizards/__init__.py
--rw-r--r--  2.0 unx    22152 b- defN 23-May-02 00:40 axonius_api_client/api/wizards/wizard.py
--rw-r--r--  2.0 unx    12642 b- defN 23-May-02 00:40 axonius_api_client/api/wizards/wizard_csv.py
--rw-r--r--  2.0 unx     5023 b- defN 23-May-02 00:40 axonius_api_client/api/wizards/wizard_text.py
--rw-r--r--  2.0 unx      324 b- defN 23-May-02 00:40 axonius_api_client/auth/__init__.py
--rw-r--r--  2.0 unx     1504 b- defN 23-May-02 00:40 axonius_api_client/auth/api_key.py
--rw-r--r--  2.0 unx     2559 b- defN 23-May-02 00:40 axonius_api_client/auth/credentials.py
--rw-r--r--  2.0 unx     4271 b- defN 23-May-02 00:40 axonius_api_client/auth/models.py
--rw-r--r--  2.0 unx      505 b- defN 23-May-02 00:40 axonius_api_client/cert_human/__init__.py
--rw-r--r--  2.0 unx    88574 b- defN 23-May-02 00:40 axonius_api_client/cert_human/all_logs_list.json
--rw-r--r--  2.0 unx    62859 b- defN 23-May-02 00:40 axonius_api_client/cert_human/all_logs_list.py
--rw-r--r--  2.0 unx      604 b- defN 23-May-02 00:40 axonius_api_client/cert_human/constants.py
--rw-r--r--  2.0 unx     7401 b- defN 23-May-02 00:40 axonius_api_client/cert_human/convert.py
--rw-r--r--  2.0 unx     3702 b- defN 23-May-02 00:40 axonius_api_client/cert_human/ct_logs.py
--rw-r--r--  2.0 unx     1374 b- defN 23-May-02 00:40 axonius_api_client/cert_human/enums.py
--rw-r--r--  2.0 unx      618 b- defN 23-May-02 00:40 axonius_api_client/cert_human/exceptions.py
--rw-r--r--  2.0 unx     9071 b- defN 23-May-02 00:40 axonius_api_client/cert_human/paths.py
--rw-r--r--  2.0 unx     9048 b- defN 23-May-02 00:40 axonius_api_client/cert_human/ssl_capture.py
--rw-r--r--  2.0 unx     1656 b- defN 23-May-02 00:40 axonius_api_client/cert_human/ssl_context.py
--rw-r--r--  2.0 unx    13297 b- defN 23-May-02 00:40 axonius_api_client/cert_human/ssl_extensions.py
--rw-r--r--  2.0 unx     4879 b- defN 23-May-02 00:40 axonius_api_client/cert_human/utils.py
--rw-r--r--  2.0 unx      307 b- defN 23-May-02 00:40 axonius_api_client/cert_human/stores/__init__.py
--rw-r--r--  2.0 unx    10738 b- defN 23-May-02 00:40 axonius_api_client/cert_human/stores/cert.py
--rw-r--r--  2.0 unx     5793 b- defN 23-May-02 00:40 axonius_api_client/cert_human/stores/cert_request.py
--rw-r--r--  2.0 unx    13661 b- defN 23-May-02 00:40 axonius_api_client/cert_human/stores/store.py
--rw-r--r--  2.0 unx    10726 b- defN 23-May-02 00:40 axonius_api_client/cli/__init__.py
--rw-r--r--  2.0 unx    11259 b- defN 23-May-02 00:40 axonius_api_client/cli/context.py
--rw-r--r--  2.0 unx     6649 b- defN 23-May-02 00:40 axonius_api_client/cli/helps.py
--rw-r--r--  2.0 unx     9240 b- defN 23-May-02 00:40 axonius_api_client/cli/options.py
--rw-r--r--  2.0 unx      492 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_account/__init__.py
+-rw-r--r--  2.0 unx    22147 b- defN 23-May-02 15:09 axonius_api_client/api/wizards/wizard.py
+-rw-r--r--  2.0 unx    12635 b- defN 23-May-02 15:09 axonius_api_client/api/wizards/wizard_csv.py
+-rw-r--r--  2.0 unx     4997 b- defN 23-May-02 15:09 axonius_api_client/api/wizards/wizard_text.py
+-rw-r--r--  2.0 unx      388 b- defN 23-May-02 15:09 axonius_api_client/auth/__init__.py
+-rw-r--r--  2.0 unx     1505 b- defN 23-May-02 15:09 axonius_api_client/auth/api_key.py
+-rw-r--r--  2.0 unx     2476 b- defN 23-May-02 15:09 axonius_api_client/auth/credentials.py
+-rw-r--r--  2.0 unx     3804 b- defN 23-May-02 15:09 axonius_api_client/auth/model.py
+-rw-r--r--  2.0 unx      778 b- defN 23-May-02 15:09 axonius_api_client/auth/null.py
+-rw-r--r--  2.0 unx    14940 b- defN 23-May-02 15:09 axonius_api_client/cli/__init__.py
+-rw-r--r--  2.0 unx    13167 b- defN 23-May-02 15:09 axonius_api_client/cli/context.py
+-rw-r--r--  2.0 unx     6753 b- defN 23-May-02 15:09 axonius_api_client/cli/helps.py
+-rw-r--r--  2.0 unx     9376 b- defN 23-May-02 15:09 axonius_api_client/cli/options.py
+-rw-r--r--  2.0 unx      546 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_account/__init__.py
 -rw-r--r--  2.0 unx      791 b- defN 23-Mar-10 18:37 axonius_api_client/cli/grp_account/cmd_get_api_keys.py
--rw-r--r--  2.0 unx      367 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_adapters/__init__.py
+-rw-r--r--  2.0 unx      506 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_adapters/__init__.py
 -rw-r--r--  2.0 unx      801 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_adapters/cmd_config_get.py
 -rw-r--r--  2.0 unx      983 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_adapters/cmd_config_update.py
 -rw-r--r--  2.0 unx     1052 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_adapters/cmd_config_update_from_json.py
 -rw-r--r--  2.0 unx     1204 b- defN 23-Apr-27 20:38 axonius_api_client/cli/grp_adapters/cmd_file_upload.py
 -rw-r--r--  2.0 unx     2098 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_adapters/cmd_get.py
 -rw-r--r--  2.0 unx     5240 b- defN 23-Apr-21 17:58 axonius_api_client/cli/grp_adapters/cmd_get_fetch_history.py
 -rw-r--r--  2.0 unx     1040 b- defN 22-Aug-08 14:28 axonius_api_client/cli/grp_adapters/cmd_get_fetch_history_filters.py
@@ -146,27 +170,27 @@
 -rw-r--r--  2.0 unx     1903 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_set_active.py
 -rw-r--r--  2.0 unx     1849 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_set_label.py
 -rw-r--r--  2.0 unx     2435 b- defN 22-Jul-05 03:10 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_test.py
 -rw-r--r--  2.0 unx     1453 b- defN 22-Jul-05 03:10 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_test_by_id.py
 -rw-r--r--  2.0 unx     3224 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_update_by_id.py
 -rw-r--r--  2.0 unx      714 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_update_by_id_from_json.py
 -rw-r--r--  2.0 unx     6523 b- defN 23-Apr-27 20:38 axonius_api_client/cli/grp_adapters/grp_cnx/grp_common.py
--rw-r--r--  2.0 unx    33887 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_adapters/grp_cnx/parsing.py
+-rw-r--r--  2.0 unx    33886 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_adapters/grp_cnx/parsing.py
 -rw-r--r--  2.0 unx     3643 b- defN 23-Apr-27 20:38 axonius_api_client/cli/grp_assets/__init__.py
--rw-r--r--  2.0 unx     1042 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_assets/cmd_count.py
--rw-r--r--  2.0 unx      844 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_assets/cmd_count_by_saved_query.py
+-rw-r--r--  2.0 unx     1158 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_assets/cmd_count.py
+-rw-r--r--  2.0 unx      960 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_assets/cmd_count_by_saved_query.py
 -rw-r--r--  2.0 unx     1263 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_assets/cmd_destroy.py
--rw-r--r--  2.0 unx     1328 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_assets/cmd_get.py
+-rw-r--r--  2.0 unx     1358 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_assets/cmd_get.py
 -rw-r--r--  2.0 unx      891 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_assets/cmd_get_by_id.py
--rw-r--r--  2.0 unx     1034 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_assets/cmd_get_by_saved_query.py
+-rw-r--r--  2.0 unx     1087 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_assets/cmd_get_by_saved_query.py
 -rw-r--r--  2.0 unx     5959 b- defN 23-Apr-27 20:38 axonius_api_client/cli/grp_assets/cmd_get_fields.py
 -rw-r--r--  2.0 unx      627 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_assets/cmd_get_fields_default.py
 -rw-r--r--  2.0 unx      741 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_assets/cmd_get_tags.py
 -rw-r--r--  2.0 unx     8652 b- defN 23-Apr-27 20:49 axonius_api_client/cli/grp_assets/cmds_run_enforcement.py
--rw-r--r--  2.0 unx    14981 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_assets/grp_common.py
+-rw-r--r--  2.0 unx    20821 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_assets/grp_common.py
 -rw-r--r--  2.0 unx      300 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_assets/grp_saved_query/__init__.py
 -rw-r--r--  2.0 unx     3943 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_add.py
 -rw-r--r--  2.0 unx     6458 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_add_from_json.py
 -rw-r--r--  2.0 unx     1544 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_add_from_wiz_csv.py
 -rw-r--r--  2.0 unx     2246 b- defN 23-Mar-10 18:37 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_copy.py
 -rw-r--r--  2.0 unx     1010 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_delete_by_name.py
 -rw-r--r--  2.0 unx     1288 b- defN 23-Apr-27 20:40 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_delete_by_tags.py
@@ -195,18 +219,18 @@
 -rw-r--r--  2.0 unx      764 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_certs/cmd_csr_get.py
 -rw-r--r--  2.0 unx      900 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_certs/cmd_from_path.py
 -rw-r--r--  2.0 unx      808 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_certs/cmd_from_url.py
 -rw-r--r--  2.0 unx     2473 b- defN 23-Apr-27 20:40 axonius_api_client/cli/grp_certs/cmd_gui_get.py
 -rw-r--r--  2.0 unx      627 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_certs/cmd_gui_info.py
 -rw-r--r--  2.0 unx     1431 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_certs/cmd_gui_reset.py
 -rw-r--r--  2.0 unx     2840 b- defN 23-Apr-27 20:40 axonius_api_client/cli/grp_certs/cmd_gui_update.py
--rw-r--r--  2.0 unx     9392 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_certs/grp_common.py
--rw-r--r--  2.0 unx      309 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_enforcements/__init__.py
+-rw-r--r--  2.0 unx     9394 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_certs/grp_common.py
+-rw-r--r--  2.0 unx      447 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/__init__.py
 -rw-r--r--  2.0 unx      942 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_copy.py
--rw-r--r--  2.0 unx      950 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_enforcements/cmd_create.py
+-rw-r--r--  2.0 unx      938 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/cmd_create.py
 -rw-r--r--  2.0 unx      752 b- defN 22-Apr-11 23:51 axonius_api_client/cli/grp_enforcements/cmd_delete.py
 -rw-r--r--  2.0 unx      831 b- defN 22-Apr-11 23:51 axonius_api_client/cli/grp_enforcements/cmd_get.py
 -rw-r--r--  2.0 unx      862 b- defN 22-Apr-11 23:51 axonius_api_client/cli/grp_enforcements/cmd_get_action_types.py
 -rw-r--r--  2.0 unx      853 b- defN 23-Apr-27 20:40 axonius_api_client/cli/grp_enforcements/cmd_run.py
 -rw-r--r--  2.0 unx      957 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_update_action_add.py
 -rw-r--r--  2.0 unx      901 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_update_action_main.py
 -rw-r--r--  2.0 unx      883 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_update_action_remove.py
@@ -222,15 +246,23 @@
 -rw-r--r--  2.0 unx      780 b- defN 22-Apr-11 23:51 axonius_api_client/cli/grp_enforcements/cmd_update_query_remove.py
 -rw-r--r--  2.0 unx      892 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_daily.py
 -rw-r--r--  2.0 unx      797 b- defN 22-Apr-11 23:51 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_discovery.py
 -rw-r--r--  2.0 unx      831 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_hourly.py
 -rw-r--r--  2.0 unx      902 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_monthly.py
 -rw-r--r--  2.0 unx      782 b- defN 22-Apr-11 23:51 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_never.py
 -rw-r--r--  2.0 unx      896 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_weekly.py
--rw-r--r--  2.0 unx    10586 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_enforcements/grp_common.py
+-rw-r--r--  2.0 unx    10537 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_common.py
+-rw-r--r--  2.0 unx      309 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/__init__.py
+-rw-r--r--  2.0 unx      741 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_count.py
+-rw-r--r--  2.0 unx      942 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_get.py
+-rw-r--r--  2.0 unx      919 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_get_filters.py
+-rw-r--r--  2.0 unx     3084 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/export_get.py
+-rw-r--r--  2.0 unx     1749 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/export_get_filters.py
+-rw-r--r--  2.0 unx    10469 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/options_get.py
+-rw-r--r--  2.0 unx      779 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_enforcements/grp_tasks/options_get_filters.py
 -rw-r--r--  2.0 unx      574 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_folders/__init__.py
 -rw-r--r--  2.0 unx      879 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_folders/cmd_create.py
 -rw-r--r--  2.0 unx      889 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_folders/cmd_delete.py
 -rw-r--r--  2.0 unx     1107 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_folders/cmd_find.py
 -rw-r--r--  2.0 unx      894 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_folders/cmd_get_tree.py
 -rw-r--r--  2.0 unx      867 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_folders/cmd_move.py
 -rw-r--r--  2.0 unx      879 b- defN 23-Mar-09 15:18 axonius_api_client/cli/grp_folders/cmd_rename.py
@@ -262,15 +294,15 @@
 -rw-r--r--  2.0 unx     1014 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_system/grp_data_scopes/cmd_update_device_scopes.py
 -rw-r--r--  2.0 unx     1042 b- defN 22-Apr-22 00:52 axonius_api_client/cli/grp_system/grp_data_scopes/cmd_update_name.py
 -rw-r--r--  2.0 unx     1008 b- defN 23-Apr-21 17:55 axonius_api_client/cli/grp_system/grp_data_scopes/cmd_update_user_scopes.py
 -rw-r--r--  2.0 unx     1963 b- defN 23-Apr-27 20:41 axonius_api_client/cli/grp_system/grp_data_scopes/grp_common.py
 -rw-r--r--  2.0 unx      303 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_discover/__init__.py
 -rw-r--r--  2.0 unx      749 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_discover/cmd_get.py
 -rw-r--r--  2.0 unx     1250 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_discover/cmd_is_data_stable.py
--rw-r--r--  2.0 unx      845 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_system/grp_discover/cmd_is_running.py
+-rw-r--r--  2.0 unx      846 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_system/grp_discover/cmd_is_running.py
 -rw-r--r--  2.0 unx     1241 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_discover/cmd_start.py
 -rw-r--r--  2.0 unx      787 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_discover/cmd_stop.py
 -rw-r--r--  2.0 unx     1128 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_discover/cmd_wait_data_stable.py
 -rw-r--r--  2.0 unx     3332 b- defN 23-Apr-27 20:41 axonius_api_client/cli/grp_system/grp_discover/grp_common.py
 -rw-r--r--  2.0 unx      277 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_meta/__init__.py
 -rw-r--r--  2.0 unx     1258 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_meta/cmd_about.py
 -rw-r--r--  2.0 unx     1114 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_meta/cmd_sizes.py
@@ -294,81 +326,104 @@
 -rw-r--r--  2.0 unx     1068 b- defN 22-Apr-22 00:52 axonius_api_client/cli/grp_system/grp_roles/cmd_update_name.py
 -rw-r--r--  2.0 unx     1172 b- defN 22-Apr-22 00:52 axonius_api_client/cli/grp_system/grp_roles/cmd_update_perms.py
 -rw-r--r--  2.0 unx     3529 b- defN 22-Apr-22 00:52 axonius_api_client/cli/grp_system/grp_roles/grp_common.py
 -rw-r--r--  2.0 unx     1012 b- defN 23-Apr-27 20:42 axonius_api_client/cli/grp_system/grp_settings/__init__.py
 -rw-r--r--  2.0 unx     1870 b- defN 23-Apr-27 20:42 axonius_api_client/cli/grp_system/grp_settings/cmd_configure_destroy.py
 -rw-r--r--  2.0 unx     1062 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_settings/cmd_get.py
 -rw-r--r--  2.0 unx     1136 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_settings/cmd_get_section.py
--rw-r--r--  2.0 unx     1236 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_system/grp_settings/cmd_get_subsection.py
+-rw-r--r--  2.0 unx     1235 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_system/grp_settings/cmd_get_subsection.py
 -rw-r--r--  2.0 unx     1359 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_settings/cmd_update_section.py
 -rw-r--r--  2.0 unx     1382 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_settings/cmd_update_section_from_json.py
--rw-r--r--  2.0 unx     1426 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection.py
--rw-r--r--  2.0 unx     1500 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection_from_json.py
+-rw-r--r--  2.0 unx     1425 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection.py
+-rw-r--r--  2.0 unx     1499 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection_from_json.py
 -rw-r--r--  2.0 unx     2434 b- defN 23-Apr-27 20:42 axonius_api_client/cli/grp_system/grp_settings/grp_common.py
 -rw-r--r--  2.0 unx      276 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_system/grp_users/__init__.py
 -rw-r--r--  2.0 unx     2530 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/cmd_add.py
 -rw-r--r--  2.0 unx     3159 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/cmd_add_from_csv.py
 -rw-r--r--  2.0 unx      752 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/cmd_delete.py
 -rw-r--r--  2.0 unx     1042 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/cmd_email_password_reset_link.py
 -rw-r--r--  2.0 unx      636 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/cmd_get.py
--rw-r--r--  2.0 unx      882 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_system/grp_users/cmd_get_by_name.py
+-rw-r--r--  2.0 unx      881 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_system/grp_users/cmd_get_by_name.py
 -rw-r--r--  2.0 unx      857 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/cmd_get_password_reset_link.py
 -rw-r--r--  2.0 unx     2702 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/cmd_update.py
 -rw-r--r--  2.0 unx     1736 b- defN 21-Dec-14 21:37 axonius_api_client/cli/grp_system/grp_users/grp_common.py
 -rw-r--r--  2.0 unx      272 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_tools/__init__.py
 -rw-r--r--  2.0 unx     1135 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_tools/cmd_help_features.py
--rw-r--r--  2.0 unx     6899 b- defN 23-May-02 00:40 axonius_api_client/cli/grp_tools/cmd_shell.py
+-rw-r--r--  2.0 unx     5956 b- defN 23-May-02 15:09 axonius_api_client/cli/grp_tools/cmd_shell.py
 -rw-r--r--  2.0 unx     1727 b- defN 23-Apr-08 13:43 axonius_api_client/cli/grp_tools/cmd_signup.py
 -rw-r--r--  2.0 unx     1005 b- defN 22-Mar-30 12:19 axonius_api_client/cli/grp_tools/cmd_sysinfo.py
 -rw-r--r--  2.0 unx     2338 b- defN 23-Apr-27 20:42 axonius_api_client/cli/grp_tools/cmd_system_status.py
 -rw-r--r--  2.0 unx     1099 b- defN 23-Mar-10 18:37 axonius_api_client/cli/grp_tools/cmd_use_token_reset_token.py
 -rw-r--r--  2.0 unx      677 b- defN 23-Apr-27 20:42 axonius_api_client/cli/grp_tools/cmd_write_config.py
 -rw-r--r--  2.0 unx     2101 b- defN 23-Mar-10 18:37 axonius_api_client/cli/grp_tools/grp_common.py
 -rw-r--r--  2.0 unx      554 b- defN 23-Mar-10 18:37 axonius_api_client/cli/grp_tools/grp_options.py
--rw-r--r--  2.0 unx      298 b- defN 23-May-02 00:40 axonius_api_client/constants/__init__.py
--rw-r--r--  2.0 unx     2973 b- defN 23-May-02 00:40 axonius_api_client/constants/adapters.py
--rw-r--r--  2.0 unx     2574 b- defN 23-May-02 00:40 axonius_api_client/constants/api.py
--rw-r--r--  2.0 unx      673 b- defN 23-May-02 00:40 axonius_api_client/constants/ctypes.py
+-rw-r--r--  2.0 unx      334 b- defN 23-May-02 15:09 axonius_api_client/constants/__init__.py
+-rw-r--r--  2.0 unx     2973 b- defN 23-May-02 15:09 axonius_api_client/constants/adapters.py
+-rw-r--r--  2.0 unx     2868 b- defN 23-May-02 15:09 axonius_api_client/constants/api.py
+-rw-r--r--  2.0 unx    18413 b- defN 23-May-02 15:09 axonius_api_client/constants/asset_helpers.py
+-rw-r--r--  2.0 unx     1258 b- defN 23-May-02 15:09 axonius_api_client/constants/ctypes.py
 -rw-r--r--  2.0 unx     5056 b- defN 23-Apr-27 20:53 axonius_api_client/constants/enforcements.py
--rw-r--r--  2.0 unx    38017 b- defN 23-May-02 00:40 axonius_api_client/constants/fields.py
--rw-r--r--  2.0 unx     2591 b- defN 23-May-02 00:40 axonius_api_client/constants/general.py
--rw-r--r--  2.0 unx     3730 b- defN 23-May-02 00:40 axonius_api_client/constants/logs.py
+-rw-r--r--  2.0 unx    38018 b- defN 23-May-02 15:09 axonius_api_client/constants/fields.py
+-rw-r--r--  2.0 unx     2669 b- defN 23-May-02 15:09 axonius_api_client/constants/general.py
+-rw-r--r--  2.0 unx     4180 b- defN 23-May-02 15:09 axonius_api_client/constants/logs.py
 -rw-r--r--  2.0 unx     1151 b- defN 22-Mar-30 12:19 axonius_api_client/constants/tables.py
--rw-r--r--  2.0 unx    15122 b- defN 23-May-02 00:40 axonius_api_client/constants/wizards.py
+-rw-r--r--  2.0 unx    15197 b- defN 23-May-02 15:09 axonius_api_client/constants/wizards.py
 -rw-r--r--  2.0 unx      119 b- defN 21-Dec-14 21:37 axonius_api_client/examples/__init__.py
--rw-r--r--  2.0 unx    18356 b- defN 23-May-02 00:40 axonius_api_client/examples/adapter_fetch_history_addin.py
+-rw-r--r--  2.0 unx    18359 b- defN 23-May-02 15:09 axonius_api_client/examples/adapter_fetch_history_addin.py
 -rw-r--r--  2.0 unx     6948 b- defN 23-Apr-27 20:53 axonius_api_client/examples/add_adapter_cnxs_from_csv.py
 -rw-r--r--  2.0 unx     3058 b- defN 23-Apr-27 20:53 axonius_api_client/examples/example_api_query_wizard.py
 -rw-r--r--  2.0 unx     2946 b- defN 23-Apr-21 17:55 axonius_api_client/examples/example_cvss_filtering.py
 -rw-r--r--  2.0 unx     2776 b- defN 21-Dec-14 21:37 axonius_api_client/examples/example_details.py
 -rw-r--r--  2.0 unx     1445 b- defN 21-Dec-14 21:37 axonius_api_client/examples/example_get_by_sq.py
--rw-r--r--  2.0 unx    11207 b- defN 23-May-02 00:40 axonius_api_client/examples/example_os_count_magic.py
+-rw-r--r--  2.0 unx    11207 b- defN 23-May-02 15:09 axonius_api_client/examples/example_os_count_magic.py
 -rw-r--r--  2.0 unx     2211 b- defN 21-Dec-14 21:37 axonius_api_client/examples/example_sw_missing.py
 -rw-r--r--  2.0 unx    13284 b- defN 23-Apr-27 20:53 axonius_api_client/examples/example_user_device_associated_agent_versions.py
 -rw-r--r--  2.0 unx     3180 b- defN 21-Dec-14 21:37 axonius_api_client/examples/example_wmi_last_used_users.py
 -rw-r--r--  2.0 unx    10240 b- defN 23-Apr-27 20:53 axonius_api_client/examples/sa_user_pairing.py
--rw-r--r--  2.0 unx     2730 b- defN 23-May-02 00:40 axonius_api_client/examples/script_base.py
+-rw-r--r--  2.0 unx     2729 b- defN 23-May-02 15:09 axonius_api_client/examples/script_base.py
 -rw-r--r--  2.0 unx     3993 b- defN 23-Apr-27 20:54 axonius_api_client/examples/update_action_config.py
 -rw-r--r--  2.0 unx    21603 b- defN 23-Mar-09 15:18 axonius_api_client/examples/user_to_device_correlation_example.py
--rw-r--r--  2.0 unx     1355 b- defN 23-May-02 00:40 axonius_api_client/examples/wip_roles.py
--rw-r--r--  2.0 unx      304 b- defN 23-May-02 00:40 axonius_api_client/parsers/__init__.py
--rw-r--r--  2.0 unx    19040 b- defN 23-May-02 00:40 axonius_api_client/parsers/config.py
--rw-r--r--  2.0 unx     9206 b- defN 23-May-02 00:40 axonius_api_client/parsers/dt_delta.py
--rw-r--r--  2.0 unx    13385 b- defN 23-May-02 00:40 axonius_api_client/parsers/fields.py
--rw-r--r--  2.0 unx    14278 b- defN 23-May-02 00:40 axonius_api_client/parsers/grabber.py
--rw-r--r--  2.0 unx     9123 b- defN 23-May-02 00:40 axonius_api_client/parsers/matcher.py
--rw-r--r--  2.0 unx     9783 b- defN 23-May-02 00:40 axonius_api_client/parsers/search_wip.py
+-rw-r--r--  2.0 unx     1354 b- defN 23-May-02 15:09 axonius_api_client/examples/wip_roles.py
+-rw-r--r--  2.0 unx      300 b- defN 23-May-02 15:09 axonius_api_client/parsers/__init__.py
+-rw-r--r--  2.0 unx    19041 b- defN 23-May-02 15:09 axonius_api_client/parsers/config.py
+-rw-r--r--  2.0 unx    13385 b- defN 23-May-02 15:09 axonius_api_client/parsers/fields.py
+-rw-r--r--  2.0 unx    14319 b- defN 23-May-02 15:09 axonius_api_client/parsers/grabber.py
+-rw-r--r--  2.0 unx     9629 b- defN 23-May-02 15:09 axonius_api_client/parsers/matcher.py
 -rw-r--r--  2.0 unx     5908 b- defN 23-Apr-27 20:57 axonius_api_client/parsers/searchers.py
 -rw-r--r--  2.0 unx    10770 b- defN 23-Apr-27 20:57 axonius_api_client/parsers/tables.py
--rw-r--r--  2.0 unx     6235 b- defN 23-May-02 00:40 axonius_api_client/parsers/url_parser.py
 -rw-r--r--  2.0 unx    19266 b- defN 23-Apr-27 20:58 axonius_api_client/parsers/wizards.py
+-rw-r--r--  2.0 unx      150 b- defN 23-May-02 15:09 axonius_api_client/projects/__init__.py
+-rw-r--r--  2.0 unx      293 b- defN 23-May-02 15:09 axonius_api_client/projects/cli_cf_token.py
+-rw-r--r--  2.0 unx     6363 b- defN 23-May-02 15:09 axonius_api_client/projects/url_parser.py
+-rw-r--r--  2.0 unx      533 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/__init__.py
+-rw-r--r--  2.0 unx    88575 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/all_logs_list.json
+-rw-r--r--  2.0 unx    62965 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/all_logs_list.py
+-rw-r--r--  2.0 unx      604 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/constants.py
+-rw-r--r--  2.0 unx     7413 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/convert.py
+-rw-r--r--  2.0 unx     3703 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/ct_logs.py
+-rw-r--r--  2.0 unx     1374 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/enums.py
+-rw-r--r--  2.0 unx      618 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/exceptions.py
+-rw-r--r--  2.0 unx     9081 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/paths.py
+-rw-r--r--  2.0 unx     9046 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/ssl_capture.py
+-rw-r--r--  2.0 unx     1656 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/ssl_context.py
+-rw-r--r--  2.0 unx    13328 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/ssl_extensions.py
+-rw-r--r--  2.0 unx     5117 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/utils.py
+-rw-r--r--  2.0 unx      307 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/stores/__init__.py
+-rw-r--r--  2.0 unx    10607 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/stores/cert.py
+-rw-r--r--  2.0 unx     5788 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/stores/cert_request.py
+-rw-r--r--  2.0 unx    13711 b- defN 23-May-02 15:09 axonius_api_client/projects/cert_human/stores/store.py
+-rw-r--r--  2.0 unx      301 b- defN 23-May-02 15:09 axonius_api_client/projects/cf_token/__init__.py
+-rw-r--r--  2.0 unx     5090 b- defN 23-May-02 15:09 axonius_api_client/projects/cf_token/cli.py
+-rw-r--r--  2.0 unx    10039 b- defN 23-May-02 15:09 axonius_api_client/projects/cf_token/constants.py
+-rw-r--r--  2.0 unx    11805 b- defN 23-May-02 15:09 axonius_api_client/projects/cf_token/flows.py
+-rw-r--r--  2.0 unx    24145 b- defN 23-May-02 15:09 axonius_api_client/projects/cf_token/tools.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-07 16:53 axonius_api_client/tests/__init__.py
--rw-r--r--  2.0 unx     8904 b- defN 23-May-02 00:40 axonius_api_client/tests/conftest.py
+-rw-r--r--  2.0 unx    12671 b- defN 23-May-02 15:09 axonius_api_client/tests/conftest.py
 -rw-r--r--  2.0 unx     8321 b- defN 23-Apr-27 21:03 axonius_api_client/tests/meta.py
--rw-r--r--  2.0 unx    10078 b- defN 23-May-02 00:40 axonius_api_client/tests/utils.py
+-rw-r--r--  2.0 unx    12347 b- defN 23-May-02 15:09 axonius_api_client/tests/utils.py
 -rw-r--r--  2.0 unx     1455 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/common.sh
 -rwxr-xr-x  2.0 unx     5219 b- defN 23-Apr-27 21:03 axonius_api_client/tests/datafiles/create_certs.sh
 -rw-r--r--  2.0 unx     1058 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/ca_ec.crt.pem
 -rw-r--r--  2.0 unx      302 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/ca_ec.key
 -rw-r--r--  2.0 unx     1594 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/ca_rsa.crt.pem
 -rw-r--r--  2.0 unx     1704 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/ca_rsa.key
 -rw-r--r--  2.0 unx     2078 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/server_ec.crt.p7b
@@ -376,67 +431,72 @@
 -rw-r--r--  2.0 unx      655 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/server_ec.csr.pem
 -rw-r--r--  2.0 unx      302 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/server_ec.key
 -rw-r--r--  2.0 unx     3146 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/server_rsa.crt.p7b
 -rw-r--r--  2.0 unx     1558 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/server_rsa.crt.pem
 -rw-r--r--  2.0 unx     1188 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/server_rsa.csr.pem
 -rw-r--r--  2.0 unx     1708 b- defN 22-Mar-30 12:19 axonius_api_client/tests/datafiles/certs/server_rsa.key
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/__init__.py
--rw-r--r--  2.0 unx    21672 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/test_api_endpoints.py
--rw-r--r--  2.0 unx     2401 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/test_signup.py
+-rw-r--r--  2.0 unx    22291 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/test_api_endpoints.py
+-rw-r--r--  2.0 unx     3044 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/test_signup.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_adapters/__init__.py
 -rw-r--r--  2.0 unx    18039 b- defN 23-Apr-21 17:58 axonius_api_client/tests/tests_api/tests_adapters/test_adapters.py
--rw-r--r--  2.0 unx    22002 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_adapters/test_cnx.py
+-rw-r--r--  2.0 unx    20408 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_adapters/test_cnx.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_asset_callbacks/__init__.py
--rw-r--r--  2.0 unx    41532 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks.py
+-rw-r--r--  2.0 unx    42093 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks.py
 -rw-r--r--  2.0 unx     1543 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_base.py
--rw-r--r--  2.0 unx     2451 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_csv.py
--rw-r--r--  2.0 unx     2721 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json.py
--rw-r--r--  2.0 unx     1140 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json_to_csv.py
--rw-r--r--  2.0 unx     2389 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_table.py
+-rw-r--r--  2.0 unx     2429 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_csv.py
+-rw-r--r--  2.0 unx     2699 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json.py
+-rw-r--r--  2.0 unx     1118 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json_to_csv.py
+-rw-r--r--  2.0 unx     2367 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_table.py
 -rw-r--r--  2.0 unx     1955 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_xlsx.py
 -rw-r--r--  2.0 unx      920 b- defN 22-Aug-08 14:28 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_xml.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_assets/__init__.py
--rw-r--r--  2.0 unx    15451 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_assets/test_assets.py
+-rw-r--r--  2.0 unx    15411 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_assets/test_assets.py
 -rw-r--r--  2.0 unx    24596 b- defN 23-Apr-27 21:06 axonius_api_client/tests/tests_api/tests_assets/test_fields.py
--rw-r--r--  2.0 unx     5056 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_assets/test_labels.py
+-rw-r--r--  2.0 unx     5040 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_assets/test_labels.py
 -rw-r--r--  2.0 unx     6385 b- defN 23-Apr-27 21:06 axonius_api_client/tests/tests_api/tests_assets/test_runner.py
--rw-r--r--  2.0 unx    38776 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_assets/test_saved_query.py
+-rw-r--r--  2.0 unx    39238 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_assets/test_saved_query.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_enforcements/__init__.py
--rw-r--r--  2.0 unx    21185 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_enforcements/test_enforcements.py
+-rw-r--r--  2.0 unx    21225 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_enforcements/test_enforcements.py
+-rw-r--r--  2.0 unx    10666 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_enforcements/test_tasks.py
 -rw-r--r--  2.0 unx       42 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_folders/__init__.py
--rw-r--r--  2.0 unx    40838 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_folders/test_folders.py
+-rw-r--r--  2.0 unx    41545 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_folders/test_folders.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_api/tests_json_api/__init__.py
 -rw-r--r--  2.0 unx     3564 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_api/tests_json_api/test_custom_fields.py
--rw-r--r--  2.0 unx     7433 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_json_api/test_json_api.py
+-rw-r--r--  2.0 unx     7419 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_json_api/test_json_api.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Feb-09 09:31 axonius_api_client/tests/tests_api/tests_openapi/__init__.py
 -rw-r--r--  2.0 unx      810 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_api/tests_openapi/test_openapi.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_parsers/__init__.py
 -rw-r--r--  2.0 unx    11092 b- defN 23-Apr-27 21:07 axonius_api_client/tests/tests_api/tests_parsers/test_config.py
 -rw-r--r--  2.0 unx     1699 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_api/tests_parsers/test_constants.py
 -rw-r--r--  2.0 unx      884 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_parsers/test_fields.py
 -rw-r--r--  2.0 unx     4394 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_parsers/test_grabber.py
 -rw-r--r--  2.0 unx     3261 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_parsers/test_tables.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_system/__init__.py
 -rw-r--r--  2.0 unx     1900 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_system/test_activity_logs.py
--rw-r--r--  2.0 unx     5158 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_system/test_dashboard.py
+-rw-r--r--  2.0 unx     5175 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_system/test_dashboard.py
 -rw-r--r--  2.0 unx    11063 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_system/test_data_scopes.py
 -rw-r--r--  2.0 unx     8914 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_system/test_instances.py
--rw-r--r--  2.0 unx     3934 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_system/test_meta.py
+-rw-r--r--  2.0 unx     4025 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_system/test_meta.py
 -rw-r--r--  2.0 unx     4525 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_api/tests_system/test_remote_support.py
--rw-r--r--  2.0 unx    13681 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_api/tests_system/test_settings.py
+-rw-r--r--  2.0 unx    13708 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_api/tests_system/test_settings.py
 -rw-r--r--  2.0 unx     8747 b- defN 23-Apr-21 17:55 axonius_api_client/tests/tests_api/tests_system/test_system_roles.py
 -rw-r--r--  2.0 unx     7858 b- defN 23-Apr-21 17:55 axonius_api_client/tests/tests_api/tests_system/test_system_users.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_wizard/__init__.py
 -rw-r--r--  2.0 unx     1905 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_api/tests_wizard/test_constants.py
 -rw-r--r--  2.0 unx    28159 b- defN 23-Apr-27 23:19 axonius_api_client/tests/tests_api/tests_wizard/test_wizard.py
 -rw-r--r--  2.0 unx    21275 b- defN 23-Apr-27 23:19 axonius_api_client/tests/tests_api/tests_wizard/test_wizard_csv.py
 -rw-r--r--  2.0 unx    23020 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_api/tests_wizard/test_wizard_parser.py
 -rw-r--r--  2.0 unx     4829 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_api/tests_wizard/test_wizard_text.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_auth/__init__.py
--rw-r--r--  2.0 unx     2810 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_auth/test_auth.py
+-rw-r--r--  2.0 unx     4424 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_auth/test_auth.py
+-rw-r--r--  2.0 unx       42 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cf_token/__init__.py
+-rw-r--r--  2.0 unx     2638 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cf_token/meta.py
+-rw-r--r--  2.0 unx     8491 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cf_token/test_flows.py
+-rw-r--r--  2.0 unx    17116 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cf_token/test_tools.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_cli/__init__.py
 -rw-r--r--  2.0 unx       42 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_folders_grp/__init__.py
 -rw-r--r--  2.0 unx     4990 b- defN 23-Apr-21 17:55 axonius_api_client/tests/tests_cli/tests_folders_grp/test_folders_grp.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_activity_logs/__init__.py
 -rw-r--r--  2.0 unx     1672 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_activity_logs/test_cmd_get.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_cli/tests_grp_adapters/__init__.py
 -rw-r--r--  2.0 unx     1027 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_adapters/test_cmd_config_get.py
@@ -450,15 +510,15 @@
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_central_core/__init__.py
 -rw-r--r--  2.0 unx      746 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_central_core/test_cmd_get.py
 -rw-r--r--  2.0 unx      776 b- defN 23-Apr-29 16:19 axonius_api_client/tests/tests_cli/tests_grp_central_core/test_cmd_restore_from_aws_s3.py
 -rw-r--r--  2.0 unx     2694 b- defN 23-Apr-29 16:20 axonius_api_client/tests/tests_cli/tests_grp_central_core/test_cmd_update.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_cli/tests_grp_cnx/__init__.py
 -rw-r--r--  2.0 unx     4088 b- defN 23-Apr-30 20:32 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add.py
 -rw-r--r--  2.0 unx     3826 b- defN 23-Apr-30 20:32 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add_from_json.py
--rw-r--r--  2.0 unx     8854 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add_multiple_from_json.py
+-rw-r--r--  2.0 unx     8469 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add_multiple_from_json.py
 -rw-r--r--  2.0 unx     3171 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_get.py
 -rw-r--r--  2.0 unx     2227 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_get_by_id.py
 -rw-r--r--  2.0 unx     2258 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_set_active.py
 -rw-r--r--  2.0 unx     2245 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_set_label.py
 -rw-r--r--  2.0 unx     2287 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_test.py
 -rw-r--r--  2.0 unx     2645 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_test_by_id.py
 -rw-r--r--  2.0 unx     1488 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_update_by_id.py
@@ -470,19 +530,33 @@
 -rw-r--r--  2.0 unx     3214 b- defN 23-Apr-27 23:20 axonius_api_client/tests/tests_cli/tests_grp_data_scopes/test_cmd_create.py
 -rw-r--r--  2.0 unx     2447 b- defN 23-Apr-27 23:20 axonius_api_client/tests/tests_cli/tests_grp_data_scopes/test_cmd_get.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_discover/__init__.py
 -rw-r--r--  2.0 unx     1491 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_discover/test_cmd_get.py
 -rw-r--r--  2.0 unx     2250 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_discover/test_cmd_multi.py
 -rw-r--r--  2.0 unx      981 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_discover/test_cmd_wait_data_stable.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Apr-11 23:51 axonius_api_client/tests/tests_cli/tests_grp_enforcements/__init__.py
--rw-r--r--  2.0 unx     4358 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_copy.py
--rw-r--r--  2.0 unx     3752 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_create.py
+-rw-r--r--  2.0 unx     4507 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_copy.py
+-rw-r--r--  2.0 unx     3946 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_create.py
 -rw-r--r--  2.0 unx     3036 b- defN 23-Apr-27 23:23 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_get.py
 -rw-r--r--  2.0 unx     3053 b- defN 23-Apr-27 23:23 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_get_action_types.py
 -rw-r--r--  2.0 unx     1112 b- defN 23-Apr-27 23:23 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_run.py
+-rw-r--r--  2.0 unx     1517 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_description.py
+-rw-r--r--  2.0 unx     2203 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_above.py
+-rw-r--r--  2.0 unx     2203 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_below.py
+-rw-r--r--  2.0 unx     1522 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_decreased.py
+-rw-r--r--  2.0 unx     1522 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_increased.py
+-rw-r--r--  2.0 unx     1507 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_only_new_assets.py
+-rw-r--r--  2.0 unx     1123 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_query.py
+-rw-r--r--  2.0 unx      904 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_query_remove.py
+-rw-r--r--  2.0 unx     3368 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_daily.py
+-rw-r--r--  2.0 unx     1043 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_discovery.py
+-rw-r--r--  2.0 unx     2499 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_hourly.py
+-rw-r--r--  2.0 unx     3414 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_monthly.py
+-rw-r--r--  2.0 unx     1007 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_never.py
+-rw-r--r--  2.0 unx     4098 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_weekly.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_meta/__init__.py
 -rw-r--r--  2.0 unx     4533 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_meta/test_cmd_about.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_meta/test_cmd_history_sizes.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_cli/tests_grp_openapi/__init__.py
 -rw-r--r--  2.0 unx     1364 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_openapi/test_cmd_get-spec.py
 -rw-r--r--  2.0 unx       42 b- defN 22-Mar-30 12:19 axonius_api_client/tests/tests_cli/tests_grp_saved_query/__init__.py
 -rw-r--r--  2.0 unx     2115 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_saved_query/base.py
@@ -504,26 +578,30 @@
 -rw-r--r--  2.0 unx     1227 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_delete.py
 -rw-r--r--  2.0 unx     1244 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_get.py
 -rw-r--r--  2.0 unx     1380 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_get_by_name.py
 -rw-r--r--  2.0 unx      656 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_get_perms.py
 -rw-r--r--  2.0 unx     1686 b- defN 22-Apr-22 00:52 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_data_scope.py
 -rw-r--r--  2.0 unx     1290 b- defN 22-Apr-22 00:52 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_name.py
 -rw-r--r--  2.0 unx     1343 b- defN 22-Apr-22 00:52 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_perms.py
+-rw-r--r--  2.0 unx       42 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_tasks/__init__.py
+-rw-r--r--  2.0 unx      608 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_count.py
+-rw-r--r--  2.0 unx     2976 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_get.py
+-rw-r--r--  2.0 unx     2055 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_get_filters.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_cli/tests_grp_tools/__init__.py
--rw-r--r--  2.0 unx     5084 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_shell.py
+-rw-r--r--  2.0 unx     5082 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_shell.py
 -rw-r--r--  2.0 unx      995 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_sysinfo.py
 -rw-r--r--  2.0 unx     1271 b- defN 23-Apr-27 21:15 axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_write_config.py
 -rw-r--r--  2.0 unx       42 b- defN 21-Dec-14 21:37 axonius_api_client/tests/tests_pkg/__init__.py
--rw-r--r--  2.0 unx     3616 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_pkg/test_connect.py
+-rw-r--r--  2.0 unx     6783 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_pkg/test_connect.py
 -rw-r--r--  2.0 unx     2481 b- defN 23-Mar-09 15:18 axonius_api_client/tests/tests_pkg/test_data.py
--rw-r--r--  2.0 unx     9962 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_pkg/test_http.py
+-rw-r--r--  2.0 unx     9914 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_pkg/test_http.py
 -rw-r--r--  2.0 unx     4045 b- defN 23-Apr-21 17:55 axonius_api_client/tests/tests_pkg/test_logs.py
--rw-r--r--  2.0 unx     6332 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_pkg/test_setup_env.py
--rw-r--r--  2.0 unx    42069 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_pkg/test_tools.py
--rw-r--r--  2.0 unx     2985 b- defN 23-May-02 00:40 axonius_api_client/tests/tests_pkg/test_url_parser.py
--rw-r--r--  2.0 unx     1064 b- defN 23-May-02 00:42 axonius_api_client-4.60.4.dist-info/LICENSE
--rw-r--r--  2.0 unx     2740 b- defN 23-May-02 00:42 axonius_api_client-4.60.4.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-May-02 00:42 axonius_api_client-4.60.4.dist-info/WHEEL
--rw-r--r--  2.0 unx       57 b- defN 23-May-02 00:42 axonius_api_client-4.60.4.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       19 b- defN 23-May-02 00:42 axonius_api_client-4.60.4.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    59521 b- defN 23-May-02 00:42 axonius_api_client-4.60.4.dist-info/RECORD
-527 files, 3109245 bytes uncompressed, 706527 bytes compressed:  77.3%
+-rw-r--r--  2.0 unx     8318 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_pkg/test_setup_env.py
+-rw-r--r--  2.0 unx    42244 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_pkg/test_tools.py
+-rw-r--r--  2.0 unx     3189 b- defN 23-May-02 15:09 axonius_api_client/tests/tests_pkg/test_url_parser.py
+-rw-r--r--  2.0 unx     1064 b- defN 23-May-02 15:09 axonius_api_client-5.0.0.dist-info/LICENSE
+-rw-r--r--  2.0 unx     2739 b- defN 23-May-02 15:09 axonius_api_client-5.0.0.dist-info/METADATA
+-rw-r--r--  2.0 unx      110 b- defN 23-May-02 15:09 axonius_api_client-5.0.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       57 b- defN 23-May-02 15:09 axonius_api_client-5.0.0.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       19 b- defN 23-May-02 15:09 axonius_api_client-5.0.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    69263 b- defN 23-May-02 15:09 axonius_api_client-5.0.0.dist-info/RECORD
+605 files, 3424983 bytes uncompressed, 789981 bytes compressed:  77.0%
```

## zipnote {}

```diff
@@ -21,17 +21,14 @@
 
 Filename: axonius_api_client/setup_env.py
 Comment: 
 
 Filename: axonius_api_client/tools.py
 Comment: 
 
-Filename: axonius_api_client/utils.py
-Comment: 
-
 Filename: axonius_api_client/version.py
 Comment: 
 
 Filename: axonius_api_client/api/__init__.py
 Comment: 
 
 Filename: axonius_api_client/api/api_endpoint.py
@@ -120,26 +117,14 @@
 
 Filename: axonius_api_client/api/folders/folders.py
 Comment: 
 
 Filename: axonius_api_client/api/json_api/__init__.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/account.py
-Comment: 
-
-Filename: axonius_api_client/api/json_api/adapters.py
-Comment: 
-
-Filename: axonius_api_client/api/json_api/assets.py
-Comment: 
-
-Filename: axonius_api_client/api/json_api/audit_logs.py
-Comment: 
-
 Filename: axonius_api_client/api/json_api/base.py
 Comment: 
 
 Filename: axonius_api_client/api/json_api/base2.py
 Comment: 
 
 Filename: axonius_api_client/api/json_api/central_core.py
@@ -189,17 +174,14 @@
 
 Filename: axonius_api_client/api/json_api/saved_queries.py
 Comment: 
 
 Filename: axonius_api_client/api/json_api/selection.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/signup.py
-Comment: 
-
 Filename: axonius_api_client/api/json_api/spaces_export.py
 Comment: 
 
 Filename: axonius_api_client/api/json_api/system_meta.py
 Comment: 
 
 Filename: axonius_api_client/api/json_api/system_roles.py
@@ -210,162 +192,252 @@
 
 Filename: axonius_api_client/api/json_api/system_users.py
 Comment: 
 
 Filename: axonius_api_client/api/json_api/time_range.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/folders/__init__.py
+Filename: axonius_api_client/api/json_api/account/__init__.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/folders/base.py
+Filename: axonius_api_client/api/json_api/account/current_user.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/folders/enforcements.py
+Filename: axonius_api_client/api/json_api/account/login_request.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/folders/queries.py
+Filename: axonius_api_client/api/json_api/account/login_response.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/tasks/__init__.py
+Filename: axonius_api_client/api/json_api/adapters/__init__.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/tasks/get_tasks.py
+Filename: axonius_api_client/api/json_api/adapters/adapter_node.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/tasks/result.py
+Filename: axonius_api_client/api/json_api/adapters/adapter_settings_response.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/tasks/task.py
+Filename: axonius_api_client/api/json_api/adapters/adapter_settings_update_request.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/tasks/task_basic.py
+Filename: axonius_api_client/api/json_api/adapters/adapters_list_response.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/tasks/task_filters.py
+Filename: axonius_api_client/api/json_api/adapters/adapters_request.py
 Comment: 
 
-Filename: axonius_api_client/api/json_api/tasks/task_full.py
+Filename: axonius_api_client/api/json_api/adapters/adapters_response.py
 Comment: 
 
-Filename: axonius_api_client/api/openapi/__init__.py
+Filename: axonius_api_client/api/json_api/adapters/clients_count.py
 Comment: 
 
-Filename: axonius_api_client/api/openapi/openapi_spec.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_create_request.py
 Comment: 
 
-Filename: axonius_api_client/api/system/__init__.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_create_response.py
 Comment: 
 
-Filename: axonius_api_client/api/system/activity_logs.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_delete_request.py
 Comment: 
 
-Filename: axonius_api_client/api/system/dashboard.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_delete_response.py
 Comment: 
 
-Filename: axonius_api_client/api/system/dashboard_spaces.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_labels_response.py
 Comment: 
 
-Filename: axonius_api_client/api/system/data_scopes.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_test_request.py
 Comment: 
 
-Filename: axonius_api_client/api/system/instances.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_update_request.py
 Comment: 
 
-Filename: axonius_api_client/api/system/meta.py
+Filename: axonius_api_client/api/json_api/adapters/cnx_update_response.py
 Comment: 
 
-Filename: axonius_api_client/api/system/remote_support.py
+Filename: axonius_api_client/api/json_api/adapters/cnxs_response.py
 Comment: 
 
-Filename: axonius_api_client/api/system/settings.py
+Filename: axonius_api_client/api/json_api/adapters/fetch_history_filters_response.py
 Comment: 
 
-Filename: axonius_api_client/api/system/signup.py
+Filename: axonius_api_client/api/json_api/adapters/fetch_history_request.py
 Comment: 
 
-Filename: axonius_api_client/api/system/system_roles.py
+Filename: axonius_api_client/api/json_api/adapters/fetch_history_response.py
 Comment: 
 
-Filename: axonius_api_client/api/system/system_users.py
+Filename: axonius_api_client/api/json_api/assets/__init__.py
 Comment: 
 
-Filename: axonius_api_client/api/wizards/__init__.py
+Filename: axonius_api_client/api/json_api/assets/asset_id_request.py
 Comment: 
 
-Filename: axonius_api_client/api/wizards/wizard.py
+Filename: axonius_api_client/api/json_api/assets/asset_id_response.py
 Comment: 
 
-Filename: axonius_api_client/api/wizards/wizard_csv.py
+Filename: axonius_api_client/api/json_api/assets/asset_request.py
 Comment: 
 
-Filename: axonius_api_client/api/wizards/wizard_text.py
+Filename: axonius_api_client/api/json_api/assets/asset_response.py
 Comment: 
 
-Filename: axonius_api_client/auth/__init__.py
+Filename: axonius_api_client/api/json_api/assets/count_request.py
 Comment: 
 
-Filename: axonius_api_client/auth/api_key.py
+Filename: axonius_api_client/api/json_api/assets/count_response.py
 Comment: 
 
-Filename: axonius_api_client/auth/credentials.py
+Filename: axonius_api_client/api/json_api/assets/destroy_request.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/assets/destroy_response.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/assets/fields_response.py
 Comment: 
 
-Filename: axonius_api_client/auth/models.py
+Filename: axonius_api_client/api/json_api/assets/history_dates_human.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/__init__.py
+Filename: axonius_api_client/api/json_api/assets/history_dates_response.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/all_logs_list.json
+Filename: axonius_api_client/api/json_api/assets/modify_tags_request.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/all_logs_list.py
+Filename: axonius_api_client/api/json_api/assets/modify_tags_response.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/constants.py
+Filename: axonius_api_client/api/json_api/assets/run_enforcement_request.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/convert.py
+Filename: axonius_api_client/api/json_api/audit_logs/__init__.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/audit_logs/audit_log_request.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/audit_logs/audit_log_response.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/folders/__init__.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/folders/base.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/folders/enforcements.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/folders/queries.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/ct_logs.py
+Filename: axonius_api_client/api/json_api/signup/__init__.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/signup/signup_request.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/signup/signup_response.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/signup/system_status.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/tasks/__init__.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/tasks/get_tasks.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/tasks/result.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/tasks/task.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/tasks/task_basic.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/tasks/task_filters.py
+Comment: 
+
+Filename: axonius_api_client/api/json_api/tasks/task_full.py
+Comment: 
+
+Filename: axonius_api_client/api/openapi/__init__.py
+Comment: 
+
+Filename: axonius_api_client/api/openapi/openapi_spec.py
+Comment: 
+
+Filename: axonius_api_client/api/system/__init__.py
+Comment: 
+
+Filename: axonius_api_client/api/system/activity_logs.py
+Comment: 
+
+Filename: axonius_api_client/api/system/dashboard.py
+Comment: 
+
+Filename: axonius_api_client/api/system/dashboard_spaces.py
+Comment: 
+
+Filename: axonius_api_client/api/system/data_scopes.py
+Comment: 
+
+Filename: axonius_api_client/api/system/instances.py
+Comment: 
+
+Filename: axonius_api_client/api/system/meta.py
+Comment: 
+
+Filename: axonius_api_client/api/system/remote_support.py
+Comment: 
+
+Filename: axonius_api_client/api/system/settings.py
+Comment: 
+
+Filename: axonius_api_client/api/system/signup.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/enums.py
+Filename: axonius_api_client/api/system/system_roles.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/exceptions.py
+Filename: axonius_api_client/api/system/system_users.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/paths.py
+Filename: axonius_api_client/api/wizards/__init__.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/ssl_capture.py
+Filename: axonius_api_client/api/wizards/wizard.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/ssl_context.py
+Filename: axonius_api_client/api/wizards/wizard_csv.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/ssl_extensions.py
+Filename: axonius_api_client/api/wizards/wizard_text.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/utils.py
+Filename: axonius_api_client/auth/__init__.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/stores/__init__.py
+Filename: axonius_api_client/auth/api_key.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/stores/cert.py
+Filename: axonius_api_client/auth/credentials.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/stores/cert_request.py
+Filename: axonius_api_client/auth/model.py
 Comment: 
 
-Filename: axonius_api_client/cert_human/stores/store.py
+Filename: axonius_api_client/auth/null.py
 Comment: 
 
 Filename: axonius_api_client/cli/__init__.py
 Comment: 
 
 Filename: axonius_api_client/cli/context.py
 Comment: 
@@ -678,14 +750,38 @@
 
 Filename: axonius_api_client/cli/grp_enforcements/cmd_update_schedule_weekly.py
 Comment: 
 
 Filename: axonius_api_client/cli/grp_enforcements/grp_common.py
 Comment: 
 
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/__init__.py
+Comment: 
+
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_count.py
+Comment: 
+
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_get.py
+Comment: 
+
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_get_filters.py
+Comment: 
+
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/export_get.py
+Comment: 
+
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/export_get_filters.py
+Comment: 
+
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/options_get.py
+Comment: 
+
+Filename: axonius_api_client/cli/grp_enforcements/grp_tasks/options_get_filters.py
+Comment: 
+
 Filename: axonius_api_client/cli/grp_folders/__init__.py
 Comment: 
 
 Filename: axonius_api_client/cli/grp_folders/cmd_create.py
 Comment: 
 
 Filename: axonius_api_client/cli/grp_folders/cmd_delete.py
@@ -978,14 +1074,17 @@
 
 Filename: axonius_api_client/constants/adapters.py
 Comment: 
 
 Filename: axonius_api_client/constants/api.py
 Comment: 
 
+Filename: axonius_api_client/constants/asset_helpers.py
+Comment: 
+
 Filename: axonius_api_client/constants/ctypes.py
 Comment: 
 
 Filename: axonius_api_client/constants/enforcements.py
 Comment: 
 
 Filename: axonius_api_client/constants/fields.py
@@ -1053,39 +1152,105 @@
 
 Filename: axonius_api_client/parsers/__init__.py
 Comment: 
 
 Filename: axonius_api_client/parsers/config.py
 Comment: 
 
-Filename: axonius_api_client/parsers/dt_delta.py
-Comment: 
-
 Filename: axonius_api_client/parsers/fields.py
 Comment: 
 
 Filename: axonius_api_client/parsers/grabber.py
 Comment: 
 
 Filename: axonius_api_client/parsers/matcher.py
 Comment: 
 
-Filename: axonius_api_client/parsers/search_wip.py
-Comment: 
-
 Filename: axonius_api_client/parsers/searchers.py
 Comment: 
 
 Filename: axonius_api_client/parsers/tables.py
 Comment: 
 
-Filename: axonius_api_client/parsers/url_parser.py
+Filename: axonius_api_client/parsers/wizards.py
+Comment: 
+
+Filename: axonius_api_client/projects/__init__.py
 Comment: 
 
-Filename: axonius_api_client/parsers/wizards.py
+Filename: axonius_api_client/projects/cli_cf_token.py
+Comment: 
+
+Filename: axonius_api_client/projects/url_parser.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/__init__.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/all_logs_list.json
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/all_logs_list.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/constants.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/convert.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/ct_logs.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/enums.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/exceptions.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/paths.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/ssl_capture.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/ssl_context.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/ssl_extensions.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/utils.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/stores/__init__.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/stores/cert.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/stores/cert_request.py
+Comment: 
+
+Filename: axonius_api_client/projects/cert_human/stores/store.py
+Comment: 
+
+Filename: axonius_api_client/projects/cf_token/__init__.py
+Comment: 
+
+Filename: axonius_api_client/projects/cf_token/cli.py
+Comment: 
+
+Filename: axonius_api_client/projects/cf_token/constants.py
+Comment: 
+
+Filename: axonius_api_client/projects/cf_token/flows.py
+Comment: 
+
+Filename: axonius_api_client/projects/cf_token/tools.py
 Comment: 
 
 Filename: axonius_api_client/tests/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/conftest.py
 Comment: 
@@ -1203,14 +1368,17 @@
 
 Filename: axonius_api_client/tests/tests_api/tests_enforcements/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_api/tests_enforcements/test_enforcements.py
 Comment: 
 
+Filename: axonius_api_client/tests/tests_api/tests_enforcements/test_tasks.py
+Comment: 
+
 Filename: axonius_api_client/tests/tests_api/tests_folders/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_api/tests_folders/test_folders.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_api/tests_json_api/__init__.py
@@ -1296,14 +1464,26 @@
 
 Filename: axonius_api_client/tests/tests_auth/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_auth/test_auth.py
 Comment: 
 
+Filename: axonius_api_client/tests/tests_cf_token/__init__.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cf_token/meta.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cf_token/test_flows.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cf_token/test_tools.py
+Comment: 
+
 Filename: axonius_api_client/tests/tests_cli/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_folders_grp/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_folders_grp/test_folders_grp.py
@@ -1434,14 +1614,56 @@
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_get_action_types.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_run.py
 Comment: 
 
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_description.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_above.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_below.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_decreased.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_increased.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_only_new_assets.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_query.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_query_remove.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_daily.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_discovery.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_hourly.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_monthly.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_never.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_weekly.py
+Comment: 
+
 Filename: axonius_api_client/tests/tests_cli/tests_grp_meta/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_meta/test_cmd_about.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_meta/test_cmd_history_sizes.py
@@ -1521,14 +1743,26 @@
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_name.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_perms.py
 Comment: 
 
+Filename: axonius_api_client/tests/tests_cli/tests_grp_tasks/__init__.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_count.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_get.py
+Comment: 
+
+Filename: axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_get_filters.py
+Comment: 
+
 Filename: axonius_api_client/tests/tests_cli/tests_grp_tools/__init__.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_shell.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_sysinfo.py
@@ -1557,26 +1791,26 @@
 
 Filename: axonius_api_client/tests/tests_pkg/test_tools.py
 Comment: 
 
 Filename: axonius_api_client/tests/tests_pkg/test_url_parser.py
 Comment: 
 
-Filename: axonius_api_client-4.60.4.dist-info/LICENSE
+Filename: axonius_api_client-5.0.0.dist-info/LICENSE
 Comment: 
 
-Filename: axonius_api_client-4.60.4.dist-info/METADATA
+Filename: axonius_api_client-5.0.0.dist-info/METADATA
 Comment: 
 
-Filename: axonius_api_client-4.60.4.dist-info/WHEEL
+Filename: axonius_api_client-5.0.0.dist-info/WHEEL
 Comment: 
 
-Filename: axonius_api_client-4.60.4.dist-info/entry_points.txt
+Filename: axonius_api_client-5.0.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: axonius_api_client-4.60.4.dist-info/top_level.txt
+Filename: axonius_api_client-5.0.0.dist-info/top_level.txt
 Comment: 
 
-Filename: axonius_api_client-4.60.4.dist-info/RECORD
+Filename: axonius_api_client-5.0.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## axonius_api_client/__init__.py

```diff
@@ -2,19 +2,24 @@
 """Python API Client for Axonius.
 
 See Also:
     :obj:`connect.Connect` for creating a client for using this package.
 
 """
 import logging
+import sys
+import pathlib
 
 from . import setup_env, version
 
 PACKAGE_ROOT: str = __package__
 PACKAGE_FILE: str = __file__
+PACKAGE_PATH: pathlib.Path = pathlib.Path(PACKAGE_FILE).parent.absolute()
+PROJECTS_PATH: pathlib.Path = PACKAGE_PATH / "projects"
+
 VERSION: str = version.__version__
 __version__ = VERSION
 
 LOG: logging.Logger = logging.getLogger(PACKAGE_ROOT)
 """root logger used by entire package, named after package."""
 
 DEFAULT_PATH: str = setup_env.DEFAULT_PATH
@@ -28,16 +33,38 @@
 
 INIT_DOTENV: str = load_dotenv()
 """Initial path to .env file that was loaded"""
 
 POST_DOTENV: dict = setup_env.get_env_ax()
 """AX.* env variables after loading dotenv."""
 
+# short term hack for projects until they are moved to their own repos
 try:
-    from . import api, auth, cert_human, cli, constants, data, exceptions, http, logs, tools
+    sys.path.insert(0, str(PROJECTS_PATH))
+    from .projects import cf_token, cert_human, url_parser
+
+except Exception:  # pragma: no cover
+    raise
+
+
+try:
+    # noinspection PyCompatibility
+    from . import (
+        api,
+        auth,
+        cli,
+        constants,
+        data,
+        exceptions,
+        http,
+        logs,
+        tools,
+        projects,
+    )
+
     from .api import (
         ActivityLogs,
         Adapters,
         ApiEndpoints,
         Cnx,
         Dashboard,
         DashboardSpaces,
@@ -54,32 +81,37 @@
         SystemRoles,
         SystemUsers,
         Users,
         Vulnerabilities,
         Wizard,
         WizardCsv,
         WizardText,
+        json_api,
     )
-    from .auth import ApiKey
+    from .auth import AuthApiKey, AuthCredentials, AuthModel, AuthNull
     from .connect import Connect
     from .features import Features
     from .http import Http
 except Exception:  # pragma: no cover
     raise
 
 
 LOG = logs.LOG
 
 __all__ = (
+    "PACKAGE_ROOT",
     # API client
     "Connect",
     # HTTP client
     "Http",
     # API authentication
-    "ApiKey",
+    "AuthApiKey",
+    "AuthModel",
+    "AuthCredentials",
+    "AuthNull",
     # API
     "Adapters",
     "Cnx",
     "Dashboard",
     "DashboardSpaces",
     "Devices",
     "Enforcements",
@@ -111,9 +143,14 @@
     "constants",
     "data",
     "exceptions",
     "http",
     "logs",
     "tools",
     "version",
+    "json_api",
+    # projects
+    "projects",
     "cert_human",
+    "cf_token",
+    "url_parser",
 )
```

## axonius_api_client/connect.py

```diff
@@ -1,66 +1,40 @@
 # -*- coding: utf-8 -*-
 """Easy all-in-one connection handler."""
-import typing as t
-import types
 import logging
 import logging.handlers
-import pathlib
+import platform
 import re
-from typing import List, Optional, Union
+import types
+import typing as t
 
 import requests
 
-from .api import (
-    ActivityLogs,
-    Adapters,
-    Dashboard,
-    DashboardSpaces,
-    DataScopes,
-    Devices,
-    Enforcements,
-    Folders,
-    Instances,
-    Meta,
-    OpenAPISpec,
-    RemoteSupport,
-    SettingsGlobal,
-    SettingsGui,
-    SettingsIdentityProviders,
-    SettingsLifecycle,
-    Signup,
-    SystemRoles,
-    SystemUsers,
-    Users,
-    Vulnerabilities,
-)
-from .auth import ApiKey, Credentials
-from .constants.api import TIMEOUT_CONNECT, TIMEOUT_RESPONSE
+from . import api, logs, tools, version
+from .projects import cert_human
+from .projects.cf_token import constants as cf_constants
+from .auth import AuthApiKey, AuthCredentials, AuthModel, AuthNull
+from .constants.ctypes import PathLike
 from .constants.logs import (
     LOG_FILE_MAX_FILES,
     LOG_FILE_MAX_MB,
     LOG_FILE_NAME,
     LOG_FILE_PATH,
     LOG_FMT_BRIEF,
     LOG_FMT_VERBOSE,
     LOG_LEVEL_API,
     LOG_LEVEL_AUTH,
     LOG_LEVEL_CONSOLE,
     LOG_LEVEL_ENDPOINTS,
     LOG_LEVEL_FILE,
-    LOG_LEVEL_HTTP,
     LOG_LEVEL_PACKAGE,
 )
 from .exceptions import ConnectError, InvalidCredentials
 from .http import Http, T_Cookies, T_Headers
-from .logs import LOG, HideFormatter, add_file, add_stderr, get_obj_log, set_log_level
 from .setup_env import get_env_ax
-from .tools import coerce_bool, coerce_int, json_dump, json_reload, sysinfo
-from .version import __version__ as VERSION
-from . import tools
 
 
 class Connect:
     """Easy all-in-one connection handler for using the API client.
 
     Examples:
         >>> #!/usr/bin/env python
@@ -75,537 +49,807 @@
         >>> # client_args = axonapi.get_env_connect(ax_env="/path/to/envfile", override=True)
         >>>
         >>> # create a client using the url, key, and secret from OS env
         >>> client = axonapi.Connect(**client_args)
         >>>
         >>> j = client.jdump  # json dump helper
         >>>
-        >>> client.start()                  # connect to axonius
+        >>> client.start()  # connect to axonius
         >>>
-        >>> # client.activity_logs          # get audit logs
-        >>> # client.adapters               # get adapters and update adapter settings
-        >>> # client.adapters.cnx           # CRUD for adapter connections
-        >>> # client.dashboard              # get/start/stop discovery cycles
-        >>> # client.devices                # get device assets
-        >>> # client.devices.fields         # get field schemas for device assets
-        >>> # client.devices.labels         # add/remove/get tags for device assets
-        >>> # client.devices.saved_queries  # CRUD for saved queries for device assets
-        >>> # client.enforcements           # CRUD for enforcements
-        >>> # client.instances              # get instances and instance meta data
-        >>> # client.meta                   # get product meta data
-        >>> # client.remote_support         # enable/disable remote support settings
-        >>> # client.settings_global        # get/update global system settings
-        >>> # client.settings_gui           # get/update gui system settings
-        >>> # client.settings_ip            # get/update identity provider system settings
-        >>> # client.settings_lifecycle     # get/update lifecycle system settings
-        >>> # client.signup                 # perform initial signup and use password reset tokens
-        >>> # client.system_roles           # CRUD for system roles
-        >>> # client.system_users           # CRUD for system users
-        >>> # client.users                  # get user assets
-        >>> # client.users.fields           # get field schemas for user assets
-        >>> # client.users.labels           # add/remove/get tags for user assets
-        >>> # client.users.saved_queries    # CRUD for saved queries for user assets
-
+        >>> # client.activity_logs                 # get audit logs
+        >>> # client.adapters                      # get adapters and update adapter settings
+        >>> # client.adapters.cnx                  # CRUD for adapter connections
+        >>> # client.dashboard                     # get/start/stop discovery cycles
+        >>> # client.dashboard_spaces              # CRUD for dashboard spaces
+        >>> # client.data_scopes                   # CRUD for data scopes
+        >>> # client.devices                       # get device assets
+        >>> # client.devices.fields                # get field schemas for device assets
+        >>> # client.devices.labels                # add/remove/get tags for device assets
+        >>> # client.devices.saved_queries         # CRUD for saved queries for device assets
+        >>> # client.enforcements                  # CRUD for enforcements
+        >>> # client.folders                       # CRUD for folders
+        >>> # client.folders.enforcements          # CRUD for enforcements folders
+        >>> # client.folders.queries               # CRUD for queries folders
+        >>> # client.instances                     # get instances and instance meta data
+        >>> # client.openapi                       # get openapi spec
+        >>> # client.meta                          # get product meta data
+        >>> # client.remote_support                # include_output/disable remote support settings
+        >>> # client.settings_global               # get/update global system settings
+        >>> # client.settings_gui                  # get/update gui system settings
+        >>> # client.settings_ip                   # get/update identity provider system settings
+        >>> # client.settings_lifecycle            # get/update lifecycle system settings
+        >>> # client.signup                        # initial signup, password resets
+        >>> # client.system_roles                  # CRUD for system roles
+        >>> # client.system_users                  # CRUD for system users
+        >>> # client.users                         # get user assets
+        >>> # client.users.fields                  # get field schemas for user assets
+        >>> # client.users.labels                  # add/remove/get tags for user assets
+        >>> # client.users.saved_queries           # CRUD for saved queries for user assets
+        >>> # client.vulnerabilities               # get vulnerability assets
+        >>> # client.vulnerabilities.fields        # get field schemas for vulnerability assets
+        >>> # client.vulnerabilities.labels        # add/remove/get tags for vulnerability assets
+        >>> # client.vulnerabilities.saved_queries # CRUD for saved queries for vulnerability assets
     """
 
     TOOLS: types.ModuleType = tools
     """Tools module."""
 
+    LOG_LOGGER: logging.Logger = logs.LOG
+    """Logger for the entire package, where console and file output handlers will be attached to."""
+
+    LOG: logging.Logger = None
+    """Logger for this class."""
+
+    LOG_HTTP_MAX: bool = False
+    """Shortcut to include_output ALL http logging *warning: very heavy log output*."""
+
+    STARTED: bool = False
+    """Flag to indicate if client has been started."""
+
+    WRAPERROR: bool = True
+    """Flag to indicate if client should wrap exceptions."""
+
+    _url: str = None
+    """Initially supplied URL of the Axonius instance."""
+
+    ARGS_HANDLER_CON: dict = None
+    """Arguments to use when setting up console logging."""
+
+    ARGS_HANDLER_FILE: dict = None
+    """Arguments to use when setting up file logging."""
+
+    ARGS_API: dict = None
+    """Arguments to use when setting up models."""
+
+    ARGS_ORIG: dict = None
+    """Original arguments supplied to the constructor."""
+
+    HANDLER_CON: t.Optional[logging.StreamHandler] = None
+    """Console logging handler."""
+
+    HANDLER_FILE: t.Optional[logging.handlers.RotatingFileHandler] = None
+    """File logging handler."""
+
+    http: Http = None
+    HTTP: Http = None
+    """HTTP client."""
+
+    auth: AuthModel = None
+    AUTH: AuthModel = None
+    """Authentication handler."""
+
+    AUTH_NULL: AuthModel = None
+    """Auth model for authenticating with no auth."""
+
+    CREDENTIALS: bool = False
+    """Flag to indicate if key & secret are actually username & password."""
+
+    API_CACHE: t.Dict[t.Type[api.ModelMixins], api.ModelMixins] = None
+    """Cache for API Models."""
+
+    API_ATTRS: t.List[str] = [
+        "activity_logs",
+        "adapters",
+        "dashboard",
+        "dashboard_spaces",
+        "data_scopes",
+        "devices",
+        "enforcements",
+        "folders",
+        "instances",
+        "meta",
+        "openapi",
+        "remote_support",
+        "settings_global",
+        "settings_gui",
+        "settings_ip",
+        "settings_lifecycle",
+        "signup",
+        "system_roles",
+        "system_users",
+        "users",
+        "vulnerabilities",
+    ]
+    """Attributes that are API Models."""
+
+    API_LOG_LEVEL: t.Union[int, str] = LOG_LEVEL_API
+    """Log level for API Models."""
+
+    REASON_RES: t.List[t.Pattern] = [
+        re.compile(r".*?object at.*?>: ([a-zA-Z0-9\]\[: ]+)"),
+        re.compile(r".*?] (.*) "),
+    ]
+    """Patterns to look for in exceptions that we can pretty up for user display."""
+
+    PKG_VERSION: str = version.__version__
+    """Version of this package."""
+
+    PY_VERSION: str = platform.python_version()
+    """Version of Python that this package is running on."""
+
+    ABOUT_CACHE: t.Optional[dict] = None
+    """Cached data from the /about endpoint."""
+
+    HTTP_MAX: str = """log_request_body = True
+log_response_body = True
+log_level_http = "debug"
+log_level_package = "debug"
+log_level_console = "debug"
+log_level_file = "debug"
+log_request_attrs = "all"
+log_response_attrs = "all"
+log_body_lines = 10000
+"""
+    """Override values used when log_http_max is True."""
+
+    HTTP_MAX_CLI: str = ", ".join(HTTP_MAX.splitlines())
+    """CLI Help string for log_http_max."""
+
     def __init__(
         self,
         url: str,
         key: str,
         secret: str,
         log_console: bool = False,
         log_file: bool = False,
         log_file_rotate: bool = False,
-        certpath: Optional[Union[str, pathlib.Path]] = None,
+        certpath: t.Optional[PathLike] = None,
         certverify: bool = False,
         certwarn: bool = True,
-        proxy: Optional[str] = None,
-        headers: Optional[T_Headers] = None,
-        cookies: Optional[T_Cookies] = None,
+        proxy: t.Optional[str] = None,
+        headers: t.Optional[T_Headers] = None,
+        cookies: t.Optional[T_Cookies] = None,
         credentials: bool = False,
+        timeout_connect: t.Optional[t.Union[int, float]] = Http.CONNECT_TIMEOUT,
+        timeout_response: t.Optional[t.Union[int, float]] = Http.RESPONSE_TIMEOUT,
+        cert_client_key: t.Optional[PathLike] = None,
+        cert_client_cert: t.Optional[PathLike] = None,
+        cert_client_both: t.Optional[PathLike] = None,
+        save_history: bool = False,
+        log_level: t.Union[str, int] = "debug",
+        log_request_attrs: t.Optional[t.Union[str, t.Iterable[str]]] = None,
+        log_response_attrs: t.Optional[t.Union[str, t.Iterable[str]]] = None,
+        log_request_body: bool = False,
+        log_response_body: bool = False,
+        log_logger: logging.Logger = LOG_LOGGER,
+        log_level_package: t.Union[str, int] = LOG_LEVEL_PACKAGE,
+        log_level_endpoints: t.Union[str, int] = LOG_LEVEL_ENDPOINTS,
+        log_level_http: t.Union[str, int] = Http.LOG_LEVEL,
+        log_level_auth: t.Union[str, int] = LOG_LEVEL_AUTH,
+        log_level_api: t.Union[str, int] = LOG_LEVEL_API,
+        log_level_console: t.Union[str, int] = LOG_LEVEL_CONSOLE,
+        log_level_file: t.Union[str, int] = LOG_LEVEL_FILE,
+        log_console_fmt: str = LOG_FMT_BRIEF,
+        log_http_max: bool = LOG_HTTP_MAX,
+        log_file_fmt: str = LOG_FMT_VERBOSE,
+        log_file_name: t.Optional[PathLike] = LOG_FILE_NAME,
+        log_file_path: t.Optional[PathLike] = LOG_FILE_PATH,
+        log_file_max_mb: int = LOG_FILE_MAX_MB,
+        log_file_max_files: int = LOG_FILE_MAX_FILES,
         log_hide_secrets: bool = True,
+        log_body_lines: int = Http.LOG_BODY_LINES,
+        wraperror: bool = True,
+        cf_token: t.Optional[str] = None,
+        cf_url: t.Optional[str] = None,
+        cf_path: t.Optional[PathLike] = cf_constants.CF_PATH,
+        cf_run: bool = cf_constants.CLIENT_RUN,
+        cf_run_login: bool = cf_constants.FLOW_RUN_LOGIN,
+        cf_run_access: bool = cf_constants.FLOW_RUN_ACCESS,
+        cf_env: bool = cf_constants.FLOW_ENV,
+        cf_echo: bool = cf_constants.FLOW_ECHO,
+        cf_echo_verbose: bool = cf_constants.FLOW_ECHO_VERBOSE,
+        cf_error: bool = cf_constants.CLIENT_ERROR,
+        cf_error_login: bool = cf_constants.FLOW_ERROR,
+        cf_error_access: bool = cf_constants.FLOW_ERROR,
+        cf_timeout_access: t.Optional[int] = cf_constants.TIMEOUT_ACCESS,
+        cf_timeout_login: t.Optional[int] = cf_constants.TIMEOUT_LOGIN,
+        http: t.Optional[Http] = None,
+        auth: t.Optional[AuthModel] = None,
+        auth_null: t.Optional[AuthModel] = None,
         **kwargs,
     ):
         """Easy all-in-one connection handler.
 
         Args:
             url: URL, hostname, or IP address of Axonius instance
             key: API Key from account page in Axonius instance
             secret: API Secret from account page in Axonius instance
-            log_console: enable logging to console
-            log_file: enable logging to file
-            certpath: path to CA bundle file to use when verifying certs offered by :attr:`url`
+            log_console: include_output logging to console
+            log_file: include_output logging to file
+            certpath: path to CA bundle file to use when verifying certs offered by url
             certverify: raise exception if cert is self-signed or only if cert is invalid
             certwarn: show insecure warning once or never show insecure warning
-            proxy: proxy to use when making https requests to :attr:`url`
+            proxy: proxy to use when making https requests to url
             headers: additional headers to supply with every request
             cookies: additional cookies to supply with every request
-            **kwargs: documented as properties
+            credentials: treat key as username as secret as password
+            timeout_connect: seconds to wait for connections to open to url
+            timeout_response: seconds to wait for responses from url
+            cert_client_key: file with private key to offer to url
+            cert_client_cert: file with client cert to offer to url
+            cert_client_both: file with client cert and private key to offer to url
+            save_history: save history of responses to Http.HISTORY
+            log_level: log level to use for this object
+            log_request_attrs: list of request attributes to log
+            log_response_attrs: list of response attributes to log
+            log_request_body: log request body
+            log_response_body: log response body
+            log_logger: Logger for the entire package, where console and file output
+                handlers will be attached to
+            log_level_package: log level to use for package root logger
+            log_level_endpoints: log level to use for endpoint loggers
+            log_level_http: log level to use for http loggers
+            log_level_auth: log level to use for auth loggers
+            log_level_api: log level to use for api loggers
+            log_level_console: log level to use for console loggers
+            log_level_file: log level to use for file loggers
+            log_console_fmt: format string to use for console logging
+            log_file_fmt: format string to use for file logging
+            log_file_name: name of file to log to
+            log_file_path: path to directory to log to
+            log_file_max_mb: max size of log file in MB
+            log_file_max_files: max number of log files to keep
+            log_body_lines: max length of request/response body to log
+            log_hide_secrets: hide secrets in logs
+            log_http_max: Shortcut to include_output ALL http logging *warning: heavy log output*
+            wraperror: wrap certain errors in a more user friendly format
+            cf_url: URL to use in `access token` and `access login` commands,
+                will fallback to url if not supplied
+            cf_token: access token supplied by user, will be checked for validity if not empty
+            cf_env: if no token supplied, try to get token from OS env var CF_TOKEN
+            cf_run: if no token supplied or in OS env vars, try to get token from `access token` and
+                `access login` commands
+            cf_run_access: if run is True, try to get token from `access token`,
+            cf_run_login: if run is True and no token returned from `access token` command,
+                try to get token from `access login` command
+            cf_path: path to cloudflared binary to run, can be full path or path in OS env var $PATH
+            cf_timeout_access: timeout for `access token` command in seconds
+            cf_timeout_login: timeout for `access login` command in seconds
+            cf_error: raise error if an invalid token is found or no token can be found
+            cf_error_access: raise exc if `access token` command fails and login is False
+            cf_error_login: raise exc if `access login` command fails
+            cf_echo: echo commands and results to stderr
+            cf_echo_verbose: echo more to stderr
+            http: http object to use for this connection
+            auth: auth model to use for this connection
+            auth_null: null auth model to use for this connection
+            **kwargs: unused
         """
-        self.url: str = url
-        """URL of Axonius instance to use"""
-
-        certwarn = coerce_bool(certwarn)
-        certverify = coerce_bool(certverify)
-        log_console = coerce_bool(log_console)
-        log_file = coerce_bool(log_file)
-        self.LOG_HIDE_SECRETS: bool = coerce_bool(log_hide_secrets)
-
-        self.LOG_HIDE_SECRETS: bool = coerce_bool(log_hide_secrets)
-        self.TIMEOUT_CONNECT: int = coerce_int(kwargs.get("timeout_connect", TIMEOUT_CONNECT))
-        """Seconds to wait for connections to open to :attr:`url` ``kwargs=timeout_connect``"""
-
-        self.TIMEOUT_RESPONSE: int = coerce_int(kwargs.get("timeout_response", TIMEOUT_RESPONSE))
-        """Seconds to wait for responses from :attr:`url` ``kwargs=timeout_response``"""
-
-        self.CERT_CLIENT_KEY: Optional[Union[str, pathlib.Path]] = kwargs.get(
-            "cert_client_key", None
-        )
-        """Private key file for cert_client_cert ``kwargs=cert_client_key``"""
-
-        self.CERT_CLIENT_CERT: Optional[Union[str, pathlib.Path]] = kwargs.get(
-            "cert_client_cert", None
-        )
-        """cert file to offer to :attr:`url` ``kwargs=cert_client_cert``"""
-
-        self.CERT_CLIENT_BOTH: Optional[Union[str, pathlib.Path]] = kwargs.get(
-            "cert_client_both", None
-        )
-        """cert file with both private key and cert to offer to :attr:`url`
-        ``kwargs=cert_client_both``"""
-
-        self.SAVE_HISTORY: bool = kwargs.get("save_history", False)
-        """append responses to :attr:`axonius_api_client.http.Http.HISTORY`
-        ``kwargs=save_history``"""
-
-        self.LOG_LEVEL: Union[str, int] = kwargs.get("log_level", "debug")
-        """log level for this class ``kwargs=log_level``"""
-
-        self.LOG_REQUEST_ATTRS: Optional[List[str]] = kwargs.get("log_request_attrs", None)
-        """request attrs to log :attr:`axonius_api_client.constants.logs.REQUEST_ATTR_MAP`
-        ``kwargs=log_request_attrs``"""
-
-        self.LOG_RESPONSE_ATTRS: Optional[List[str]] = kwargs.get("log_response_attrs", None)
-        """response attrs to log :attr:`axonius_api_client.constants.logs.RESPONSE_ATTR_MAP`
-        ``kwargs=log_response_attrs``"""
-
-        self.LOG_REQUEST_BODY: bool = kwargs.get("log_request_body", False)
-        """log request bodies ``kwargs=log_request_body``"""
-
-        self.LOG_RESPONSE_BODY: bool = kwargs.get("log_response_body", False)
-        """log response bodies ``kwargs=log_response_body``"""
-
-        self.LOG_LOGGER: logging.Logger = kwargs.get("log_logger", LOG)
-        """logger to use as package root logger ``kwargs=log_logger``"""
-
-        self.LOG_LEVEL_PACKAGE: Union[str, int] = kwargs.get("log_level_package", LOG_LEVEL_PACKAGE)
-        """log level for entire package ``kwargs=log_level_package``"""
-
-        self.LOG_LEVEL_ENDPOINTS: Union[str, int] = kwargs.get(
-            "log_level_endpoints", LOG_LEVEL_ENDPOINTS
-        )
-        """log level for entire package ``kwargs=log_level_package``"""
-
-        self.LOG_LEVEL_HTTP: Union[str, int] = kwargs.get("log_level_http", LOG_LEVEL_HTTP)
-        """log level for :obj:`axonius_api_client.http.Http` ``kwargs=log_level_http``"""
-
-        self.LOG_LEVEL_AUTH: Union[str, int] = kwargs.get("log_level_auth", LOG_LEVEL_AUTH)
-        """log level for :obj:`axonius_api_client.auth.models.Mixins` ``kwargs=log_level_auth``"""
-
-        self.LOG_LEVEL_API: Union[str, int] = kwargs.get("log_level_api", LOG_LEVEL_API)
-        """log level for :obj:`axonius_api_client.api.mixins.ModelMixins`
-        ``kwargs=log_level_api``"""
-
-        self.LOG_LEVEL_CONSOLE: Union[str, int] = kwargs.get("log_level_console", LOG_LEVEL_CONSOLE)
-        """log level for logs sent to console ``kwargs=log_level_console``"""
-
-        self.LOG_LEVEL_FILE: Union[str, int] = kwargs.get("log_level_file", LOG_LEVEL_FILE)
-        """log level for logs sent to file ``kwargs=log_level_file``"""
-
-        self.LOG_CONSOLE_FMT: str = kwargs.get("log_console_fmt", LOG_FMT_BRIEF)
-        """logging format to use for logs sent to console ``kwargs=log_console_fmt``"""
-
-        self.LOG_FILE_FMT: str = kwargs.get("log_file_fmt", LOG_FMT_VERBOSE)
-        """logging format to use for logs sent to file ``kwargs=log_file_fmt``"""
-
-        self.LOG_FILE_NAME: Union[str, pathlib.Path] = kwargs.get("log_file_name", LOG_FILE_NAME)
-        """name of file to write logs to under :attr:`LOG_FILE_PATH` ``kwargs=log_file_name``"""
-
-        self.LOG_FILE_PATH: Union[str, pathlib.Path] = kwargs.get("log_file_path", LOG_FILE_PATH)
-        """path to write :attr:`LOG_FILE_NAME` to ``kwargs=log_file_path``"""
-
-        self.LOG_FILE_MAX_MB: int = kwargs.get("log_file_max_mb", LOG_FILE_MAX_MB)
-        """rollover file logs at this many MB ``kwargs=log_file_max_mb``"""
-
-        self.LOG_FILE_MAX_FILES: int = kwargs.get("log_file_max_files", LOG_FILE_MAX_FILES)
-        """number of rollover file logs to keep ``kwargs=log_file_max_files``"""
-
-        self.WRAPERROR: bool = coerce_bool(kwargs.get("wraperror", True))
-        """wrap errors in human friendly way or show full traceback ``kwargs=wraperror``"""
-
-        self.LOG: logging.Logger = get_obj_log(obj=self, level=self.LOG_LEVEL)
-        """logger object to use"""
-
-        set_log_level(obj=self.LOG_LOGGER, level=self.LOG_LEVEL_PACKAGE)
-
-        from .api.api_endpoint import LOGGER as LOGGER_ENDPOINT
-
-        set_log_level(obj=LOGGER_ENDPOINT, level=self.LOG_LEVEL_ENDPOINTS)
-
-        self.STARTED: bool = False
-        """track if :meth:`start` has been called"""
-
-        self.HANDLER_FILE: t.Optional[logging.handlers.RotatingFileHandler] = None
-        """file logging handler"""
-
-        self.HANDLER_CON: t.Optional[logging.StreamHandler] = None
-        """console logging handler"""
-        HideFormatter.HIDE_ENABLED = self.LOG_HIDE_SECRETS
-
-        if log_console:
-            self.HANDLER_CON = add_stderr(
-                obj=self.LOG_LOGGER, level=self.LOG_LEVEL_CONSOLE, fmt=self.LOG_CONSOLE_FMT
-            )
-
-        if log_file:
-            self.HANDLER_FILE = add_file(
-                obj=self.LOG_LOGGER,
-                level=self.LOG_LEVEL_FILE,
-                file_path=self.LOG_FILE_PATH,
-                file_name=self.LOG_FILE_NAME,
-                max_mb=self.LOG_FILE_MAX_MB,
-                max_files=self.LOG_FILE_MAX_FILES,
-                fmt=self.LOG_FILE_FMT,
-            )
-        if log_file_rotate:
-            self.do_rollover()
-
-        self.HTTP_ARGS: dict = {
+        self._url: str = url
+        self.__key: str = key
+        self.__secret: str = secret
+        self.CLIENT = self
+        self.API_CACHE: dict = {}
+        self.ARGS_ORIG: dict = kwargs
+        self.LOG_LOGGER: logging.Logger = log_logger
+        self.LOG: logging.Logger = logs.get_obj_log(obj=self, log_level=log_level)
+        self.LOG_HTTP_MAX: bool = tools.coerce_bool(log_http_max)
+        self.CREDENTIALS: bool = tools.coerce_bool(credentials)
+
+        if self.LOG_HTTP_MAX:
+            log_request_body = True
+            log_response_body = True
+            log_level_http = "debug"
+            log_level_package = "debug"
+            log_level_console = "debug"
+            log_level_file = "debug"
+            log_request_attrs = "all"
+            log_response_attrs = "all"
+            if not isinstance(log_body_lines, int) or (
+                isinstance(log_body_lines, int) and log_body_lines < 10000
+            ):
+                log_body_lines = 10000
+
+        self.ARGS_HANDLER_CON: dict = {
+            "obj": log_logger,
+            "level": log_level_console,
+            "fmt": log_console_fmt,
+        }
+        self.ARGS_HANDLER_FILE: dict = {
+            "obj": log_logger,
+            "level": log_level_file,
+            "file_path": log_file_path,
+            "file_name": log_file_name,
+            "max_mb": log_file_max_mb,
+            "max_files": log_file_max_files,
+            "fmt": log_file_fmt,
+        }
+        self.ARGS_HTTP: dict = {
             "url": url,
             "https_proxy": proxy,
             "certpath": certpath,
             "certwarn": certwarn,
             "certverify": certverify,
-            "cert_client_both": self.CERT_CLIENT_BOTH,
-            "cert_client_cert": self.CERT_CLIENT_CERT,
-            "cert_client_key": self.CERT_CLIENT_KEY,
-            "log_level": self.LOG_LEVEL_HTTP,
-            "log_request_attrs": self.LOG_REQUEST_ATTRS,
-            "log_response_attrs": self.LOG_RESPONSE_ATTRS,
-            "log_request_body": self.LOG_REQUEST_BODY,
-            "log_response_body": self.LOG_RESPONSE_BODY,
-            "save_history": self.SAVE_HISTORY,
-            "connect_timeout": self.TIMEOUT_CONNECT,
-            "response_timeout": self.TIMEOUT_RESPONSE,
+            "cert_client_both": cert_client_both,
+            "cert_client_cert": cert_client_cert,
+            "cert_client_key": cert_client_key,
+            "log_level": log_level_http,
+            "log_body_lines": log_body_lines,
+            "log_request_attrs": log_request_attrs,
+            "log_response_attrs": log_response_attrs,
+            "log_request_body": log_request_body,
+            "log_response_body": log_response_body,
+            "save_history": save_history,
+            "connect_timeout": timeout_connect,
+            "response_timeout": timeout_response,
             "headers": headers,
             "cookies": cookies,
+            "cf_url": cf_url,
+            "cf_token": cf_token,
+            "cf_env": cf_env,
+            "cf_run": cf_run,
+            "cf_run_login": cf_run_login,
+            "cf_run_access": cf_run_access,
+            "cf_path": cf_path,
+            "cf_timeout_access": cf_timeout_access,
+            "cf_timeout_login": cf_timeout_login,
+            "cf_error": cf_error,
+            "cf_error_access": cf_error_access,
+            "cf_error_login": cf_error_login,
+            "cf_echo": cf_echo,
+            "cf_echo_verbose": cf_echo_verbose,
         }
-        """arguments to use for creating :attr:`HTTP`"""
-
-        self.HTTP = Http(**self.HTTP_ARGS)
-        """:obj:`axonius_api_client.http.Http` client to use for :attr:`AUTH`"""
-        self.AUTH_ARGS: dict = {"log_level": self.LOG_LEVEL_AUTH}
-
-        if credentials:
-            self.AUTH_ARGS.update({"username": key, "password": secret})
-            self.AUTH = Credentials(http=self.HTTP, **self.AUTH_ARGS)
-            """:obj:`Credentials` auth method to use for all API models"""
-        else:
-            self.AUTH_ARGS.update({"key": key, "secret": secret})
-            self.AUTH = ApiKey(http=self.HTTP, **self.AUTH_ARGS)
-            """:obj:`ApiKey` auth method to use for all API models"""
-
-        self.API_ARGS: dict = {"auth": self.AUTH, "log_level": self.LOG_LEVEL_API}
-        """arguments to use for all API models"""
-
-        self.SIGNUP = Signup(**self.HTTP_ARGS)
-        """Easy access to signup."""
-
-        self.HTTP.CLIENT = self
 
+        self.set_wraperror(wraperror)
+        self.set_log_hide_secrets(value=log_hide_secrets)
+        self.set_log_level_api(value=log_level_api)
+        self.set_log_level_package(value=log_level_package)
+        self.set_log_level_endpoints(value=log_level_endpoints)
+        self.control_log_file(enable=log_file, rotate=log_file_rotate)
+        self.control_log_console(enable=log_console)
+        self.HTTP = self.http = self._init_http(http=http)
+        self.AUTH = self.auth = self._init_auth(auth=auth, log_level=log_level_auth)
+        self.AUTH_NULL: AuthModel = self._init_auth_null(
+            auth_null=auth_null, log_level=log_level_auth
+        )
         self._init()
 
-    def do_rollover(self):
-        """Rollover log file."""
-        if self.HANDLER_FILE:
-            LOG.info("Forcing file logs to rotate")
-            self.HANDLER_FILE.flush()
-            try:
-                self.HANDLER_FILE.doRollover()
-                LOG.info("Forced file logs to rotate")
-            except Exception as exc:
-                LOG.exception("Failed to force file logs to rotate: %s", exc)
-
     def start(self):
         """Connect to and authenticate with Axonius."""
         if not self.STARTED:
-            sysinfo_dump = json_dump(sysinfo())
-            LOG.debug(f"SYSTEM INFO: {sysinfo_dump}")
+            sysinfo_dump: dict = tools.sysinfo()
+            self.LOG.debug(f"SYSTEM INFO: {tools.json_dump(sysinfo_dump)}")
 
             try:
                 self.AUTH.login()
             except Exception as exc:
                 if not self.WRAPERROR:
                     raise
 
-                pre = f"Unable to connect to {self.HTTP.url!r}"
+                pre = f"Unable to connect to {self.url!r}"
+                connect_exc = ConnectError(f"{pre}: {exc}")
 
                 if isinstance(exc, requests.ConnectTimeout):
                     timeout = self.HTTP.CONNECT_TIMEOUT
                     msg = f"{pre}: connection timed out after {timeout} seconds"
                     connect_exc = ConnectError(msg)
                 elif isinstance(exc, requests.ConnectionError):
                     reason = self._get_exc_reason(exc=exc)
                     connect_exc = ConnectError(f"{pre}: {reason}")
                 elif isinstance(exc, InvalidCredentials):
                     connect_exc = ConnectError(f"{pre}: Invalid Credentials supplied")
-                else:
-                    connect_exc = ConnectError(f"{pre}: {exc}")
+
                 connect_exc.exc = exc
                 raise connect_exc
 
             self.STARTED = True
-            LOG.info(str(self))
+            self.LOG.info(str(self))
 
+    # --> MODELS
     @property
-    def signup(self) -> Signup:
-        """Work with signup endpoints."""
-        if not hasattr(self, "_signup"):  # pragma: no cover
-            self._signup = Signup(**self.HTTP_ARGS)
-        return self._signup
+    def activity_logs(self) -> api.ActivityLogs:
+        """Work with activity logs."""
+        return self._get_model(model=api.ActivityLogs)
 
     @property
-    def users(self) -> Users:
-        """Work with user assets."""
-        self.start()
-        if not hasattr(self, "_users"):
-            self._users = Users(**self.API_ARGS)
-        return self._users
+    def adapters(self) -> api.Adapters:
+        """Work with adapters and adapter connections."""
+        return self._get_model(model=api.Adapters)
 
     @property
-    def vulnerabilities(self) -> Vulnerabilities:
-        """Work with user assets."""
-        self.start()
-        if not hasattr(self, "_vulnerabilities"):
-            self._vulnerabilities = Vulnerabilities(**self.API_ARGS)
-        return self._vulnerabilities
+    def dashboard(self) -> api.Dashboard:
+        """Work with discovery cycles."""
+        return self._get_model(model=api.Dashboard)
+
+    @property
+    def dashboard_spaces(self) -> api.DashboardSpaces:
+        """Work with dashboard spaces."""
+        return self._get_model(model=api.DashboardSpaces)
+
+    @property
+    def data_scopes(self) -> api.DataScopes:
+        """Work with data scopes."""
+        return self._get_model(model=api.DataScopes)
 
     @property
-    def devices(self) -> Devices:
+    def devices(self) -> api.Devices:
         """Work with device assets."""
-        self.start()
-        if not hasattr(self, "_devices"):
-            self._devices = Devices(**self.API_ARGS)
-        return self._devices
+        return self._get_model(model=api.Devices)
 
     @property
-    def adapters(self) -> Adapters:
-        """Work with adapters and adapter connections."""
-        self.start()
-        if not hasattr(self, "_adapters"):
-            self._adapters = Adapters(**self.API_ARGS)
-        return self._adapters
+    def enforcements(self) -> api.Enforcements:
+        """Work with Enforcement Center."""
+        return self._get_model(model=api.Enforcements)
+
+    @property
+    def folders(self) -> api.Folders:
+        """Work with folders for enforcements and queries."""
+        return self._get_model(model=api.Folders)
 
     @property
-    def instances(self) -> Instances:
+    def instances(self) -> api.Instances:
         """Work with instances."""
-        self.start()
-        if not hasattr(self, "_instances"):
-            self._instances = Instances(**self.API_ARGS)
-        return self._instances
+        return self._get_model(model=api.Instances)
 
     @property
-    def activity_logs(self) -> ActivityLogs:
-        """Work with activity logs."""
-        self.start()
-        if not hasattr(self, "_activity_logs"):
-            self._activity_logs = ActivityLogs(**self.API_ARGS)
-        return self._activity_logs
+    def openapi(self) -> api.OpenAPISpec:
+        """Work with the OpenAPI specification file."""
+        return self._get_model(model=api.OpenAPISpec)
 
     @property
-    def remote_support(self) -> RemoteSupport:
-        """Work with configuring remote support."""
-        self.start()
-        if not hasattr(self, "_remote_support"):
-            self._remote_support = RemoteSupport(**self.API_ARGS)
-        return self._remote_support
+    def meta(self) -> api.Meta:
+        """Work with instance metadata."""
+        return self._get_model(model=api.Meta)
 
     @property
-    def dashboard(self) -> Dashboard:
-        """Work with discovery cycles."""
-        self.start()
-        if not hasattr(self, "_dashboard"):
-            self._dashboard = Dashboard(**self.API_ARGS)
-        return self._dashboard
+    def remote_support(self) -> api.RemoteSupport:
+        """Work with configuring remote support."""
+        return self._get_model(model=api.RemoteSupport)
 
     @property
-    def dashboard_spaces(self) -> DashboardSpaces:
-        """Work with dashboard spaces."""
-        self.start()
-        if not hasattr(self, "_dashboard_spaces"):
-            self._dashboard_spaces = DashboardSpaces(**self.API_ARGS)
-        return self._dashboard_spaces
+    def settings_global(self) -> api.SettingsGlobal:
+        """Work with core system settings."""
+        return self._get_model(model=api.SettingsGlobal)
 
     @property
-    def enforcements(self) -> Enforcements:
-        """Work with Enforcement Center."""
-        self.start()
-        if not hasattr(self, "_enforcements"):
-            self._enforcements = Enforcements(**self.API_ARGS)
-        return self._enforcements
+    def settings_gui(self) -> api.SettingsGui:
+        """Work with gui system settings."""
+        return self._get_model(model=api.SettingsGui)
 
     @property
-    def system_users(self) -> SystemUsers:
-        """Work with system users."""
-        self.start()
-        if not hasattr(self, "_system_users"):
-            self._system_users = SystemUsers(**self.API_ARGS)
-        return self._system_users
+    def settings_ip(self) -> api.SettingsIdentityProviders:
+        """Work with identity providers settings."""
+        return self._get_model(model=api.SettingsIdentityProviders)
 
     @property
-    def system_roles(self) -> SystemRoles:
-        """Work with system roles."""
-        self.start()
-        if not hasattr(self, "_system_roles"):
-            self._system_roles = SystemRoles(**self.API_ARGS)
-        return self._system_roles
+    def settings_lifecycle(self) -> api.SettingsLifecycle:
+        """Work with lifecycle system settings."""
+        return self._get_model(model=api.SettingsLifecycle)
 
     @property
-    def meta(self) -> Meta:
-        """Work with instance metadata."""
-        self.start()
-        if not hasattr(self, "_meta"):
-            self._meta = Meta(**self.API_ARGS)
-        return self._meta
+    def signup(self) -> api.Signup:
+        """Perform initial signup, password reset, and other unauthenticated endpoints."""
+        return self._get_model(model=api.Signup, start=False, auth=self.AUTH_NULL)
 
     @property
-    def settings_ip(self) -> SettingsIdentityProviders:
-        """Work with identity providers settings."""
-        self.start()
-        if not hasattr(self, "_settings_ip"):
-            self._settings_ip = SettingsIdentityProviders(**self.API_ARGS)
-        return self._settings_ip
+    def system_users(self) -> api.SystemUsers:
+        """Work with system users."""
+        return self._get_model(model=api.SystemUsers)
 
     @property
-    def settings_global(self) -> SettingsGlobal:
-        """Work with core system settings."""
-        self.start()
-        if not hasattr(self, "_settings_global"):
-            self._settings_global = SettingsGlobal(**self.API_ARGS)
-        return self._settings_global
+    def system_roles(self) -> api.SystemRoles:
+        """Work with system roles."""
+        return self._get_model(model=api.SystemRoles)
 
     @property
-    def settings_gui(self) -> SettingsGui:
-        """Work with gui system settings."""
-        self.start()
-        if not hasattr(self, "_settings_gui"):
-            self._settings_gui = SettingsGui(**self.API_ARGS)
-        return self._settings_gui
+    def users(self) -> api.Users:
+        """Work with user assets."""
+        return self._get_model(model=api.Users)
 
     @property
-    def settings_lifecycle(self) -> SettingsLifecycle:
-        """Work with lifecycle system settings."""
-        self.start()
-        if not hasattr(self, "_settings_lifecycle"):
-            self._settings_lifecycle = SettingsLifecycle(**self.API_ARGS)
-        return self._settings_lifecycle
+    def vulnerabilities(self) -> api.Vulnerabilities:
+        """Work with vulnerability assets."""
+        return self._get_model(model=api.Vulnerabilities)
 
-    def __str__(self) -> str:
-        """Show object info."""
-        client = getattr(self, "HTTP", "")
-        url = getattr(client, "URL", self.HTTP_ARGS["url"])
-        ax_env = get_env_ax()
-        banner = ax_env.get("AX_BANNER")
-        banner = f"[{banner}]" if banner else ""
-        pkg_ver = f"API Client v{VERSION}"
+    def set_wraperror(self, value: bool = True):
+        """Set whether to wrap errors in a more user-friendly format."""
+        self.WRAPERROR = tools.coerce_bool(value)
 
-        if self.STARTED:
-            about = self.meta.about(error=False)
-            if about:
-                version = about.get("Version", "") or about.get("Installed Version", "") or "DEMO"
-                build_date = about.get("Build Date", "")
-                bits = [f"version: {version}", f"(RELEASE DATE: {build_date})"]
-            else:
-                version = "unknown (no permissions)"
-                bits = [f"version: {version}"]
+    # <-- METHODS
 
-            msg = [f"Connected to {url!r}", *bits]
-        else:
-            msg = [f"Not connected to {url!r}"]
+    @staticmethod
+    def set_log_hide_secrets(value: bool = True):
+        """Set whether to hide secrets in logs."""
+        logs.HideFormatter.HIDE_ENABLED = tools.coerce_bool(value)
+
+    def set_log_level_console(self, value: t.Union[str, int] = LOG_LEVEL_CONSOLE):
+        """Set the log level for this client's console output."""
+        if isinstance(self.ARGS_HANDLER_CON, dict):
+            self.ARGS_HANDLER_CON["level"] = logs.str_level(value)
+        if self.HANDLER_CON:
+            logs.set_log_level(obj=self.HANDLER_CON, level=value)
+
+    def set_log_level_file(self, value: t.Union[str, int] = LOG_LEVEL_FILE):
+        """Set the log level for this client's file output."""
+        if isinstance(self.ARGS_HANDLER_FILE, dict):
+            self.ARGS_HANDLER_FILE["level"] = logs.str_level(value)
+        if self.HANDLER_FILE:
+            logs.set_log_level(obj=self.HANDLER_FILE, level=value)
 
-        bits = [x for x in [*msg, pkg_ver, banner] if x]
-        return " ".join(bits)
+    def set_log_level_api(self, value: t.Union[str, int] = LOG_LEVEL_API):
+        """Set the log level for this client's api objects."""
+        self.API_LOG_LEVEL: str = logs.str_level(value)
+        for obj in self.API_CACHE.values():
+            if isinstance(obj, api.ModelMixins):
+                logs.set_log_level(obj=obj.LOG, level=self.API_LOG_LEVEL)
+
+    def set_log_level_connect(self, value: t.Union[str, int] = "debug"):
+        """Set the log level for this client."""
+        logs.set_log_level(obj=self.LOG, level=value)
+
+    def set_log_level_http(self, value: t.Union[str, int] = Http.LOG_LEVEL):
+        """Set the log level for this client's http object."""
+        if isinstance(self.HTTP, Http):
+            logs.set_log_level(obj=self.HTTP.LOG, level=value)
+
+    def set_log_level_auth(self, value: t.Union[str, int] = LOG_LEVEL_AUTH):
+        """Set the log level for this client's auth objects."""
+        for obj in self.AUTH, self.AUTH_NULL:
+            if isinstance(obj, AuthModel):
+                logs.set_log_level(obj=obj.LOG, level=value)
+
+    def set_log_level_package(self, value: t.Union[str, int] = LOG_LEVEL_PACKAGE):
+        """Set the log level for this client's package."""
+        logs.set_log_level(obj=self.LOG_LOGGER, level=value)
 
-    def __repr__(self) -> str:
-        """Show object info."""
-        return self.__str__()
+    @staticmethod
+    def set_log_level_endpoints(value: t.Union[str, int] = LOG_LEVEL_ENDPOINTS):
+        """Set the log level for this client's endpoints."""
+        from .api.api_endpoint import LOGGER as LOGGER_ENDPOINT
+
+        logs.set_log_level(obj=LOGGER_ENDPOINT, level=value)
+
+    def control_log_console(self, enable: bool = False) -> bool:
+        """Add logging to console for this client."""
+        enable = tools.coerce_bool(enable)
+        if enable and not self.HANDLER_CON:
+            self.HANDLER_CON = logs.add_stderr(**self.ARGS_HANDLER_CON)
+            self.LOG.debug("Logging to console enabled.")
+            return True
+        if not enable and self.HANDLER_CON:
+            self.LOG.debug("Logging to console disabled.")
+            self.HANDLER_CON.close()
+            logs.del_stderr(obj=self.LOG_LOGGER)
+            self.HANDLER_CON = None
+            return True
+        return False
+
+    def control_log_file(self, enable: bool = False, rotate: bool = False) -> bool:
+        """Add logging to file for this client."""
+        enable = tools.coerce_bool(enable)
+        if enable and not self.HANDLER_FILE:
+            self.HANDLER_FILE = logs.add_file(**self.ARGS_HANDLER_FILE)
+            self.LOG.debug("Logging to file enabled.")
+            return True
+        self.rotate_log_files(value=rotate)
+        if not enable and self.HANDLER_FILE:
+            self.LOG.debug("Logging to file disabled.")
+            self.HANDLER_FILE.close()
+            logs.del_file(obj=self.LOG_LOGGER)
+            self.HANDLER_FILE = None
+            return True
+        return False
+
+    def rotate_log_files(self, value: bool = False):
+        """Rollover log file."""
+        value = tools.coerce_bool(value)
+        if value and self.HANDLER_FILE:
+            self.LOG.debug("Forcing file logs to rotate")
+            self.HANDLER_FILE.flush()
+            try:
+                self.HANDLER_FILE.doRollover()
+                self.LOG.debug("Forced file logs to rotate")
+            except Exception as exc:  # pragma: no cover
+                self.LOG.exception("Failed to force file logs to rotate: %s", exc)
 
     @property
-    def build_date(self) -> str:
-        """Pass."""
-        return self.meta.about().get("Build Date", "")
+    def url(self) -> str:
+        """Get the URL of the current instance."""
+        return self.HTTP.url if self.HTTP else self._url
+
+    @property
+    def api_keys(self) -> dict:
+        """Get the API keys for the current user."""
+        return self.AUTH.get_api_keys()
+
+    @property
+    def current_user(self) -> api.json_api.account.CurrentUser:
+        """Get the current user."""
+        return self.AUTH.get_current_user()
+
+    @property
+    def about(self):
+        """Cached data from the /about endpoint."""
+        if self.ABOUT_CACHE:
+            return self.ABOUT_CACHE
+        value = self.meta.about(error=False)
+        if value:
+            self.ABOUT_CACHE = value
+        return value
 
     @property
     def version(self) -> str:
-        """Pass."""
-        about = self.meta.about()
-        version = about.get("Version", "") or about.get("Installed Version", "") or "DEMO"
-        return version.replace("_", ".")
+        """Get the Axonius instance version."""
+        data = "none yet"
+        if self.STARTED:
+            data = self.about.get("Version") or self.about.get("Installed Version") or "DEMO"
+            data = data.replace("_", ".")
+        return data
 
     @property
-    def openapi(self) -> OpenAPISpec:
-        """Work with the OpenAPI specification file."""
-        self.start()
-        if not hasattr(self, "_openapi"):
-            self._openapi = OpenAPISpec(**self.API_ARGS)
-        return self._openapi
+    def build_date(self) -> str:
+        """Get the Axonius instance build date."""
+        data = "none yet"
+        if self.STARTED:
+            data = self.about.get("Build Date", "UNKNOWN")
+        return data
 
     @property
-    def data_scopes(self) -> DataScopes:
-        """Work with data scopes."""
-        self.start()
-        if not hasattr(self, "_data_scopes"):
-            self._data_scopes = DataScopes(**self.API_ARGS)
-        return self._data_scopes
+    def str_ax_version(self) -> str:
+        """Get the Axonius instance version & build date for use in str."""
+        days = f"({tools.dt_days_ago(self.build_date)} days ago)"
+        return f"Axonius Version {self.version!r}, Build Date: {self.build_date!r} {days}"
 
     @property
-    def folders(self) -> Folders:
-        """Work with data scopes."""
-        self.start()
-        if not hasattr(self, "_folders"):
-            self._folders = Folders(**self.API_ARGS)
-        return self._folders
+    def str_ax_user(self) -> str:
+        """Get the Axonius instance user for use in str."""
+        value = "User: ??"
+        if self.STARTED:
+            value = self.current_user.str_connect
+        return value
 
     @property
-    def api_keys(self) -> dict:
-        """Get the API keys for the current user."""
-        return self.AUTH.get_api_keys()
+    def ssl_days_left(self) -> t.Optional[int]:
+        """Get the number of days left until the SSL certificate expires."""
+        value = None
+        if isinstance(self.HTTP, Http):
+            cert: t.Optional[cert_human.Cert] = self.HTTP.get_cert()
+            if isinstance(cert, cert_human.Cert):
+                value = tools.dt_days_ago(cert.not_valid_after, from_now=False)
+        return value
+
+    @property
+    def str_ax_cert(self) -> str:
+        """Get the Axonius instance SSL certificate for use in str."""
+        value = "SSL: ??"
+        if isinstance(self.HTTP, Http):
+            cert: t.Optional[cert_human.Cert] = self.HTTP.get_cert()
+            if isinstance(cert, cert_human.Cert):
+                dt = str(cert.not_valid_after)
+                value = f"SSL Issued To: {cert.subject_short!r}, Expires On: {dt!r}"
+        return value
+
+    @property
+    def str_state(self) -> str:
+        """Get the connection state for use in str."""
+        value = "Not connected"
+        if self.STARTED:
+            value = "Connected"
+        value = f"{value} to {self.url!r}, CLIENT v{self.PKG_VERSION}, PYTHON v{self.PY_VERSION}"
+        banner = get_env_ax().get("AX_BANNER")
+        if banner:
+            value = f"{value} [{banner}]"
+        return value
+
+    @staticmethod
+    def jdump(obj: t.Any, **kwargs) -> None:
+        """Print object as JSON."""
+        tools.jdump(obj=obj, **kwargs)
+
+    def __str__(self) -> str:
+        """Show object info."""
+        items: t.List[str] = [
+            self.str_state,
+            self.str_ax_version,
+            self.str_ax_cert,
+            self.str_ax_user,
+        ]
+        return "\n".join(x for x in items if x)
+
+    def __repr__(self) -> str:
+        """Show object info."""
+        return self.__str__()
 
     @classmethod
     def _get_exc_reason(cls, exc: Exception) -> str:
         """Trim exceptions down to a more user-friendly display.
 
         Uses :attr:`REASON_RES` to do regex substitutions.
         """
         reason = str(exc)
         for reason_re in cls.REASON_RES:
             if reason_re.search(reason):
                 return reason_re.sub(r"\1", reason).rstrip("')")
         return reason
 
-    @staticmethod
-    def jdump(obj, **kwargs):  # pragma: no cover
-        """JSON dump utility."""
-        print(json_reload(obj, **kwargs))
-
-    REASON_RES: List[t.Pattern] = [
-        re.compile(r".*?object at.*?\>\: ([a-zA-Z0-9\]\[: ]+)"),
-        re.compile(r".*?\] (.*) "),
-    ]
-    """patterns to look for in exceptions that we can pretty up for user display."""
+    def _check_binding(self, value: t.Any) -> t.Any:
+        """Check if an object is already bound to a different client."""
+        client = getattr(value, "CLIENT", value)
+        if isinstance(client, self.__class__) and client is not self:
+            raise ConnectError(
+                f"{value} is already set to {client!r} and cannot be set to {self!r}"
+            )
+        setattr(value, "CLIENT", self)
+        return value
+
+    def _init_http(self, http: t.Optional[Http] = None) -> Http:
+        """Initialize the HTTP object."""
+        if not isinstance(http, Http):
+            http: Http = Http(**self.ARGS_HTTP)
+        return self._check_binding(http)
+
+    def _init_auth(
+        self, auth: t.Optional[AuthModel] = None, log_level: t.Union[str, int] = LOG_LEVEL_AUTH
+    ) -> AuthModel:
+        """Initialize the Auth object."""
+        if not isinstance(auth, AuthModel):
+            if self.CREDENTIALS:
+                auth: AuthCredentials = AuthCredentials(
+                    username=self.__key,
+                    password=self.__secret,
+                    http=self.http,
+                    log_level=log_level,
+                )
+            else:
+                auth: AuthApiKey = AuthApiKey(
+                    key=self.__key,
+                    secret=self.__secret,
+                    http=self.http,
+                    log_level=log_level,
+                )
+        return self._check_binding(auth)
+
+    def _init_auth_null(
+        self, auth_null: t.Optional[AuthModel] = None, log_level: t.Union[str, int] = LOG_LEVEL_AUTH
+    ) -> AuthModel:
+        """Initialize the null Auth object."""
+        if not isinstance(auth_null, AuthModel):
+            auth_null: AuthNull = AuthNull(http=self.http, log_level=log_level)
+        return self._check_binding(auth_null)
 
     def _init(self):
-        """Pass."""
-        pass
+        """Custom init for this class."""
+
+    def _get_model(
+        self, model: t.Type[api.ModelMixins], start: bool = True, auth: t.Optional[AuthModel] = None
+    ) -> t.Any:
+        """Create or get an API model.
+
+        Args:
+            model: model to create or get
+            start: start :attr:`AUTH` if not already started
+            auth: auth to use for this model, if not supplied default to :attr:`AUTH`
+
+        Returns:
+            model instance
+        """
+        if start:
+            self.start()
+
+        if model in self.API_CACHE:
+            return self.API_CACHE[model]
+
+        if not isinstance(auth, AuthModel):
+            auth = self.AUTH
+
+        self.API_CACHE[model] = model(auth=auth, log_level=self.API_LOG_LEVEL)
+        return self.API_CACHE[model]
```

## axonius_api_client/exceptions.py

```diff
@@ -137,18 +137,18 @@
         self.sqs = sqs
         self.details = details
         self.msg = f"Saved Query not found with {details}"
         try:
             barrier = "#" * 80
             barrier = f"\n{barrier}\n"
             details = barrier + barrier.join([x.str_details for x in sqs])
-            self.tablemsg = [self.msg, "", details, "", self.msg]
+            self.msg_table = [self.msg, "", details, "", self.msg]
         except Exception:
-            self.tablemsg = tablize_sqs(data=sqs, err=self.msg)
-        super().__init__(self.tablemsg)
+            self.msg_table = tablize_sqs(data=sqs, err=self.msg)
+        super().__init__(self.msg_table)
 
 
 class SavedQueryTagsNotFoundError(SavedQueryNotFoundError):
     """Error when something is not found."""
 
     def __init__(self, value: t.List[str], valid: t.List[str]):
         """Pass."""
@@ -216,14 +216,16 @@
 class CnxTestError(CnxError):
     """Errors when testing a connections configuration."""
 
 
 class CnxAddError(CnxError):
     """Errors when adding a new connection."""
 
+    cnx_new: t.ClassVar[t.Optional[dict]] = None
+
 
 class ResponseError(ApiError):
     """Errors when checking responses."""
 
     def __init__(
         self, msg: t.Optional[str] = None, response=None, exc: t.Optional[Exception] = None
     ):
@@ -498,15 +500,15 @@
             value (t.Any):
                 The input value.
             encoding_format (t.Optional[str], optional):
                 The encoding format used for decoding.
             encoding_errors (t.Optional[str], optional):
                 The error handling method used for decoding.
         """
-        from .utils import trim_value_repr
+        from .tools import trim_value_repr
 
         self.value: bytes = value
         self.encoding_format: t.Optional[str] = encoding_format
         self.encoding_errors: t.Optional[str] = encoding_errors
         super().__init__(
             f"The input byte string {trim_value_repr(self.value)} cannot be decoded to a valid "
             f"ObjectId string using encoding_format {self.encoding_format!r} "
@@ -523,15 +525,15 @@
 
     def __init__(self, value: str):
         """Initialize a new instance of the InvalidObjectIdError exception.
 
         Args:
             value (str): The input string.
         """
-        from .utils import trim_value_repr
+        from .tools import trim_value_repr
 
         self.value: t.Any = value
         super().__init__(f"The input string {trim_value_repr(self.value)} is not a valid ObjectId.")
 
 
 class InvalidTypeError(TypeError, AxonError):
     """Raised when an input value is not one of the allowed types.
@@ -546,15 +548,15 @@
 
         Args:
             value (t.Any): The input value.
             allowed_types (t.Optional[t.Tuple[type]], optional): The tuple of allowed types.
         """
         self.value: t.Any = value
         self.allowed_types: t.Optional[t.Tuple[type]] = allowed_types
-        from .utils import get_type_str
+        from .tools import get_type_str
 
         super().__init__(
             f"The input value {self.value!r} type {get_type_str(self.value)} is not "
             f"one of the allowed types: {get_type_str(allowed_types)}."
         )
```

## axonius_api_client/http.py

```diff
@@ -1,244 +1,493 @@
 # -*- coding: utf-8 -*-
 """HTTP client."""
-import typing as t
 import logging
 import pathlib
+import typing as t
 import warnings
-from typing import Any, List, Optional, Pattern, Union
 
+import OpenSSL
 import requests
 import requests.cookies
 import requests.structures
 import urllib3
 import urllib3.exceptions
 
-from . import cert_human
+from . import version
+from .projects import cert_human
+from .projects.cf_token import constants as cf_constants
+from .projects.cf_token.flows import flow_get_token
+from .projects.cf_token.tools import is_url, get_env_url
+from .projects.url_parser import UrlParser
 from .constants.api import TIMEOUT_CONNECT, TIMEOUT_RESPONSE
-from .constants.ctypes import PatternLikeListy
+from .constants.ctypes import PathLike, PatternLikeListy
 from .constants.logs import LOG_LEVEL_HTTP, MAX_BODY_LEN, REQUEST_ATTR_MAP, RESPONSE_ATTR_MAP
 from .exceptions import HttpError
 from .logs import get_obj_log, set_log_level
-from .parsers.url_parser import UrlParser
+
 from .setup_env import get_env_user_agent
-from .tools import coerce_str, join_url, json_log, listify, path_read, tilde_re
-from .version import __version__
+from .tools import (
+    coerce_bool,
+    coerce_int_float,
+    coerce_str,
+    join_url,
+    json_log,
+    listify,
+    path_read,
+    tilde_re,
+)
 
 INJECT_RESULTS: t.Tuple[bool, t.List[str]] = cert_human.ssl_capture.inject_into_urllib3()
 T_Cookies: t.Type = t.Union[dict, requests.cookies.RequestsCookieJar]
 T_Headers: t.Type = t.Union[dict, requests.structures.CaseInsensitiveDict]
 
-HIDE_HEADERS: t.List[str] = [
+HIDE_HEADERS: t.Tuple[str, ...] = (
     "~cookie",
     "~auth",
     "~token",
     "~^cf_",
     "~secret",
     "~key",
     "~username",
     "~password",
-]
+)
+
+
+def is_headers(value: t.Any) -> bool:
+    """Check if token is a valid headers object."""
+    return isinstance(value, (dict, requests.structures.CaseInsensitiveDict))
+
+
+def is_cookies(value: t.Any) -> bool:
+    """Check if token is a valid cookies object."""
+    return isinstance(value, (dict, requests.cookies.RequestsCookieJar))
 
 
 class Http:
     """HTTP client that wraps around :obj:`requests.Session`."""
 
-    HIDE_STR = "*********"
+    session: requests.Session = None
+    """Requests session object."""
+
+    LOG: logging.Logger = None
+    """Logger for this object."""
+
+    HISTORY: t.List[requests.Response] = None
+    """History of all requests made."""
+
+    LAST_REQUEST: t.Optional[requests.PreparedRequest] = None
+    """Last request made."""
+
+    LAST_RESPONSE: t.Optional[requests.Response] = None
+    """Last response received."""
+
+    SAVE_HISTORY: bool = False
+    """Save history of requests."""
+
+    SAVE_LAST: bool = True
+    """Save last request and response."""
+
+    URLPARSED: UrlParser = None
+    """Parsed URL object."""
+
+    url: str = None
+    """URL to use for requests."""
+
+    CERT_PATH: t.Optional[PathLike] = None
+    """Path to CA cert file."""
+
+    CERT_WARN: bool = True
+    """Warn if CA cert is not found."""
+
+    CERT_VERIFY: bool = False
+    """Verify server cert."""
+
+    HTTP_HEADERS: t.Optional[T_Headers] = None
+    """Headers to use for all requests."""
+
+    HTTP_COOKIES: t.Optional[T_Cookies] = None
+    """Cookies to use for all requests."""
+
+    CERT_CLIENT_BOTH: t.Optional[PathLike] = None
+    """Path to client cert and key file."""
+
+    CERT_CLIENT_CERT: t.Optional[PathLike] = None
+    """Path to client cert file."""
+
+    CERT_CLIENT_KEY: t.Optional[PathLike] = None
+    """Path to client key file."""
+
+    CONNECT_TIMEOUT: t.Optional[t.Union[int, float]] = TIMEOUT_CONNECT
+    """Timeout for connecting to server."""
+
+    RESPONSE_TIMEOUT: t.Optional[t.Union[int, float]] = TIMEOUT_RESPONSE
+    """Timeout for receiving response from server."""
+
+    HTTP_PROXY: t.Optional[str] = None
+    """Proxy to use for HTTP requests."""
+
+    HTTPS_PROXY: t.Optional[str] = None
+    """Proxy to use for HTTPS requests."""
+
+    LOG_BODY_LINES: int = MAX_BODY_LEN
+    """Max length of request and response bodies to log."""
+
+    LOG_HIDE_HEADERS: t.Optional[PatternLikeListy] = HIDE_HEADERS
+    """Headers to hide when logging."""
+
+    LOG_HIDE_STR: str = "*********"
+    """String to use to hide sensitive data in logs."""
+
+    LOG_LEVEL: t.Union[int, str] = LOG_LEVEL_HTTP
+    """Log level to use for this object."""
+
+    LOG_LEVEL_URLLIB: t.Union[int, str] = "warning"
+    """Log level to use for urllib3."""
+
+    LOG_REQUEST_ATTRS: t.Optional[t.List[str]] = None
+    """Attributes of request to log."""
+
+    LOG_REQUEST_BODY: bool = False
+    """Log request body."""
+
+    LOG_RESPONSE_ATTRS: t.Optional[t.List[str]] = None
+    """Attributes of response to log."""
+
+    LOG_RESPONSE_BODY: bool = False
+    """Log response body."""
+
+    URL_CERT: t.Optional[cert_human.Cert] = None
+    """Cert object for URL."""
+
+    URL_CERT_CHAIN: t.Optional[t.List[cert_human.Cert]] = None
+    """Cert chain for URL."""
+
+    CLIENT: t.Optional[object] = None
+    """Client object that created this object."""
+    # TBD: Connect needs an interface for proper type hinting without circular reference
 
     def __init__(
         self,
-        url: Union[UrlParser, str],
-        certpath: Optional[Union[str, pathlib.Path]] = None,
-        certwarn: bool = True,
-        certverify: bool = False,
-        headers: Optional[T_Headers] = None,
-        cookies: Optional[T_Cookies] = None,
+        url: t.Union[UrlParser, str],
+        certpath: t.Optional[PathLike] = None,
+        certwarn: bool = CERT_WARN,
+        certverify: bool = CERT_VERIFY,
+        headers: t.Optional[T_Headers] = None,
+        cookies: t.Optional[T_Cookies] = None,
+        cert_client_both: t.Optional[PathLike] = None,
+        cert_client_cert: t.Optional[PathLike] = None,
+        cert_client_key: t.Optional[PathLike] = None,
+        connect_timeout: t.Optional[t.Union[int, float]] = CONNECT_TIMEOUT,
+        response_timeout: t.Optional[t.Union[int, float]] = RESPONSE_TIMEOUT,
+        http_proxy: t.Optional[str] = None,
+        https_proxy: t.Optional[str] = None,
+        log_body_lines: int = LOG_BODY_LINES,
+        log_hide_headers: t.Optional[PatternLikeListy] = HIDE_HEADERS,
+        log_hide_str: t.Optional[str] = LOG_HIDE_STR,
+        log_level: t.Union[int, str] = LOG_LEVEL,
+        log_level_urllib: t.Union[int, str] = LOG_LEVEL_URLLIB,
+        log_request_attrs: t.Optional[t.Union[str, t.Iterable[str]]] = None,
+        log_request_body: bool = LOG_REQUEST_BODY,
+        log_response_attrs: t.Optional[t.Union[str, t.Iterable[str]]] = None,
+        log_response_body: bool = LOG_RESPONSE_BODY,
+        save_history: bool = SAVE_HISTORY,
+        save_last: bool = SAVE_LAST,
+        cf_token: t.Optional[str] = None,
+        cf_url: t.Optional[str] = None,
+        cf_path: t.Optional[PathLike] = cf_constants.CF_PATH,
+        cf_run: bool = cf_constants.CLIENT_RUN,
+        cf_run_login: bool = cf_constants.FLOW_RUN_LOGIN,
+        cf_run_access: bool = cf_constants.FLOW_RUN_ACCESS,
+        cf_env: bool = cf_constants.FLOW_ENV,
+        cf_echo: bool = cf_constants.FLOW_ECHO,
+        cf_echo_verbose: bool = cf_constants.FLOW_ECHO_VERBOSE,
+        cf_error: bool = cf_constants.CLIENT_ERROR,
+        cf_error_login: bool = cf_constants.FLOW_ERROR,
+        cf_error_access: bool = cf_constants.FLOW_ERROR,
+        cf_timeout_access: t.Optional[int] = cf_constants.TIMEOUT_ACCESS,
+        cf_timeout_login: t.Optional[int] = cf_constants.TIMEOUT_LOGIN,
         **kwargs,
     ):
         """HTTP client that wraps around :obj:`requests.Session`.
 
         Notes:
             * If certpath is supplied, certverify is ignored
             * private key supplied to cert_client_key or cert_client_both
               can **NOT** be password encrypted
 
         Args:
             url: URL, hostname, or IP address of Axonius instance
-            certpath: path to CA bundle file to use when verifying certs offered by :attr:`url`
-            certverify: raise exception if cert is self-signed or only if cert is invalid
+            certpath: token to CA bundle file to use when verifying certs offered by :attr:`url`
             certwarn: show insecure warning once or never show insecure warning
-            proxy: proxy to use when making https requests to :attr:`url`
+            certverify: raise exception if cert is self-signed or only if cert is invalid
+            headers: headers to send with every request
+            cookies: cookies to send with every request
+            log_level: log level to use for this object
+            log_body_lines: max length of request and response bodies to log
+            log_hide_headers: headers to hide when logging
+            save_last: save last request and response to :attr:`last_request` and
+                :attr:`last_response`
+            save_history: save all requests and responses to :attr:`history`
+            connect_timeout: seconds to wait for connections to open to :attr:`url`
+            response_timeout: seconds to wait for responses from :attr:`url`
+            log_request_body: log the request body
+            log_response_body: log the response body
+            http_proxy: proxy to use when making http requests to :attr:`url`
+            https_proxy: proxy to use when making https requests to :attr:`url`
+            log_level_urllib: log level to use for urllib3
+            cert_client_key: path to client private key file
+            cert_client_both: path to client private key and cert file
+            cert_client_cert: path to client cert file
+            cf_url: URL to use in `access token` and `access login` commands,
+                will fallback to url if not supplied
+            cf_token: access token supplied by user, will be checked for validity if not empty
+            cf_env: if no token supplied, try to get token from OS env var CF_TOKEN
+            cf_run: if no token supplied or in OS env vars, try to get token from `access token` and
+                `access login` commands
+            cf_run_access: if run is True, try to get token from `access token`,
+            cf_run_login: if run is True and no token returned from `access token` command,
+                try to get token from `access login` command
+            cf_path: path to cloudflared binary to run, can be full path or path in OS env var $PATH
+            cf_timeout_access: timeout for `access token` command in seconds
+            cf_timeout_login: timeout for `access login` command in seconds
+            cf_error: raise error if an invalid token is found or no token can be found
+            cf_error_access: raise exc if `access token` command fails and login is False
+            cf_error_login: raise exc if `access login` command fails
+            cf_echo: echo commands and results to stdout
+            cf_echo_verbose: echo checks to stdout
+            **kwargs: no longer used, will throw a deprecation warning
 
         Raises:
             :exc:`HttpError`:
-
                 - if either cert_client_cert or cert_client_key are supplied, and the other is
                   not supplied
                 - if any of cert_path, cert_client_cert, cert_client_key, or cert_client_both
                   are supplied and the file does not exist
         """
-        self.LOG_LEVEL: Union[str, int] = kwargs.get("log_level", LOG_LEVEL_HTTP)
-        """log level for this class ``kwargs=log_level``"""
-
+        self.KWARGS: dict = kwargs
+        self.session = requests.Session()
+        self.LOG_LEVEL: t.Union[int, str] = log_level
         self.LOG: logging.Logger = get_obj_log(obj=self, level=self.LOG_LEVEL)
-        """Logger for this object."""
 
-        self.LOG_BODY_MAX_LEN = MAX_BODY_LEN
-        self.URLPARSED: UrlParser = self.parse_url(url=url)
+        self.HISTORY: t.List[requests.Response] = []
+        self.LAST_REQUEST: t.Optional[requests.PreparedRequest] = None
+        self.LAST_RESPONSE: t.Optional[requests.Response] = None
+
+        self.LOG_BODY_LINES: t.Optional[int] = coerce_int_float(log_body_lines, error=False)
+        self.LOG_HIDE_HEADERS: PatternLikeListy = tilde_re(listify(log_hide_headers))
+        self.LOG_HIDE_STR: t.Optional[str] = log_hide_str
+        self.LOG_LEVEL_URLLIB: str = log_level_urllib
+        self.LOG_REQUEST_BODY: bool = coerce_bool(log_request_body)
+        self.LOG_RESPONSE_BODY: bool = coerce_bool(log_response_body)
 
-        self.url: str = self.URLPARSED.url
-        """URL to connect to"""
+        self.log_request_attrs = log_request_attrs
+        self.log_response_attrs = log_response_attrs
 
-        self.LOG_HIDE_HEADERS: PatternLikeListy = tilde_re(
-            listify(kwargs.get("log_hide_headers", HIDE_HEADERS))
-        )
-        """Headers to hide when logging."""
-
-        self.SAVE_LAST: bool = kwargs.get("save_last", True)
-        """save requests to :attr:`LAST_REQUEST` and responses to :attr:`LAST_RESPONSE`
-        ``kwargs=save_last``"""
-
-        self.SAVE_HISTORY: bool = kwargs.get("save_history", False)
-        """Append all responses to :attr:`HISTORY` ``kwargs=save_history``"""
-
-        self.CONNECT_TIMEOUT: int = kwargs.get("connect_timeout", TIMEOUT_CONNECT)
-        """seconds to wait for connections to open to :attr:`url` ``kwargs=connect_timeout``"""
-
-        self.RESPONSE_TIMEOUT: int = kwargs.get("response_timeout", TIMEOUT_RESPONSE)
-        """seconds to wait for responses from :attr:`url` ``kwargs=response_timeout``"""
-
-        self.LOG_REQUEST_BODY: bool = kwargs.get("log_request_body", False)
-        """Log the full request body ``kwargs=log_request_body``"""
-
-        self.LOG_RESPONSE_BODY: bool = kwargs.get("log_response_body", False)
-        """Log the full response body. ``kwargs=log_response_body``"""
-
-        self.HTTP_PROXY: Optional[str] = kwargs.get("http_proxy", None)
-        """HTTP proxy to use. ``kwargs=http_proxy``"""
+        self.URLPARSED: UrlParser = self.parse_url(url=url)
+        self.url: str = self.URLPARSED.url
 
-        self.HTTPS_PROXY: Optional[str] = kwargs.get("https_proxy", None)
-        """HTTPS proxy to use. ``kwargs=https_proxy``"""
+        self.HTTP_HEADERS: T_Headers = headers if is_headers(headers) else {}
+        self.HTTP_COOKIES: T_Cookies = cookies if is_cookies(cookies) else {}
 
-        self.LOG_REQUEST_ATTRS: Optional[List[str]] = kwargs.get("log_request_attrs", None)
-        """request attrs to log :attr:`axonius_api_client.constants.logs.REQUEST_ATTR_MAP`
-        ``kwargs=log_request_attrs``"""
+        self.set_cf_token(
+            url=cf_url,
+            token=cf_token,
+            run=cf_run,
+            path=cf_path,
+            run_login=cf_run_login,
+            run_access=cf_run_access,
+            env=cf_env,
+            echo=cf_echo,
+            echo_verbose=cf_echo_verbose,
+            error=cf_error,
+            error_login=cf_error_login,
+            error_access=cf_error_access,
+            timeout_access=cf_timeout_access,
+            timeout_login=cf_timeout_login,
+        )
 
-        self.LOG_RESPONSE_ATTRS: Optional[List[str]] = kwargs.get("log_response_attrs", None)
-        """response attrs to log :attr:`axonius_api_client.constants.logs.RESPONSE_ATTR_MAP`
-        ``kwargs=log_response_attrs``"""
+        self.CERT_PATH: t.Optional[PathLike] = certpath
+        self.CERT_WARN: bool = coerce_bool(certwarn)
+        self.CERT_VERIFY: bool = certverify
 
-        self.LOG_LEVEL_URLLIB: str = kwargs.get("log_level_urllib", "warning")
-        """logging level for low-level urllib library. ``kwargs=log_level_urllib``"""
+        self.CERT_CLIENT_BOTH: t.Optional[PathLike] = cert_client_both
+        self.CERT_CLIENT_CERT: t.Optional[PathLike] = cert_client_cert
+        self.CERT_CLIENT_KEY: t.Optional[PathLike] = cert_client_key
 
-        self.CERT_CLIENT_KEY: Optional[Union[str, pathlib.Path]] = kwargs.get(
-            "cert_client_key", None
+        self.CONNECT_TIMEOUT: t.Optional[t.Union[int, float]] = coerce_int_float(
+            connect_timeout, error=False
         )
-        """Private key file for cert_client_cert ``kwargs=cert_client_key``"""
-
-        self.CERT_CLIENT_CERT: Optional[Union[str, pathlib.Path]] = kwargs.get(
-            "cert_client_cert", None
+        self.RESPONSE_TIMEOUT: t.Optional[t.Union[int, float]] = coerce_int_float(
+            response_timeout, error=False
         )
-        """cert file to offer to :attr:`url` ``kwargs=cert_client_cert``"""
 
-        self.CERT_CLIENT_BOTH: Optional[Union[str, pathlib.Path]] = kwargs.get(
-            "cert_client_both", None
-        )
-        """cert file with both private key and cert to offer to :attr:`url`
-        ``kwargs=cert_client_both``"""
-
-        self.LAST_REQUEST = None
-        """:obj:`requests.PreparedRequest`: last request sent"""
-
-        self.LAST_RESPONSE = None
-        """:obj:`requests.Response`: last response received"""
+        self.HTTP_PROXY: t.Optional[str] = http_proxy
+        self.HTTPS_PROXY: t.Optional[str] = https_proxy
 
-        self.HISTORY = []
-        """:obj:`list` of :obj:`requests.Response`: all responses received."""
+        self.SAVE_HISTORY: bool = coerce_bool(save_history)
+        self.SAVE_LAST: bool = coerce_bool(save_last)
 
-        self.CERT_PATH: Optional[Union[str, pathlib.Path]] = certpath
-        self.CERT_VERIFY: bool = certverify
-        self.CERT_WARN: bool = certwarn
-        self.HTTP_HEADERS: T_Headers = (
-            headers if isinstance(headers, (dict, requests.structures.CaseInsensitiveDict)) else {}
-        )
-        self.HTTP_COOKIES: T_Cookies = (
-            cookies if isinstance(cookies, (dict, requests.cookies.RequestsCookieJar)) else {}
-        )
-        self.log_request_attrs: Optional[List[str]] = self.LOG_REQUEST_ATTRS
-        self.log_response_attrs: Optional[List[str]] = self.LOG_RESPONSE_ATTRS
-        self.session: requests.Session = requests.Session()
         self.set_urllib_warnings()
         self.set_urllib_log()
         self.new_session()
         self._init()
 
-    def get_cert(self) -> cert_human.Cert:
-        """Pass."""
-        response = self(verify=False)
-        cert = response.raw.captured_cert
-        source = {
-            "url": self.url,
-            "method": f"{self.__class__.__module__}.{self.__class__.__name__}.get_cert",
-        }
-        return cert_human.Cert(cert=cert, source=source)
+    def set_cf_token(
+        self,
+        url: t.Optional[str] = None,
+        token: t.Optional[str] = None,
+        env: bool = cf_constants.FLOW_ENV,
+        run: bool = cf_constants.FLOW_RUN,
+        run_access: bool = cf_constants.FLOW_RUN_ACCESS,
+        run_login: bool = cf_constants.FLOW_RUN_LOGIN,
+        path: t.Union[str, bytes, pathlib.Path] = cf_constants.CF_PATH,
+        timeout_access: t.Optional[int] = cf_constants.TIMEOUT_ACCESS,
+        timeout_login: t.Optional[int] = cf_constants.TIMEOUT_LOGIN,
+        error: bool = cf_constants.FLOW_ERROR,
+        error_access: bool = cf_constants.FLOW_ERROR,
+        error_login: bool = cf_constants.FLOW_ERROR,
+        echo: bool = cf_constants.FLOW_ECHO,
+        echo_verbose: bool = cf_constants.FLOW_ECHO_VERBOSE,
+    ) -> t.Optional[str]:
+        """Set the Cloudflare access token to use for requests.
 
-    def get_cert_chain(self) -> List[cert_human.Cert]:
-        """Pass."""
-        response = self(verify=False)
-        chain = response.raw.captured_chain or [response.raw.captured_cert]
-        source = {
-            "url": self.url,
-            "method": f"{self.__class__.__module__}.{self.__class__.__name__}.get_cert_chain",
-        }
-        return [cert_human.Cert(cert=x, source=source) for x in chain]
+        Notes:
+            - If `token` is supplied, it will be checked for validity
+            - If `token` is not supplied, and `env` is True, try to get a token
+              from the OS environment variables CF_TOKEN or AX_TOKEN
+            - If `token` is not supplied or defined in OS environment variables
+              and `run` is True, try to get a token from the command `$path access token`
+            - If `token` is not supplied or defined in OS environment variables
+              or returned from the command `$path access token` and `login` is True,
+              try to get a token from the command `$path access login`
+
+        Args:
+            url: URL to use in `access token` and `access login` commands,
+                will default to self.url if not supplied
+            token: access token supplied by user, will be checked for validity if not empty
+            env: if no token supplied, try to get token from OS env var CF_TOKEN
+            run: if no token supplied or in OS env vars, try to get token from `access token` and
+                `access login` commands
+            run_access: if run is True, try to get token from `access token`,
+            run_login: if run is True and no token returned from `access token` command, try to get
+                token from `access login` command
+            path: path to cloudflared binary to run, can be full path or path in OS env var $PATH
+            timeout_access: timeout for `access token` command in seconds
+            timeout_login: timeout for `access login` command in seconds
+            error: raise error if an invalid token is found or no token can be found
+            error_access: raise exc if `access token` command fails and login is False
+            error_login: raise exc if `access login` command fails
+            echo: echo commands and results to stdout
+            echo_verbose: echo checks to stdout
+
+        Returns:
+            None or token, depending on `error` and `error_access` and `error_login`
+        """
+        url = url if is_url(url) else get_env_url(error=True, error_empty=False) or self.url
+        token = flow_get_token(
+            url=url,
+            path=path,
+            timeout_access=timeout_access,
+            timeout_login=timeout_login,
+            error=error,
+            error_access=error_access,
+            error_login=error_login,
+            token=token,
+            env=env,
+            run=run,
+            run_login=run_login,
+            run_access=run_access,
+            echo=echo,
+            echo_verbose=echo_verbose,
+        )
+        self.HTTP_HEADERS["cf-access-token"] = token
+        self.session.headers["cf-access-token"] = token
+        return token
+
+    def safe_request(self, error: bool = False, **kwargs) -> t.Optional[requests.Response]:
+        """Make a request, but catch all exceptions and return None."""
+        kwargs.setdefault("verify", False)
+        kwargs.setdefault("connect_timeout", 10)
+        kwargs.setdefault("response_timeout", 10)
+        # noinspection PyBroadException
+        try:
+            return self(**kwargs)
+        except Exception:  # pragma: no cover
+            if error:
+                raise
+        return None  # pragma: no cover
+
+    def get_cert(self, error: bool = False) -> t.Optional[cert_human.Cert]:
+        """Get the SSL certificate from url."""
+        if not isinstance(self.URL_CERT, cert_human.Cert):
+            response: t.Optional[requests.Response] = self.safe_request(error=error)
+            value = None
+            if response:
+                cert: OpenSSL.crypto.X509 = response.raw.captured_cert
+                source: dict = {"url": self.url, "method": f"{self.get_cert.__name__}"}
+                value = cert_human.Cert(cert=cert, source=source)
+            self.URL_CERT = value
+        return self.URL_CERT
+
+    def get_cert_chain(self, error: bool = False) -> t.List[cert_human.Cert]:
+        """Get the SSL certificate chain from url."""
+        if not (isinstance(self.URL_CERT_CHAIN, list) and self.URL_CERT_CHAIN):
+            response: t.Optional[requests.Response] = self.safe_request(error=error)
+            value = []
+            if response:
+                chain: t.List[OpenSSL.crypto.X509] = listify(response.raw.captured_chain)
+                source: dict = {"url": self.url, "method": f"{self.get_cert_chain.__name__}"}
+                value = [cert_human.Cert(cert=x, source=source) for x in chain]
+            self.URL_CERT_CHAIN = value
+        return self.URL_CERT_CHAIN
 
-    def parse_url(self, url: Union[str, UrlParser]) -> UrlParser:
+    def parse_url(self, url: t.Union[str, UrlParser]) -> UrlParser:
         """Pass."""
         if isinstance(url, UrlParser):
             ret = url
             self.LOG.debug(f"Using supplied {ret}")
         else:
             ret = UrlParser(url=url, default_scheme="https")
             self.LOG.debug(f"Parsed {url} into {ret}")
         return ret
 
     def new_session(self):
-        """Pass."""
+        """Create a new session object."""
         self.session: requests.Session = requests.Session()
         self.set_session_headers()
         self.set_session_cookies()
         self.set_session_proxies()
         self.set_session_verify()
         self.set_session_cert()
 
     def set_session_headers(self):
-        """Pass."""
+        """Configure :attr:`session` headers with :attr:`HTTP_HEADERS`."""
         self.session.headers.update(self.HTTP_HEADERS)
 
     def set_session_cookies(self):
-        """Pass."""
+        """Configure :attr:`session` cookies with :attr:`HTTP_COOKIES`."""
         self.session.cookies.update(self.HTTP_COOKIES)
 
     def set_session_proxies(self):
-        """Pass."""
+        """Configure :attr:`session` proxies."""
         self.session.proxies = {"https": self.HTTPS_PROXY, "http": self.HTTP_PROXY}
 
     def set_session_verify(self):
-        """Pass."""
-        if self.CERT_PATH:
+        """Configure :attr:`session` verify with a cert bundle or a bool."""
+        if self.CERT_PATH:  # pragma: no cover
             # TBD: verify cert bundle
             self.CERT_PATH, _ = path_read(obj=self.CERT_PATH, binary=True)
             self.LOG.debug(f"Resolved cert verify to {self.CERT_PATH}")
             self.session.verify = str(self.CERT_PATH)
         else:
             self.session.verify = self.CERT_VERIFY
             self.LOG.debug(f"Resolved cert verify to {self.CERT_VERIFY}")
 
     def set_session_cert(self):
-        """Pass."""
+        """Configure :attr:`session` with the client cert."""
         if self.CERT_CLIENT_BOTH:
             # TBD: verify cert and key
             self.CERT_CLIENT_BOTH, _ = path_read(obj=self.CERT_CLIENT_BOTH, binary=True)
             self.LOG.debug(
                 f"Resolved client cert with both cert and key to {self.CERT_CLIENT_BOTH}"
             )
             self.session.cert = str(self.CERT_CLIENT_BOTH)
@@ -256,35 +505,39 @@
             self.LOG.debug(f"Resolved client cert with cert only to {self.CERT_CLIENT_CERT}")
 
             self.CERT_CLIENT_KEY, _ = path_read(obj=self.CERT_CLIENT_KEY, binary=True)
             self.LOG.debug(f"Resolved client cert with key only to {self.CERT_CLIENT_KEY}")
             self.session.cert = (str(self.CERT_CLIENT_CERT), str(self.CERT_CLIENT_KEY))
 
     def set_urllib_warnings(self):
-        """Pass."""
+        """Filter urllib warnings to show once or ignore.
+
+        Notes:
+            if self.CERT_WARN is True, show warning once
+            if self.CERT_WARN is False, ignore warning
+        """
         if self.CERT_WARN is True:
             warnings.simplefilter("once", urllib3.exceptions.InsecureRequestWarning)
         elif self.CERT_WARN is False:
             warnings.simplefilter("ignore", urllib3.exceptions.InsecureRequestWarning)
 
     def set_urllib_log(self):
-        """Pass."""
-        urllog = logging.getLogger("urllib3.connectionpool")
-        set_log_level(obj=urllog, level=self.LOG_LEVEL_URLLIB)
+        """Set the urllib3 logging level to :attr:`LOG_LEVEL_URLLIB`."""
+        set_log_level(obj=logging.getLogger("urllib3.connectionpool"), level=self.LOG_LEVEL_URLLIB)
 
     def __call__(
         self,
-        path: Optional[str] = None,
-        route: Optional[str] = None,
+        path: t.Optional[str] = None,
+        route: t.Optional[str] = None,
         method: str = "get",
-        data: Optional[str] = None,
-        params: Optional[dict] = None,
-        headers: Optional[dict] = None,
-        cookies: Optional[dict] = None,
-        json: Optional[dict] = None,
+        data: t.Optional[str] = None,
+        params: t.Optional[dict] = None,
+        headers: t.Optional[dict] = None,
+        cookies: t.Optional[dict] = None,
+        json: t.Optional[dict] = None,
         files: tuple = None,
         **kwargs,
     ):
         """Create, prepare, and then send a request using :attr:`session`.
 
         Args:
             path: path to append to :attr:`url`
@@ -303,20 +556,21 @@
                 * verify: verification of cert for this request
                 * cert: client cert to offer for this request
 
         Returns:
             :obj:`requests.Response`
         """
 
-        def log_if_headers(msg: str):
+        def log_if_headers(msg: str):  # pragma: no cover
             """Pass."""
             if "headers" in self.log_request_attrs:
                 self.LOG.debug(msg)
 
-        if not hasattr(self, "session") or kwargs.get("session_reset", False) is True:
+        session_reset = kwargs.get("session_reset", False)
+        if not hasattr(self, "session") or session_reset is True:  # pragma: no cover
             self.new_session()
 
         url = join_url(self.url, path, route)
 
         this_headers = {}
         this_headers.update(headers or {})
         this_headers.setdefault("User-Agent", self.user_agent)
@@ -380,15 +634,16 @@
     def __repr__(self) -> str:
         """Show object info."""
         return self.__str__()
 
     @property
     def user_agent(self) -> str:
         """Value to use in User-Agent header."""
-        return get_env_user_agent() or f"{__name__}.{self.__class__.__name__}/{__version__}"
+        ver = version.__version__
+        return get_env_user_agent() or f"{__name__}.{self.__class__.__name__}/{ver}"
 
     def _do_log_request(self, request):
         """Log attributes and/or body of a request.
 
         Args:
             request (:obj:`requests.PreparedRequest`): prepared request to log attrs/body of
         """
@@ -412,26 +667,27 @@
 
         Args:
             headers: headers to clean values of
         """
 
         def getval(key, value):
             """Pass."""
-            skey = str(key).lower()
-            for check in self.LOG_HIDE_HEADERS:
-                if (isinstance(check, str) and check.lower() == skey) or (
-                    isinstance(check, Pattern) and check.search(key)
-                ):
-                    return self.HIDE_STR
+            if isinstance(self.LOG_HIDE_STR, str) and self.LOG_HIDE_STR:
+                skey = str(key).lower()
+                for check in self.LOG_HIDE_HEADERS:
+                    if (isinstance(check, str) and check.lower() == skey) or (
+                        isinstance(check, t.Pattern) and check.search(key)
+                    ):
+                        return self.LOG_HIDE_STR
             return value
 
         # noinspection PyBroadException
         try:
             return {k: getval(k, v) for k, v in headers.items()}
-        except Exception:
+        except Exception:  # pragma: no cover
             return headers
 
     def _do_log_response(self, response):
         """Log attributes and/or body of a response.
 
         Args:
             response (:obj:`requests.Response`): response to log attrs/body of
@@ -449,57 +705,57 @@
             )
             self.LOG.debug(f"RESPONSE ATTRS: {lattrs}")
 
         if self.LOG_RESPONSE_BODY:
             self.LOG.debug(self.log_body(body=response.text, body_type="RESPONSE", src=response))
 
     @property
-    def log_request_attrs(self) -> List[str]:
+    def log_request_attrs(self) -> t.List[str]:
         """Get the request attributes that should be logged."""
         return self._get_log_attrs("request")
 
     @log_request_attrs.setter
-    def log_request_attrs(self, value: List[str]):
+    def log_request_attrs(self, value: t.List[str]):
         """Set the request attributes that should be logged."""
         attr_map = REQUEST_ATTR_MAP
         attr_type = "request"
         self._set_log_attrs(attr_map=attr_map, attr_type=attr_type, value=value)
 
     @property
-    def log_response_attrs(self) -> List[str]:
+    def log_response_attrs(self) -> t.List[str]:
         """Get the response attributes that should be logged."""
         return self._get_log_attrs("response")
 
     @log_response_attrs.setter
-    def log_response_attrs(self, value: List[str]):
+    def log_response_attrs(self, value: t.List[str]):
         """Set the response attributes that should be logged."""
         attr_map = RESPONSE_ATTR_MAP
         attr_type = "response"
         self._set_log_attrs(attr_map=attr_map, attr_type=attr_type, value=value)
 
-    def _get_log_attrs(self, attr_type: str) -> List[str]:
+    def _get_log_attrs(self, attr_type: str) -> t.List[str]:
         """Get the log attributes for a specific type.
 
         Args:
             attr_type: 'request' or 'response'
         """
         return getattr(self, "_LOG_ATTRS", {}).get(attr_type, [])
 
-    def _set_log_attrs(self, attr_map: dict, attr_type: str, value: Union[str, List[str]]):
+    def _set_log_attrs(self, attr_map: dict, attr_type: str, value: t.Union[str, t.List[str]]):
         """Set the log attributes for a specific type.
 
         Args:
             attr_map: map of attributes to format strings
             attr_type: 'request' or 'response'
             value: user supplied attrs to log
         """
         if not hasattr(self, "_LOG_ATTRS"):
             self._LOG_ATTRS = {"response": [], "request": []}
 
-        value = [x.lower().strip() for x in listify(value)]
+        value = [x.lower().strip() for x in listify(value) if isinstance(x, str)]
 
         if not value:
             self._LOG_ATTRS[attr_type] = []
             return
 
         log_attrs = self._LOG_ATTRS[attr_type]
 
@@ -513,22 +769,22 @@
         for item in value:
             if item in attr_map:
                 value = attr_map[item]
                 entry = f"{item}={value}"
                 if entry not in log_attrs:
                     log_attrs.append(entry)
 
-    def log_body(self, body: Any, body_type: str, src: t.Optional[t.Any] = None) -> str:
+    def log_body(self, body: t.Any, body_type: str, src: t.Optional[t.Any] = None) -> str:
         """Get a string for logging a request or response body.
 
         Args:
             body: content to log
             body_type: 'request' or 'response'
             src: source of the body
 
         """
-        body = json_log(obj=coerce_str(value=body), trim=self.LOG_BODY_MAX_LEN)
+        body = json_log(obj=coerce_str(value=body), trim=self.LOG_BODY_LINES)
         return f"{body_type} BODY from {src}:\n{body}"
 
     def _init(self):
         """Pass."""
         pass
```

## axonius_api_client/setup_env.py

```diff
@@ -1,29 +1,57 @@
 # -*- coding: utf-8 -*-
-"""Tools for getting OS env vars."""
+"""Tools for getting OS env vars.
+
+TODO: This whole module needs to be refactored.
+
+It was originally intended as a quick hack to preload
+environment variables before importing the whole package
+in order to overcome limitations in older versions of python.
+
+get_env_connect should be converted to use click options,
+but due to time constraints CONNECT_SCHEMAS is used instead.
+
+We will need to refactor cli/__init__.py so that the Connect
+options are defined in a separate module that can be imported
+by both cli/__init__.py and setup_env.py
+"""
+import enum
 import logging
 import os
 import pathlib
-from typing import List, Optional, Tuple, Union
+import pprint
+import sys
+import typing as t
 
 import dotenv
 
 LOGGER = logging.getLogger("axonius_api_client.setup_env")
 """Logger to use"""
 dotenv.main.logger = LOGGER
 
-YES: List[str] = ["1", "true", "t", "yes", "y", "on"]
-"""Values that should be considered as truthy"""
+YES: t.List[str] = ["1", "true", "t", "yes", "y", "on"]
+"""Values that should be considered as true"""
 
-NO: List[str] = ["0", "false", "f", "no", "n", "off"]
-"""Values that should be considered as falsey"""
+NO: t.List[str] = ["0", "false", "f", "no", "n", "off"]
+"""Values that should be considered as false"""
 
 KEY_PRE: str = "AX_"
+CF_PRE: str = "CF_"
 """Prefix for axonapi related OS env vars"""
 
+KEY_CF_TOKEN = f"{CF_PRE}TOKEN"
+KEY_CF_RUN = f"{CF_PRE}RUN"
+KEY_CF_ERROR = f"{CF_PRE}ERROR"
+KEY_CF_PATH = f"{CF_PRE}PATH"
+
+CF_TOKEN_DEFAULT = None
+CF_PATH_DEFAULT = "cloudflared"
+CF_RUN_DEFAULT = "no"
+CF_ERROR_DEFAULT = "no"
+
 KEY_DEFAULT_PATH: str = f"{KEY_PRE}PATH"
 """OS env to use for :attr:`DEFAULT_PATH` instead of CWD"""
 
 KEY_ENV_FILE: str = f"{KEY_PRE}ENV_FILE"
 """OS env to use for .env file name"""
 
 KEY_ENV_PATH: str = f"{KEY_PRE}ENV"
@@ -57,14 +85,17 @@
 
 KEY_DEBUG_PRINT: str = f"{KEY_PRE}DEBUG_PRINT"
 """OS env to use print() instead of LOGGER.debug()"""
 
 KEY_USER_AGENT: str = f"{KEY_PRE}USER_AGENT"
 """OS env to use a custom User Agent string."""
 
+KEY_CREDENTIALS = f"{KEY_PRE}CREDENTIALS"
+DEFAULT_CREDENTIALS: str = "no"
+
 DEFAULT_DEBUG: str = "no"
 """Default for :attr:`KEY_DEBUG`"""
 
 DEFAULT_EXTRA_WARN: str = "yes"
 
 DEFAULT_DEBUG_PRINT: str = "no"
 """Default for :attr:`KEY_DEBUG_PRINT`"""
@@ -74,236 +105,638 @@
 
 DEFAULT_CERTWARN: str = "yes"
 """Default for :attr:`KEY_CERTWARN`"""
 
 DEFAULT_ENV_FILE: str = ".env"
 """Default for :attr:`KEY_ENV_FILE`"""
 
-KEYS_HIDDEN: List[str] = [KEY_KEY, KEY_SECRET]
-"""List of keys to hide in :meth:`get_env_ax`"""
+KEYS_HIDDEN: t.List[str] = [KEY_KEY, KEY_SECRET, KEY_CF_TOKEN]
+"""t.List of keys to hide in :meth:`get_env_ax`"""
+
+KEY_MATCHES: t.List[str] = ["password", "secret", "token", "key"]
+"""t.List of key partial matches to hide in :meth:`get_env_ax`"""
 
 HIDDEN: str = "_HIDDEN_"
 """Value to use for hidden keys in :meth:`get_env_ax`"""
 
+EMPTY_STRINGS = ["", "none", "null", "nil"]
+EMPTY_OBJECTS = [None, [], {}, set(), tuple(), "", b""]
+
+
+CONNECT_SCHEMAS: dict = {
+    "url": {
+        "env": KEY_URL,
+        "arg": "url",
+        "default": None,
+        "type": "string",
+        "description": "API URL",
+        "empty_ok": False,
+    },
+    "key": {
+        "env": KEY_KEY,
+        "arg": "key",
+        "default": None,
+        "type": "string",
+        "description": "API Key",
+        "empty_ok": False,
+    },
+    "secret": {
+        "env": KEY_SECRET,
+        "arg": "secret",
+        "default": None,
+        "type": "string",
+        "description": "API Secret",
+        "empty_ok": False,
+    },
+    "certwarn": {
+        "env": KEY_CERTWARN,
+        "arg": "certwarn",
+        "default": DEFAULT_CERTWARN,
+        "type": "boolean",
+        "description": "Enable/disable cert warnings",
+    },
+    "credentials": {
+        "env": KEY_CREDENTIALS,
+        "arg": "credentials",
+        "default": DEFAULT_CREDENTIALS,
+        "type": "boolean",
+        "description": "Treat key/secret as username/password",
+    },
+    "cf_token": {
+        "env": KEY_CF_TOKEN,
+        "arg": "cf_token",
+        "default": CF_TOKEN_DEFAULT,
+        "type": "string",
+        "description": "Cloudflare access token",
+        "empty_ok": True,
+    },
+    "cf_path": {
+        "env": KEY_CF_PATH,
+        "arg": "cf_path",
+        "default": CF_PATH_DEFAULT,
+        "type": "string",
+        "description": "Path to cloudflared binary to run if cf_run is True",
+        "empty_ok": True,
+    },
+    "cf_run": {
+        "env": KEY_CF_RUN,
+        "arg": "cf_run",
+        "default": CF_RUN_DEFAULT,
+        "type": "boolean",
+        "description": (
+            "If cf_token not supplied, run cloudflared binary in cf_path to get Cloudflare "
+            "access token"
+        ),
+    },
+    "cf_error": {
+        "env": KEY_CF_ERROR,
+        "arg": "cf_error",
+        "default": CF_ERROR_DEFAULT,
+        "type": "boolean",
+        "description": (
+            "If cf_token not supplied, raise an error if a token cannot be obtained from "
+            "cloudflared binary in cf_path"
+        ),
+    },
+}
+# TBD convert to click options (need to refactor cli/__init__.py to do this properly)
+
+
+def is_empty_object(value: t.Any) -> bool:
+    """Check if value is an empty object.
+
+    Args:
+        value: value to check
+
+    Returns:
+        bool: True if value is an empty object
+    """
+    return value in EMPTY_OBJECTS
+
+
+def is_empty_string(value: t.Any) -> bool:
+    """Check if value is an empty string.
+
+    Args:
+        value: value to check
+
+    Returns:
+        bool: True if value is an empty string
+    """
+    value = bytes_to_str(value)
+    is_str = isinstance(value, str) and value.strip()
+    return True if not is_str else value.strip().lower() in EMPTY_STRINGS
+
+
+def is_empty(value: t.Any) -> bool:
+    """Check if value is empty.
+
+    Args:
+        value: value to check
+
+    Returns:
+        bool: True if value is empty
+    """
+    return is_empty_object(value) or is_empty_string(value)
+
+
+ENV_NAME = f"dotenv file named {DEFAULT_ENV_FILE!r} (override with ${KEY_ENV_FILE})"
+
+
+class Results(enum.Enum):
+    """Enum for find_dotenv results."""
+
+    supplied: str = "user supplied .env file as find_dotenv(ax_env=...)"
+    env_path: str = f"OS environment variable ${KEY_ENV_PATH}"
+    default_path: str = f"OS environment variable ${KEY_DEFAULT_PATH} or current working directory"
+    find_dotenv_cwd: str = f"Walk to root from the current working directory to find a {ENV_NAME}"
+    find_dotenv_script: str = (
+        f"Walk to root from the directory of the currently running script to find a {ENV_NAME} "
+        "(does not work in interactive mode or `sys.frozen=True`)"
+    )
+    not_found: str = f"No {ENV_NAME} found"
+
+
+def spew(
+    msg: str,
+    debug: t.Optional[bool] = None,
+    debug_print: t.Optional[bool] = None,
+) -> None:  # pragma: no cover
+    """Print a message to stdout."""
+    if DEBUG_PRINT is True or debug_print is True:
+        print(msg, file=sys.stderr)
+    if DEBUG is True or debug is True:
+        LOGGER.debug(msg)
+
+
+def get_file_or_dir_with_file(
+    path: t.Union[str, bytes, pathlib.Path], filename: t.Union[str, bytes, pathlib.Path]
+) -> t.Optional[pathlib.Path]:
+    """Check if path is a file or dir with a file.
+
+    Args:
+        path: path to check
+        filename: filename to check for
+
+    Returns:
+        pathlib.Path: path to file if found, else None
+    """
+    path = bytes_to_str(path)
+    if isinstance(path, str) and path.strip():
+        path = pathlib.Path(path.strip()).expanduser().resolve()
+    if isinstance(path, pathlib.Path) and path.exists():
+        # if it is a dir, append env_file to it
+        if path.is_dir():
+            path = path / filename
+        # TBD: check if file is readable
+        # TBD: check file size
+        if path.is_file():
+            return path
+    return None
+
 
 def find_dotenv(
-    ax_env: Optional[Union[str, pathlib.Path]] = None, default: str = os.getcwd()
-) -> Tuple[str, str]:
+    ax_env: t.Optional[t.Union[str, bytes, pathlib.Path]] = None,
+    filename: t.Optional[str] = DEFAULT_ENV_FILE,
+    default: t.Optional[t.Union[str, bytes, pathlib.Path]] = os.getcwd(),
+    check_ax_env: bool = True,
+    check_default: bool = True,
+    check_walk_cwd: bool = True,
+    check_walk_script: bool = True,
+    debug: bool = True,
+) -> t.Tuple[str, str]:
     """Find a .env file.
 
     Args:
         ax_env: manual path to look for .env file
-        default: default path to use if :attr:`KEY_DEFAULT_PATH` is not set
+        filename: name of the .env file to look for (not a path, just a filename),
+            override with $AX_ENV_FILE
+        default: default path to use if ax_env or $AX_PATH is not supplied (default is CWD)
+        check_ax_env: check if $value is file or $value/$filename is file from $AX_ENV
+        check_default: check if $value is file or $value/$filename is file $AX_PATH with default
+            as default value
+        check_walk_cwd: walk to root to find `filename` from current working directory
+        check_walk_script: walk to root to find `filename` from running scripts directory
+            (does not work in interactive mode or `sys.frozen=True`)
+        debug: enable debug output
 
     Notes:
         Order of operations:
-
             * Check for ax_env for .env (or dir with .env in it)
             * Check for OS env var :attr:`KEY_ENV_PATH` for .env (or dir with .env in it)
             * Check for OS env var :attr:`KEY_DEFAULT_PATH` as dir with .env in it
             * use dotenv.find_dotenv() to walk tree from CWD
             * use dotenv.find_dotenv() to walk tree from package root
     """
-    env_file = get_env_str(key=KEY_ENV_FILE, default=DEFAULT_ENV_FILE)
-    if ax_env:
-        found_env = pathlib.Path(ax_env).expanduser().resolve()
-        found_env = found_env / env_file if found_env.is_dir() else found_env
-        if found_env.is_file():
-            return "supplied", str(found_env)
-
-    found_env = get_env_path(key=KEY_ENV_PATH, get_dir=False)
-    if found_env and found_env.exists():
-        found_env = found_env / env_file if found_env.is_dir() else found_env
-        if found_env.is_file():
-            return "env_path", str(found_env)
-
-    found_env = get_env_path(key=KEY_DEFAULT_PATH, default=default)
-    if found_env and found_env.exists():
-        found_env = found_env / env_file if found_env.is_dir() else found_env
-        if found_env.is_file():
-            return "default_path", str(found_env)
-
-    found_env = dotenv.find_dotenv(filename=env_file, usecwd=True) or ""
-    if found_env and pathlib.Path(found_env).is_file():
-        return "find_dotenv_cwd", found_env
-
-    found_env = dotenv.find_dotenv(filename=env_file, usecwd=False) or ""
-    if found_env and pathlib.Path(found_env).is_file():
-        return "find_dotenv_pkg", found_env
+    _spew: callable = lambda x: spew(f"find_dotenv(): {x}", debug)
 
-    return "not_found", ""
-
-
-def load_dotenv(ax_env: Optional[Union[str, pathlib.Path]] = None, **kwargs) -> str:
+    # $AX_ENV_FILE=".env"
+    # name of the file to look for - should not be a full path
+    # just the name of the .env file we will look for by default when
+    # a directory is supplied instead of a file for AX_ENV
+    filename = get_env_str(key=KEY_ENV_FILE, default=filename)
+
+    _r = Results.supplied
+    found: t.Optional[pathlib.Path] = get_file_or_dir_with_file(path=ax_env, filename=filename)
+    _spew(f"ax_env={ax_env!r}, found={found!r}, ({_r.value})")
+    if found:
+        return _r.name, str(found)
+
+    if check_ax_env:
+        _r = Results.env_path
+        from_ax_env: t.Optional[str] = get_env_str(key=KEY_ENV_PATH, default="", empty_ok=True)
+        found: t.Optional[pathlib.Path] = get_file_or_dir_with_file(
+            path=from_ax_env, filename=filename
+        )
+        _spew(f"${KEY_ENV_PATH}={from_ax_env!r}, found={found!r} ({_r.value})")
+        if found:
+            return _r.name, str(found)
+
+    if check_default:
+        _r = Results.default_path
+        from_default_path: t.Optional[str] = get_env_str(
+            key=KEY_DEFAULT_PATH, default=default, empty_ok=True
+        )
+        found: t.Optional[pathlib.Path] = get_file_or_dir_with_file(
+            path=from_default_path, filename=filename
+        )
+        _spew(f"${KEY_DEFAULT_PATH}={from_default_path!r}, found={found!r} ({_r.value})")
+        if found:
+            return _r.name, str(found)
+
+    if check_walk_cwd:
+        _r = Results.find_dotenv_cwd
+        found_env: t.Optional[str] = dotenv.find_dotenv(filename=filename, usecwd=True)
+        _spew(f"found={found_env!r} ({_r.value})")
+        if found_env:
+            return _r.name, found_env
+
+    if check_walk_script:
+        _r = Results.find_dotenv_script
+        found_env: t.Optional[str] = dotenv.find_dotenv(filename=filename, usecwd=False)
+        _spew(f"found={found_env!r} ({_r.value})")
+        if found_env:
+            return _r.name, found_env
+
+    _r = Results.not_found
+    found: str = ""
+    _spew(f"found={found!r} ({_r.value})")
+    return _r.name, found
+
+
+LOADED = {}
+
+
+class MSG:
+    """Messages for :func:`load_dotenv` and :func:`find_dotenv`."""
+
+    not_found = "Could not find"
+    already_loaded = "Override is False, not loading already loaded"
+    loading = "Loading .env with override"
+
+
+def load_dotenv(
+    ax_env: t.Optional[t.Union[str, bytes, pathlib.Path]] = None,
+    override: t.Optional[bool] = None,
+    debug: t.Optional[bool] = True,
+    verbose: t.Optional[bool] = None,
+    **kwargs,
+) -> str:
     """Load a '.env' file as environment variables accessible to this package.
 
     Args:
         ax_env: path to .env file to load, if directory will look for '.env' in that directory
-        **kwargs: passed to dotenv.load_dotenv()
+        override: override existing env vars with those in .env file
+        debug: enable debug output
+        verbose: enable verbose output in dotenv.load_dotenv
+        kwargs: additional keyword arguments to pass to :func:`find_dotenv`
     """
-    src, ax_env = find_dotenv(ax_env=ax_env)
+    _spew: callable = lambda x: spew(f"load_dotenv(): {x}", debug)
+
+    src, ax_env = find_dotenv(ax_env=ax_env, debug=debug, **kwargs)
+    desc = f".env file from {src!r} ax_env={str(ax_env)!r}"
+
+    if not ax_env:
+        _spew(f"{MSG.not_found} {desc}")
+        return ax_env
+
+    override = (
+        override
+        if isinstance(override, bool)
+        else get_env_bool(key=KEY_OVERRIDE, default=DEFAULT_OVERRIDE)
+    )
+    load_key = str(ax_env)
+    if load_key in LOADED and override is not True:
+        loaded = LOADED[load_key]
+        src = loaded["src"]
+        ax_env = loaded["ax_env"]
+        desc = f".env file from {src!r} ax_env={str(ax_env)!r}"
+        _spew(f"{MSG.already_loaded} {desc}")
+        return str(ax_env)
+
+    LOADED[load_key] = loaded = {
+        "src": src,
+        "ax_env": ax_env,
+        "override": override,
+    }
+    _spew(f"{MSG.loading} {override} from {src!r} ax_env={str(ax_env)!r}")
+    pre = f"{KEY_PRE} and {CF_PRE} env vars"
+    loaded["before"] = before = get_env_ax(hide=False)
+    _spew(f"{pre} before load dotenv:\n{pprint.pformat(hide_values(before))}")
+    dotenv.load_dotenv(dotenv_path=ax_env, verbose=verbose, override=override)
+    loaded["after"] = after = get_env_ax(hide=False)
+    changed = [k for k in before if k not in after or before[k] != after[k]]
+    added = [k for k in after if k not in before]
+    _spew(
+        f"{pre} after load dotenv changed={changed}, added={added}:\n"
+        f"{pprint.pformat(hide_values(after))}"
+    )
+    return str(ax_env)
+
+
+def get_env_ax_env() -> str:
+    """Get the value of the OS env var :attr:`KEY_ENV_PATH`."""
+    return get_env_str(key=KEY_ENV_PATH, default=None, empty_ok=True)
+
+
+def get_env_str(
+    key: str,
+    default: t.Any = None,
+    empty_ok: bool = False,
+    lower: bool = False,
+    strip: bool = True,
+    description: t.Optional[str] = None,
+) -> str:
+    """Get an OS env var.
+
+    Args:
+        key: OS env key
+        default: default to use if not found
+        empty_ok: do not throw an exc if the key's value is empty
+        lower: lowercase the value
+        strip: strip the value
+        description: description of the env var
 
-    override = get_env_bool(key=KEY_OVERRIDE, default=DEFAULT_OVERRIDE)
-    DEBUG_LOG(f"Loading .env with override {override} from {src!r} {str(ax_env)!r}")
-    if pathlib.Path(ax_env).is_file():
-        DEBUG_LOG(f"{KEY_PRE}.* env vars before load dotenv: {get_env_ax()}")
-        dotenv.load_dotenv(dotenv_path=ax_env, verbose=DEBUG, override=override)
-        DEBUG_LOG(f"{KEY_PRE}.* env vars after load dotenv: {get_env_ax()}")
-    return ax_env
+    Raises:
+        :exc:`ValueError`: OS env var value is empty and empty_ok is False
+    """
+    env_value = os.environ.get(key, "")
+    is_empty_env = is_empty(env_value)
+    resolved = bytes_to_str(default if is_empty_env else env_value)
+    resolved = resolved.strip() if strip and isinstance(resolved, str) else resolved
+    resolved = resolved.lower() if lower and isinstance(resolved, str) else resolved
+    is_empty_resolved = is_empty(resolved)
+    if is_empty_resolved and not empty_ok:
+        msgs = [
+            "Error in OS environment variable",
+            f"  Description: {description!r}",
+            f"  Is Empty OK?: {empty_ok!r}",
+            "",
+            f"  OS environment variable Name: {key!r}",
+            f"  OS environment variable Value: {env_value!r}",
+            f"  OS environment variable Value is empty: {is_empty_env!r}",
+            "",
+            f"  Default Value: {default!r}",
+            f"  Resolved Value: {resolved!r}",
+            f"  Resolved Value is empty: {is_empty_resolved!r}",
+        ]
+        ax_dotenv = get_env_ax_env()
+        ax_dot = f'({KEY_ENV_PATH}="{ax_dotenv}")'
+
+        msgs += [
+            "",
+            f"Must specify {key!r} in .env {ax_dot} file or in OS environment variable, i.e.:",
+            f'  {key}="{env_value}"',
+        ]
+        raise ValueError("\n".join(msgs))
+    return resolved
 
 
-def get_env_bool(key: str, default: Optional[bool] = None) -> bool:
+def get_env_bool(key: str, default: t.Any = None, description: t.Optional[str] = None) -> bool:
     """Get an OS env var and turn convert it to a boolean.
 
     Args:
         key: OS env key
         default: default to use if not found
+        description: description of env var for error message
 
     Raises:
-        :exc:`ValueError`: OS env var value is not able to be converted to bool
+        :exc:`ValueError`: OS env var value is not bool
     """
-    value = get_env_str(key=key, default=default, lower=True)
+    value = get_env_str(key=key, default=default, lower=True, description=description)
     if value in YES or value is True:
         return True
-
     if value in NO or value is False:
         return False
-
     msg = [
         f"Supplied value {value!r} for OS environment variable {key!r} must be one of:",
         f"  For true: {', '.join(YES)}",
         f"  For false: {', '.join(NO)}",
     ]
     raise ValueError("\n".join(msg))
 
 
-def get_env_str(
-    key: str, default: Optional[str] = None, empty_ok: bool = False, lower: bool = False
-) -> str:
-    """Get an OS env var.
+def get_env_extra_warn(
+    ax_env: t.Optional[t.Union[str, bytes, pathlib.Path]] = None,
+    **kwargs,
+) -> bool:
+    """Get AX_CERT_VERIFY from OS env vars.
 
     Args:
-        key: OS env key
-        default: default to use if not found
-        empty_ok: dont throw an exc if the key's value is empty
-        lower: lowercase the value
-
-    Raises:
-        :exc:`ValueError`: OS env var value is empty and empty_ok is False
+        ax_env: path to .env file to load, if not supplied will find a '.env'
+        kwargs: passed to :func:`load_dotenv`
     """
-    orig_value = os.environ.get(key, "").strip()
-    value = orig_value
-
-    if default is not None and value in [None, ""]:
-        value = default
-
-    if not empty_ok and value in [None, ""]:
-        raise ValueError(
-            f"OS environment variable {key!r} is empty with value {orig_value!r}\n"
-            f"Must specify {key!r} in .env file or in OS environment variable"
-        )
-
-    value = value.lower() if lower and isinstance(value, str) else value
-    return value
+    load_dotenv(ax_env=ax_env, **kwargs)
+    return get_env_bool(key=KEY_EXTRA_WARN, default=DEFAULT_EXTRA_WARN)
 
 
 def get_env_path(
-    key: str, default: Optional[str] = None, get_dir: bool = True
-) -> Union[pathlib.Path, str]:
+    key: str,
+    default: t.Optional[str] = None,
+    get_dir: bool = True,
+) -> t.Union[pathlib.Path, str]:
     """Get a path from an OS env var.
 
     Args:
         key: OS env var to get path from
         default: default path to use if OS env var not set
-        get_dir: return directory containing file of path is file
+        get_dir: if path is file, return directory containing file
     """
     value = get_env_str(key=key, default=default, empty_ok=True)
     if value:
         value = pathlib.Path(value).expanduser().resolve()
         if get_dir and value.is_file():
             value = value.parent
     return value or ""
 
 
 def get_env_csv(
-    key: str, default: Optional[str] = None, empty_ok: bool = False, lower: bool = False
-) -> List[str]:
+    key: str,
+    default: t.Optional[str] = None,
+    empty_ok: bool = False,
+    lower: bool = False,
+) -> t.List[str]:
     """Get an OS env var as a CSV.
 
     Args:
         key: OS env key
         default: default to use if not found
-        empty_ok: dont throw an exc if the key's value is empty
+        empty_ok: do not throw an exc if the key's value is empty
         lower: lowercase the value
     """
     value = get_env_str(key=key, default=default, empty_ok=empty_ok, lower=lower)
     value = [y for y in [x.strip() for x in value.split(",")] if y]
     return value
 
 
-def get_env_user_agent(**kwargs) -> str:
-    """Pass."""
-    load_dotenv(**kwargs)
+def get_env_user_agent(
+    ax_env: t.Optional[t.Union[str, bytes, pathlib.Path]] = None,
+    **kwargs: t.Any,
+) -> str:
+    """Get AX_USER_AGENT from OS env vars.
+
+    Args:
+        ax_env: path to .env file to load, if not supplied will find a '.env'
+        **kwargs: passed to :func:`load_dotenv`
+    """
+    load_dotenv(ax_env=ax_env, **kwargs)
     return get_env_str(key=KEY_USER_AGENT, default="", empty_ok=True)
 
 
-def get_env_connect(**kwargs) -> dict:
-    """Get URL, API key, API secret, and certwarn from OS env vars.
+def load_schema(schema: dict, kwargs: t.Optional[dict] = None) -> t.Any:
+    """Load a schema from an OS env var."""
+    kwargs = {} if not isinstance(kwargs, dict) else kwargs
+    arg = schema["arg"]
+    schema_type = schema["type"]
+    env_key = schema["env"]
+    empty_ok = schema.get("empty_ok", False)
+    default = kwargs[arg] if arg in kwargs else schema.get("default", None)
+    description = schema.get("description", "")
+
+    if schema_type == "boolean":
+        return get_env_bool(key=env_key, default=default, description=description)
+
+    return get_env_str(
+        key=env_key,
+        default=default,
+        description=description,
+        empty_ok=empty_ok,
+    )
+
+
+def get_env_connect(
+    ax_env: t.Optional[t.Union[str, bytes, pathlib.Path]] = None,
+    load_override: t.Optional[bool] = None,
+    load_filename: t.Optional[str] = DEFAULT_ENV_FILE,
+    load_default: t.Optional[t.Union[str, bytes, pathlib.Path]] = os.getcwd(),
+    load_check_ax_env: bool = True,
+    load_check_default: bool = True,
+    load_check_walk_cwd: bool = True,
+    load_check_walk_script: bool = True,
+    debug: t.Optional[bool] = True,
+    **kwargs,
+) -> dict:
+    """Get connect arguments that start with AX_ or CF_ from OS env vars.
+
+    Notes:
+        Arguments are defined in :data:`CONNECT_SCHEMAS`.
 
     Args:
-        **kwargs: passed to :meth:`load_dotenv`
+        ax_env: path to .env file to load, if not supplied will find a '.env'
+        debug: if True, will print debug messages
+        load_override: if True, will override any existing OS env vars with values
+        load_filename: filename to load from
+        load_default: default path to use if OS env var not set
+        load_check_ax_env: if True, will check for AX_ENV_PATH in OS env vars
+        load_check_default: if True, will check for DEFAULT_ENV_FILE in OS env vars
+        load_check_walk_cwd: if True, will walk up from cwd looking for DEFAULT_ENV_FILE
+        load_check_walk_script: if True, will walk up from script looking for DEFAULT_ENV_FILE
+        **kwargs: checked for argument defaults to use instead of schema defaults
     """
-    load_dotenv(**kwargs)
-    return {
-        "url": get_env_str(key=KEY_URL),
-        "key": get_env_str(key=KEY_KEY),
-        "secret": get_env_str(key=KEY_SECRET),
-        "certwarn": get_env_bool(key=KEY_CERTWARN, default=DEFAULT_CERTWARN),
-    }
-
-
-def get_env_features(**kwargs) -> List[str]:
+    load_dotenv(
+        ax_env=ax_env,
+        debug=debug,
+        override=load_override,
+        filename=load_filename,
+        default=load_default,
+        check_ax_env=load_check_ax_env,
+        check_default=load_check_default,
+        check_walk_cwd=load_check_walk_cwd,
+        check_walk_script=load_check_walk_script,
+    )
+    return {k: load_schema(v, kwargs) for k, v in CONNECT_SCHEMAS.items()}
+
+
+def get_env_features(
+    ax_env: t.Optional[t.Union[str, bytes, pathlib.Path]] = None,
+    **kwargs,
+) -> t.List[str]:
     """Get list of features to enable from OS env vars.
 
     Args:
-        **kwargs: passed to :meth:`load_dotenv`
+        ax_env: path to .env file to load, if not supplied will find a '.env'
+        kwargs: passed to :func:`load_dotenv`
+
     """
-    load_dotenv(**kwargs)
-    value = get_env_csv(key=KEY_FEATURES, default="", empty_ok=True, lower=True)
-    return value
+    load_dotenv(ax_env=ax_env, **kwargs)
+    return get_env_csv(key=KEY_FEATURES, default="", empty_ok=True, lower=True)
 
 
-def get_env_ax():
-    """Get all axonapi related OS env vars."""
-    value = {k: v for k, v in os.environ.items() if k.startswith(KEY_PRE)}
-    value = {k: HIDDEN if k in KEYS_HIDDEN else v for k, v in value.items()}
+def hide_value(key: str, value: t.Any) -> t.Any:
+    """Hide sensitive values."""
+    if key in KEYS_HIDDEN:
+        return HIDDEN
+    for check in KEY_MATCHES:
+        if check in str(key).lower().strip():
+            return HIDDEN
     return value
 
 
-def set_env(key: str, value: str, **kwargs) -> Tuple[str, Tuple[bool, str, str]]:
-    """Set an environment variable in .env file."""
-    from . import INIT_DOTENV as ax_env
+def hide_values(data: dict) -> dict:
+    """Hide sensitive values."""
+    return {k: hide_value(k, v) for k, v in data.items()}
 
-    ax_env = ax_env or DEFAULT_ENV_FILE
-    return dotenv.set_key(dotenv_path=ax_env, key_to_set=key, value_to_set=str(value))
 
+def get_env_ax(hide: bool = True) -> dict:
+    """Get all axonapi related OS env vars."""
+    data = {k: v for k, v in os.environ.items() if k.startswith(KEY_PRE) or k.startswith(CF_PRE)}
+    return hide_values(data) if hide else data
 
-def get_env_extra_warn(**kwargs) -> str:
-    """Pass."""
-    load_dotenv(**kwargs)
-    return get_env_bool(key=KEY_EXTRA_WARN, default=DEFAULT_EXTRA_WARN)
+
+def set_env(
+    key: str,
+    value: t.Any,
+    ax_env: t.Optional[t.Union[str, bytes, pathlib.Path]] = None,
+    quote_mode: str = "always",
+    export: bool = False,
+    encoding: str = "utf-8",
+) -> t.Tuple[t.Optional[bool], str, str]:
+    """Set an environment variable in .env file."""
+    from . import INIT_DOTENV
+
+    ax_env = ax_env or INIT_DOTENV or DEFAULT_ENV_FILE
+    value = str(bytes_to_str(value))
+    return dotenv.set_key(
+        dotenv_path=ax_env,
+        key_to_set=key,
+        value_to_set=value,
+        quote_mode=quote_mode,
+        export=export,
+        encoding=encoding,
+    )
+
+
+def bytes_to_str(value: t.Any, encoding: str = "utf-8", errors: str = "ignore") -> t.Any:
+    """Convert bytes to str."""
+    if isinstance(value, bytes):
+        value = value.decode(encoding, errors=errors)
+    return value
 
 
 DEBUG_PRINT: bool = get_env_bool(key=KEY_DEBUG_PRINT, default=DEFAULT_DEBUG_PRINT)
 """Use print() instead of LOGGER.debug()."""
 
-DEBUG_USE = print if DEBUG_PRINT else LOGGER.debug
-"""use print or LOGGER.debug()"""
-
 DEBUG: bool = get_env_bool(key=KEY_DEBUG, default=DEFAULT_DEBUG)
 """Enable package wide debugging."""
 
-DEBUG_LOG = DEBUG_USE if DEBUG else lambda x: x
-"""Function to use for debug logging"""
-
 DEFAULT_PATH: str = str(get_env_path(key=KEY_DEFAULT_PATH, default=os.getcwd()))
 """Default path to use throughout this package"""
```

## axonius_api_client/tools.py

```diff
@@ -1,38 +1,45 @@
 # -*- coding: utf-8 -*-
 """Utilities and tools."""
 import codecs
 import csv
 import dataclasses
+import datetime
 import inspect
 import io
 import ipaddress
 import json
 import logging
 import pathlib
 import platform
 import re
 import sys
 import types
 import typing as t
-from datetime import datetime, timedelta, timezone
+import uuid
 from itertools import zip_longest
 from urllib.parse import urljoin
 
 import bson
 import click
 import dateutil.parser
 import dateutil.relativedelta
 import dateutil.tz
 import marshmallow
 
 from . import INIT_DOTENV, PACKAGE_FILE, PACKAGE_ROOT, VERSION
 from .constants.api import GUI_PAGE_SIZES, REFRESH, FolderDefaults
-from .constants.ctypes import PathLike, PatternLike, PatternLikeListy
-
+from .constants.ctypes import (
+    PathLike,
+    PatternLike,
+    PatternLikeListy,
+    TypeDate,
+    TypeFloat,
+    TypeDelta,
+)
 from .constants.general import (
     DAYS_MAP,
     DEBUG_ARGS,
     DEBUG_TMPL,
     EMAIL_RE,
     ERROR_ARGS,
     ERROR_TMPL,
@@ -40,21 +47,24 @@
     HUMAN_SIZES,
     NO,
     OK_ARGS,
     OK_TMPL,
     SECHO_ARGS,
     SPLITTER,
     TRIM_MSG,
+    TRIM_POST,
     URL_STARTS,
     WARN_ARGS,
     WARN_TMPL,
     YES,
+    YES_STR,
+    NO_STR,
 )
 from .constants.logs import MAX_BODY_LEN
-from .exceptions import ToolsError
+from .exceptions import FormatError, ToolsError
 from .setup_env import find_dotenv, get_env_ax
 
 LOG: logging.Logger = logging.getLogger(PACKAGE_ROOT).getChild("tools")
 
 
 def type_str(
     value: t.Any, max_len: int = 60, join: t.Optional[str] = ", "
@@ -78,22 +88,21 @@
     expanduser: bool = True,
     as_file: bool = False,
     as_dir: bool = False,
     exts: t.Optional[t.List[str]] = None,
 ) -> pathlib.Path:
     """Convert a str into a fully resolved & expanded Path object."""
     check_type(value=path, exp=(str, bytes, pathlib.Path))
+    path = bytes_to_str(value=path, encoding=path_encoding, strict=path_strict)
 
-    if isinstance(path, bytes):
-        path = bytes_to_str(value=path, strict=path_strict, encoding=path_encoding)
-        resolved = pathlib.Path(path.splitlines()[0])
-    elif isinstance(path, str):
-        resolved = pathlib.Path(path.splitlines()[0])
-    elif isinstance(path, pathlib.Path):
+    if isinstance(path, pathlib.Path):
         resolved = path
+    else:
+        path = bytes_to_str(value=path, encoding=path_encoding, strict=path_strict)
+        resolved = pathlib.Path(path.splitlines()[0])
 
     if expanduser:
         resolved = resolved.expanduser()
 
     if resolve:
         resolved = resolved.resolve()
 
@@ -125,15 +134,15 @@
 
     return False
 
 
 def listify(
     obj: t.Any = None,
     split: bool = False,
-    split_max: int = -1,
+    split_max: t.Optional[int] = -1,
     split_sep: t.Optional[PatternLike] = SPLITTER,
     strip: bool = True,
     strip_chars: t.Optional[str] = None,
     encoding: str = "utf-8",
     errors: str = "replace",
     dictkeys: bool = False,
     **kwargs,
@@ -149,15 +158,16 @@
           otherwise return as a list of obj
 
     Args:
         obj: object to coerce to list
         dictkeys: if obj is dict, return list of keys of obj
     """
 
-    def do_strip(values):
+    def stripper(values: t.List[str]) -> t.List[str]:
+        """Pass."""
         return [y for y in [x.strip(strip_chars) for x in values] if y] if strip else values
 
     if "value" in kwargs:
         obj = kwargs["value"]
 
     if obj is None:
         return []
@@ -167,179 +177,171 @@
         return list(obj)
     if isinstance(obj, dict):
         return list(obj) if dictkeys else [obj]
     if split:
         if isinstance(obj, bytes):
             obj = obj.decode(encoding=encoding, errors=errors)
         if isinstance(obj, str):
+            split_args = (
+                {"maxsplit": split_max} if isinstance(split_max, int) and split_max >= 0 else {}
+            )
             if is_pattern(value=split_sep):
-                split_max = 0 if split_max == -1 else split_max
-                return do_strip(split_sep.split(obj, maxsplit=split_max))
-            return do_strip(obj.split(sep=split_sep, maxsplit=split_max))
+                return stripper(split_sep.split(obj, **split_args))
+            return stripper(obj.split(sep=split_sep, **split_args))
+
     return [obj]
 
 
 def grouper(iterable: t.Iterable, n: int, fillvalue: t.Optional[t.Any] = None) -> t.Iterator:
     """Split an iterable into chunks.
 
     Args:
-        iterable: iterable to split into chunks of size n
+        iterable: iterable to split into chunks of size `n`
         n: length to split iterable into
         fillvalue: value to use as filler for last chunk
     """
     return zip_longest(*([iter(iterable)] * n), fillvalue=fillvalue)
 
 
 def coerce_int(
     obj: t.Any,
     max_value: t.Optional[int] = None,
     min_value: t.Optional[int] = None,
     allow_none: bool = False,
+    as_none: t.Any = None,
     valid_values: t.Optional[t.List[int]] = None,
     errmsg: t.Optional[str] = None,
-) -> int:
+) -> t.Optional[int]:
     """Convert an object into int.
 
     Args:
         obj: object to convert to int
 
     Raises:
-        :exc:`ToolsError`: if obj is not able to be converted to int
+        :exc:`ToolsError`: if obj is not int
     """
     if allow_none and (obj is None or str(obj).lower().strip() in ["none", "null"]):
-        return None
+        return as_none
 
     pre = f"{errmsg}\n" if errmsg else ""
 
     try:
         value = int(obj)
-    except Exception:
-        raise ToolsError(f"{pre}Supplied value {obj!r} of type {trype(obj)} is not an integer.")
+    except Exception as exc:
+        raise ToolsError(
+            f"{pre}Supplied value {obj!r} of type {trype(obj)} is not an integer.\n{exc}"
+        )
 
     if max_value is not None and value > max_value:
         raise ToolsError(f"{pre}Supplied value {obj!r} is greater than max value of {max_value}.")
 
     if min_value is not None and value < min_value:
         raise ToolsError(f"{pre}Supplied value {obj!r} is less than min value of {min_value}.")
 
     if valid_values and value not in valid_values:
         raise ToolsError(f"{pre}Supplied value {obj!r} is not one of {valid_values}.")
 
     return value
 
 
-# def coerce_int_float(value: t.Union[int, float, str]) -> t.Union[int, float]:
-#     """Convert an object into int or float.
-
-#     Args:
-#         obj: object to convert to int or float
-
-#     Raises:
-#         :exc:`ToolsError`: if obj is not able to be converted to int or float
-#     """
-#     if isinstance(value, float):
-#         return value
-
-#     if isinstance(value, int):
-#         return value
-
-#     if isinstance(value, str):
-#         value = value.strip()
-
-#         if value.isdigit():
-#             return int(value)
-
-#         if value.replace(".", "").isdigit():
-#             return float(value)
-
-#     vtype = trype(value)
-#     raise ToolsError(f"Supplied value {value!r} of type {vtype} is not an integer or float.")
-
-
 def coerce_int_float(
     value: t.Union[int, float, str, bytes],
     as_float: bool = False,
     error: bool = True,
     ret_value: bool = False,
+    as_none: t.Any = None,
 ) -> t.Optional[t.Union[int, float]]:
     """Coerce a value into int or float.
 
     Args:
         value (t.Union[int, float, str, bytes]): value to coerce
         as_float (bool, optional): if value is an integer, coerce to float
+        error (bool, optional): raise error if value cannot be coerced
+        ret_value (bool, optional): return value if it can be coerced
+        as_none (t.Any, optional): value to return if value cannot be coerced
 
     Returns:
         t.Optional[t.Union[int, float]]:
             float or int if value is int, float, int as str, or float as str; else None
     """
     if isinstance(value, (float, int)):
         return float(value) if as_float else value
 
     value = bytes_to_str(value=value)
 
-    if is_str(value=value):
+    if isinstance(value, str):
         value = value.strip()
 
         if "." in value and value.replace(".", "").isdigit():
             return float(value)
 
         if value.isdigit():
             return float(value) if as_float else int(value)
 
     if error:
         raise ToolsError(
             f"Supplied value {value!r} of type {trype(value)} is not an integer or float."
         )
 
-    return value if ret_value is True else None
+    return value if ret_value is True else as_none
 
 
 def coerce_bool(
-    obj: t.Any, errmsg: t.Optional[str] = None, error: bool = True, allow_none: bool = False
+    obj: t.Any,
+    errmsg: t.Optional[str] = None,
+    error: bool = True,
+    allow_none: bool = False,
+    as_none: t.Any = None,
+    as_original: bool = False,
 ) -> bool:
     """Convert an object into bool.
 
     Args:
         obj: object to coerce to bool, will check against
             :data:`axonius_api_client.constants.general.YES` and
             :data:`axonius_api_client.constants.general.NO`
 
     Raises:
-        :exc:`ToolsError`: obj is not able to be converted to bool
+        :exc:`ToolsError`: obj is not bool
     """
-    if allow_none and (obj is None or str(obj).lower().strip() in ["none", "null"]):
-        return None
 
-    def combine(obj):
-        return ", ".join([f"{x!r}" for x in obj])
+    def combine(items):
+        return ", ".join([f"{x!r}" for x in items])
 
     coerce_obj = obj
 
+    if isinstance(obj, bytes):
+        coerce_obj = coerce_obj.decode("utf-8", errors="ignore")
+
     if isinstance(obj, str):
         coerce_obj = coerce_obj.lower().strip()
 
     if coerce_obj in YES:
         return True
 
     if coerce_obj in NO:
         return False
-    if error is True:
-        vtype = trype(obj)
-        msg = listify(errmsg)
-        msg += [
-            f"Supplied value {coerce_obj!r} of type {vtype} must be one of:",
-            f"  For True: {combine(YES)}",
-            f"  For False: {combine(NO)}",
-        ]
-        raise ToolsError("\n".join(msg))
-    return obj
+
+    if not error or (
+        allow_none and (obj is None or str(coerce_obj).lower().strip() in ["none", "null"])
+    ):
+        return obj if as_original else as_none
+
+    vtype = trype(obj)
+    msg = listify(errmsg)
+    msg += [
+        f"Supplied value {coerce_obj!r} of type {vtype} must be one of:",
+        f"  For True: {combine(YES)}",
+        f"  For False: {combine(NO)}",
+    ]
+    raise ToolsError("\n".join(msg))
 
 
 def is_str(value: t.Any, not_empty: bool = True) -> bool:
-    """Check if value is non empty string."""
+    """Check if value is non-empty string."""
     return not (not_empty and not value) if isinstance(value, str) else False
 
 
 def is_email(value: t.Any) -> bool:
     """Check if a value is a valid email."""
     return is_str(value=value, not_empty=True) and bool(EMAIL_RE.fullmatch(value))
 
@@ -419,15 +421,15 @@
     def __init__(self, *args, **kwargs):
         """Pass."""
         self.fallback = kwargs.pop("fallback", None)
         super().__init__(*args, **kwargs)
 
     def default(self, obj):
         """Pass."""
-        if isinstance(obj, datetime):
+        if isinstance(obj, datetime.datetime):
             return obj.isoformat()
 
         if has_to_dict(obj):
             return obj.to_dict()
 
         if callable(getattr(self, "fallback", None)):
             return self.fallback(obj)
@@ -438,15 +440,15 @@
 def has_to_dict(obj: t.Any) -> bool:
     """Pass."""
     return hasattr(obj, "to_dict") and callable(obj.to_dict)
 
 
 def json_dump(
     obj: t.Any,
-    indent: int = 2,
+    indent: t.Optional[int] = 2,
     sort_keys: bool = False,
     error: bool = True,
     fallback: t.Any = str,
     to_dict: bool = True,
     cls: t.Type = AxJSONEncoder,
     **kwargs,
 ) -> t.Any:
@@ -721,60 +723,72 @@
                 fh.close()
         except Exception:
             pass
 
 
 def dt_parse_uuid(
     value: str, default_tz_utc: bool = False, allow_none=False, error: bool = False
-) -> t.Optional[datetime]:
+) -> t.Optional[datetime.datetime]:
     """Parse the date from an object UUID."""
     if allow_none and (value is None or str(value).lower().strip() in ["none", "null"]):
         return None
     try:
         return dt_parse(obj=bson.ObjectId(value).generation_time, default_tz_utc=default_tz_utc)
     except Exception:
         if error:
             raise
         return None
 
 
 def dt_parse(
-    obj: t.Union[str, timedelta, datetime], default_tz_utc: bool = False, allow_none=False
-) -> t.Optional[datetime]:
+    obj: t.Optional[t.Union[TypeDate, t.List[TypeDate]]],
+    default_tz_utc: bool = False,
+    allow_none=False,
+    as_none: t.Any = None,
+    as_tz: t.Optional[t.Union[str, datetime.tzinfo]] = datetime.timezone.utc,
+) -> t.Optional[t.Union[datetime.datetime, t.List[datetime.datetime]]]:
     """Parse a str, datetime, or timedelta into a datetime object.
 
-    Notes:
-        * :obj:`str`: will be parsed into datetime obj
-        * :obj:`datetime.timedelta`: will be parsed into datetime obj as now - timedelta
-        * :obj:`datetime.datetime`: will be re-parsed into datetime obj
-
     Args:
         obj: object or list of objects to parse into datetime
-    """
-    if allow_none and (obj is None or str(obj).lower().strip() in ["none", "null"]):
-        return None
-
-    if isinstance(obj, list) and all([isinstance(x, (str, datetime, timedelta)) for x in obj]):
-        return [dt_parse(obj=x) for x in obj]
+        default_tz_utc: if no timezone is found, assume UTC
+        allow_none: if obj is None or "none" or "null", return None
+        as_none: if allow_none and obj is None, return this value
+        as_tz: if not None, convert to this timezone
+    """
+    if isinstance(obj, list) and all(
+        [isinstance(x, (str, datetime.datetime, datetime.timedelta)) for x in obj]
+    ):
+        return [
+            dt_parse(obj=x, default_tz_utc=default_tz_utc, as_none=as_none, allow_none=allow_none)
+            for x in obj
+        ]
 
-    if isinstance(obj, datetime):
+    if isinstance(obj, datetime.datetime):
         obj = str(obj)
 
-    if isinstance(obj, timedelta):
+    if isinstance(obj, datetime.timedelta):
         obj = str(dt_now() - obj)
 
-    value = dateutil.parser.parse(obj)
-
-    if default_tz_utc and not value.tzinfo:
-        value = value.replace(tzinfo=dateutil.tz.tzutc())
-
+    obj = bytes_to_str(obj)
+    if allow_none and (obj is None or str(obj).lower().strip() in ["none", "null"]):
+        value = as_none
+    else:
+        value = dateutil.parser.parse(obj)
+    if isinstance(value, datetime.datetime):
+        if default_tz_utc and not value.tzinfo:
+            value = value.replace(tzinfo=as_tz)
+        if isinstance(as_tz, datetime.tzinfo):
+            value = value.astimezone(as_tz)
     return value
 
 
-def dt_parse_tmpl(obj: t.Union[str, timedelta, datetime], tmpl: str = "%Y-%m-%d") -> str:
+def dt_parse_tmpl(
+    obj: t.Union[str, datetime.timedelta, datetime.datetime], tmpl: str = "%Y-%m-%d"
+) -> str:
     """Parse a string into the format used by the REST API.
 
     Args:
         obj: date time to parse using :meth:`dt_parse`
         tmpl: strftime template to convert obj into
     """
     valid_fmts = [
@@ -792,55 +806,85 @@
                 f"Could not parse date {obj!r} of type {vtype}"
                 f", try a string in the format of:{valid}"
             )
         )
 
 
 def dt_now(
-    delta: t.Optional[timedelta] = None,
-    tz: timezone = dateutil.tz.tzutc(),
+    delta: t.Optional[datetime.timedelta] = None,
+    tz: t.Optional[datetime.timezone] = datetime.timezone.utc,
 ) -> datetime:
     """Get the current datetime in for a specific tz.
 
     Args:
         delta: convert delta into datetime str instead of returning now
         tz: timezone to return datetime in
     """
-    if isinstance(delta, timedelta):
+    if isinstance(delta, datetime.timedelta):
         return dt_parse(obj=delta)
-    return datetime.now(tz)
+    return datetime.datetime.now(tz)
 
 
 def dt_now_file(fmt: str = FILE_DATE_FMT, **kwargs):
     """Pass."""
     return dt_now(**kwargs).strftime(fmt)
 
 
-def dt_sec_ago(obj: t.Union[str, timedelta, datetime], exact: bool = False) -> int:
+def dt_sec_ago(
+    obj: t.Union[str, datetime.timedelta, datetime.datetime], exact: bool = False, places: int = 2
+) -> int:
     """Get number of seconds ago a given datetime was.
 
     Args:
         obj: parsed by :meth:`dt_parse` into a datetime obj
+        exact: if True, return exact seconds, otherwise round to places
+        places: number of places to round to
     """
     obj = dt_parse(obj=obj)
     now = dt_now(tz=obj.tzinfo)
     value = (now - obj).total_seconds()
-    return value if exact else round(value)
+    return value if exact else round(value, places)
+
+
+def dt_days_ago(
+    obj: t.Union[str, datetime.timedelta, datetime.datetime],
+    error: bool = False,
+    from_now: bool = True,
+) -> t.Optional[int]:
+    """Get number of days ago a given datetime was.
+
+    Args:
+        obj: parsed by :meth:`dt_sec_ago` into seconds ago
+        error: if True, raise any errors, otherwise return None
+        from_now: if True, return days from now, otherwise return days ago
+    """
+    # noinspection PyBroadException
+    try:
+        obj: datetime.datetime = dt_parse(obj=obj)
+        now: datetime.datetime = dt_now(tz=obj.tzinfo)
+        delta: datetime.timedelta = now - obj if from_now else obj - now
+        return delta.days
+    except Exception:
+        if error:
+            raise
+        return None
 
 
-def dt_min_ago(obj: t.Union[str, timedelta, datetime]) -> int:
+def dt_min_ago(obj: t.Union[str, datetime.timedelta, datetime.datetime]) -> int:
     """Get number of minutes ago a given datetime was.
 
     Args:
         obj: parsed by :meth:`dt_sec_ago` into seconds ago
     """
     return round(dt_sec_ago(obj=obj) / 60)
 
 
-def dt_days_left(obj: t.Optional[t.Union[str, timedelta, datetime]]) -> t.Optional[int]:
+def dt_days_left(
+    obj: t.Optional[t.Union[str, datetime.timedelta, datetime.datetime]]
+) -> t.Optional[int]:
     """Get number of days left until a given datetime.
 
     Args:
         obj: parsed by :meth:`dt_sec_ago` into days left
     """
     ret = None
     if obj:
@@ -848,15 +892,15 @@
         now = dt_now(tz=obj.tzinfo)
         seconds = (obj - now).total_seconds()
         ret = round(seconds / 60 / 60 / 24)
     return ret
 
 
 def dt_within_min(
-    obj: t.Union[str, timedelta, datetime],
+    obj: t.Union[str, datetime.timedelta, datetime.datetime],
     n: t.Optional[t.Union[str, int]] = None,
 ) -> bool:
     """Check if given datetime is within the past n minutes.
 
     Args:
         obj: parsed by :meth:`dt_min_ago` into minutes ago
         n: int of :meth:`dt_min_ago` should be greater than or equal to
@@ -1090,15 +1134,15 @@
     do_strip: bool = True,
     lower: bool = True,
     empty: bool = False,
 ) -> t.List[str]:
     """Split a string or list of strings into a list of strings.
 
     Args:
-        obj: string or list of strings to split
+        obj: string or list of strings to `split`
         split: character to split on
         strip: characters to strip
         do_strip: strip each item from the split
         lower: lowercase each item from the split
         empty: remove empty items post split
     """
     if obj is None:
@@ -1129,55 +1173,55 @@
             x = x.strip(strip)
         if not empty and not x:
             continue
         ret.append(x)
     return ret
 
 
-def echo_debug(msg: str, **kwargs):
+def echo_debug(msg: t.Optional[t.Union[str, t.List[str]]] = None, **kwargs):
     """Echo a message to console.
 
     Args:
         msg: message to echo
         kwargs: passed to ``echo``
     """
     kwargs.setdefault("style_args", DEBUG_ARGS)
     kwargs.setdefault("style_tmpl", DEBUG_TMPL)
     kwargs.setdefault("log_level", "debug")
     return echo(msg=msg, **kwargs)
 
 
-def echo_ok(msg: str, **kwargs):
+def echo_ok(msg: t.Optional[t.Union[str, t.List[str]]] = None, **kwargs):
     """Echo a message to console.
 
     Args:
         msg: message to echo
         kwargs: passed to ``echo``
     """
     kwargs.setdefault("style_args", OK_ARGS)
     kwargs.setdefault("style_tmpl", OK_TMPL)
     kwargs.setdefault("log_level", "info")
     return echo(msg=msg, **kwargs)
 
 
-def echo_warn(msg: str, **kwargs):
+def echo_warn(msg: t.Optional[t.Union[str, t.List[str]]] = None, **kwargs):
     """Echo a warning message to console.
 
     Args:
         msg: message to echo
         kwargs: passed to ``echo``
     """
     kwargs.setdefault("style_args", WARN_ARGS)
     kwargs.setdefault("style_tmpl", WARN_TMPL)
     kwargs.setdefault("log_level", "warning")
     kwargs["do_echo"] = True
     return echo(msg=msg, **kwargs)
 
 
-def echo_error(msg: str, abort=True, **kwargs):
+def echo_error(msg: t.Optional[t.Union[str, t.List[str]]] = None, abort: bool = True, **kwargs):
     """Echo an error message to console.
 
     Args:
         msg: message to echo
         abort: call sys.exit(1) after echoing message
         kwargs: passed to ``echo``
     """
@@ -1283,16 +1327,16 @@
     return info
 
 
 def calc_percent(part: t.Union[int, float], whole: t.Union[int, float], places: int = 2) -> float:
     """Calculate the percentage of part out of whole.
 
     Args:
-        part: number to get percent of whole
-        whole: number to caclulate against part
+        part: number to get percent of `whole`
+        whole: number to calculate against `part`
         places: number of decimal places to return
     """
     if 0 in [part, whole]:
         value = 0.00
     elif part > whole:
         value = 100.00
     else:
@@ -1644,26 +1688,37 @@
 
 
 def is_url(value: str) -> bool:
     """Pass."""
     return isinstance(value, str) and any([value.startswith(x) for x in URL_STARTS])
 
 
-def bytes_to_str(value: t.Any, encoding: str = "utf-8", errors: str = "replace") -> t.Any:
+def bytes_to_str(
+    value: t.Any, encoding: str = "utf-8", ignore: bool = False, strict: bool = False
+) -> t.Any:
     """Convert value to str if value is bytes.
 
     Args:
         value (t.Any): value to convert to str
         encoding (str, optional): encoding to use
-        errors (str, optional): how to handle errors
+        ignore (bool, optional): ignore errors instead of replacing them
+        strict (bool, optional): raise an exception if there are decoding errors
 
     Returns:
-        t.Any: str if value is bytes, else orginal value
+        t.Any: str if value is bytes, else original value
     """
-    return value.decode(encoding=encoding, errors=errors) if isinstance(value, bytes) else value
+    if isinstance(value, bytes):
+        if strict:
+            errors = "strict"
+        elif ignore:
+            errors = "ignore"
+        else:
+            errors = "replace"
+        return value.decode(encoding, errors=errors)
+    return value
 
 
 def strip_str(value: t.Any) -> t.Union[str, t.Any]:
     """Strip a value if it is a string."""
     return value.strip() if isinstance(value, str) else value
 
 
@@ -1752,31 +1807,32 @@
     if hasattr(cls, "__module__") and hasattr(cls, "__name__"):
         return f"{cls.__module__}.{cls.__name__}"
 
     return str(value)
 
 
 def csv_writer(
-    rows: t.List[dict],
-    columns: t.Optional[t.List[str]] = None,
+    rows: t.Iterable[dict],
+    columns: t.Optional[t.Iterable[str]] = None,
     quotes: str = "nonnumeric",
     dialect: str = "excel",
     line_ending: str = "\n",
     stream: t.Optional[t.IO] = None,
     key_extra_error: bool = False,
     write_headers: bool = True,
     key_missing_value: t.Optional[t.Any] = None,
 ) -> str:  # pragma: no cover
     """Pass."""
     quotes: int = getattr(csv, f"QUOTE_{quotes.upper()}")
-    if not isinstance(columns, (list, tuple)):
+    if not isinstance(columns, (list, tuple)) and write_headers:
         columns: t.List[str] = []
         # this will kill a generator
-        for row in rows:
-            columns += [x for x in row if x not in columns]
+        if write_headers:
+            for row in rows:
+                columns += [x for x in row if x not in columns]
 
     extrasaction: str = "raise" if key_extra_error else "ignore"
     stream: t.IO = stream if is_file_like(stream) else io.StringIO()
     writer: csv.DictWriter = csv.DictWriter(
         stream,
         fieldnames=columns,
         quoting=quotes,
@@ -1786,15 +1842,15 @@
         extrasaction=extrasaction,
     )
     if write_headers:
         writer.writerow(dict(zip(columns, columns)))
     for row in rows:
         writer.writerow(row)
     stream.seek(0)
-    content: str = stream.getvalue()
+    content: t.Union[str, bytes] = stream.getvalue()
     return content
 
 
 def parse_int_min_max(value, default=0, min_value=None, max_value=None):
     """Pass."""
     if isinstance(value, str) and value.isdigit():
         value = int(value)
@@ -2101,31 +2157,34 @@
 
     Returns:
         t.Any: t.Pattern if value is str or bytes and starts with prefix, else original value
     """
     if isinstance(value, (list, tuple)):
         return [coerce_str_re(value=x, prefix=prefix) for x in value]
 
+    prefix_len: int = len(prefix)
+
     value = bytes_to_str(value=value)
-    return (
-        re.compile(value[1:], re.I) if (is_str(value=value) and value.startswith(prefix)) else value
-    )
+    if is_str(value=value) and value.startswith(prefix):
+        value = re.compile(value[prefix_len:], re.I)
+
+    return value
 
 
 def human_size(
     value: t.Union[int, str, bytes, float], decimals: int = 2, error: bool = True
 ) -> str:
-    """Convert bytes to human readable.
+    """Convert bytes to human-readable.
 
     Args:
         value (t.Union[int, str, bytes, float]): value to coerce into int/float
         decimals (int, optional): number of decimal places to include in str output
 
     Returns:
-        str: human readable size of value if value is int, float, int as str, or float as str
+        str: human-readable size of value if value is int, float, int as str, or float as str
              else empty str
     """
     value = coerce_int_float(value=value, error=error)
     if isinstance(value, (int, float)):
         for size in HUMAN_SIZES:
             if value < 1024.0:
                 return f"{value:0.{decimals}f} {size}"
@@ -2332,43 +2391,67 @@
 
 
 def parse_refresh(
     value: t.Any = REFRESH,
     elapsed: bool = True,
     refresh_elapsed: t.Optional[t.Union[int, float]] = None,
 ) -> bool:
-    """Check if value is True or is int/float and minimum < elapsed >= value."""
-    # if bytes, convert to str
-    value = bytes_to_str(value=value)
+    """Check if an object should be refreshed.
 
-    # try to coerce to bool
-    value = coerce_bool(obj=value, error=False)
-    if isinstance(value, bool):
-        return value
+    Notes:
+        If the value is bytes, it will be converted to string.
+        If the value is string, it will try to safely coerce to float, int, or bool.
+        If the value is True or False it will be returned as is.
+        If the value is a float or int and refresh_elapsed is a float or int and elapsed is True
+        it will return True if the value is greater than refresh_elapsed.
 
-    if elapsed is True and isinstance(refresh_elapsed, (int, float)):
-        # try to coerce to int or float
-        value = coerce_int_float(value=value, error=False, ret_value=True)
-        if isinstance(value, (int, float)) and refresh_elapsed >= value:
-            return True
+    Args:
+        value: The value to check.
+        elapsed: If True, check if the value is an int or float and if it is greater than
+            refresh_elapsed if refresh_elapsed is an int or float.
+        refresh_elapsed: The elapsed time to check against if elapsed is True.
+    """
+    if isinstance(value, bytes):
+        value = value.decode("utf-8", errors="ignore")
 
-    return False
+    if isinstance(value, str):
+        check_value = value.strip().lower()
+        if "." in check_value and check_value.replace(".", "").isdigit():
+            value = float(check_value)
+        elif check_value.isdigit():
+            value = int(check_value)
+        elif check_value in YES_STR:
+            value = True
+        elif check_value in NO_STR:
+            value = False
+
+    if value is True:
+        return True
+    if value is False:
+        return False
+
+    return (
+        elapsed is True
+        and isinstance(value, (int, float))
+        and isinstance(refresh_elapsed, (int, float))
+        and refresh_elapsed >= value
+    )
 
 
 def get_diff_seconds(
-    start: t.Optional[datetime] = None,
-    stop: t.Optional[datetime] = None,
+    start: t.Optional[datetime.datetime] = None,
+    stop: t.Optional[datetime.datetime] = None,
     places: t.Optional[int] = 5,
 ) -> t.Optional[float]:
     """Pass."""
     seconds: t.Optional[float] = None
     if start is not None:
-        start: datetime = dt_parse(obj=start)
-        stop: datetime = dt_now if stop is None else dt_parse(stop)
-        delta: timedelta = stop - start
+        start: datetime.datetime = dt_parse(obj=start)
+        stop: datetime.datetime = dt_now if stop is None else dt_parse(stop)
+        delta: datetime.timedelta = stop - start
         seconds: float = delta.total_seconds()
         if isinstance(places, int):
             seconds: float = float(f"{seconds:.{places}f}")
     return seconds
 
 
 def score_prefix(value: str, prefix: t.Optional[t.List[str]] = None, join: str = "_") -> str:
@@ -2449,7 +2532,128 @@
     """Determine if value is a subclass of the type for this field."""
     if expected_types is None:
         return as_none
     try:
         return issubclass(value, expected_types)
     except TypeError:
         return False
+
+
+def get_query_id(value: t.Optional[t.Union[str, bytes, uuid.UUID]] = None) -> str:
+    """Get or build a query id."""
+    if isinstance(value, uuid.UUID):
+        return str(value)
+    if isinstance(value, (str, bytes)) and value.strip():
+        try:
+            return str(uuid.UUID(bytes_to_str(value)))
+        except ValueError:
+            pass
+    return str(uuid.uuid4())
+
+
+def jdump(obj, **kwargs):
+    """JSON dump utility."""
+    data = json_reload(obj, **kwargs)
+    print(data)
+
+
+def is_subclass_safe(value: t.Any, expected_type: t.Any) -> bool:
+    """Check if value is a subclass of a type."""
+    try:
+        return issubclass(value, expected_type)
+    except TypeError:
+        return False
+
+
+def trim_value_repr(
+    value: t.Any, max_length: t.Optional[int] = 30, trim_post: t.Optional[str] = TRIM_POST
+) -> str:
+    """Trim the value to a maximum length and appends info about the number of trimmed characters.
+
+    Args:
+        value: The input value to be trimmed.
+        max_length: The maximum allowed length of the value repr.
+        trim_post: The string to add
+
+    Returns:
+        str: The trimmed value representation with info about the number of trimmed characters.
+    """
+    value_repr: str = repr(value)
+    if isinstance(max_length, int) and max_length >= 0:
+        original_length: int = len(value_repr)
+        if original_length > max_length:
+            value_repr: str = value_repr[:max_length]
+            modified_length: int = len(value_repr)
+            if isinstance(trim_post, str):
+                removed_length: int = original_length - modified_length
+                locals_dict: t.Dict[str, t.Any] = locals()
+                try:
+                    value_repr += trim_post.format(**locals_dict)
+                except KeyError as exc:
+                    raise FormatError(template=trim_post, error=exc, kwargs=locals_dict)
+    return value_repr
+
+
+def coerce_seconds(value: t.Optional[TypeFloat] = None) -> t.Optional[float]:
+    """Coerce a value to seconds."""
+    if value is None:
+        return None
+
+    if isinstance(value, float):
+        return value
+
+    if isinstance(value, int):
+        return float(value)
+
+    value = bytes_to_str(value)
+    if isinstance(value, str) and value.strip():
+        try:
+            return float(value.strip())
+        except ValueError:
+            raise ValueError(f"Invalid float/integer provided for seconds: {value!r}")
+    return None
+
+
+def coerce_delta(value: t.Optional[TypeDelta] = None) -> t.Optional[datetime.timedelta]:
+    """Coerce a value to a timedelta.
+
+    Args:
+        value: value to coerce - if None or already a timedelta, return as is, otherwise
+            pass to :meth:`coerce_seconds` and return as a timedelta if it returns a float,
+            otherwise return None
+    """
+    if value is None or isinstance(value, datetime.timedelta):
+        return value
+    value = coerce_seconds(value=value)
+    return datetime.timedelta(seconds=value) if isinstance(value, float) else None
+
+
+def coerce_date_delta(
+    value: t.Optional[TypeDate] = None,
+    add: t.Optional[TypeDelta] = None,
+    subtract: t.Optional[TypeDelta] = None,
+) -> t.Optional[datetime.datetime]:
+    """Coerce a value to a date.
+
+    Notes:
+        Value will be parsed by :meth:`dt_parse` into a datetime.datetime or None.
+        If add or subtract are provided, value will be added to or subtracted from
+        the parsed value or now if value not provided.
+
+    Args:
+        value: value to coerce
+        add: seconds to add to value
+        subtract: seconds to subtract from value
+
+    Returns:
+        datetime.datetime: coerced value
+    """
+    parsed: t.Optional[datetime.datetime] = dt_parse(value, allow_none=True)
+    add: t.Optional[datetime.timedelta] = coerce_delta(add)
+    subtract: t.Optional[datetime.timedelta] = coerce_delta(subtract)
+    if isinstance(add, datetime.timedelta):
+        parsed = parsed or dt_now()
+        parsed += add
+    if isinstance(subtract, datetime.timedelta):
+        parsed = parsed or dt_now()
+        parsed -= subtract
+    return parsed
```

## axonius_api_client/version.py

```diff
@@ -1,10 +1,10 @@
 # -*- coding: utf-8 -*-
 """Version information for this package."""
-__version__ = "4.60.4"
+__version__ = "5.0.0"
 VERSION: str = __version__
 """Version of package."""
 
 __url__ = "https://github.com/Axonius/axonius_api_client"
 URL: str = __url__
 """URL of package."""
```

## axonius_api_client/api/__init__.py

```diff
@@ -1,16 +1,17 @@
 # -*- coding: utf-8 -*-
 """API library package."""
 from . import api_endpoints, json_api
 from .adapters import Adapters, Cnx
 from .api_endpoint import ApiEndpoint
 from .api_endpoints import ApiEndpoints
-from .assets import Devices, Runner, Users, Vulnerabilities
+from .assets import Devices, Runner, Users, Vulnerabilities, AssetMixin
 from .enforcements import Enforcements
 from .folders import Folders
+from .mixins import ChildMixins, ModelMixins
 from .openapi import OpenAPISpec
 from .system import (
     ActivityLogs,
     Dashboard,
     DashboardSpaces,
     DataScopes,
     Instances,
@@ -53,8 +54,11 @@
     "ApiEndpoints",
     "ApiEndpoint",
     "json_api",
     "OpenAPISpec",
     "DataScopes",
     "Vulnerabilities",
     "Folders",
+    "ModelMixins",
+    "ChildMixins",
+    "AssetMixin",
 )
```

## axonius_api_client/api/api_endpoint.py

```diff
@@ -26,37 +26,37 @@
 from .json_api.base import BaseModel, BaseSchema, BaseSchemaJson
 
 LOGGER: logging.Logger = logging.getLogger(name=__name__)
 set_log_level(obj=LOGGER, level=LOG_LEVEL_ENDPOINTS)
 
 
 def check_mappings(endpoint: "ApiEndpoint"):
-    """Pass."""
+    """Check that the mappings for an endpoint are valid."""
     model_attrs: t.List[str] = ["request_model_cls", "response_model_cls"]
     schema_attrs: t.List[str] = ["request_schema_cls", "response_schema_cls"]
 
     for attr in model_attrs:
         value = getattr(endpoint, attr)
         check_model_cls(obj=value, src=attr)
 
     for attr in schema_attrs:
         value = getattr(endpoint, attr)
         check_schema_cls(obj=value, src=attr)
 
 
 def check_model_cls(obj: type, src: str):
-    """Pass."""
+    """Check that supplied object is a subclass of BaseModel."""
     invalid = [BaseModel]
     if obj:
         if not inspect.isclass(obj) or obj in invalid or not issubclass(obj, BaseModel):
             raise ValueError(f"{src} {obj} must be a subclass of {BaseModel}")
 
 
 def check_schema_cls(obj: type, src: str):
-    """Pass."""
+    """Check that supplied object is a subclass of BaseSchema."""
     invalid = [BaseSchema, BaseSchemaJson]
     if obj:
         if not inspect.isclass(obj) or obj in invalid or not issubclass(obj, BaseSchema):
             raise ValueError(f"{src} {obj} must be a subclass of {BaseSchema}")
 
 
 @dataclasses.dataclass(eq=True, frozen=True)
@@ -91,22 +91,23 @@
     """Do not serialize the request object when sending the request."""
 
     response_as_text: bool = False
     """Do not serialize the response object when receiving the response."""
 
     response_json_error: bool = True
     """Throw errors if the JSON can not be serialized."""
+    log: t.ClassVar[logging.Logger] = LOGGER.getChild("ApiEndpoint")
 
     def __str__(self):
         """Get a pretty str for this object."""
         items = "\n  " + ",\n  ".join(self.str_properties) + ",\n"
         return f"{self.__class__.__name__}({items})"
 
     def __post_init__(self):
-        """Pass."""
+        """Dataclass post init."""
         super().__setattr__("log", LOGGER.getChild(self.__class__.__name__))
         check_mappings(endpoint=self)
 
     @property
     def str_properties(self) -> t.List[str]:
         """Get the properties for this endpoint as a list of strs."""
         return [
@@ -115,101 +116,121 @@
             f"request_model={get_cls_path(self.request_model_cls)}",
             f"response_schema={get_cls_path(self.response_schema_cls)}",
             f"response_model={get_cls_path(self.response_model_cls)}",
         ]
 
     def perform_request(
         self, http: Http, request_obj: t.Optional[BaseModel] = None, raw: bool = False, **kwargs
-    ) -> t.Union[BaseModel, JSON_TYPES, t.Any]:
-        """Perform a request to this endpoint using an http object.
+    ) -> t.Any:
+        """Perform a request to this endpoint using a http object.
 
         Args:
             http (Http): HTTP object to use to send request
             request_obj (t.Optional[BaseModel], optional): dataclass containing
                 object to serialize for the request
             raw (bool): return the raw requests.Response object
             **kwargs: passed to :meth:`perform_request_raw` and :meth:`handle_response`
 
         Returns:
-            t.Union[BaseModel, JSON_TYPES]: the data loaded from the response received
+            the data loaded from the response received
         """
         self.log.debug(f"{self!r} Performing request with request_obj type {type(request_obj)}")
-        kwargs["response"] = response = self.perform_request_raw(
+        response: requests.Response = self.perform_request_raw(
             http=http, request_obj=request_obj, **kwargs
         )
         self.log.debug(f"{self!r} Received response {response}")
+        kwargs["response"] = response
         return response if raw else self.handle_response(http=http, **kwargs)
 
     def perform_request_raw(
         self, http: Http, request_obj: t.Optional[BaseModel] = None, **kwargs
-    ) -> t.Union[BaseModel, JSON_TYPES]:
-        """Perform a request to this endpoint using an http object.
+    ) -> requests.Response:
+        """Perform a request to this endpoint using a http object.
 
         Args:
             http (Http): HTTP object to use to send request
             request_obj (t.Optional[BaseModel], optional): dataclass containing
                 object to serialize for the request
             **kwargs: passed to :meth:`get_http_args` and :meth:`Http.__call__`
 
         Returns:
-            t.Union[BaseModel, JSON_TYPES]: the data loaded from the response received
+            requests.Response: the response received
         """
-        http_args = self.get_http_args(request_obj=request_obj, **kwargs)
-        response = http(**http_args)
-        return response
+        return http(**self.get_http_args(request_obj=request_obj, **kwargs))
 
-    def load_request(self, **kwargs) -> t.Union[BaseModel, dict, None]:
+    def load_request(
+        self,
+        remove_unknown_arguments: bool = False,
+        warn_unknown_arguments: bool = False,
+        reraise: bool = RERAISE,
+        **kwargs,
+    ) -> t.Any:
         """Create a dataclass for a request_obj to send using :meth:`perform_request`.
 
         Args:
+            remove_unknown_arguments (bool, optional): remove unknown arguments from kwargs
+            warn_unknown_arguments (bool, optional): warn about unknown arguments in kwargs
+            reraise (bool, optional): reraise exceptions
             **kwargs: passed to :meth:`BaseModel.load_request` if :attr:`request_model_cls`
                 is set
 
         Returns:
             t.Union[BaseModel, dict, None]: Loaded dataclass object or dict of kwargs or None if
                 kwargs empty
         """
         load_cls = self.request_load_cls
+
         ret = kwargs or None
         if load_cls:
+            args_cleaner = getattr(load_cls, "remove_unknown_arguments", False)
+            if callable(args_cleaner):
+                kwargs, _ = args_cleaner(
+                    kwargs=kwargs,
+                    remove_unknown_arguments=remove_unknown_arguments,
+                    warn_unknown_arguments=warn_unknown_arguments,
+                )
             self.log.debug(f"{self!r} Loading request with load_cls {load_cls} kwargs {kwargs}")
             try:
                 ret = load_cls.load_request(**kwargs)
             except Exception as exc:
+                if reraise:
+                    raise
                 err = "Failed to load request object"
                 details = [f"cls: {load_cls}", f"kwargs: {json_log(kwargs)}"]
                 raise RequestLoadObjectError(api_endpoint=self, err=err, details=details, exc=exc)
 
             self.log.debug(f"{self!r} Loaded request into {load_cls}")
         return ret
 
     def load_response(
-        self, data: dict, http: Http, unloaded: bool = False, reraise: bool = RERAISE, **kwargs
+        self, data: dict, http: Http, unloaded: bool = False, **kwargs
     ) -> t.Union[BaseModel, JSON_TYPES]:
         """Load the response data into a dataclass model object.
 
         Args:
             data (dict): JSON data received from response
             http (Http): HTTP object used to receive response
             unloaded (bool): return the data without loading it into a dataclass model
             **kwargs: passed to :meth:`BaseSchema.load_response` or :meth:`BaseModel.load_response`
 
         Returns:
             t.Union[BaseModel, JSON_TYPES]: Loaded dataclass model or JSON data
         """
+        kwargs["reraise"] = kwargs.get("reraise", RERAISE)
+
         if not unloaded:
             load_cls = self.response_load_cls
             if load_cls:
                 self.log.debug(
                     f"{self!r} Loading response with data type {type(data)}, load_cls={load_cls}"
                 )
                 try:
                     data = load_cls.load_response(data=data, http=http, **kwargs)
                 except Exception as exc:
-                    if reraise:
+                    if kwargs["reraise"]:
                         raise
                     err = "Failed to load response object"
                     details = [f"load_cls: {load_cls}"]
                     raise ResponseLoadObjectError(
                         api_endpoint=self, err=err, details=details, exc=exc
                     )
 
@@ -228,52 +249,57 @@
 
     def handle_response(
         self, http: Http, response: requests.Response, **kwargs
     ) -> t.Union[BaseModel, JSON_TYPES]:
         """Get the response data.
 
         Args:
-            http (Http): HTTP object used to receive response
+            http (Http): HTTP object used to receive `response`
             response (requests.Response): response to handle
             **kwargs: passed to :meth:`get_response_json` and :meth:`load_response`
 
         Returns:
             t.Union[BaseModel, JSON_TYPES]: Loaded dataclass model or JSON data
 
         """
         if self.response_as_text:
             data = response.text
             self.check_response_status(http=http, response=response, **kwargs)
         else:
-            data = self.get_response_json(response=response)
+            data = self.get_response_json(response=response, **kwargs)
             self.check_response_status(http=http, response=response, **kwargs)
             data = self.load_response(
                 http=http, response=response, **combo_dicts(kwargs, data=data)
             )
+            # noinspection PyBroadException
             try:
                 setattr(data, "RESPONSE", response)
             except Exception:
                 pass
         return data
 
-    def get_response_json(self, response: requests.Response) -> JSON_TYPES:
+    def get_response_json(self, response: requests.Response, **kwargs) -> JSON_TYPES:
         """Get the JSON from a response.
 
         Args:
             response (requests.Response): response to handle
 
         Raises:
             JsonInvalidError: if response can not be deserialized from JSON
 
         Returns:
             JSON_TYPES: deserialized JSON from response
         """
+        reraise = kwargs.get("reraise", RERAISE)
+
         try:
             return response.json()
         except Exception as exc:
+            if reraise:
+                raise
             msg = f"Response has invalid JSON\nWhile in {self}"
             if self.response_json_error:
                 raise JsonInvalidError(msg=msg, response=response, exc=exc)
             return response.text
 
     def check_response_status(
         self,
@@ -281,29 +307,31 @@
         response: requests.Response,
         response_status_hook: t.Optional[callable] = None,
         **kwargs,
     ):
         """Check the status code of a response.
 
         Args:
-            http (Http): HTTP object used to receive response
+            http (Http): HTTP object used to receive `response`
             response (requests.Response): response to handle
             response_status_hook (t.Optional[callable], optional): callable to perform
                 extra checks of response status that takes args: http, response, kwargs
             **kwargs: Passed to `response_status_hook` if supplied, if hook returns truthy
                 no more status checks are done
 
         Notes:
             If response_status_hook returns True, the rest of the check_response_status
             workflow will be skipped
 
         Raises:
-            InvalidCredentials: if response has has a 401 status code
+            InvalidCredentials: if response has a 401 status code
             ResponseNotOk: if response has a bad status code
         """
+        kwargs.setdefault("reraise", RERAISE)
+
         if callable(response_status_hook):
             hook_ret = response_status_hook(http=http, response=response, **kwargs)
             if hook_ret is True:
                 return
 
         msgs = [
             f"Response has a bad HTTP status code: {response.status_code}",
@@ -313,14 +341,16 @@
         if response.status_code == 401:
             msgs.append("Invalid credentials")
             raise InvalidCredentials(msg="\n".join(msgs), response=response)
 
         try:
             response.raise_for_status()
         except Exception as exc:
+            if kwargs["reraise"]:
+                raise
             raise ResponseNotOk(msg="\n".join(msgs), response=response, exc=exc)
 
     def get_http_args(
         self,
         request_obj: t.Optional[BaseModel] = None,
         http_args: t.Optional[dict] = None,
         **kwargs,
@@ -332,98 +362,105 @@
             http_args (t.Optional[dict], optional): Additional arguments to add
             **kwargs: passed to :meth:`dump_path` and :meth:`dump_object`
 
         Returns:
             dict: The arguments to make the request using :obj:`Http`.
         """
         self.check_request_obj(request_obj=request_obj)
-        args = {}
-        args["method"] = self.method
-        args["path"] = self.dump_path(request_obj=request_obj, http_args=http_args, **kwargs)
-        args.update(self.dump_object(request_obj=request_obj, **kwargs))
-        args.update(self.http_args or {})
-        args.update(http_args or {})
+        data_args: dict = self.dump_object(request_obj=request_obj, **kwargs)
+        path: str = self.dump_path(request_obj=request_obj, http_args=http_args, **kwargs)
+        base_args: dict = dict(path=path, method=self.method)
+        args: dict = combo_dicts(self.http_args, data_args, base_args, http_args)
         self.check_missing_args(args=args)
         return args
 
-    def _get_dump_object_method(
-        self, request_obj: t.Optional[BaseModel] = None, **kwargs
-    ) -> t.Tuple[str, callable]:
+    def dump_object(self, request_obj: t.Any = None, **kwargs) -> dict:
+        """Dump a request object to a python object.
+
+        Args:
+            request_obj (t.Any, optional): dataclass model to serialize for request
+            **kwargs: passed to dump_method
+
+        Returns:
+            dict: dict with 'json' or 'params' keys to send to :meth:`Http.__call__`.
+        """
+        data: dict = {}
+        if request_obj and not self.request_as_none:
+            data_key, dump_method = self._get_dump_method(request_obj=request_obj)
+            data[data_key] = self._call_dump_method(
+                **combo_dicts(kwargs, dump_method=dump_method, request_obj=request_obj)
+            )
+        return data
+
+    # noinspection PyUnusedLocal
+    def _get_dump_method(
+        self, request_obj: t.Optional[BaseModel] = None
+    ) -> t.Tuple[str, t.Optional[callable]]:
         """Get the method that should be used to dump a model and the arg for the request."""
-        if self.method == "get" and callable(getattr(request_obj, "dump_request_params", None)):
-            dump_method = request_obj.dump_request_params
+        dump_get: t.Optional[callable] = getattr(request_obj, "dump_request_params", None)
+        dump_post: t.Optional[callable] = getattr(request_obj, "dump_request", None)
+        key: str = ""
+        dump_method: t.Optional[callable] = None
+        if self.method == "get" and callable(dump_get):
+            dump_method = dump_get
             key = "params"
-        elif callable(getattr(request_obj, "dump_request", None)):
-            dump_method = request_obj.dump_request
+        elif callable(dump_post):
+            dump_method = dump_post
             key = "json"
-        else:
+        elif request_obj:
             err = f"Request object does not have dump_request methods: {request_obj!r}"
             details = [
                 f"request_obj type: {type(request_obj)}",
                 f"request_obj: {request_obj!r}",
             ]
             raise RequestFormatObjectError(api_endpoint=self, err=err, details=details)
         return key, dump_method
 
-    def _call_dump_object_method(
+    def _call_dump_method(
         self, dump_method: callable, request_obj: t.Optional[BaseModel] = None, **kwargs
     ) -> dict:
         """Pass."""
-        kwargs = combo_dicts(kwargs, schema_cls=self.request_schema_cls)
+        kwargs.setdefault("reraise", RERAISE)
+        kwargs.setdefault("schema_cls", self.request_schema_cls)
+
         try:
             return dump_method(**kwargs)
         except Exception as exc:
+            if kwargs["reraise"]:
+                raise
             err = f"Request formatting failed for object: {request_obj!r}"
             details = [
                 f"dump_method: {dump_method}",
                 f"kwargs: {json_log(kwargs)}",
                 f"request_obj: {request_obj!r}",
             ]
             raise RequestFormatObjectError(api_endpoint=self, err=err, details=details, exc=exc)
 
-    def dump_object(self, request_obj: t.Optional[BaseModel] = None, **kwargs) -> dict:
-        """Serialize a dataclass model for a request.
-
-        Args:
-            request_obj (t.Optional[BaseModel], optional): dataclass model to serialize for request
-            **kwargs: passed to :meth:`BaseModel.dump_request_params` if method is GET, else
-                passed to :meth:`BaseModel.dump_request`
-
-        Returns:
-            dict: dict with 'json' or 'params' key to send to :meth:`Http.__call__`.
-
-        Raises:
-            RequestFormatObjectError: if dumping the request_obj fails
-        """
-        ret = {}
-        if request_obj and not self.request_as_none:
-            key, dump_method = self._get_dump_object_method(request_obj=request_obj, **kwargs)
-            ret[key] = self._call_dump_object_method(
-                dump_method=dump_method, request_obj=request_obj, **kwargs
-            )
-        return ret
-
     def dump_path(self, request_obj: t.Optional[BaseModel] = None, **kwargs) -> str:
         """Get the path to use for this endpoint.
 
         Args:
             request_obj (t.Optional[BaseModel], optional): dataclass model used as part
                 of the string formatting for :attr:`path`
             **kwargs: Used as part of the string formatting for :attr:`path`
 
         Returns:
             str: formatted string of :attr:`path`
         """
-        kwargs = combo_dicts(kwargs, path=self.path)
+        kwargs.setdefault("path", self.path)
+        kwargs.setdefault("reraise", RERAISE)
+
         cls_dump = getattr(request_obj, "dump_request_path", None)
         method = cls_dump if callable(cls_dump) else self.path.format
 
         try:
             return method(**kwargs)
         except Exception as exc:
+            if kwargs["reraise"]:
+                raise
             err = f"Request formatting failed for path: {self.path!r}"
             details = [
                 f"method: {method}",
                 f"kwargs: {json_log(kwargs)}",
                 f"request_obj: {request_obj!r}",
             ]
             raise RequestFormatPathError(api_endpoint=self, err=err, details=details, exc=exc)
@@ -448,14 +485,15 @@
 
         Args:
             request_obj (BaseModel): request object to check
 
         Raises:
             RequestObjectTypeError: if request_obj is not an instance of :attr:`request_model_cls`
         """
-        if self.request_model_cls and not isinstance(request_obj, self.request_model_cls):
-            err = f"Request object must be of type {self.request_model_cls!r}"
-            details = [
-                f"Request object type supplied: {type(request_obj)}",
-                f"Request object supplied: {request_obj!r}",
-            ]
-            raise RequestObjectTypeError(api_endpoint=self, err=err, details=details)
+        if self.request_model_cls and not self.request_as_none:
+            if not isinstance(request_obj, self.request_model_cls):
+                err = f"Request object must be of type {self.request_model_cls!r}"
+                details = [
+                    f"Request object type supplied: {type(request_obj)}",
+                    f"Request object supplied: {request_obj!r}",
+                ]
+                raise RequestObjectTypeError(api_endpoint=self, err=err, details=details)
```

## axonius_api_client/api/api_endpoints.py

```diff
@@ -1,38 +1,61 @@
 # -*- coding: utf-8 -*-
-"""Models for API requests & responses."""
+"""Containers for API Endpoint definitions."""
 import dataclasses
-from typing import Dict
+import typing as t
 
 from ..data import BaseData
 from . import json_api
 from .api_endpoint import ApiEndpoint
 
 
 class ApiEndpointGroup(BaseData):
-    """Pass."""
+    """Container for API endpoint definitions."""
 
     @classmethod
-    def get_endpoints(cls) -> Dict[str, ApiEndpoint]:
-        """Pass."""
-        return {x.name: x.default for x in cls.get_fields()}
+    def get_subgroups(cls, recursive: bool = False) -> t.Dict[str, "ApiEndpointGroup"]:
+        """Get all subgroups defined in this group."""
+        groups: t.Dict[str, "ApiEndpointGroup"] = {
+            x.name: x.default for x in cls.get_fields() if isinstance(x.default, ApiEndpointGroup)
+        }
+        if recursive:
+            for group_name, group in list(groups.items()):
+                for subgroup_name, subgroup in group.get_subgroups(recursive=recursive).items():
+                    groups[f"{group_name}.{subgroup_name}"] = subgroup
+        return groups
+
+    @classmethod
+    def get_endpoints(cls, recursive: bool = False) -> t.Dict[str, ApiEndpoint]:
+        """Get all endpoints defined in this group."""
+        endpoints: t.Dict[str, ApiEndpoint] = {
+            x.name: x.default for x in cls.get_fields() if isinstance(x.default, ApiEndpoint)
+        }
+        if recursive:
+            for group_name, group in cls.get_subgroups(recursive=recursive).items():
+                for endpoint_name, endpoint in group.get_endpoints(recursive=recursive).items():
+                    endpoints[f"{group_name}.{endpoint_name}"] = endpoint
+        return endpoints
 
     def __str__(self):
         """Pass."""
-        names = [x.name for x in self.get_fields()]
-        return f"{self.__class__.__name__}(endpoints={names})"
+        items: t.List[str] = [
+            f"endpoints={list(self.get_endpoints())}",
+            f"subgroups={list(self.get_subgroups())}",
+        ]
+        items: str = ", ".join(items)
+        return f"{self.__class__.__name__}({items})"
 
     def __repr__(self):
         """Pass."""
         return self.__str__()
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class DashboardSpaces(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with dashboard spaces."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/dashboard",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.dashboard_spaces.SpacesDetailsSchema,
@@ -84,106 +107,107 @@
         response_schema_cls=json_api.dashboard_spaces.ExportableSpacesResponseSchema,
         response_model_cls=json_api.dashboard_spaces.ExportableSpacesResponse,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Assets(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with assets."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/{asset_type}",
+        path="api/{asset_type}",
         request_schema_cls=json_api.assets.AssetRequestSchema,
         request_model_cls=json_api.assets.AssetRequest,
         response_schema_cls=None,
         response_model_cls=json_api.assets.AssetsPage,
     )
     # PBUG: include_notes=True ignored if fields are specified
 
     get_by_id: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/{asset_type}/{internal_axon_id}",
-        request_schema_cls=None,
-        request_model_cls=None,
-        response_schema_cls=None,
+        path="api/{asset_type}/{internal_axon_id}",
+        request_schema_cls=json_api.assets.AssetByIdRequestSchema,
+        request_model_cls=json_api.assets.AssetByIdRequest,
+        response_schema_cls=json_api.assets.AssetByIdSchema,
         response_model_cls=json_api.assets.AssetById,
+        request_as_none=True,
     )
     # loose model!
 
     count: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/{asset_type}/count",
+        path="api/{asset_type}/count",
         request_schema_cls=json_api.assets.CountRequestSchema,
         request_model_cls=json_api.assets.CountRequest,
-        response_schema_cls=None,
+        response_schema_cls=json_api.assets.CountSchema,
         response_model_cls=json_api.assets.Count,
     )
     # PBUG: returns None until celery finished, want a blocking return until celery returns
 
     fields: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/{asset_type}/fields",
+        path="api/{asset_type}/fields",
         request_schema_cls=None,
         request_model_cls=None,
-        response_schema_cls=json_api.generic.MetadataSchema,
-        response_model_cls=json_api.generic.Metadata,
+        response_schema_cls=json_api.assets.FieldsSchema,
+        response_model_cls=json_api.assets.Fields,
     )
 
     destroy: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/{asset_type}/destroy",
-        request_schema_cls=None,
+        path="api/{asset_type}/destroy",
+        request_schema_cls=json_api.assets.DestroyRequestSchema,
         request_model_cls=json_api.assets.DestroyRequest,
-        response_schema_cls=json_api.generic.MetadataSchema,
-        response_model_cls=json_api.generic.Metadata,
+        response_schema_cls=json_api.assets.DestroySchema,
+        response_model_cls=json_api.assets.Destroy,
     )
     # PBUG: returns 403 status code "You are lacking some permissions for this request"
     # PBUG: REST API0: historical_prefix hardcoded to 'historical_users_'
     # PBUG: request not modeled
 
     tags_get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/{asset_type}/labels",
+        path="api/{asset_type}/labels",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.generic.StrValueSchema,
         response_model_cls=json_api.generic.StrValue,
     )
 
     tags_get_expirable_names: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/{asset_type}/expirable_tags_names",
+        path="api/{asset_type}/expirable_tags_names",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.generic.StrValueSchema,
         response_model_cls=json_api.generic.StrValue,
     )
 
     tags_add: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/{asset_type}/labels",
-        request_schema_cls=json_api.assets.ModifyTagsSchema,
-        request_model_cls=json_api.assets.ModifyTags,
-        response_schema_cls=json_api.generic.IntValueSchema,
-        response_model_cls=json_api.generic.IntValue,
+        path="api/{asset_type}/labels",
+        request_schema_cls=json_api.assets.ModifyTagsRequestSchema,
+        request_model_cls=json_api.assets.ModifyTagsRequest,
+        response_schema_cls=json_api.assets.ModifyTagsSchema,
+        response_model_cls=json_api.assets.ModifyTags,
     )
 
     tags_remove: ApiEndpoint = ApiEndpoint(
         method="delete",
-        path="api/V4.0/{asset_type}/labels",
-        request_schema_cls=json_api.assets.ModifyTagsSchema,
-        request_model_cls=json_api.assets.ModifyTags,
-        response_schema_cls=json_api.generic.IntValueSchema,
-        response_model_cls=json_api.generic.IntValue,
+        path="api/{asset_type}/labels",
+        request_schema_cls=json_api.assets.ModifyTagsRequestSchema,
+        request_model_cls=json_api.assets.ModifyTagsRequest,
+        response_schema_cls=json_api.assets.ModifyTagsSchema,
+        response_model_cls=json_api.assets.ModifyTags,
     )
 
     history_dates: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/dashboard/get_allowed_dates",
+        path="api/dashboard/get_allowed_dates",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.assets.HistoryDatesSchema,
         response_model_cls=json_api.assets.HistoryDates,
     )
 
     run_enforcement: ApiEndpoint = ApiEndpoint(
@@ -195,15 +219,15 @@
         response_model_cls=None,
         response_as_text=True,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class FoldersQueries(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with folders for saved queries."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/queries/folders",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.folders.queries.FoldersSchema,
@@ -245,15 +269,15 @@
         response_schema_cls=json_api.folders.queries.MoveFolderResponseSchema,
         response_model_cls=json_api.folders.queries.MoveFolderResponseModel,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class FoldersEnforcements(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with folders for enforcements."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/enforcements_folders",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.folders.enforcements.FoldersSchema,
@@ -295,15 +319,15 @@
         response_schema_cls=json_api.folders.enforcements.MoveFolderResponseSchema,
         response_model_cls=json_api.folders.enforcements.MoveFolderResponseModel,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class SavedQueries(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with saved queries."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/queries/saved",
         request_schema_cls=json_api.saved_queries.SavedQueryGetSchema,
         request_model_cls=json_api.saved_queries.SavedQueryGet,
         response_schema_cls=json_api.saved_queries.SavedQuerySchema,
@@ -382,95 +406,95 @@
         response_schema_cls=json_api.saved_queries.SavedQuerySchema,
         response_model_cls=json_api.saved_queries.SavedQuery,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Instances(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with Axonius instances."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/instances",
+        path="api/instances",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.instances.InstanceSchema,
         response_model_cls=json_api.instances.Instance,
     )
 
     delete: ApiEndpoint = ApiEndpoint(
         method="delete",
-        path="api/V4.0/instances",
+        path="api/instances",
         request_schema_cls=None,
         request_model_cls=json_api.instances.InstanceDeleteRequest,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
     # TBUG: need testrail integration to automate tests
 
     update_attrs: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/instances",
+        path="api/instances",
         request_schema_cls=None,
         request_model_cls=json_api.instances.InstanceUpdateAttributesRequest,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
     update_active: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/instances",
+        path="api/instances",
         request_schema_cls=None,
         request_model_cls=json_api.instances.InstanceUpdateActiveRequest,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
     # TBUG: need testrail integration to automate tests
 
     factory_reset: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/factory_reset",
+        path="api/factory_reset",
         request_schema_cls=json_api.instances.FactoryResetRequestSchema,
         request_model_cls=json_api.instances.FactoryResetRequest,
         response_schema_cls=json_api.instances.FactoryResetSchema,
         response_model_cls=json_api.instances.FactoryReset,
     )
 
     admin_script_upload_start: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/configuration/upload_file",
+        path="api/settings/configuration/upload_file",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
 
     admin_script_upload_chunk: ApiEndpoint = ApiEndpoint(
         method="patch",
-        path="api/V4.0/settings/configuration/upload_file?patch={uuid}",
+        path="api/settings/configuration/upload_file?patch={uuid}",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
 
     admin_script_execute: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/configuration/execute/{uuid}",
+        path="api/settings/configuration/execute/{uuid}",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
 
@@ -503,85 +527,85 @@
     )
     # PBUG: not modeled in any way shape or form
     # PBUG: returns string 'Tunnel is not enabled on system' if FF: enable_saas is False
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class CentralCore(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with Axonius Central Cores."""
 
     settings_get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/central_core",
+        path="api/settings/central_core",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.system_settings.SystemSettingsSchema,
         response_model_cls=json_api.system_settings.SystemSettings,
     )
 
     settings_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/central_core",
+        path="api/settings/central_core",
         request_schema_cls=json_api.central_core.CentralCoreSettingsUpdateSchema,
         request_model_cls=json_api.central_core.CentralCoreSettingsUpdate,
         response_schema_cls=json_api.system_settings.SystemSettingsSchema,
         response_model_cls=json_api.system_settings.SystemSettings,
     )
 
     restore_aws: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/settings/central_core/restore",
+        path="api/settings/central_core/restore",
         request_schema_cls=json_api.central_core.CentralCoreRestoreAwsRequestSchema,
         request_model_cls=json_api.central_core.CentralCoreRestoreAwsRequest,
         response_schema_cls=json_api.central_core.CentralCoreRestoreSchema,
         response_model_cls=json_api.central_core.CentralCoreRestore,
         http_args={"response_timeout": 3600},
     )
     # PBUG: need other restore types added eventually
     # TBUG: need testrail integration to automate tests
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class SystemSettings(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with Axonius system settings."""
 
     # PBUG: schema differences between settings update and get
     # PBUG: no configName returned in get
     # PBUG: update request expects configName and pluginId, which is not returned by get
     # PBUG: update response returns config_name and pluginId, which are not returned by get
     settings_get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/plugins/{plugin_name}/{config_name}",
+        path="api/settings/plugins/{plugin_name}/{config_name}",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.system_settings.SystemSettingsSchema,
         response_model_cls=json_api.system_settings.SystemSettings,
     )
 
     settings_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/plugins/{plugin_name}/{config_name}",
+        path="api/settings/plugins/{plugin_name}/{config_name}",
         request_schema_cls=json_api.system_settings.SystemSettingsUpdateSchema,
         request_model_cls=json_api.system_settings.SystemSettingsUpdate,
         response_schema_cls=json_api.system_settings.SystemSettingsSchema,
         response_model_cls=json_api.system_settings.SystemSettings,
     )
 
     feature_flags_get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/plugins/gui/FeatureFlags",
+        path="api/settings/plugins/gui/FeatureFlags",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.system_settings.FeatureFlagsSchema,
         response_model_cls=json_api.system_settings.FeatureFlags,
     )
 
     meta_about: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/meta/about",
+        path="api/settings/meta/about",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.system_meta.SystemMetaSchema,
         response_model_cls=None,
     )
     meta_about2: ApiEndpoint = ApiEndpoint(
         method="get",
@@ -599,277 +623,286 @@
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
     )
 
     historical_sizes: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/historical_sizes",
+        path="api/settings/historical_sizes",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
     )
     # PBUG: response is not jsonapi model
 
     file_upload: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/plugins/{plugin}/upload_file",
+        path="api/settings/plugins/{plugin}/upload_file",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.generic.ApiBaseSchema,
         response_model_cls=json_api.generic.ApiBase,
         http_args_required=["files", "data"],
     )
 
     cert_uploaded: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/certificate/global_ssl",
+        path="api/certificate/global_ssl",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
     )
 
     gui_cert_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/certificate/global_ssl",
+        path="api/certificate/global_ssl",
         request_schema_cls=None,
         request_model_cls=json_api.system_settings.CertificateUpdateRequest,
         response_schema_cls=None,
         response_model_cls=None,
     )
     # PBUG: not modeled (not even anything, just returns "True")
 
     gui_cert_info: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/certificate/details",
+        path="api/certificate/details",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.system_settings.CertificateDetailsSchema,
         response_model_cls=json_api.system_settings.CertificateDetails,
     )
 
     gui_cert_reset: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/certificate/reset_to_defaults",
+        path="api/certificate/reset_to_defaults",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.generic.BoolValueSchema,
         response_model_cls=json_api.generic.BoolValue,
     )
     # PBUG: bool value useless to return here, return cert details or something at least
 
     cert_settings: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/certificate/certificate_settings",
+        path="api/certificate/certificate_settings",
         request_schema_cls=json_api.system_settings.CertificateConfigSchema,
         request_model_cls=json_api.system_settings.CertificateConfig,
         response_schema_cls=json_api.generic.BoolValueSchema,
         response_model_cls=json_api.generic.BoolValue,
     )
     # PBUG: dicts not modeled
     # PBUG: bool value useless
 
     csr_get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/certificate/csr",
+        path="api/certificate/csr",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     csr_create: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/certificate/csr",
+        path="api/certificate/csr",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.generic.BoolValueSchema,
         response_model_cls=json_api.generic.BoolValue,
         http_args_required=["json"],
     )
 
     csr_cancel: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/certificate/cancel_csr",
+        path="api/certificate/cancel_csr",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.generic.BoolValueSchema,
         response_model_cls=json_api.generic.BoolValue,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class RemoteSupport(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with remote support."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/maintenance",
+        path="api/settings/maintenance",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.remote_support.RemoteSupportSchema,
         response_model_cls=json_api.remote_support.RemoteSupport,
     )
     # PBUG: response is not jsonapi model
 
     temporary_enable: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/settings/maintenance",
+        path="api/settings/maintenance",
         request_schema_cls=json_api.remote_support.UpdateTemporaryRequestSchema,
         request_model_cls=json_api.remote_support.UpdateTemporaryRequest,
         response_schema_cls=None,
         response_model_cls=json_api.remote_support.UpdateTemporaryResponse,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
     temporary_disable: ApiEndpoint = ApiEndpoint(
         method="delete",
-        path="api/V4.0/settings/maintenance",
+        path="api/settings/maintenance",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: response is not jsonapi model
 
     permanent_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/maintenance",
+        path="api/settings/maintenance",
         request_schema_cls=json_api.remote_support.UpdatePermanentRequestSchema,
         request_model_cls=json_api.remote_support.UpdatePermanentRequest,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
     analytics_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/maintenance",
+        path="api/settings/maintenance",
         request_schema_cls=json_api.remote_support.UpdateAnalyticsRequestSchema,
         request_model_cls=json_api.remote_support.UpdateAnalyticsRequest,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
     troubleshooting_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/maintenance",
+        path="api/settings/maintenance",
         request_schema_cls=json_api.remote_support.UpdateTroubleshootingRequestSchema,
         request_model_cls=json_api.remote_support.UpdateTroubleshootingRequest,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class SystemUsers(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with Axonius user accounts."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/users",
+        path="api/settings/users",
         request_schema_cls=json_api.resources.ResourcesGetSchema,
         request_model_cls=json_api.resources.ResourcesGet,
         response_schema_cls=json_api.system_users.SystemUserSchema,
         response_model_cls=json_api.system_users.SystemUser,
     )
 
     create: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/settings/users",
+        path="api/settings/users",
         request_schema_cls=json_api.system_users.SystemUserCreateSchema,
         request_model_cls=json_api.system_users.SystemUserCreate,
         response_schema_cls=json_api.system_users.SystemUserSchema,
         response_model_cls=json_api.system_users.SystemUser,
     )
 
     delete: ApiEndpoint = ApiEndpoint(
         method="delete",
-        path="api/V4.0/settings/users/{uuid}",
-        request_schema_cls=None,
+        path="api/settings/users/{uuid}",
+        request_schema_cls=json_api.resources.ResourceDeleteSchema,
         request_model_cls=json_api.resources.ResourceDelete,
         response_schema_cls=json_api.generic.MetadataSchema,
         response_model_cls=json_api.generic.Metadata,
     )
 
     update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/users/{uuid}",
+        path="api/settings/users/{uuid}",
         request_schema_cls=json_api.system_users.SystemUserUpdateSchema,
         request_model_cls=json_api.system_users.SystemUserUpdate,
         response_schema_cls=json_api.system_users.SystemUserSchema,
         response_model_cls=json_api.system_users.SystemUser,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class PasswordReset(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with password resets."""
 
     create: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/settings/users/tokens/generate",
+        path="api/settings/users/tokens/generate",
         request_schema_cls=None,
         request_model_cls=json_api.password_reset.CreateRequest,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
     send: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/settings/users/tokens/notify",
+        path="api/settings/users/tokens/notify",
         request_schema_cls=None,
         request_model_cls=json_api.password_reset.SendRequest,
         response_schema_cls=None,
         response_model_cls=json_api.password_reset.SendResponse,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
     validate: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/users/tokens/validate/{token}",
+        path="api/settings/users/tokens/validate/{token}",
         request_schema_cls=None,
         request_model_cls=json_api.password_reset.ValidateRequest,
         response_schema_cls=None,
         response_model_cls=json_api.password_reset.ValidateResponse,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
     use: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/settings/users/tokens/reset",
+        path="api/settings/users/tokens/reset",
         request_schema_cls=None,
         request_model_cls=json_api.password_reset.UseRequest,
         response_schema_cls=None,
         response_model_cls=json_api.password_reset.UseResponse,
     )
     # PBUG: request is not jsonapi model
     # PBUG: response is not jsonapi model
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Tasks(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with enforcement tasks."""
+
+    count: ApiEndpoint = ApiEndpoint(
+        method="get",
+        path="api/enforcements/tasks/count",
+        request_schema_cls=json_api.tasks.GetTasksSchema,
+        request_model_cls=json_api.tasks.GetTasks,
+        response_schema_cls=json_api.generic.IntValueSchema,
+        response_model_cls=json_api.generic.IntValue,
+    )
 
     get_basic: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/enforcements/tasks",
         request_schema_cls=json_api.tasks.GetTasksSchema,
         request_model_cls=json_api.tasks.GetTasks,
         response_schema_cls=json_api.tasks.TaskBasicSchema,
@@ -894,15 +927,15 @@
         response_model_cls=json_api.tasks.TaskFilters,
     )
 
 
 # PBUG: so many things wrong with this
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Enforcements(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with enforcements."""
 
     tasks: Tasks = Tasks()
 
     get_sets: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/enforcements",
         request_schema_cls=json_api.resources.ResourcesGetSchema,
@@ -998,15 +1031,15 @@
         response_schema_cls=json_api.generic.ListDictValueSchema,
         response_model_cls=json_api.generic.ListDictValue,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class SystemRoles(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with Axonius roles."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/settings/roles",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.system_roles.SystemRoleSchema,
@@ -1022,15 +1055,15 @@
         response_schema_cls=json_api.system_roles.SystemRoleSchema,
         response_model_cls=json_api.system_roles.SystemRole,
     )
 
     delete: ApiEndpoint = ApiEndpoint(
         method="delete",
         path="api/settings/roles/{uuid}",
-        request_schema_cls=None,
+        request_schema_cls=json_api.resources.ResourceDeleteSchema,
         request_model_cls=json_api.resources.ResourceDelete,
         response_schema_cls=json_api.generic.MetadataSchema,
         response_model_cls=json_api.generic.Metadata,
         request_as_none=True,
     )
 
     update: ApiEndpoint = ApiEndpoint(
@@ -1051,66 +1084,66 @@
         response_model_cls=None,
     )
     # PBUG: response is not jsonapi model
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Lifecycle(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with the discover lifecycle."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/dashboard/lifecycle",
+        path="api/dashboard/lifecycle",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.lifecycle.LifecycleSchema,
         response_model_cls=json_api.lifecycle.Lifecycle,
     )
 
     start: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/settings/run_manual_discovery",
+        path="api/settings/run_manual_discovery",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: response is not jsonapi model
 
     stop: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/settings/stop_research_phase",
+        path="api/settings/stop_research_phase",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         response_as_text=True,
     )
     # PBUG: response is not jsonapi model
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Adapters(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with adapters."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/adapters",
+        path="api/adapters",
         request_schema_cls=json_api.adapters.AdaptersRequestSchema,
         request_model_cls=json_api.adapters.AdaptersRequest,
         response_schema_cls=json_api.adapters.AdapterSchema,
         response_model_cls=json_api.adapters.Adapter,
         http_args={"response_timeout": 3600},
     )
     # PBUG: REST API0: this can take forever to return with get_clients=True
 
     get_basic: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/adapters/list",
+        path="api/adapters/list",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.adapters.AdaptersListSchema,
         response_model_cls=json_api.adapters.AdaptersList,
     )
 
     get_fetch_history_filters: ApiEndpoint = ApiEndpoint(
@@ -1131,164 +1164,200 @@
         response_model_cls=json_api.adapters.AdapterFetchHistory,
         # response_schema_cls=None,
         # response_model_cls=None,
     )
 
     settings_get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/adapters/{adapter_name}/advanced_settings",
+        path="api/adapters/{adapter_name}/advanced_settings",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.adapters.AdapterSettingsSchema,
         response_model_cls=json_api.adapters.AdapterSettings,
     )
 
     settings_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/adapters/{adapter_name}/{config_name}",
-        request_schema_cls=json_api.adapters.AdapterSettingsUpdateSchema,
+        path="api/adapters/{adapter_name}/{config_name}",
+        request_schema_cls=json_api.adapters.AdapterSettingsUpdateUpdateSchema,
         request_model_cls=json_api.adapters.AdapterSettingsUpdate,
         response_schema_cls=json_api.system_settings.SystemSettingsSchema,
         response_model_cls=json_api.system_settings.SystemSettings,
     )
 
     file_upload: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/adapters/{adapter_name}/{node_id}/upload_file",
+        path="api/adapters/{adapter_name}/{node_id}/upload_file",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
         http_args_required=["files", "data"],
     )
     # PBUG: if Content-Type is not multipart, server returns a 401/unauthorized
     # PBUG: response not modeled correctly!
     # PBUG: can get filename returned in response?
 
-    labels_get: ApiEndpoint = ApiEndpoint(
+    cnx_get_labels: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/adapters/labels",
+        path="api/adapters/labels",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.adapters.CnxLabelsSchema,
         response_model_cls=json_api.adapters.CnxLabels,
     )
 
     cnx_get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/adapters/{adapter_name}/connections",
+        path="api/adapters/{adapter_name}/connections",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=json_api.adapters.Cnxs,
     )
 
     cnx_create: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/adapters/{adapter_name}/connections",
+        path="api/adapters/{adapter_name}/connections",
         request_schema_cls=json_api.adapters.CnxCreateRequestSchema,
         request_model_cls=json_api.adapters.CnxCreateRequest,
-        response_schema_cls=json_api.adapters.CnxModifyResponseSchema,
-        response_model_cls=json_api.adapters.CnxModifyResponse,
+        response_schema_cls=json_api.adapters.CnxCreateSchema,
+        response_model_cls=json_api.adapters.CnxCreate,
     )
 
     cnx_update: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/adapters/{adapter_name}/connections/{uuid}",
+        path="api/adapters/{adapter_name}/connections/{uuid}",
         request_schema_cls=json_api.adapters.CnxUpdateRequestSchema,
         request_model_cls=json_api.adapters.CnxUpdateRequest,
-        response_schema_cls=json_api.adapters.CnxModifyResponseSchema,
-        response_model_cls=json_api.adapters.CnxModifyResponse,
+        response_schema_cls=json_api.adapters.CnxUpdateSchema,
+        response_model_cls=json_api.adapters.CnxUpdate,
     )
 
     cnx_test: ApiEndpoint = ApiEndpoint(
         method="put",
-        path="api/V4.0/adapters/{adapter_name}/connections/test",
+        path="api/adapters/{adapter_name}/connections/test",
         request_schema_cls=json_api.adapters.CnxTestRequestSchema,
         request_model_cls=json_api.adapters.CnxTestRequest,
         response_schema_cls=None,
         response_model_cls=None,
     )
 
     cnx_delete: ApiEndpoint = ApiEndpoint(
         method="delete",
-        path="api/V4.0/adapters/{adapter_name}/connections/{uuid}",
+        path="api/adapters/{adapter_name}/connections/{uuid}",
         request_schema_cls=json_api.adapters.CnxDeleteRequestSchema,
         request_model_cls=json_api.adapters.CnxDeleteRequest,
         response_schema_cls=json_api.adapters.CnxDeleteSchema,
         response_model_cls=json_api.adapters.CnxDelete,
     )
     # PBUG: returns non-conformant json str in 'client_id' key i.e.:
     # "{'client_id': 'https://10.0.0.111_test1'}"
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Signup(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with initial signup."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/signup",
+        path="api/signup",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.generic.BoolValueSchema,
         response_model_cls=json_api.generic.BoolValue,
     )
 
+    expired: ApiEndpoint = ApiEndpoint(
+        method="get",
+        path="api/system/expired",
+        request_schema_cls=None,
+        request_model_cls=None,
+        response_schema_cls=json_api.generic.BoolValueSchema,
+        response_model_cls=json_api.generic.BoolValue,
+    )
+
+    license_status: ApiEndpoint = ApiEndpoint(
+        method="get",
+        path="api/license_status",
+        request_schema_cls=None,
+        request_model_cls=None,
+        response_schema_cls=json_api.generic.BoolValueSchema,
+        response_model_cls=json_api.generic.BoolValue,
+    )
+
+    get_indication_color: ApiEndpoint = ApiEndpoint(
+        method="get",
+        path="api/get_master_indication_color",
+        request_schema_cls=None,
+        request_model_cls=None,
+        response_schema_cls=json_api.generic.StrValueSchema,
+        response_model_cls=json_api.generic.StrValue,
+    )
+
+    get_login_options: ApiEndpoint = ApiEndpoint(
+        method="get",
+        path="api/get_login_options",
+        request_schema_cls=None,
+        request_model_cls=None,
+        response_schema_cls=json_api.generic.MetadataSchema,
+        response_model_cls=json_api.generic.Metadata,
+    )
+
     perform: ApiEndpoint = ApiEndpoint(
         method="post",
-        path="api/V4.0/signup",
+        path="api/signup",
         request_schema_cls=json_api.signup.SignupRequestSchema,
         request_model_cls=json_api.signup.SignupRequest,
         response_schema_cls=json_api.signup.SignupResponseSchema,
         response_model_cls=json_api.signup.SignupResponse,
     )
 
     status: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/status",
+        path="api/status",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.signup.SystemStatusSchema,
         response_model_cls=json_api.signup.SystemStatus,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class AuditLogs(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with audit logs."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
-        path="api/V4.0/settings/audit",
+        path="api/settings/audit",
         request_schema_cls=json_api.audit_logs.AuditLogRequestSchema,
         request_model_cls=json_api.audit_logs.AuditLogRequest,
         response_schema_cls=json_api.audit_logs.AuditLogSchema,
         response_model_cls=json_api.audit_logs.AuditLog,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class OpenAPISpec(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with OPENAPI."""
 
     get_spec: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/open_api_yaml",
         request_schema_cls=None,
         request_model_cls=None,
         response_as_text=True,
         response_schema_cls=None,
         response_model_cls=None,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class DataScopes(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with data scopes."""
 
     get: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/settings/data_scope",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=json_api.data_scopes.DataScopeDetailsSchema,
@@ -1321,40 +1390,49 @@
         response_schema_cls=json_api.generic.MetadataSchema,
         response_model_cls=json_api.generic.Metadata,
     )
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
 class Account(ApiEndpointGroup):
-    """Pass."""
+    """Container for all API endpoints for working with Axonius accounts."""
 
     login: ApiEndpoint = ApiEndpoint(
         method="post",
         path="api/login",
         request_schema_cls=json_api.account.LoginRequestSchema,
         request_model_cls=json_api.account.LoginRequest,
         response_schema_cls=json_api.account.LoginResponseSchema,
         response_model_cls=json_api.account.LoginResponse,
     )
 
+    get_current_user: ApiEndpoint = ApiEndpoint(
+        method="get",
+        path="api/login",
+        request_schema_cls=None,
+        request_model_cls=None,
+        response_schema_cls=json_api.account.CurrentUserSchema,
+        response_model_cls=json_api.account.CurrentUser,
+    )
+
     get_api_keys: ApiEndpoint = ApiEndpoint(
         method="get",
         path="api/settings/api_key",
         request_schema_cls=None,
         request_model_cls=None,
         response_schema_cls=None,
         response_model_cls=None,
     )
 
     validate: ApiEndpoint = SystemSettings.get_constants
 
 
 @dataclasses.dataclass(eq=True, frozen=True, repr=False)
-class ApiEndpoints(BaseData):
-    """Pass."""
+class ApiEndpointsGroups(ApiEndpointGroup):
+    """Container for all API endpoint groups."""
 
     instances: Instances = Instances()
     central_core: CentralCore = CentralCore()
     system_settings: SystemSettings = SystemSettings()
     remote_support: RemoteSupport = RemoteSupport()
     system_users: SystemUsers = SystemUsers()
     system_roles: SystemRoles = SystemRoles()
@@ -1369,23 +1447,9 @@
     openapi: OpenAPISpec = OpenAPISpec()
     data_scopes: DataScopes = DataScopes()
     dashboard_spaces: DashboardSpaces = DashboardSpaces()
     folders_queries: FoldersQueries = FoldersQueries()
     folders_enforcements: FoldersEnforcements = FoldersEnforcements()
     account: Account = Account()
 
-    @classmethod
-    def get_groups(cls) -> Dict[str, ApiEndpointGroup]:
-        """Pass."""
-        return {x.name: x.default for x in cls.get_fields()}
-
-    def __str__(self):
-        """Pass."""
-        names = [x.name for x in self.get_fields()]
-        return f"{self.__class__.__name__}(groups={names})"
-
-    def __repr__(self):
-        """Pass."""
-        return self.__str__()
-
 
-ApiEndpoints = ApiEndpoints()
+ApiEndpoints = ApiEndpointsGroups()
```

## axonius_api_client/api/mixins.py

```diff
@@ -1,40 +1,66 @@
 # -*- coding: utf-8 -*-
 """API model base classes and mixins."""
 import logging
+import typing as t
 
-from .. import auth
+from ..auth import AuthModel
 from ..constants.logs import LOG_LEVEL_API
+from ..http import Http
 from ..logs import get_obj_log
 
 
 class Model:
     """API model base class."""
 
+    http: t.ClassVar[Http] = None
+    """HTTP client to use to send requests."""
 
+    auth: t.ClassVar[AuthModel] = None
+    """Authentication object to use to send requests."""
+
+    LOG: t.ClassVar[logging.Logger] = None
+    """Logger for this object."""
+
+
+# noinspection PyUnusedLocal
 class ModelMixins(Model):
     """Mixins for API Models."""
 
-    def __init__(self, auth: auth.Model, **kwargs):
+    LOG: logging.Logger = None
+    """Logger for this object."""
+
+    auth: AuthModel = None
+    """Authentication model with bound Http object to use for requests."""
+
+    http: Http = None
+    """Http object to use for requests."""
+
+    def __init__(
+        self,
+        auth: AuthModel,
+        log_level: t.Union[int, str] = LOG_LEVEL_API,
+        **kwargs,
+    ):
         """Mixins for API Models.
 
         Args:
             auth: object to use for auth and sending API requests
+            log_level: logging level to use for this objects logger
             **kwargs: passed to :meth:`_init`
         """
-        log_level = kwargs.get("log_level", LOG_LEVEL_API)
         self.LOG: logging.Logger = get_obj_log(obj=self, level=log_level)
-        """Logger for this object."""
-        self.auth = auth
-        """:obj:`axonius_api_client.auth.models.Mixins` authentication object."""
-        self.http = auth.http
-        """:obj:`axonius_api_client.http.Http` client to use to send requests,"""
-
+        self.auth: AuthModel = auth
+        self.http: Http = auth.http
         self._init(**kwargs)
-        auth.check_login()
+        self._init_auth(**kwargs)
+
+    def _init_auth(self, **kwargs):
+        """Post init method for subclasses to use for overriding auth setup."""
+        self.auth.login()
 
     def _init(self, **kwargs):
         """Post init method for subclasses to use for extra setup."""
         pass
 
     def __str__(self) -> str:
         """Show info for this model object."""
@@ -44,27 +70,39 @@
         return f"{cls.__module__}.{cls.__name__}(auth={auth!r}, url={url!r})"
 
     def __repr__(self) -> str:
         """Show info for this model object."""
         return self.__str__()
 
 
-class ChildMixins:
+class ChildMixins(Model):
     """Mixins model for API child objects."""
 
+    parent: Model = None
+    """Parent API model of this child."""
+
+    LOG: logging.Logger = None
+    """Logger for this object."""
+
+    auth: AuthModel = None
+    """Authentication model with bound Http object to use for requests."""
+
+    http: Http = None
+    """Http object to use for requests."""
+
     def __init__(self, parent: Model):
         """Mixins model for API child objects.
 
         Args:
             parent: parent API model of this child
         """
-        self.parent = parent
-        self.http = parent.http
-        self.auth = parent.auth
-        self.LOG = parent.LOG.getChild(self.__class__.__name__)
+        self.parent: Model = parent
+        self.http: Http = parent.http
+        self.auth: AuthModel = parent.auth
+        self.LOG: logging.Logger = parent.LOG.getChild(self.__class__.__name__)
         self._init(parent=parent)
 
     def _init(self, parent: Model):
         """Post init method for subclasses to use for extra setup.
 
         Args:
             parent: parent API model of this child
```

## axonius_api_client/api/adapters/adapters.py

```diff
@@ -10,20 +10,20 @@
 from ...exceptions import ApiError, NotFoundError  # , StopFetch
 from ...parsers.config import config_build, config_unchanged, config_unknown
 from ...parsers.tables import tablize_adapters
 from ...tools import path_read
 from ..api_endpoints import ApiEndpoints
 from ..json_api.adapters import (
     Adapter,
-    AdapterFetchHistory,
-    AdapterFetchHistoryFilters,
     AdapterFetchHistoryRequest,
     AdapterSettings,
     AdaptersList,
     CnxLabels,
+    AdapterFetchHistory,
+    AdapterFetchHistoryFilters,
 )
 from ..json_api.paging_state import PagingState
 from ..json_api.system_settings import SystemSettings
 from ..json_api.time_range import UnitTypes
 from ..mixins import ModelMixins
 
 HIST_MOD = AdapterFetchHistory
@@ -457,15 +457,15 @@
             ...     field_name="name_of_field",
             ... )
             >>> file_uuid
             {'uuid': '5f78b7dee33f0a113700a6fc', 'filename': 'name_of_file'}
 
         Args:
             name: name of adapter to upload file to
-            node: name of node to to upload file to
+            node: name of node to upload file to
             field_name: name of field (should match configuration schema key name)
             file_name: name of file to upload
             file_content: content of file to upload
             file_content_type: mime type of file to upload
 
         Returns:
             dict: with keys 'filename' and 'uuid'
@@ -618,15 +618,15 @@
 
     def _get_labels(self) -> CnxLabels:
         """Get labels metadata for all connections.
 
         Returns:
             CnxLabels: dataclass model containing response
         """
-        api_endpoint = ApiEndpoints.adapters.labels_get
+        api_endpoint = ApiEndpoints.adapters.cnx_get_labels
         response = api_endpoint.perform_request(http=self.http)
         return response
 
     def _get_fetch_history_filters(self) -> AdapterFetchHistoryFilters:
         """Get filter values for use in adapters history."""
         api_endpoint = ApiEndpoints.adapters.get_fetch_history_filters
         response = api_endpoint.perform_request(http=self.http)
```

## axonius_api_client/api/adapters/cnx.py

```diff
@@ -35,15 +35,15 @@
     json_load,
     listify,
     pathify,
     pathlib,
     strip_right,
 )
 from ..api_endpoints import ApiEndpoints
-from ..json_api.adapters import CnxDelete, CnxModifyResponse, Cnxs
+from ..json_api.adapters import CnxCreate, CnxDelete, CnxLabels, Cnxs, CnxUpdate
 from ..json_api.instances import Tunnel
 from ..mixins import ChildMixins
 
 
 class Cnx(ChildMixins):
     """API model for working with adapter connections.
 
@@ -841,15 +841,15 @@
             value: file to upload
             schema: connection configuration schema of type "file"
             callbacks: callbacks supplied
             source: description of what called this method
 
         Raises:
             :exc:`ConfigInvalidValue`: When value is a path that does not exist, or
-                a dictionary that does not have 'uuid' and 'filename' keys',
+                a dictionary that does not have 'uuid' and 'filename' keys,
                 or if value is not a file
         """
 
         def fail(msg):
             sinfo = config_info(schema=schema, value=str(value), source=source)
             msgs = [sinfo, "", f"supplied value: {orig!r}", f"parsed value: {value!r}", msg]
             raise ConfigInvalidValue("\n".join(msgs))
@@ -897,15 +897,15 @@
         connection_discovery: Optional[dict] = None,
         is_instances_mode: bool = False,
         save_and_fetch: bool = True,
         active: bool = True,
         connection_label: Optional[str] = None,
         tunnel_id: Optional[str] = None,
         response_status_hook: Optional[Callable] = None,
-    ) -> CnxModifyResponse:
+    ) -> CnxCreate:
         """Pass."""
         api_endpoint = ApiEndpoints.adapters.cnx_create
         request_obj = api_endpoint.load_request(
             connection=connection,
             connection_discovery=connection_discovery,
             instance=instance_id,
             instance_name=instance_name,
@@ -967,14 +967,22 @@
         """
         adapter_name += "" if adapter_name.endswith("_adapter") else "_adapter"
         api_endpoint = ApiEndpoints.adapters.cnx_get
         ret = api_endpoint.perform_request(http=self.auth.http, adapter_name=adapter_name)
         ret.adapter_name = strip_right(obj=adapter_name, fix="_adapter")
         return ret
 
+    def _get_labels(self) -> CnxLabels:
+        """Get all connection labels.
+
+        Returns:
+            CnxLabels: dataclass model containing response
+        """
+        return ApiEndpoints.adapters.cnx_get_labels.perform_request(http=self.auth.http)
+
     def _delete(
         self,
         uuid: str,
         adapter_name: str,
         instance_id: str,
         instance_name: str,
         delete_entities: bool = False,
@@ -1023,15 +1031,15 @@
         tunnel_id: Optional[str] = None,
         connection_discovery: Optional[dict] = None,
         is_instances_mode: bool = False,
         save_and_fetch: bool = True,
         active: bool = True,
         connection_label: Optional[str] = None,
         response_status_hook: Optional[Callable] = None,
-    ) -> CnxModifyResponse:
+    ) -> CnxUpdate:
         """Pass.
 
         Args:
             uuid (str): uuid of connection
             connection (dict): new configuration to apply
             instance_id (str): UUID of instance to move connection to
             instance_name (str): NAME of instance to move connection to
```

## axonius_api_client/api/asset_callbacks/__init__.py

```diff
@@ -1,20 +1,21 @@
 # -*- coding: utf-8 -*-
 """Callbacks for formatting asset data and exporting to various formats."""
-from .base import Base
+from .base import Base, ExportMixins
 from .base_csv import Csv
 from .base_json import Json
 from .base_json_to_csv import JsonToCsv
 from .base_table import Table
 from .base_xlsx import Xlsx
 from .base_xml import Xml
 from .tools import CB_MAP, get_callbacks_cls
 
 __all__ = (
     "Base",
+    "ExportMixins",
     "Csv",
     "Json",
     "Table",
     "Xlsx",
     "Xml",
     "JsonToCsv",
     "get_callbacks_cls",
```

## axonius_api_client/api/asset_callbacks/base.py

```diff
@@ -1,39 +1,39 @@
 # -*- coding: utf-8 -*-
 """Base callbacks."""
 import logging
 import pathlib
 import re
 import sys
+import typing as t
 from typing import IO, Generator, List, Optional, Tuple, Union
 
 from ... import DEFAULT_PATH
 from ...constants.api import FIELD_JOINER, FIELD_TRIM_LEN, FIELD_TRIM_STR
 from ...constants.fields import (
     AGG_ADAPTER_NAME,
     FIELDS_DETAILS,
     FIELDS_DETAILS_EXCLUDE,
     FIELDS_ENTITY_PASSTHRU,
     SCHEMAS_CUSTOM,
 )
 from ...exceptions import ApiError
-
-# from ...parsers.fields import schema_custom
-from ...tools import calc_percent  # json_dump,
 from ...tools import (
     PathLike,
+    calc_percent,
     check_path_is_not_dir,
     coerce_int,
     dt_now,
     echo_debug,
     echo_error,
     echo_ok,
     echo_warn,
     get_path,
     get_paths_format,
+    is_subclass_safe,
     join_kv,
     listify,
     longest_str,
     path_backup_file,
     strip_right,
 )
 
@@ -56,23 +56,23 @@
         * :meth:`args_map` for callback generic arguments to format assets.
         * :meth:`args_map_custom` for callback specific arguments to format and export data.
 
     """
 
     @classmethod
     def args_map(cls) -> dict:
-        """Get all of the argument names and their defaults for this callbacks object.
+        """Get all the argument names and their defaults for this callbacks object.
 
         Examples:
             Create a ``client`` using :obj:`axonius_api_client.connect.Connect` and assume
             ``apiobj`` is either ``client.devices`` or ``client.users``
 
             >>> apiobj = client.devices  # or client.users
 
-            Flatten complex fields -  Will take all sub fields of complex fields and put them
+            Flatten complex fields -  Will take all sub-fields of complex fields and put them
             on the root level with their values index correlated to each other.
 
             >>> assets = apiobj.get(fields=["network_interfaces"], field_flatten=True)
 
             Explode a single field - will take that field and create new rows for list item.
 
             >>> assets = apiobj.get(field_explode="hostname")
@@ -119,15 +119,15 @@
 
             >>> assets = apiobj.get(report_software_whitelist=["chrome", "^adobe.*acrobat"])
 
             Echo to STDERR progress messages.
 
             >>> assets = apiobj.get(do_echo=True)
 
-            Change the amount of assets that echo page progress if do_echo is true.
+            Change the amount of assets that echo page progress when do_echo is true.
 
             >>> assets = apiobj.get(do_echo=True, page_progress=100)
 
             Supply a set of custom callbacks to process each row before all builtin callbacks
             are run. Custom callbacks receive two arguments: ``self`` (the current callback object)
             and ``rows`` (the current rows being processed). Custom callbacks must return a list of
             rows.
@@ -187,14 +187,17 @@
             "report_software_whitelist": [],
             "page_progress": 10000,
             "do_echo": False,
             "custom_cbs": [],
             "debug_timing": False,
             "explode_entities": False,
             "include_dates": False,
+            "csv_field_flatten": True,
+            "csv_field_join": True,
+            "csv_field_null": True,
         }
 
     def get_arg_value(self, arg: str) -> Union[str, list, bool, int]:
         """Get an argument value.
 
         Args:
             arg: key to get from :attr:`GETARGS` with a default value from :meth:`args_map`
@@ -218,16 +221,16 @@
         getargs: dict = None,
     ):
         """Callbacks base class for assets.
 
         Args:
             apiobj (:obj:`axonius_api_client.api.assets.asset_mixin.AssetMixin`): Asset object
                 that created this callback
-            store: store tracker of assets get method that created this callback
-            state: state tracker of assets get method that created this callback
+            store: store tracker of get method that created this callback
+            state: state tracker of get method that created this callback
             getargs: kwargs passed to assets get method that created this callback
         """
         self.LOG: logging.Logger = apiobj.LOG.getChild(self.__class__.__name__)
         """logger for this object."""
 
         self.APIOBJ = apiobj
         self.ALL_SCHEMAS: dict = apiobj.fields.get()
@@ -673,19 +676,19 @@
         # force it into a list of items
         items = listify(row.pop(field, []))
 
         for sub_schema in self.get_sub_schemas(schema=schema):
             sub_field = sub_schema["name_qual"]
             sub_short = sub_schema["name"]
 
-            # for each sub field, ensure there is an empty list to store values
+            # for each sub-field, ensure there is an empty list to store values
             row[sub_field] = []
 
-            # for each complex item, remove the sub field, force it into a list,
-            # and append it to the sub fields fully qualified name at the root row level
+            # for each complex item, remove the sub-field, force it into a list,
+            # and append it to the sub-fields fully qualified name at the root row level
             for item in items:
                 value = item.pop(sub_short, null_value)
                 value = value if isinstance(value, list) else [value]
                 row[sub_field] += value
 
     def do_explode_entities(self, rows: Union[List[dict], dict]) -> List[dict]:
         """Explode a row into a row for each asset entity.
@@ -887,34 +890,34 @@
 
     def add_include_dates(self, rows: Union[List[dict], dict]) -> List[dict]:
         """Process report: Add dates (history and current)
 
         Args:
             rows: rows to process
         """
+
         def _add_date(row):
             row.update(updater)
             return row
 
         rows = listify(rows)
         enabled = self.get_arg_value("include_dates")
         if not enabled:
             return rows
 
         history_date = self.STORE.get("history_date_parsed")
         current_date = str(dt_now())
         schemas = SCHEMAS_CUSTOM["include_dates"]
         updater = {
-                schemas["history_date"]["name_qual"]: history_date,
-                schemas["current_date"]["name_qual"]: current_date
+            schemas["history_date"]["name_qual"]: history_date,
+            schemas["current_date"]["name_qual"]: current_date,
         }
         rows = [_add_date(row) for row in rows]
         return rows
 
-
     def add_report_adapters_missing(self, rows: Union[List[dict], dict]) -> List[dict]:
         """Process report: Missing adapters.
 
         Args:
             rows: rows to process
         """
         rows = listify(rows)
@@ -978,15 +981,15 @@
             )
         return self._excluded_schemas
 
     def echo(
         self,
         msg: str,
         debug: bool = False,
-        error: Union[bool, Exception] = False,
+        error: Union[bool, t.Type[Exception]] = False,
         warning: bool = False,
         level: str = "info",
         level_debug: str = "debug",
         level_error: str = "error",
         level_warning: str = "warning",
         abort: bool = True,
     ):
@@ -1012,26 +1015,26 @@
                 echo_debug(msg=msg)
             else:
                 echo_ok(msg=msg)
         else:
             if error:
                 getattr(self.LOG, level_error)(msg)
                 if abort:
-                    if not isinstance(error, Exception):
+                    if not is_subclass_safe(error, Exception):
                         error = ApiError
                     raise error(msg)
             elif warning:
                 getattr(self.LOG, level_warning)(msg)
             elif debug:
                 getattr(self.LOG, level_debug)(msg)
             else:
                 getattr(self.LOG, level)(msg)
 
     def get_sub_schemas(self, schema: dict) -> Generator[dict, None, None]:
-        """Get all the schemas of sub fields for a complex field.
+        """Get all the schemas of sub-fields for a complex field.
 
         Args:
             schema: schema of complex field
         """
         sub_schemas = listify(schema.get("sub_fields"))
         for sub_schema in sub_schemas:
             if self.is_excluded(schema=sub_schema) or not sub_schema["is_root"]:
@@ -1414,14 +1417,17 @@
     "do_echo": "Echo messages to console",
     "custom_cbs": "Custom callbacks to perform on assets",
     "json_flat": "For JSON Export: Use JSONL format",
     "csv_key_miss": "For CSV Export: Value to use when keys are missing",
     "csv_key_extras": "For CSV Export: What to do with extra CSV columns",
     "csv_dialect": "For CSV Export: CSV Dialect to use",
     "csv_quoting": "For CSV Export: CSV quoting style",
+    "csv_field_flatten": "For CSV/XLSX Export: Enable flattening of complex fields",
+    "csv_field_join": "For CSV/XLSX Export: Enable joining of list fields",
+    "csv_field_null": "For CSV/XLSX Export: Enable null values for missing fields",
     "export_file": "File to export data to",
     "export_path": "Directory to export data to",
     "export_overwrite": "Overwrite export_file if it exists",
     "export_schema": "Export schema of fields",
     "export_fd": "Export to a file descriptor",
     "export_fd_close": "Close the file descriptor when done",
     "export_backup": "If export_file exists, rename it with the datetime",
```

## axonius_api_client/api/asset_callbacks/base_csv.py

```diff
@@ -109,17 +109,17 @@
                 "csv_quoting": "nonnumeric",
             }
         )
         return args
 
     def _init(self, **kwargs):
         """Override arguments to make export readable."""
-        self.set_arg_value("field_null", True)
-        self.set_arg_value("field_flatten", True)
-        self.set_arg_value("field_join", True)
+        self.set_arg_value("field_null", self.get_arg_value("csv_field_null"))
+        self.set_arg_value("field_flatten", self.get_arg_value("csv_field_flatten"))
+        self.set_arg_value("field_join", self.get_arg_value("csv_field_join"))
 
     def start(self, **kwargs):
         """Start this callbacks object."""
         super(Csv, self).start(**kwargs)
         self.open_fd()
 
     def do_start(self, **kwargs):
```

## axonius_api_client/api/asset_callbacks/base_xlsx.py

```diff
@@ -99,17 +99,17 @@
                 "xlsx_cell_format": {"text_wrap": True},
             }
         )
         return args
 
     def _init(self, **kwargs):
         """Override defaults to make export readable."""
-        self.set_arg_value("field_null", True)
-        self.set_arg_value("field_flatten", True)
-        self.set_arg_value("field_join", True)
+        self.set_arg_value("field_null", self.get_arg_value("csv_field_null"))
+        self.set_arg_value("field_flatten", self.get_arg_value("csv_field_flatten"))
+        self.set_arg_value("field_join", self.get_arg_value("csv_field_join"))
 
     def start(self, **kwargs):
         """Start this callbacks object."""
         super(Xlsx, self).start(**kwargs)
         self.do_start(**kwargs)
 
     def do_start(self, **kwargs):
```

## axonius_api_client/api/asset_callbacks/tools.py

```diff
@@ -1,25 +1,25 @@
 # -*- coding: utf-8 -*-
 """Tools for loading callbacks."""
-from typing import Dict
+import typing as t
 
 from ...exceptions import ApiError
 from ...tools import get_subcls
 from .base import Base
 
-CB_MAP: Dict[str, Base] = {
+CB_MAP: t.Dict[str, t.Type[Base]] = {
     Base.CB_NAME: Base,
     **{x.CB_NAME: x for x in get_subcls(cls=Base)},
 }
 """Map of export name to callbacks class."""
 
 CB_DEF: str = Base.CB_NAME
 
 
-def get_callbacks_cls(export: str = CB_DEF) -> Base:
+def get_callbacks_cls(export: str = CB_DEF) -> t.Type[Base]:
     """Get a callback class.
 
     Args:
         export: export format from asset object get method to map to a callback object
             must be one of :data:`CB_MAP`
     """
     export = export or CB_DEF
```

## axonius_api_client/api/assets/asset_mixin.py

```diff
@@ -1,42 +1,49 @@
 # -*- coding: utf-8 -*-
 """API model mixin for device and user assets."""
 import datetime
 import pathlib
 import time
 import types
 import typing as t
+import uuid
 
 import cachetools
 
 from ...constants.api import DEFAULT_CALLBACKS_CLS, MAX_PAGE_SIZE, PAGE_SIZE
 from ...constants.fields import AXID
 from ...exceptions import ApiError, NotFoundError, ResponseNotOk, StopFetch
 from ...parsers.grabber import Grabber
 from ...tools import PathLike, dt_now, dt_now_file, get_subcls, json_dump, listify
-from .. import json_api
-from ..api_endpoints import ApiEndpoints
+from ..api_endpoints import ApiEndpoint, ApiEndpoints
+from ..asset_callbacks.tools import Base as BaseCallbacks
 from ..asset_callbacks.tools import get_callbacks_cls
+from ..json_api.assets import (
+    AssetById,
+    AssetRequest,
+    AssetsPage,
+    AssetTypeHistoryDates,
+    Count,
+    CountRequest,
+    HistoryDates,
+)
 from ..mixins import ModelMixins
 from ..wizards import Wizard, WizardCsv, WizardText
 from .runner import ENFORCEMENT, Runner
 
 GEN_TYPE = t.Union[t.Generator[dict, None, None], t.List[dict]]
 HISTORY_DATES_OBJ_CACHE = cachetools.TTLCache(maxsize=1, ttl=300)
 HISTORY_DATES_CACHE = cachetools.TTLCache(maxsize=1, ttl=300)
 
 
+# noinspection PyAttributeOutsideInit,PyShadowingBuiltins
 class AssetMixin(ModelMixins):
     """API model mixin for device and user assets.
 
     Examples:
-        Create a ``client`` using :obj:`axonius_api_client.connect.Connect` and assume
-        ``apiobj`` is either ``client.devices`` or ``client.users``
-
-        >>> apiobj = client.devices  # or client.users
 
         * Get count of assets: :meth:`count`
         * Get count of assets from a saved query: :meth:`count_by_saved_query`
         * Get assets: :meth:`get`
         * Get assets from a saved query: :meth:`get_by_saved_query`
         * Get the full data set for a single asset: :meth:`get_by_id`
         * Work with saved queries: :obj:`axonius_api_client.api.assets.saved_query.SavedQuery`
@@ -45,26 +52,25 @@
 
     See Also:
         This object is not usable directly, it only stores the logic that is common for working
         with the various asset types:
 
         * Device assets :obj:`axonius_api_client.api.assets.devices.Devices`
         * User assets :obj:`axonius_api_client.api.assets.users.Users`
-
     """
 
     ASSET_TYPE: str = ""
 
     @classmethod
     def asset_types(cls) -> t.List[str]:
         """Pass."""
         return [x.ASSET_TYPE for x in cls.asset_modules()]
 
     @classmethod
-    def asset_modules(cls) -> t.List["AssetMixin"]:
+    def asset_modules(cls) -> t.List[t.Type["AssetMixin"]]:
         """Pass."""
         return get_subcls(AssetMixin)
 
     def run_enforcement(
         self,
         eset: ENFORCEMENT,
         ids: t.Union[str, t.List[str]],
@@ -78,52 +84,55 @@
         src_fields: t.Optional[t.List[str]] = None,
         check_stdin: bool = True,
         grabber: t.Optional[Grabber] = None,
     ) -> Runner:
         """Run an enforcement set against a manually selected list of assets.
 
         Examples:
-            '''Get a list of assets from a query and manually extract the IDs.
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> WIZ: str = "simple os.type equals Windows"  # "query of assets to target"
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Get a list of assets from a query and manually extract the IDs.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            ITEMS = apiobj.get(wiz_entries=WIZ)
-            IDS = [x['internal_axon_id'] for x in ITEMS]
-            runner = apiobj.run_enforcement(eset=ESET, ids=IDS, verified=True)
-            print(runner)
-            '''
+            >>> ITEMS: list[dict] = apiobj.get(wiz_entries=WIZ)
+            >>> IDS: list[str] = list(map(lambda x: x['internal_axon_id'], ITEMS))
+            >>> RUNNER: Runner = apiobj.run_enforcement(eset=ESET, ids=IDS, verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
               verify_count=True,
               prompt=False,
               grabber=None,
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             ids (t.Union[str, t.List[str]]): Asset IDs to run Enforcement Set against,
                 csv-like string or list of csv-like strings
             verify_and_run (bool, optional): if false, return the Runner object
                 to use manually. if true, run :method:`Runner.verify_and_run`
                 before returning the Runner object
             verified (bool): $ids already verified, just run $eset against $ids
             verify_count (bool): Verify that the count of $query equals the count of $ids
             prompt (bool): Prompt user for verification when applicable.
             do_echo (bool): Echo output to console as well as log
             refetch (bool): refetch $eset even if it is already a model
-            check_stdin (bool): check if stdin is a TTY when prompting
+            src_query (str): query to use to get $ids
+            src_fields (list): fields to use to get $ids
+            check_stdin (bool): error if stdin is a TTY when prompting
             grabber: (grabber): Grabber used to get IDs
 
         Returns:
             Runner: Runner object used to verify and run $eset
         """
         runner = Runner(
             apiobj=self,
@@ -133,14 +142,15 @@
             verify_count=verify_count,
             prompt=prompt,
             do_echo=do_echo,
             refetch=refetch,
             src_query=src_query,
             src_fields=src_fields,
             grabber=grabber,
+            check_stdin=check_stdin,
         )
         if verify_and_run:
             runner.verify_and_run()
         return runner
 
     def run_enforcement_from_items(
         self,
@@ -150,25 +160,28 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a list of dicts or strs and run $eset against them.
 
         Examples:
-            '''Get a list of assets from a query and use the grabber get the IDs.
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> WIZ: str = "simple os.type equals Windows"  # "query of assets to target"
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Get a list of assets from a query and use the grabber get the IDs.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            ITEMS = apiobj.get(wiz_entries=WIZ)
-            runner = apiobj.run_enforcement_from_items(eset=ESET, items=ITEMS, verified=True)
-            print(runner)
-            '''
+            >>> ITEMS: list[dict] = apiobj.get(wiz_entries=WIZ)
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_items(eset=ESET, items=ITEMS,
+            ... verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -176,17 +189,15 @@
               prompt=False,
               grabber=Grabber(
               count_supplied=31,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source=None,
-            ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             items (t.Union[str, t.List[str], dict, t.List[dict], types.GeneratorType]): list of
                 strs or dicts to grab Asset IDs from
             keys (t.Union[str, t.List[str]]): additional keys for grabber to look for Asset IDs in
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
@@ -215,30 +226,33 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a JSON string with a list of dicts and run $eset against them.
 
         Examples:
-            '''Get a list of assets from a query and export the assets to a JSON str
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> WIZ: str = "simple os.type equals Windows"  # "query of assets to target"
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Get a list of assets from a query and export the assets to a JSON str
             then run an enforcement against all asset IDs from the JSON str.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            import io
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            FH = io.StringIO()
-            z = apiobj.get(wiz_entries=WIZ, export="json", export_fd=FH, export_fd_close=False)
-            FH.seek(0)
-            ITEMS = FH.getvalue()
-            runner = apiobj.run_enforcement_from_json(eset=ESET, items=ITEMS, verified=True)
-            print(runner)
-            '''
+            >>> import io
+            >>> FH = io.StringIO()
+            >>> _ = apiobj.get(wiz_entries=WIZ, export="json", export_fd=FH, export_fd_close=False)
+            >>> FH.seek(0)
+            >>> ITEMS: str = FH.getvalue()
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_json(eset=ESET, items=ITEMS,
+            ... verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -248,30 +262,24 @@
               count_supplied=31,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_json items type=str, length=15519 post_load type=list, length=31',
             ),
             )
-            '''
 
-            '''Get a list of assets from a query and export the assets to a JSON file
+            Get a list of assets from a query and export the assets to a JSON file
             then run an enforcement against all asset IDs from the JSON file.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            import pathlib
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            PATH = pathlib.Path("data.json")
-            z = apiobj.get(wiz_entries=WIZ, export="json", export_file=PATH, export_overwrite=True)
-            runner = apiobj.run_enforcement_from_json(eset=ESET, items=PATH, verified=True)
-            print(runner)
-            '''
+            >>> import pathlib
+            >>> PATH: pathlib.Path = pathlib.Path("data.json")
+            >>> _ = apiobj.get(wiz_entries=WIZ, export="json", export_file=PATH)
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_json(eset=ESET, items=PATH,
+            ... verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -281,15 +289,14 @@
               count_supplied=31,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_json items type=PosixPath, length=None post_load type=list, length=31',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             items (t.Union[str, bytes, t.IO, pathlib.Path]): json str, handle for file containing
                 json str, or pathlib.Path of path containing json str
             keys (t.Union[str, t.List[str]]): additional keys for grabber to look for Asset IDs in
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
@@ -318,30 +325,33 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a JSONL string with one dict per line and run $eset against them.
 
         Examples:
-            '''Get a list of assets from a query and export the assets to a JSONL str
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> WIZ: str = "simple os.type equals Windows"  # "query of assets to target"
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Get a list of assets from a query and export the assets to a JSONL str
             then run an enforcement against all asset IDs from the JSONL str.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            import io
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            FH = io.StringIO()
-            z = apiobj.get(
-              wiz_entries=WIZ, export="json", json_flat=True, export_fd=FH, export_fd_close=False)
-            FH.seek(0)
-            runner = apiobj.run_enforcement_from_jsonl(eset=ESET, items=FH, verified=True)
-            print(runner)
-            '''
+            >>> import io
+            >>> FH = io.StringIO()
+            >>> _ = apiobj.get(wiz_entries=WIZ, export="json", json_flat=True,
+            ... export_fd=FH, export_fd_close=False)
+            >>> FH.seek(0)
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_jsonl(eset=ESET, items=FH,
+            ... verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -351,32 +361,26 @@
               count_supplied=31,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_jsonl items type=StringIO, length=None post_load type=list, length=31',
             ),
             )
-            '''
 
-            '''Get a list of assets from a query and export the assets to a JSONL file
+            Get a list of assets from a query and export the assets to a JSONL file
             then run an enforcement against all asset IDs from the JSONL file.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            import pathlib
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            PATH = pathlib.Path("data.jsonl")
-            z = apiobj.get(
-              wiz_entries=WIZ, export="json", json_flat=True, export_file=PATH,
-              export_overwrite=True)
-            runner = apiobj.run_enforcement_from_jsonl(eset=ESET, items=PATH, verified=True)
-            print(runner)
-            '''
+            >>> import pathlib
+            >>> PATH = pathlib.Path("data.jsonl")
+            >>> _ = apiobj.get(
+            ...  wiz_entries=WIZ, export="json", json_flat=True, export_file=PATH,
+            ... export_overwrite=True)
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_jsonl(eset=ESET, items=PATH,
+            ... verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -386,15 +390,14 @@
               count_supplied=31,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_jsonl items type=PosixPath, length=None post_load type=list, length=31',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             items (t.Union[str, bytes, t.IO, pathlib.Path]): jsonl str, handle for file containing
                 jsonl str, or pathlib.Path of path containing jsonl str
             keys (t.Union[str, t.List[str]]): additional keys for grabber to look for Asset IDs in
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
@@ -424,32 +427,35 @@
         do_raise_grab: bool = False,
         load_args: t.Optional[dict] = None,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a CSV string and run $eset against them.
 
         Examples:
-            '''Get a list of assets from a query and export the assets to a JSONL str
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> WIZ: str = "simple os.type equals Windows"  # "query of assets to target"
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Get a list of assets from a query and export the assets to a JSONL str
             then run an enforcement against all asset IDs from the JSONL str.
             We can also use a CSV file exported from the GUI.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            from axonius_api_client.tools import bom_strip
-            import io
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            FH = io.StringIO()
-            z = apiobj.get(wiz_entries=WIZ, export="csv", export_fd=FH, export_fd_close=False)
-            FH.seek(0)
-            ITEMS = bom_strip(FH.getvalue())
-            runner = apiobj.run_enforcement_from_csv(eset=ESET, items=ITEMS, verified=True)
-            print(runner)
-            '''
+
+            >>> import io
+            >>> FH: io.StringIO = io.StringIO()
+            >>> _ = apiobj.get(wiz_entries=WIZ, export="csv", export_fd=FH, export_fd_close=False)
+            >>> FH.seek(0)
+            >>> ITEMS: str = axonapi.tools.bom_strip(FH.getvalue())
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_csv(eset=ESET, items=ITEMS,
+            ... verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -459,31 +465,25 @@
               count_supplied=33,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_csv items type=str, length=6556 post_load type=list, length=33',
             ),
             )
-            '''
 
-            '''Get a list of assets from a query and export the assets to a CSV file
+            Get a list of assets from a query and export the assets to a CSV file
             then run an enforcement against all asset IDs from the CSV file.
             We can also use a CSV file exported from the GUI.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            import pathlib
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            PATH = pathlib.Path("data.csv")
-            z = apiobj.get(wiz_entries=WIZ, export="csv", export_file=PATH, export_overwrite=True)
-            runner = apiobj.run_enforcement_from_csv(eset=ESET, items=PATH, verified=True)
-            print(runner)
-            '''
+            >>> import pathlib
+            >>> PATH: pathlib.Path = pathlib.Path("data.csv")
+            >>> _ = apiobj.get(wiz_entries=WIZ, export="csv", export_file=PATH)
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_csv(eset=ESET, items=PATH,
+            ... verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -493,24 +493,24 @@
               count_supplied=33,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_csv items type=PosixPath, length=None post_load type=list, length=33',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             items (t.Union[str, bytes, t.IO, pathlib.Path]): csv str, handle for file containing
                 csv str, or pathlib.Path of path containing csv str
             keys (t.Union[str, t.List[str]]): additional keys for grabber to look for Asset IDs in
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
             do_raise_grab (bool, optional): Throw an error if grabber fails to find an Asset ID
                 in any items
+            load_args: passed to :func:`pandas.read_csv`
             **kwargs: passed to :method:`run_enforcement`
 
         Returns:
             Runner: Runner object used to verify and run $eset
         """
         kwargs["grabber"] = grabber = Grabber.from_csv(
             items=items,
@@ -531,32 +531,35 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         r"""Get Asset IDs from a text string and run $eset against them.
 
         Examples:
-            '''Get a list of assets from a query and export the assets to a text file
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> WIZ: str = "simple os.type equals Windows"  # "query of assets to target"
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Get a list of assets from a query and export the assets to a text file
             then run an enforcement against all asset IDs from the text file.
-            All lines will have any non alpha-numeric characters removed from them and if a
-            32 character alpha numeric string is found it is considered an Asset ID.
+            All lines will have any non-alphanumeric characters removed from them and if a
+            32 character alphanumeric string is found it is considered an Asset ID.
             We know assets are valid because we just got them, so we pass verified=True.
-            '''
-            import pathlib
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            WIZ = "simple os.type equals Windows"  # "query of assets to target"
-            ESET = "test"  # "name or uuid of enforcement set"
-            PATH = pathlib.Path("data.txt")
-            ASSETS = apiobj.get(wiz_entries=WIZ)
-            IDS = [x['internal_axon_id'] for x in ASSETS]
-            PATH.write_text('\n'.join(IDS))
-            runner = apiobj.run_enforcement_from_text(eset=ESET, items=PATH, verified=True)
-            print(runner)
-            '''
+            >>> import pathlib
+            >>> PATH: pathlib.Path = pathlib.Path("data.txt")
+            >>> ITEMS: list[dict] = apiobj.get(wiz_entries=WIZ)
+            >>> IDS: list[str] = list(map(lambda x: x['internal_axon_id'], ITEMS))
+            >>> PATH.write_text('\n'.join(IDS))
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_text(
+            ... eset=ESET, items=PATH, verified=True)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -566,15 +569,14 @@
               count_supplied=31,
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_text items type=PosixPath, length=None',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             items (t.Union[str, bytes, t.IO, pathlib.Path]): text str, handle for file containing
                 text str, or pathlib.Path of path containing text str
             keys (t.Union[str, t.List[str]]): n/a
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
@@ -603,25 +605,27 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a JSON file with a list of dicts and run $eset against them.
 
         Examples:
-            '''Run an enforcement against all asset IDs from a JSON file.
-            We are unsure if Asset IDs are still valid for this instance so
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Run an enforcement against all asset IDs from a JSON file.
+            We are unsure if Asset IDs are still valid for this instance of Axonius, so
             we do not pass verified=True.
-            '''
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            PATH = "data.json"
-            ESET = "test"  # "name or uuid of enforcement set"
-            runner = apiobj.run_enforcement_from_json_path(eset=ESET, path=PATH)
-            print(runner)
-            '''
+            >>> PATH: str = "data.json"
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_json_path(eset=ESET, path=PATH)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=31,
               verified=True,
@@ -633,15 +637,14 @@
               do_echo=True,
               do_raise=False,
               source='from_json_path /Users/jimbo/gh/Axonius/axonapi/data.json /
             from_json items type=PosixPath, length=None post_load
             type=list, length=31',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             path (PathLike): str or pathlib.Path of path containing json str
             keys (t.Union[str, t.List[str]]): additional keys for grabber to look for Asset IDs in
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
             do_raise_grab (bool, optional): Throw an error if grabber fails to find an Asset ID
@@ -669,25 +672,27 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a JSONL file with one dict per line and run $eset against them.
 
         Examples:
-            '''Run an enforcement against all asset IDs from a JSONL file.
-            We are unsure if Asset IDs are still valid for this instance so
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Run an enforcement against all asset IDs from a JSONL file.
+            We are unsure if Asset IDs are still valid for this instance, so
             we do not pass verified=True.
-            '''
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            PATH = "data.jsonl"
-            ESET = "test"  # "name or uuid of enforcement set"
-            runner = apiobj.run_enforcement_from_jsonl_path(eset=ESET, path=PATH)
-            print(runner)
-            '''
+            >>> PATH: str = "data.jsonl"
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_jsonl_path(eset=ESET, path=PATH)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=31,
               verified=True,
@@ -698,15 +703,14 @@
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_jsonl_path /Users/jimbo/gh/Axonius/axonapi/data.jsonl /
             from_jsonl items type=PosixPath, length=None post_load type=list, length=31',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             path (PathLike): str or pathlib.Path of path containing jsonl str
             keys (t.Union[str, t.List[str]]): additional keys for grabber to look for Asset IDs in
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
             do_raise_grab (bool, optional): Throw an error if grabber fails to find an Asset ID
@@ -734,25 +738,27 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a CSV file and run $eset against them.
 
         Examples:
-            '''Run an enforcement against all asset IDs from a JSONL file.
-            We are unsure if Asset IDs are still valid for this instance so
-            we do not pass verified=True.
-            '''
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            PATH = "data.csv"
-            ESET = "test"  # "name or uuid of enforcement set"
-            runner = apiobj.run_enforcement_from_csv_path(eset=ESET, path=PATH)
-            print(runner)
-            '''
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Run an enforcement against all asset IDs from a JSONL file.
+            We are unsure if Asset IDs are still valid for this instance,
+            so we do not pass verified=True.
+            >>> PATH: str = "data.csv"
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_csv_path(eset=ESET, path=PATH)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=31,
               verified=True,
@@ -763,15 +769,14 @@
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_csv_path /Users/jimbo/gh/Axonius/axonapi/data.csv /
             from_csv items type=PosixPath, length=None post_load type=list, length=33',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             path (PathLike): str or pathlib.Path of path containing csv str
             keys (t.Union[str, t.List[str]]): additional keys for grabber to look for Asset IDs in
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
             do_raise_grab (bool, optional): Throw an error if grabber fails to find an Asset ID
@@ -799,27 +804,29 @@
         do_echo_grab: bool = True,
         do_raise_grab: bool = False,
         **kwargs,
     ) -> Runner:
         """Get Asset IDs from a text file and run $eset against them.
 
         Examples:
-            '''Run an enforcement against all asset IDs from a text file.
-            All lines will have any non alpha-numeric characters removed from them and if a
-            32 character alpha numeric string is found it is considered an Asset ID.
-            We are unsure if Asset IDs are still valid for this instance so
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> ESET: str = "test"  # "name or uuid of enforcement set"
+
+            Run an enforcement against all asset IDs from a text file.
+            All lines will have any non-alphanumeric characters removed from them and if a
+            32 character alphanumeric string is found it is considered an Asset ID.
+            We are unsure if Asset IDs are still valid for this instance, so
             we do not pass verified=True.
-            '''
-            client = globals()['client']  # instance of axonius_api_client.Connect
-            apiobj = client.devices  # client.devices, client.users, or client.vulnerabilities
-            PATH = "data.txt"
-            ESET = "test"  # "name or uuid of enforcement set"
-            runner = apiobj.run_enforcement_from_text_path(eset=ESET, path=PATH)
-            print(runner)
-            '''
+            >>> PATH: str = "data.txt"
+            >>> RUNNER: Runner = apiobj.run_enforcement_from_text_path(eset=ESET, path=PATH)
+            >>> print(RUNNER)
             Runner(
               state='Ran Enforcement Set against 31 supplied Asset IDs',
               eset='test',
               executed=True,
               count_ids=31,
               count_result=None,
               verified=True,
@@ -830,15 +837,14 @@
               count_found=31,
               do_echo=True,
               do_raise=False,
               source='from_text_path /Users/jimbo/gh/Axonius/axonapi/data.txt /
             from_text items type=PosixPath, length=None post_load type=generator, length=None',
             ),
             )
-            '''
 
         Args:
             eset (ENFORCEMENT): name, uuid, or Enforcement Set object to run
             path (PathLike): str or pathlib.Path of path containing text str
             keys (t.Union[str, t.List[str]]): n/a
             do_echo_grab (bool, optional): Echo output of Asset ID grabber to console as well as log
             do_raise_grab (bool, optional): Throw an error if grabber fails to find an Asset ID
@@ -856,152 +862,180 @@
             source=kwargs.pop("source", None),
         )
         kwargs["ids"] = grabber.axids
         return self.run_enforcement(eset=eset, **kwargs)
 
     @property
     def enforcements(self):
-        """Work with data scopes."""
+        """Work with enforcements."""
         if not hasattr(self, "_enforcements"):
             from ..enforcements import Enforcements
 
             self._enforcements: Enforcements = Enforcements(auth=self.auth)
         return self._enforcements
 
+    # noinspection PyUnusedLocal
     def count(
         self,
         query: t.Optional[str] = None,
         history_date: t.Optional[t.Union[str, datetime.timedelta, datetime.datetime]] = None,
         history_days_ago: t.Optional[int] = None,
         history_exact: bool = False,
         wiz_entries: t.Optional[t.Union[t.List[dict], t.List[str], dict, str]] = None,
+        wiz_parsed: t.Optional[t.List[dict]] = None,
+        history_date_parsed: t.Optional[str] = None,
         use_cache_entry: bool = False,
+        use_heavy_fields_collection: bool = False,
+        frontend_sent_time: t.Optional[datetime.datetime] = None,
+        query_id: t.Optional[t.Union[str, uuid.UUID]] = None,
         saved_query_id: t.Optional[str] = None,
+        request_obj: t.Optional[CountRequest] = None,
+        http_args: t.Optional[dict] = None,
+        sleep: t.Optional[t.Union[int, float]] = 0.5,
         **kwargs,
     ) -> int:
         """Get the count of assets from a query.
 
         Examples:
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
             Get count of all assets
-
-            >>> count = apiobj.count()
-
+            >>> path: int = apiobj.count()
             Get count of all assets for a given date
-
-            >>> count = apiobj.count(history_date="2020-09-29")
-
+            >>> path: int = apiobj.count(history_date="2020-09-29")
             Get count of assets matching a query built by the GUI query wizard
-
-            >>> query='(specific_data.data.name == "test")'
-            >>> count = apiobj.count(query=query)
-
+            >>> use_query: str = '(specific_data.data.name == "test")'
+            >>> path: int = apiobj.count(query=use_query)
             Get count of assets matching a query built by the API client query wizard
-
-            >>> entries = [{'type': 'simple', 'value': 'name equals test'}]
-            >>> count = apiobj.count(wiz_entries=entries)
+            >>> entries: str = 'simple name equals test'
+            >>> path: int = apiobj.count(wiz_entries=entries)
+            Same as above but using a list of dicts instead of a string for wiz_entries
+            >>> entries: t.List[dict] = [{'type': 'simple', 'path': 'name equals test'}]
+            >>> path: int = apiobj.count(wiz_entries=entries)
 
         Args:
-            query: if supplied, only return the count of assets that match the query
-                if not supplied, the count of all assets will be returned
+            query: only return the count of assets that match the query
             history_date: return asset count for a given historical date
-            wiz_entries: wizard expressions to create query from
-
-        """
-        wiz_parsed = self.get_wiz_entries(wiz_entries=wiz_entries)
-
+            history_days_ago: return asset count for a given historical date that is N days ago
+            history_exact: if True, return the exact asset count for a given historical date
+                if False, return the asset count for the closest historical date
+            wiz_entries: build a query from the entries and return the count of
+                assets that match the query
+            wiz_parsed: previously parsed wiz_entries
+            history_date_parsed: previously parsed history_date
+            use_cache_entry: if True, use the last query that was run to get the count
+            use_heavy_fields_collection: if True, use the HEAVV fields collection to get the count
+            frontend_sent_time: time that the query was sent from the frontend
+            query_id: ID to identify this query
+            saved_query_id: ID of saved query that count is being issued for
+            request_obj: request object to use instead of building one
+            http_args: args to pass to http request
+            sleep: time to sleep between requests
+            **kwargs: sent to :meth:`build_count_request`
+        """
+        request_obj = self.build_count_request(
+            request_obj=request_obj,
+            filter=query,
+            history=history_date_parsed,
+            use_cache_entry=use_cache_entry,
+            saved_query_id=saved_query_id,
+            query_id=query_id,
+            use_heavy_fields_collection=use_heavy_fields_collection,
+            frontend_sent_time=frontend_sent_time,
+            **kwargs,
+        )
+        if not isinstance(http_args, dict):
+            http_args = {}
+        if not isinstance(wiz_parsed, dict):
+            wiz_parsed = self.get_wiz_entries(wiz_entries=wiz_entries)
         if isinstance(wiz_parsed, dict):
-            if wiz_parsed.get("query"):
-                query = wiz_parsed["query"]
-
-        history_date = self.get_history_date(
-            date=history_date, days_ago=history_days_ago, exact=history_exact
-        )
-
-        value = None
-
-        while value is None:
-            value = self._count(
-                filter=query,
-                history_date=history_date,
-                use_cache_entry=use_cache_entry,
-            ).value
-            use_cache_entry = True
-
-        return value
+            wiz_query: t.Optional[str] = wiz_parsed.get("query")
+            if isinstance(wiz_query, str) and wiz_query:
+                request_obj.filter = wiz_query
+        if not history_date_parsed and not request_obj.history:
+            request_obj.history = self.get_history_date(
+                date=history_date, days_ago=history_days_ago, exact=history_exact
+            )
+        count: t.Optional[int] = None
+        while not isinstance(count, int):
+            response: Count = self._count(request_obj=request_obj, http_args=http_args)
+            count: t.Optional[int] = response.value
+            if isinstance(count, int):
+                break
+            request_obj.use_cache_entry = True
+            if isinstance(sleep, (int, float)):
+                time.sleep(sleep)
+        return count
 
     def count_by_saved_query(self, name: str, **kwargs) -> int:
         """Get the count of assets for a query defined in a saved query.
 
         Examples:
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
             Get count of assets returned from a saved query
-
-            >>> count = apiobj.count_by_saved_query(name="test")
-
+            >>> count: int = apiobj.count_by_saved_query(name="test")
             Get count of assets returned from a saved query for a given date
-
-            >>> count = apiobj.count_by_saved_query(name="test", history_date="2020-09-29")
+            >>> count: int = apiobj.count_by_saved_query(name="test", history_date="2020-09-29")
 
         Args:
             name: saved query to get count of assets from
             kwargs: supplied to :meth:`count`
         """
         sq = self.saved_query.get_by_multi(sq=name)
-        kwargs["query"] = sq["view"]["query"]["filter"]
+        _view: dict = sq.get("view", {})
+        _query: dict = _view.get("query", {})
+        kwargs["query"]: t.Optional[str] = _query.get("filter")
         kwargs["saved_query_id"] = sq["id"]
         return self.count(**kwargs)
 
     def get(self, generator: bool = False, **kwargs) -> GEN_TYPE:
         r"""Get assets from a query.
 
         Examples:
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
             Get all assets with the default fields defined in the API client
-
-            >>> assets = apiobj.get()
-
-            Get all assets using an iterator
-
-            >>> assets = [x for x in apiobj.get(generator=True)]
-
-            Get all assets with fields that equal names
-
-            >>> assets = apiobj.get(fields=["os.type", "aws:aws_device_type"])
-
-            Get all assets with fields that fuzzy match names and no default fields
-
-            >>> assets = apiobj.get(fields_fuzzy=["last", "os"], fields_default=False)
-
-            Get all assets with fields that regex match names a
-
-            >>> assets = apiobj.get(fields_regex=["^os\."])
-
-            Get all assets with all root fields for an adapter
-
-            >>> assets = apiobj.get(fields_root="aws")
-
+            >>> assets: list[dict] = apiobj.get()
+            Get all assets using a generator
+            >>> assets: list[dict] = list(apiobj.get(generator=True))
+            Get all assets and include more fields
+            >>> fields: list[str] = ["os.type", "aws:aws_device_type"]
+            >>> assets: list[dict] = apiobj.get(fields=fields)
+            Get all assets and include fields that fuzzy match names and no default fields
+            >>> fields_fuzzy: list[str] = ["last", "os"]
+            >>> assets: list[dict] = apiobj.get(fields_fuzzy=fields_fuzzy, fields_default=False)
+            Get all assets and include fields that regex match names a
+            >>> fields_regex: list[str] = ["^os\."]
+            >>> assets: list[dict] = apiobj.get(fields_regex=fields_regex)
+            Get all assets and include all root fields for an adapter
+            >>> assets: list[dict] = apiobj.get(fields_root="aws")
             Get all assets for a given date in history and sort the rows on a field
-
-            >>> assets = apiobj.get(history_date="2020-09-29", sort_field="name")
-
-            Get all assets with details of which adapter connection provided the agg data
-
-            >>> assets = apiobj.get(include_details=True)
-
+            >>> assets: list[dict] = apiobj.get(history_date="2020-09-29", sort_field="name")
+            Get all assets with details of which adapter connection provided the aggregated data
+            >>> assets: list[dict] = apiobj.get(include_details=True)
             Get assets matching a query built by the GUI query wizard
-
-            >>> query='(specific_data.data.name == "test")'
-            >>> assets = apiobj.get(query=query)
-
+            >>> query: str ='(specific_data.data.name == "test")'
+            >>> assets: list[dict] = apiobj.get(query=query)
             Get assets matching a query built by the API client query wizard
-
-            >>> entries=[{'type': 'simple', 'value': 'name equals test'}]
-            >>> assets = apiobj.get(wiz_entries=entries)
+            >>> wiz_entries: list[dict] = [{'type': 'simple', 'path': 'name equals test'}]
+            >>> assets: list[dict] = apiobj.get(wiz_entries=wiz_entries)
 
         See Also:
             This method is used by all other get* methods under the hood and their kwargs are
-            passed thru to this method and passed to :meth:`get_generator` which are then passed
+            passed through to this method and passed to :meth:`get_generator` which are then passed
             to whatever callback is used based on the ``export`` argument.
 
             If ``export`` is not supplied, see
             :meth:`axonius_api_client.api.asset_callbacks.base.Base.args_map`.
 
             If ``export`` equals ``json``, see
             :meth:`axonius_api_client.api.asset_callbacks.base_json.Json.args_map`.
@@ -1014,59 +1048,25 @@
 
             If ``export`` equals ``table``, see
             :meth:`axonius_api_client.api.asset_callbacks.base_table.Table.args_map`.
 
             If ``export`` equals ``xlsx``, see
             :meth:`axonius_api_client.api.asset_callbacks.base_xlsx.Xlsx.args_map`.
 
+            :obj:`axonius_api_client.constants.asset_helpers.ASSET_HELPERS` for a list of
+            helpers that translate between GUI titles, API request attributes, and saved query
+            paths.
+
         Args:
             generator: return an iterator for assets that will yield rows as they are fetched
             **kwargs: passed to :meth:`get_generator`
         """
         gen = self.get_generator(**kwargs)
         return gen if generator else list(gen)
 
-    def get_wiz_entries(
-        self, wiz_entries: t.Optional[t.Union[t.List[dict], t.List[str], dict, str]] = None
-    ) -> t.Optional[dict]:
-        """Pass."""
-        wiz_entries = listify(wiz_entries) if wiz_entries else []
-        if not wiz_entries:
-            return None
-
-        if all([isinstance(x, dict) for x in wiz_entries]):
-            return self.wizard.parse(entries=wiz_entries)
-
-        if all([isinstance(x, str) for x in wiz_entries]):
-            return self.wizard_text.parse(content=wiz_entries)
-
-        raise ApiError("wiz_entries must be a single or list of dict or str")
-
-    def get_sort_field(
-        self, field: t.Optional[str] = None, descending: bool = False
-    ) -> t.Optional[str]:
-        """Pass."""
-        if isinstance(field, str) and field:
-            field = self.fields.get_field_name(value=field)
-            field = f"-{field}" if descending else field
-        else:
-            field = None
-        return field
-
-    def get_history_date(
-        self,
-        date: t.Optional[t.Union[str, datetime.timedelta, datetime.datetime]] = None,
-        days_ago: t.Optional[int] = None,
-        exact: bool = False,
-    ) -> t.Optional[str]:
-        """Pass."""
-        if date is not None or days_ago is not None:
-            return self.history_dates_obj().get_date(date=date, days_ago=days_ago, exact=exact)
-        return None
-
     def get_generator(
         self,
         query: t.Optional[str] = None,
         fields: t.Optional[t.Union[t.List[str], str]] = None,
         fields_manual: t.Optional[t.Union[t.List[str], str]] = None,
         fields_regex: t.Optional[t.Union[t.List[str], str]] = None,
         fields_regex_root_only: bool = True,
@@ -1077,29 +1077,76 @@
         max_rows: t.Optional[int] = None,
         max_pages: t.Optional[int] = None,
         row_start: int = 0,
         page_size: int = MAX_PAGE_SIZE,
         page_start: int = 0,
         page_sleep: int = 0,
         export: str = DEFAULT_CALLBACKS_CLS,
-        include_notes: bool = False,
-        include_details: bool = False,
         sort_field: t.Optional[str] = None,
         sort_descending: bool = False,
         history_date: t.Optional[t.Union[str, datetime.timedelta, datetime.datetime]] = None,
         history_days_ago: t.Optional[int] = None,
         history_exact: bool = False,
         wiz_entries: t.Optional[t.Union[t.List[dict], t.List[str], dict, str]] = None,
-        saved_query_id: t.Optional[str] = None,
+        wiz_parsed: t.Optional[dict] = None,
+        file_date: t.Optional[str] = None,
+        use_heavy_fields_collection: bool = False,
+        sort_field_parsed: t.Optional[str] = None,
+        search: t.Optional[str] = None,
+        history_date_parsed: t.Optional[str] = None,
+        field_filters: t.Optional[t.List[dict]] = None,
+        excluded_adapters: t.Optional[t.List[dict]] = None,
+        asset_excluded_adapters: t.Optional[t.List[dict]] = None,
+        asset_filters: t.Optional[t.List[dict]] = None,
         expressions: t.Optional[t.List[dict]] = None,
+        fields_parsed: t.Optional[t.Union[dict, t.List[str]]] = None,
+        include_details: bool = False,
+        include_notes: bool = False,
+        use_cursor: bool = True,
+        cursor_id: t.Optional[str] = None,
+        saved_query_id: t.Optional[str] = None,
+        query_id: t.Optional[t.Union[str, uuid.UUID]] = None,
+        is_refresh: bool = False,
+        null_for_non_exist: bool = False,
+        source_component: t.Optional[str] = None,
+        frontend_sent_time: t.Optional[datetime.datetime] = None,
+        filter_out_non_existing_fields: bool = True,
+        complex_fields_preview_limit: t.Optional[int] = None,
+        max_field_items: t.Optional[int] = None,
+        initial_count: t.Optional[int] = None,
+        request_obj: t.Optional[AssetRequest] = None,
+        export_templates: t.Optional[dict] = None,
         http_args: t.Optional[dict] = None,
         **kwargs,
     ) -> t.Generator[dict, None, None]:
         """Get assets from a query.
 
+        See Also:
+            If ``export`` is not supplied, see
+            :meth:`axonius_api_client.api.asset_callbacks.base.Base.args_map`.
+
+            If ``export`` equals ``json``, see
+            :meth:`axonius_api_client.api.asset_callbacks.base_json.Json.args_map`.
+
+            If ``export`` equals ``csv``, see
+            :meth:`axonius_api_client.api.asset_callbacks.base_csv.Csv.args_map`.
+
+            If ``export`` equals ``json_to_csv``, see
+            :meth:`axonius_api_client.api.asset_callbacks.base_json_to_csv.JsonToCsv.args_map`.
+
+            If ``export`` equals ``table``, see
+            :meth:`axonius_api_client.api.asset_callbacks.base_table.Table.args_map`.
+
+            If ``export`` equals ``xlsx``, see
+            :meth:`axonius_api_client.api.asset_callbacks.base_xlsx.Xlsx.args_map`.
+
+            :obj:`axonius_api_client.constants.asset_helpers.ASSET_HELPERS` for a list of
+            helpers that translate between GUI titles, API request attributes, and saved query
+            paths.
+
         Args:
             query: if supplied, only get the assets that match the query
             fields: fields to return for each asset (will be validated)
             fields_manual: fields to return for each asset (will NOT be validated)
             fields_regex: regex of fields to return for each asset
             fields_regex_root_only: only match fields_regex values against root fields
             fields_fuzzy: string to fuzzy match of fields to return for each asset
@@ -1111,66 +1158,133 @@
             row_start: start at row N
             page_size: fetch N rows per page
             page_start: start at page N
             page_sleep: sleep for N seconds between each page fetch
             export: export assets using a callback method
             include_notes: include any defined notes for each adapter
             include_details: include details fields showing the adapter source of agg values
+            saved_query_id: ID of saved query this fetch is associated with
+            expressions: expressions used by query wizard to create query
             sort_field: sort the returned assets on a given field
             sort_descending: reverse the sort of the returned assets
             history_date: return assets for a given historical date
             history_days_ago: return assets for a history date N days ago
             history_exact: Use the closest match for history_date and history_days_ago
             wiz_entries: wizard expressions to create query from
+            file_date: string to use in filename templates for {DATE}
+            wiz_parsed: parsed output from a query wizard
+            fields_parsed: previously parsed fields
+            sort_field_parsed: previously parsed sort field
+            history_date_parsed: previously parsed history date
+            initial_count: previously fetched initial count
+            search: search string to use for this query
+            use_heavy_fields_collection: unknown
+            use_cursor: use cursor based pagination
+            field_filters: field filters to apply to this query
+            excluded_adapters: adapters to exclude from this query
+            asset_excluded_adapters: adapters to exclude from this query
+            asset_filters: asset filters to apply to this query
+            cursor_id: cursor ID to use for this query
+            query_id: query ID to use for this query
+            is_refresh: is this a refresh query
+            null_for_non_exist: return null for non existent fields
+            source_component: source component to use for this query
+            export_templates: filename template replacement mappings
+            filter_out_non_existing_fields: filter out fields that do not exist
+            complex_fields_preview_limit: limit the number of complex fields to preview
+            max_field_items: max number of items to return for a field
+            frontend_sent_time: frontend sent time to use for this query
+            http_args: http args to pass to :meth:`axonius_api_client.http.Http.__call__` for each
+                page fetched
+            request_obj: request object to use for this query
             **kwargs: passed thru to the asset callback defined in ``export``
         """
-        wiz_parsed: t.Optional[dict] = kwargs.get(
-            "_wiz_parsed", self.get_wiz_entries(wiz_entries=wiz_entries)
+        request_obj: AssetRequest = self.build_get_request(
+            request_obj=request_obj,
+            search=search,
+            filter=query,
+            history=history_date_parsed,
+            sort=sort_field_parsed,
+            field_filters=field_filters,
+            excluded_adapters=excluded_adapters,
+            asset_filters=asset_filters,
+            expressions=expressions,
+            include_details=include_details,
+            include_notes=include_notes,
+            use_cursor=use_cursor,
+            cursor_id=cursor_id,
+            saved_query_id=saved_query_id,
+            query_id=query_id,
+            source_component=source_component,
+            frontend_sent_time=frontend_sent_time,
+            filter_out_non_existing_fields=filter_out_non_existing_fields,
+            is_refresh=is_refresh,
+            null_for_non_exist=null_for_non_exist,
+            max_field_items=max_field_items,
+            complex_fields_preview_limit=complex_fields_preview_limit,
+            use_heavy_fields_collection=use_heavy_fields_collection,
+            asset_excluded_adapters=asset_excluded_adapters,
         )
+        request_obj.get_metadata = True
+        request_obj.use_cursor = use_cursor
+
+        if not isinstance(http_args, dict):
+            http_args: dict = {}
+
+        if not isinstance(wiz_parsed, dict):
+            wiz_parsed: dict = self.get_wiz_entries(wiz_entries=wiz_entries)
 
         if isinstance(wiz_parsed, dict):
-            if wiz_parsed.get("query"):
-                query = wiz_parsed["query"]
-            if wiz_parsed.get("expressions"):
-                expressions = wiz_parsed["expressions"]
-
-        fields_parsed: t.List[str] = kwargs.get(
-            "_fields_parsed",
-            self.fields.validate(
+            wiz_query: t.Optional[str] = wiz_parsed.get("query")
+            wiz_expressions: t.Optional[t.List[dict]] = wiz_parsed.get("expressions")
+            if isinstance(wiz_query, str) and wiz_query:
+                query = wiz_query
+            if isinstance(wiz_expressions, list) and wiz_expressions:
+                request_obj.expressions = wiz_expressions
+
+        if not isinstance(fields_parsed, (list, tuple)):
+            fields_parsed = self.fields.validate(
                 fields=fields,
                 fields_manual=fields_manual,
                 fields_regex=fields_regex,
                 fields_regex_root_only=fields_regex_root_only,
                 fields_default=fields_default,
                 fields_root=fields_root,
                 fields_fuzzy=fields_fuzzy,
                 fields_error=fields_error,
-            ),
-        )
+            )
 
-        sort_field_parsed: t.Optional[str] = kwargs.get(
-            "_sort_field_parsed", self.get_sort_field(field=sort_field, descending=sort_descending)
-        )
+        if not isinstance(sort_field_parsed, str):
+            request_obj.sort = sort_field_parsed = self.get_sort_field(
+                field=sort_field, descending=sort_descending
+            )
 
-        history_date_parsed: t.Optional[str] = kwargs.get(
-            "_history_date_parsed",
-            self.get_history_date(
+        if not isinstance(history_date_parsed, (str, datetime.datetime)):
+            request_obj.history = history_date_parsed = self.get_history_date(
                 date=history_date, days_ago=history_days_ago, exact=history_exact
-            ),
-        )
+            )
 
-        initial_count: int = kwargs.get(
-            "_initial_count", self.count(query=query, history_date=history_date)
-        )
+        if not isinstance(initial_count, int):
+            initial_count: int = self.count(
+                query=query,
+                frontend_sent_time=request_obj.frontend_sent_time,
+                history_date_parsed=history_date_parsed,
+                query_id=request_obj.query_id,
+                saved_query_id=request_obj.saved_query_id,
+                use_heavy_fields_collection=request_obj.use_heavy_fields_collection,
+            )
 
-        file_date: str = kwargs.get("_file_date", dt_now_file())
-        export_templates: dict = {
-            "{DATE}": file_date,
-            "{HISTORY_DATE}": history_date_parsed or file_date,
-        }
+        if not isinstance(file_date, str):
+            file_date: str = dt_now_file()
+
+        if not isinstance(export_templates, dict):
+            export_templates = {}
+
+        export_templates.setdefault("{DATE}", file_date)
+        export_templates.setdefault("{HISTORY_DATE}", history_date_parsed or file_date)
 
         store: dict = {
             "export": export,
             "query": query,
             "fields_parsed": fields_parsed,
             "fields": fields,
             "fields_regex": fields_regex,
@@ -1192,121 +1306,265 @@
             "max_pages": max_pages,
             "page_size": page_size,
             "page_sleep": page_sleep,
             "page_start": page_start,
             "row_start": row_start,
             "initial_count": initial_count,
             "export_templates": export_templates,
+            "request_obj": request_obj,
         }
-
-        state = json_api.assets.AssetsPage.create_state(
+        state: dict = AssetsPage.create_state(
             max_pages=max_pages,
             max_rows=max_rows,
             page_sleep=page_sleep,
             page_size=page_size,
             page_start=page_start,
             row_start=row_start,
             initial_count=initial_count,
         )
-
-        callbacks_cls = get_callbacks_cls(export=export)
-        callbacks = callbacks_cls(apiobj=self, getargs=kwargs, state=state, store=store)
-
-        self.LAST_CALLBACKS = callbacks
+        callbacks_cls: t.Type[BaseCallbacks] = get_callbacks_cls(export=export)
+        callbacks: BaseCallbacks = callbacks_cls(
+            apiobj=self, getargs=kwargs, state=state, store=store
+        )
+        self.LAST_CALLBACKS: BaseCallbacks = callbacks
         callbacks.start()
-
         self.LOG.info(f"STARTING FETCH store={json_dump(store)}")
         self.LOG.debug(f"STARTING FETCH state={json_dump(state)}")
 
         while not state["stop_fetch"]:
-            try:
-                start_dt = dt_now()
-
-                page = self._get(
-                    include_details=store["include_details"],
-                    include_notes=store["include_notes"],
-                    sort=store["sort_field_parsed"],
-                    history_date=store["history_date_parsed"],
-                    filter=store["query"],
-                    fields=store["fields_parsed"],
-                    cursor_id=state["page_cursor"],
-                    offset=state["rows_offset"],
-                    limit=state["page_size"],
-                    saved_query_id=saved_query_id,
-                    expressions=expressions,
-                    always_cached_query=False,
-                    use_cache_entry=False,
-                    get_metadata=True,
-                    use_cursor=True,
-                    http_args=http_args,
-                )
+            request_obj.filter = store["query"]
+            request_obj.fields = {self.ASSET_TYPE: store["fields_parsed"]}
+            request_obj.include_details = store["include_details"]
+            request_obj.include_notes = store["include_notes"]
+            request_obj.set_offset(state["rows_offset"])
+            request_obj.set_limit(state["page_size"])
 
-                state = page.process_page(state=state, start_dt=start_dt, apiobj=self)
+            try:
+                start_dt: datetime.datetime = dt_now()
+                page: AssetsPage = self._get(request_obj=request_obj, http_args=http_args)
 
+                if request_obj.use_cursor:
+                    request_obj.cursor_id = page.cursor
+                state: dict = page.process_page(state=state, start_dt=start_dt, apiobj=self)
                 for row in page.assets:
-                    state = page.start_row(state=state, apiobj=self, row=row)
+                    state: dict = page.start_row(state=state, apiobj=self, row=row)
                     yield from listify(obj=callbacks.process_row(row=row))
-                    state = page.process_row(state=state, apiobj=self, row=row)
-
-                state = page.process_loop(state=state, apiobj=self)
-
+                    state: dict = page.process_row(state=state, apiobj=self, row=row)
+                state: dict = page.process_loop(state=state, apiobj=self)
                 time.sleep(state["page_sleep"])
             except StopFetch as exc:
                 self.LOG.debug(f"Received {type(exc)}: {exc.reason}")
                 break
-
         self.LOG.info(f"FINISHED FETCH store={json_dump(store)}")
         self.LOG.debug(f"FINISHED FETCH state={json_dump(state)}")
-
         callbacks.stop()
 
-    def get_by_saved_query(self, name: str, **kwargs) -> GEN_TYPE:
+    def get_by_saved_query(
+        self,
+        name: str,
+        include_fields: bool = True,
+        include_excluded_adapters: bool = True,
+        include_asset_excluded_adapters: bool = True,
+        include_field_filters: bool = True,
+        include_asset_filters: bool = True,
+        **kwargs,
+    ) -> GEN_TYPE:
         """Get assets that would be returned by a saved query.
 
         Examples:
             First, create a ``client`` using :obj:`axonius_api_client.connect.Connect` and assume
             ``apiobj`` is ``client.devices`` or ``client.users``
-
-            >>> apiobj = client.devices
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
 
             Get assets from a saved query with complex fields flattened
+            >>> assets: t.List[dict] = apiobj.get_by_saved_query(name="test", field_flatten=True)
 
-            >>> assets = apiobj.get_by_saved_query(name="test", field_flatten=True)
-
-        Notes:
-            The query and the fields defined in the saved query will be used to
-            get the assets.
+        See Also:
+            :obj:`axonius_api_client.constants.asset_helpers.ASSET_HELPERS` for a list of
+            helpers that translate between GUI titles, API request attributes, and saved query
+            paths.
 
         Args:
             name: name of saved query to get assets from
+            include_fields: include fields from saved query
+            include_excluded_adapters: include column filters for excluded adapters from saved query
+            include_asset_excluded_adapters: include column filters for asset excluded adapters
+                from saved query
+            include_field_filters: include column filters for field filters from saved query
+            include_asset_filters: include column filters for asset filters from saved query
             **kwargs: passed to :meth:`get`
         """
         sq = self.saved_query.get_by_multi(sq=name)
-        kwargs["query"] = sq["view"]["query"]["filter"]
-        kwargs["fields_manual"] = sq["view"]["fields"]
+        _view: dict = sq.get("view", {})
+        _query: dict = _view.get("query", {})
+
         kwargs["saved_query_id"] = sq["id"]
-        kwargs["expressions"] = sq["view"]["query"]["expressions"]
-        kwargs.setdefault("fields_default", False)
+        kwargs["query"] = _query.get("filter")
+        kwargs["expressions"] = _query.get("expressions")
+
+        if include_fields:
+            kwargs["fields_manual"] = _view.get("fields")
+
+        if include_excluded_adapters:
+            kwargs["excluded_adapters"] = _view.get("colExcludedAdapters")
+
+        if include_asset_excluded_adapters:
+            kwargs["asset_excluded_adapters"] = _view.get("assetExcludeAdapters")
+
+        if include_field_filters:
+            kwargs["field_filters"] = _view.get("fieldFilters")
+
+        if include_asset_filters:
+            kwargs["asset_filters"] = _view.get("assetFilters")
+
+        if kwargs.get("fields_manual"):
+            kwargs.setdefault("fields_default", False)
+
         return self.get(**kwargs)
 
+    def get_wiz_entries(
+        self, wiz_entries: t.Optional[t.Union[t.List[dict], t.List[str], dict, str]] = None
+    ) -> t.Optional[dict]:
+        """Build a query and expressions.
+
+        Examples:
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+
+            None is returned if no wiz_entries are passed
+            >>> items = None
+            >>> parsed = apiobj.get_wiz_entries(wiz_entries=items)
+            >>> print(parsed)
+            None
+
+            A string or list of strings will be parsed into a query and expressions:
+            >>> items = "simple hostname contains test"
+            >>> parsed = apiobj.get_wiz_entries(wiz_entries=items)
+            >>> client.jdump(parsed)
+           {
+              "expressions": [
+                {
+                  "bracketWeight": 0,
+                  "children": [
+                    {
+                      "condition": "",
+                      "expression": {
+                        "compOp": "",
+                        "field": "",
+                        "filteredAdapters": null,
+                        "value": null
+                      },
+                      "i": 0
+                    }
+                  ],
+                  "compOp": "contains",
+                  "field": "specific_data.data.hostname",
+                  "fieldType": "axonius",
+                  "filter": "(\"specific_data.data.hostname\" == regex(\"test\", \"i\"))",
+                  "filteredAdapters": null,
+                  "leftBracket": false,
+                  "logicOp": "",
+                  "not": false,
+                  "rightBracket": false,
+                  "value": "test"
+                }
+              ],
+              "query": "(\"specific_data.data.hostname\" == regex(\"test\", \"i\"))"
+            }
+
+            A dict or list of dicts will be parsed into a query and expressions
+            >>> items = {"type": "simple", "value": "hostname contains test"}
+            >>> parsed = client.devices.get_wiz_entries(items)
+            >>> # same output as above
+
+        Args:
+            wiz_entries: list of dicts or list of strings or single dict or single string
+        """
+        wiz_entries = listify(wiz_entries) if wiz_entries else []
+        if not wiz_entries:
+            return None
+
+        if all([isinstance(x, dict) for x in wiz_entries]):
+            return self.wizard.parse(entries=wiz_entries)
+
+        if all([isinstance(x, str) for x in wiz_entries]):
+            return self.wizard_text.parse(content=wiz_entries)
+
+        raise ApiError("wiz_entries must be a single or list of dict or str")
+
+    def get_sort_field(
+        self, field: t.Optional[str] = None, descending: bool = False, validate: bool = True
+    ) -> t.Optional[str]:
+        """Build the parsed sort field based off of field and descending.
+
+        Args:
+            field: field to sort by
+            descending: if True, sort descending
+            validate: if True, validate field name
+
+        Returns:
+            field (prefixed with - if descending), None if field is None
+        """
+        if isinstance(field, str) and field:
+            if validate:
+                field = self.fields.get_field_name(value=field)
+            field = f"-{field}" if descending else field
+        else:
+            field = None
+        return field
+
+    def get_history_date(
+        self,
+        date: t.Optional[t.Union[str, datetime.timedelta, datetime.datetime]] = None,
+        days_ago: t.Optional[int] = None,
+        exact: bool = False,
+    ) -> t.Optional[str]:
+        """Get a history date.
+
+        Args:
+            date: date to get
+            days_ago: days ago to get
+            exact: if True, do not round to the nearest day
+
+        Returns:
+            date in YYYY-MM-DD format or None
+        """
+        if date is not None or days_ago is not None:
+            return self.history_dates_obj().get_date(date=date, days_ago=days_ago, exact=exact)
+        return None
+
     def get_by_id(self, id: str) -> dict:
         """Get the full data set of all adapters for a single asset.
 
         Examples:
-            >>> asset = apiobj.get_by_id(id="3d69adf54879faade7a44068e4ecea6e")
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> assets: list[dict] = apiobj.get(max_rows=1)
+            >>> asset_id: str = assets[0]["internal_axon_id"]
+            >>> asset: dict = apiobj.get_by_id(id=as)
 
         Args:
             id: internal_axon_id of asset to get all data set for
 
         Raises:
             :exc:`NotFoundError`: if id is not found
 
         """
         try:
-            return self._get_by_id(id=id).asset
+            return self._get_by_id(id=id).to_dict()
         except ResponseNotOk as exc:
             if exc.response.status_code == 404:
                 asset_type = self.ASSET_TYPE
                 msg = f"Failed to find {asset_type} asset with internal_axon_id of {id!r}"
                 raise NotFoundError(msg)
             raise  # pragma: no cover
 
@@ -1342,15 +1600,15 @@
         """Build a query to get assets where field in values.
 
         Notes:
             It is better to use :attr:`wizard`, :attr:`wizard_text`, or :attr:`wizard_csv`
             to build queries!
 
         Args:
-            values: list of values that must match field
+            values: list of values that must match `field`
             field: name of field to query against
             not_flag: prefix query with 'not'
             pre: query to add to the beginning of the query
             post: query to add to the end of the query
             field_manual: consider supplied field as a fully qualified field name
             **kwargs: passed to :meth:`get`
         """
@@ -1385,17 +1643,17 @@
         """Build a query to get assets where field regex matches a value.
 
         Notes:
             It is better to use :attr:`wizard`, :attr:`wizard_text`, or :attr:`wizard_csv`
             to build queries!
 
         Args:
-            value: regex that must match field
+            value: regex that must match `field`
             field: name of field to query against
-            case_insensitive: ignore case when performing the regex match
+            cast_insensitive: ignore case when performing the regex match
             not_flag: prefix query with 'not'
             pre: query to add to the beginning of the query
             post: query to add to the end of the query
             field_manual: consider supplied field as a fully qualified field name
             **kwargs: passed to :meth:`get`
         """
         field = self.fields.get_field_name(value=field, field_manual=field_manual)
@@ -1422,15 +1680,15 @@
         """Build a query to get assets where field equals a value.
 
         Notes:
             It is better to use :attr:`wizard`, :attr:`wizard_text`, or :attr:`wizard_csv`
             to build queries!
 
         Args:
-            value: value that must equal field
+            value: value that must equal `field`
             field: name of field to query against
             not_flag: prefix query with 'not'
             pre: query to add to the beginning of the query
             post: query to add to the end of the query
             field_manual: consider supplied field as a fully qualified field name
             **kwargs: passed to :meth:`get`
         """
@@ -1444,15 +1702,15 @@
             post=post,
             not_flag=not_flag,
         )
 
         return self.get(**kwargs)
 
     @cachetools.cached(cache=HISTORY_DATES_OBJ_CACHE)
-    def history_dates_obj(self) -> json_api.assets.AssetTypeHistoryDates:
+    def history_dates_obj(self) -> AssetTypeHistoryDates:
         """Pass."""
         return self._history_dates().parsed[self.ASSET_TYPE]
 
     @cachetools.cached(cache=HISTORY_DATES_CACHE)
     def history_dates(self) -> dict:
         """Get all known historical dates."""
         return self._history_dates().value[self.ASSET_TYPE]
@@ -1479,14 +1737,69 @@
 
         lines = [pre, inner, post]
         query = " ".join([x.strip() for x in lines if x.strip()]).strip()
 
         self.LOG.debug(f"Built query: {query!r}")
         return query
 
+    @staticmethod
+    def build_get_request(
+        request_obj: t.Optional[AssetRequest] = None,
+        offset: t.Optional[int] = 0,
+        limit: t.Optional[int] = PAGE_SIZE,
+        remove_unknown_arguments: bool = True,
+        warn_unknown_arguments: bool = True,
+        **kwargs,
+    ) -> AssetRequest:
+        """Build a request object for a get assets request.
+
+        Args:
+            request_obj: request object to use
+            offset: offset to start at
+            limit: number of assets to return
+            remove_unknown_arguments: remove unknown arguments from kwargs
+            warn_unknown_arguments: warn about unknown arguments
+            **kwargs: passed to :meth:`load_request`
+        """
+        if not isinstance(request_obj, AssetRequest):
+            api_endpoint: ApiEndpoint = ApiEndpoints.assets.get
+            request_obj: AssetRequest = api_endpoint.load_request(
+                remove_unknown_arguments=remove_unknown_arguments,
+                warn_unknown_arguments=warn_unknown_arguments,
+                **kwargs,
+            )
+            request_obj.set_limit(limit)
+            request_obj.set_offset(offset)
+        return request_obj
+
+    @staticmethod
+    def build_count_request(
+        request_obj: t.Optional[CountRequest] = None,
+        remove_unknown_arguments: bool = True,
+        warn_unknown_arguments: bool = True,
+        **kwargs,
+    ) -> CountRequest:
+        """Build a request object for a get asset count request.
+
+        Args:
+            request_obj: request object to use
+            remove_unknown_arguments: remove unknown arguments from kwargs
+            warn_unknown_arguments: warn about unknown arguments
+            **kwargs: passed to :meth:`load_request`
+        """
+        if not isinstance(request_obj, CountRequest):
+            api_endpoint: ApiEndpoint = ApiEndpoints.assets.count
+            kwargs, _ = CountRequest.remove_unknown_arguments(
+                kwargs=kwargs,
+                remove_unknown_arguments=remove_unknown_arguments,
+                warn_unknown_arguments=warn_unknown_arguments,
+            )
+            request_obj: CountRequest = api_endpoint.load_request(**kwargs)
+        return request_obj
+
     @property
     def data_scopes(self):
         """Work with data scopes."""
         if not hasattr(self, "_data_scopes"):
             from ..system import DataScopes
 
             self._data_scopes: DataScopes = DataScopes(auth=self.auth)
@@ -1521,135 +1834,103 @@
 
         self.wizard_text: WizardText = WizardText(apiobj=self)
         """Query wizard builder from text."""
 
         self.wizard_csv: WizardCsv = WizardCsv(apiobj=self)
         """Query wizard builder from CSV."""
 
-        self.LAST_GET: dict = {}
-        """Request object sent for last :meth:`get` request"""
+        self.LAST_GET: t.Optional[dict] = None
+        """Request object sent for last :meth:`_get` request"""
 
-        self.LAST_GET_REQUEST_OBJ: json_api.assets.AssetRequest = None
-        """Request data model sent for last :meth:`get` request"""
+        self.LAST_COUNT: t.Optional[dict] = None
+        """Request object sent for last :meth:`_count` request"""
 
-        self.LAST_CALLBACKS: Base = None
+        self.LAST_GET_REQUEST_OBJ: t.Optional[AssetRequest] = None
+        """Request data model sent for last :meth:`_get` request"""
+
+        self.LAST_COUNT_REQUEST_OBJ: t.Optional[CountRequest] = None
+        """Request data model sent for last :meth:`_count` request"""
+
+        self.LAST_CALLBACKS: t.Optional[Base] = None
         """Callbacks object used for last :meth:`get` request."""
 
         super(AssetMixin, self)._init(**kwargs)
 
     def _get(
         self,
-        always_cached_query: bool = False,
-        use_cache_entry: bool = False,
-        include_details: bool = False,
-        include_notes: bool = False,
-        get_metadata: bool = True,
-        use_cursor: bool = True,
-        sort_descending: bool = False,
-        history_date: t.Optional[str] = None,
-        filter: t.Optional[str] = None,
-        cursor_id: t.Optional[str] = None,
-        sort: t.Optional[str] = None,
-        excluded_adapters: t.Optional[dict] = None,
-        field_filters: t.Optional[dict] = None,
-        fields: t.Optional[dict] = None,
-        saved_query_id: t.Optional[str] = None,
-        expressions: t.Optional[t.List[dict]] = None,
-        offset: int = 0,
-        limit: int = PAGE_SIZE,
+        request_obj: t.Optional[AssetRequest] = None,
+        offset: t.Optional[int] = 0,
+        limit: t.Optional[int] = PAGE_SIZE,
         http_args: t.Optional[dict] = None,
-    ) -> json_api.assets.AssetsPage:
-        """Private API method to get a page of assets.
+        **kwargs,
+    ) -> AssetsPage:
+        """Private API method to get a page of assets using a request object.
 
         Args:
-            always_cached_query (bool, optional): UNK
-            use_cache_entry (bool, optional): UNK
-            include_details (bool, optional): include details fields showing the adapter source
-                of agg values
-            include_notes (bool, optional): Description
-            get_metadata (bool, optional): Description
-            use_cursor (bool, optional): Description
-            sort_descending (bool, optional): reverse the sort of the returned assets
-            history_date (t.Optional[str], optional): return assets for a given historical date
-            filter (t.Optional[str], optional): Description
-            cursor_id (t.Optional[str], optional): Description
-            sort (t.Optional[str], optional): Description
-            excluded_adapters (t.Optional[dict], optional): Description
-            field_filters (t.Optional[dict], optional): Description
-            fields (t.Optional[dict], optional): CSV or list of fields to include in return
-            offset (int, optional): Description
-            limit (int, optional): Description
-
-        """
-        asset_type = self.ASSET_TYPE
-        api_endpoint = ApiEndpoints.assets.get
-        request_obj = api_endpoint.load_request(
-            always_cached_query=always_cached_query,
-            use_cache_entry=use_cache_entry,
-            include_details=include_details,
-            include_notes=include_notes,
-            get_metadata=get_metadata,
-            use_cursor=use_cursor,
-            filter=filter,
-            cursor_id=cursor_id,
-            history=history_date,
-            fields={self.ASSET_TYPE: listify(fields)},
-            sort=sort,
-            excluded_adapters=excluded_adapters or {},
-            field_filters=field_filters or {},
-            saved_query_id=saved_query_id,
-            expressions=expressions or [],
+            request_obj: request object to use
+            offset: offset to start at
+            limit: number of assets to return
+            http_args: arguments to pass to :meth:`requests.Session.request`
+            **kwargs: passed to :meth:`build_get_request`
+        """
+        request_obj: AssetRequest = self.build_get_request(
+            request_obj=request_obj, offset=offset, limit=limit, **kwargs
+        )
+        self.LOG.debug(f"Getting {self.ASSET_TYPE} assets with request {json_dump(request_obj)}")
+        self.LAST_GET_REQUEST_OBJ: AssetRequest = request_obj
+        self.LAST_GET: dict = request_obj.to_dict()
+        api_endpoint: ApiEndpoint = ApiEndpoints.assets.get
+        response: AssetsPage = api_endpoint.perform_request(
+            http=self.auth.http,
+            request_obj=request_obj,
+            asset_type=self.ASSET_TYPE,
+            http_args=http_args,
         )
-        request_obj.set_page(limit=limit, offset=offset)
-        self.LAST_GET_REQUEST_OBJ = request_obj
-        self.LAST_GET = request_obj.to_dict()
-        http_args = http_args or {}
-        return api_endpoint.perform_request(
-            http=self.auth.http, request_obj=request_obj, asset_type=asset_type, http_args=http_args
+        return response
+
+    def _count(
+        self,
+        request_obj: t.Optional[CountRequest] = None,
+        http_args: t.Optional[dict] = None,
+        **kwargs,
+    ) -> Count:
+        """Direct API method to get the count of assets using a request object.
+
+        Args:
+            request_obj: request object to use
+            http_args: Arguments to pass to the HTTP request
+            kwargs: Arguments to pass to :meth:`build_count_request`
+        """
+        request_obj = self.build_count_request(request_obj=request_obj, **kwargs)
+        self.LOG.debug(
+            f"Getting count of {self.ASSET_TYPE} assets with request {json_dump(request_obj)}"
+        )
+        self.LAST_COUNT_REQUEST_OBJ: CountRequest = request_obj
+        self.LAST_COUNT: dict = request_obj.to_dict()
+        api_endpoint: ApiEndpoint = ApiEndpoints.assets.count
+        response: Count = api_endpoint.perform_request(
+            http=self.auth.http,
+            request_obj=request_obj,
+            asset_type=self.ASSET_TYPE,
+            http_args=http_args,
         )
+        return response
 
-    def _get_by_id(self, id: str) -> json_api.assets.AssetById:
+    def _get_by_id(self, id: str) -> AssetById:
         """Private API method to get the full metadata of all adapters for a single asset.
 
         Args:
             id: asset to get all metadata for
         """
         asset_type = self.ASSET_TYPE
         api_endpoint = ApiEndpoints.assets.get_by_id
         return api_endpoint.perform_request(
             http=self.auth.http, asset_type=asset_type, internal_axon_id=id
         )
 
-    def _count(
-        self,
-        filter: t.Optional[str] = None,
-        history_date: t.Optional[str] = None,
-        use_cache_entry: bool = False,
-        saved_query_id: t.Optional[str] = None,
-    ) -> json_api.assets.Count:
-        """Private API method to get the count of assets.
-
-        Args:
-            filter (t.Optional[str], optional): if supplied,
-                only return the count of assets that match the query
-            history_date (t.Optional[t.Union[str, timedelta, datetime]], optional): Description
-            use_cache_entry (bool, optional): Description
-
-        """
-        asset_type = self.ASSET_TYPE
-        api_endpoint = ApiEndpoints.assets.count
-        request_obj = api_endpoint.load_request(
-            use_cache_entry=use_cache_entry,
-            filter=filter,
-            saved_query_id=saved_query_id,
-        )
-        return api_endpoint.perform_request(
-            http=self.auth.http, request_obj=request_obj, asset_type=asset_type
-        )
-
     def _destroy(self, destroy: bool, history: bool) -> dict:  # pragma: no cover
         """Private API method to destroy ALL assets.
 
         Args:
             destroy: Must be true in order to actually perform the delete
             history: Also delete all historical information
         """
@@ -1660,15 +1941,15 @@
             history=history,
         )
 
         return api_endpoint.perform_request(
             http=self.auth.http, request_obj=request_obj, asset_type=asset_type
         )
 
-    def _history_dates(self) -> json_api.assets.HistoryDates:
+    def _history_dates(self) -> HistoryDates:
         """Private API method to get all known historical dates."""
         api_endpoint = ApiEndpoints.assets.history_dates
         return api_endpoint.perform_request(http=self.auth.http)
 
     def _run_enforcement(
         self,
         name: str,
@@ -1676,15 +1957,15 @@
         include: bool = True,
         fields: t.Optional[t.List[str]] = None,
         query: t.Optional[str] = "",
     ) -> None:
         """Run an enforcement set manually against a list of assets internal_axon_ids.
 
         Args:
-            name (str): Name of enforcement set to exectue
+            name (str): Name of enforcement set to execute
             ids (t.List[str]): internal_axon_id's of assets to run enforcement set against
             include (bool, optional): select IDs in DB or IDs NOT in DB
             fields (t.Optional[t.List[str]], optional): list of fields used to select assets
             query (str, optional): filter used to select assets
 
         Returns:
             TYPE: Empty response
```

## axonius_api_client/api/assets/devices.py

```diff
@@ -129,15 +129,15 @@
         kwargs["values"] = values
         return self.get_by_values(**kwargs)
 
     def get_by_ip_regex(self, value: str, **kwargs) -> GEN_TYPE:  # pragma: no cover
         """Build a query to get assets where :attr:`FIELD_IP` regex matches value.
 
         Args:
-            values: regex of ip address to match to match
+            values: regex of ip address to match
             **kwargs: passed to :meth:`get_by_value_regex`
         """
         kwargs["field"] = self.FIELD_IP
         kwargs["field_manual"] = True
         kwargs["value"] = value
         return self.get_by_value_regex(**kwargs)
```

## axonius_api_client/api/assets/fields.py

```diff
@@ -1,14 +1,14 @@
 # -*- coding: utf-8 -*-
 """API for working with fields for assets."""
 import re
+import typing as t
 from typing import List, Optional, Tuple, Union
 
 from cachetools import TTLCache, cached
-from fuzzyfinder import fuzzyfinder
 
 from ...constants.fields import (
     AGG_ADAPTER_ALTS,
     AGG_ADAPTER_NAME,
     FUZZY_SCHEMAS_KEYS,
     GET_SCHEMA_KEYS,
     GET_SCHEMAS_KEYS,
@@ -18,14 +18,40 @@
 from ...parsers.fields import parse_fields, schema_custom
 from ...tools import listify, split_str, strip_right
 from .. import json_api
 from ..api_endpoints import ApiEndpoints
 from ..mixins import ChildMixins
 
 
+def fuzzyfinder(value: str, collection: t.Iterable, accessor: t.Callable = lambda x: x):
+    """Fuzzy finder
+
+    Args:
+        value: A partial string which is typically entered by a user.
+        collection: A collection of strings which will be filtered based on the `value`.
+        accessor: If the `collection` is not an iterable of strings, then use the accessor
+         to fetch the string that will be used for fuzzy matching.
+
+    Returns:
+        suggestions: A list of suggestions narrowed down from `collection` using the `value`.
+    """
+    suggestions = []
+    value = str(value) if not isinstance(value, str) else value
+    pat = ".*?".join(map(re.escape, value))
+    pat = "(?=({0}))".format(pat)  # lookahead regex to manage overlapping matches
+    regex = re.compile(pat, re.IGNORECASE)
+    for item in collection:
+        r = list(regex.finditer(accessor(item)))
+        if r:
+            best = min(r, key=lambda x: len(x.group(1)))  # find the shortest match
+            suggestions.append((len(best.group(1)), best.start(), accessor(item), item))
+
+    return (z[-1] for z in sorted(suggestions))
+
+
 class Fields(ChildMixins):
     """API for working with fields for the parent asset type.
 
     Examples:
         Create a ``client`` using :obj:`axonius_api_client.connect.Connect` and assume
         ``apiobj`` is either ``client.devices`` or ``client.users``
 
@@ -71,15 +97,15 @@
         fields_regex_root_only: bool = True,
         fields_manual: Optional[Union[List[str], str]] = None,
         fields_fuzzy: Optional[Union[List[str], str]] = None,
         fields_default: bool = True,
         fields_error: bool = True,
         fields_root: Optional[str] = None,
         empty_ok: bool = False,
-    ) -> List[dict]:
+    ) -> List[str]:
         """Get the fully qualified field names for getting asset data.
 
         Examples:
             * ``fields`` gets parsed by :meth:`get_field_names_eq`
             * ``fields_regex`` gets parsed by :meth:`get_field_names_re`
             * ``fields_fuzzy`` gets parsed by :meth:`get_field_names_fuzzy`
             * ``fields_root`` gets parsed by :meth:`get_field_names_root`
@@ -98,27 +124,26 @@
             fields_manual: list of already fully qualified field names
             fields_fuzzy: list of fields to fuzzy match against their base name or title
             fields_default: include the default fields defined in the parent API object
             fields_root: include all root fields from a single adapter
 
         Raises:
             :exc:`ApiError`: if no fields selected after all processing is done
-
         """
 
         def add(items):
-            for item in items:
+            for item in listify(items):
                 if item not in selected:
                     selected.append(item)
 
         fields = listify(obj=fields)
         fields_manual = listify(obj=fields_manual)
         fields_fuzzy = listify(obj=fields_fuzzy)
 
-        selected = []
+        selected: t.List[str] = []
 
         if fields_default and not fields_root:
             add(self.parent.fields_default)
 
         if fields_root:
             add(self.get_field_names_root(adapter=fields_root))
 
@@ -164,15 +189,15 @@
 
             >>> apiobj.fields.get_field_name(value='aws:aws_device_type')
             'adapters_data.aws_adapter.aws_device_type'
 
         Args:
             value: field to find in format of ``adapter_name:field_name``
             field_manual: treat the field name as fully qualified
-            fields_custom: custom schemas to search thru in addition to API schemas
+            fields_custom: custom schemas to search in addition to API schemas
             key: key of schema to return, or if empty return the schema itself
 
         Raises:
             :exc:`ApiError`: if more than one field found in value after splitting it
         """
         if field_manual and key:
             return value
@@ -253,20 +278,22 @@
         matches = []
 
         for adapter_re, fields_re in splits:
             adapters = self.get_adapter_names(value=adapter_re)
 
             for adapter in adapters:
                 for field_re in fields_re:
-                    fschemas = self.get_field_schemas(value=field_re, schemas=fields[adapter])
-                    fschemas = [x for x in fschemas if x["name_base"] not in ["all", "raw_data"]]
+                    found_schemas = self.get_field_schemas(value=field_re, schemas=fields[adapter])
+                    found_schemas = [
+                        x for x in found_schemas if x["name_base"] not in ["all", "raw_data"]
+                    ]
                     if root_only:
-                        fschemas = [x for x in fschemas if x["is_root"]]
+                        found_schemas = [x for x in found_schemas if x["is_root"]]
 
-                    names = [x[key] for x in fschemas]
+                    names = [x[key] for x in found_schemas]
                     matches += [x for x in names if x not in matches]
         return matches
 
     def get_field_names_eq(
         self,
         value: Union[str, List[str]],
         key: str = "name_qual",
@@ -363,15 +390,15 @@
         See Also:
             :meth:`get_field_names_root`
 
         Notes:
             root fields are fields that are fields that are not sub-fields of complex fields
 
             For instance 'specific_data.data.network_interfaces.ips' is NOT a root field,
-            since 'ips' is a sub field of 'specific_data.data.network_interfaces'
+            since 'ips' is a sub-field of 'specific_data.data.network_interfaces'
 
         """
         fields = self.get()
         adapter = self.get_adapter_name(value=adapter)
         schemas = fields[adapter]
 
         matches = [x for x in schemas if x.get("selectable") and x.get("is_root")]
```

## axonius_api_client/api/assets/runner.py

```diff
@@ -215,15 +215,15 @@
 
         return
 
     def run(self, force: bool = False):
         """Run the Enforcement Set.
 
         Args:
-            force (bool, optional): if verified=False or already run, ignore and run anyways
+            force (bool, optional): if verified=False or already run, ignore and run anyway
 
         """
         self.init()
         self.state = self._tstate_run_eval
 
         sargs = {"exc": True}
         post = self._tforce_n
```

## axonius_api_client/api/assets/saved_query.py

```diff
@@ -35,15 +35,19 @@
 class SavedQuery(ChildMixins):
     """API object for working with saved queries for the parent asset type.
 
     Examples:
         Create a ``client`` using :obj:`axonius_api_client.connect.Connect` and assume
         ``apiobj`` is either ``client.devices`` or ``client.users``
 
-        >>> apiobj = client.devices  # or client.users
+        >>> import axonius_api_client as axonapi
+        >>> connect_args: dict = axonapi.get_env_connect()
+        >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+        >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+        >>>       # or client.users or client.vulnerabilities
 
         * Get a saved query by name: :meth:`get_by_name`
         * Get a saved query by UUID: :meth:`get_by_uuid`
         * Get a saved query by tags: :meth:`get_by_tags`
         * Get all saved query tags: :meth:`get_tags`
         * Get all saved queries: :meth:`get`
         * Add a saved query: :meth:`add`
@@ -55,30 +59,33 @@
         * User assets :obj:`axonius_api_client.api.assets.users.Users`
 
     """
 
     @property
     def folders(self) -> FoldersQueries:
         """Get the folders api for this object type."""
+        # noinspection PyUnresolvedReferences
         return self.auth.http.CLIENT.folders.queries
 
     def update_folder(
         self,
         sq: MULTI,
         folder: t.Union[str, FolderModel],
         as_dataclass: bool = AS_DATACLASS,
         create: bool = FolderDefaults.create_action,
         echo: bool = FolderDefaults.echo_action,
     ) -> t.Union[dict, models.SavedQuery]:
         """Update the name of a Saved Query.
 
         Args:
-            sq (MULTI): str with name or uuid, or saved query dict or dataclass
-            value (str): new name
-            as_dataclass (bool, optional): Return saved query dataclass instead of dict
+            sq: str with name or uuid, or saved query dict or dataclass
+            folder: new name
+            as_dataclass: Return saved query dataclass instead of dict
+            create: create folder if it does not exist
+            echo: echo status to stdout
 
         Returns:
             t.Union[dict, models.SavedQuery]: saved query dataclass or dict
         """
         sq = self.get_by_multi(sq=sq, as_dataclass=True)
         updated_obj = sq.move(
             folder=folder,
@@ -143,15 +150,15 @@
     def update_sort(
         self,
         sq: MULTI,
         field: t.Optional[str] = None,
         descending: bool = True,
         as_dataclass: bool = AS_DATACLASS,
     ) -> t.Union[dict, models.SavedQuery]:
-        """Update the sort of a Saved Query.
+        """Update the sort field of a Saved Query.
 
         Args:
             sq (MULTI): str with name or uuid, or saved query dict or dataclass
             field (t.Optional[str], optional): field to sort results on
             descending (bool, optional): sort descending or ascending
             as_dataclass (bool, optional): Return saved query dataclass instead of dict
 
@@ -882,15 +889,15 @@
             ...     description="meep meep",
             ...     tags=["nyuck1", "nyuck2", "nyuck3"],
             ... )
 
         Notes:
             Saved Queries created without expressions will not be editable using the query wizard
             in the GUI. Use :obj:`axonius_api_client.api.wizards.wizard.Wizard` to produce a query
-            and it's accordant expressions for the GUI query wizard.
+            and expressions for the GUI query wizard.
 
         Args:
             as_dataclass (bool, optional): return saved query dataclass or dict
             **kwargs: passed to :meth:`build_add_model`
 
         Returns:
             t.Union[dict, models.SavedQuery]: saved query dataclass or dict
@@ -929,23 +936,33 @@
         **kwargs,
     ) -> models.SavedQueryCreate:
         """Create a saved query.
 
         Examples:
             Create a saved query using a :obj:`axonius_api_client.api.wizards.wizard.Wizard`
 
-            >>> parsed = apiobj.wizard_text.parse(content="simple hostname contains blah")
-            >>> query = parsed["query"]
-            >>> expressions = parsed["expressions"]
+            >>> import axonius_api_client as axonapi
+            >>> connect_args: dict = axonapi.get_env_connect()
+            >>> client: axonapi.Connect = axonapi.Connect(**connect_args)
+            >>> apiobj: axonapi.api.assets.AssetMixin = client.devices
+            >>>       # or client.users or client.vulnerabilities
+            >>> wiz: str = "simple hostname contains blah"
+            >>> parsed: dict = apiobj.wizard_text.parse(content=wiz)
+            >>> sq_name: str = "test"
+            >>> sq_query: str = parsed["query"]
+            >>> sq_expressions: list[dict] = parsed["expressions"]
+            >>> sq_description: str = "meep meep"
+            >>> sq_tags: list[str] = ["nice1", "nice2", "nice3"]
             >>> sq = apiobj.saved_query.add(
-            ...     name="test",
-            ...     query=query,
-            ...     expressions=expressions,
-            ...     description="meep meep",
-            ...     tags=["nyuck1", "nyuck2", "nyuck3"],
+            ...     name=sq_name,
+            ...     query=sq_query,
+            ...     expressions=sq_expressions,
+            ...     description=sq_description,
+            ...     tags=sq_tags,
+            ...     as_dataclass=True,
             ... )
 
         Notes:
             Saved Queries created without expressions will not be editable using the query wizard
             in the GUI. Use :obj:`axonius_api_client.api.wizards.wizard.Wizard` to produce a query
             and it's accordant expressions for the GUI query wizard.
```

## axonius_api_client/api/enforcements/enforcements.py

```diff
@@ -37,48 +37,50 @@
 MULTI_ACTION_TYPE = t.Union[str, dict, ActionType]
 CACHE_GET = TTLCache(maxsize=1024, ttl=60)
 
 
 class Enforcements(ModelMixins):
     """API working with enforcements.
 
-    Whats the deal with BASIC vs FULL?
+    What's the deal with BASIC vs FULL?
 
-    The REST API exposes an endpoint to page through the enforcement sets, but the details
-    about the actions and triggers configured are limited.
+    The REST API exposes an endpoint to page through the enforcement sets,
+    but the details about the actions and triggers configured are limited.
 
-    In order to get the full details of the actions and triggers, one call must be made to
-    fetch the FULL details of each enforcement set by UUID.
+    In order to get the full details of the actions and triggers, one call
+    must be made to fetch the FULL details of each enforcement set by UUID.
 
-    To make things more fun, the FULL model is lacking details that are only provided by the BASIC
-    model: triggers_last_triggered, triggers_times_triggered, updated_by, last_triggered,
-    last_updated. Also the "human friendly" description of trigger schedule is only available from
-    BASIC.
+    To make things more fun, the FULL model is lacking details that are
+    only provided by the BASIC model:
+    triggers_last_triggered, triggers_times_triggered, updated_by, last_triggered,
+    last_updated.
 
-    We overcome this by getting the FULL object and attaching the BASIC object as FULL.BASIC.
+    Finally, the "human friendly" description of trigger schedule is only available from BASIC.
 
-    TBD:
-        get_tasks
+    We overcome this by getting the FULL object and attaching the BASIC object as FULL.BASIC.
     """
 
     @property
     def folders(self) -> FoldersEnforcements:
         """Get the folders api for this object type."""
+        # noinspection PyUnresolvedReferences
         return self.auth.http.CLIENT.folders.enforcements
 
     def get_set(
         self,
         value: MULTI_SET,
         refetch: bool = True,
         cache: t.Optional[t.List[EnforcementFullModel]] = None,
     ) -> EnforcementFullModel:
         """Get an enforcement set by name or UUID.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
+            value: enforcement set model or str with name or uuid
+            refetch: refetch the enforcement set from the API
+            cache: cache of enforcement sets
 
         Raises:
             ApiError: if invalid type supplied for value
             NotFoundError: if not found
 
         Returns:
             EnforcementFullModel: enforcement set model
@@ -146,27 +148,27 @@
     ) -> t.Union[
         t.Generator[t.Union[EnforcementBasicModel, EnforcementFullModel], None, None],
         t.List[t.Union[EnforcementBasicModel, EnforcementFullModel]],
     ]:
         """Get all enforcement sets.
 
         Args:
-            generator (bool, optional): return an iterator for objects
-            full (bool, optional): get the full model of each enforcement set
+            generator: return an iterator for objects
+            full: get the full model of each enforcement set
         """
         gen = self.get_sets_generator(full=full)
         return gen if generator else list(gen)
 
     def get_sets_generator(
         self, full: bool = True
     ) -> t.Generator[t.Union[EnforcementBasicModel, EnforcementFullModel], None, None]:
         """Get all enforcement sets using a generator.
 
         Args:
-            full (bool, optional): get the full model of each enforcement set
+            full: get the full model of each enforcement set
 
         Yields:
             t.Generator[t.Union[EnforcementBasicModel, EnforcementFullModel], None, None]: Generator
         """
         offset = 0
 
         while True:
@@ -179,15 +181,15 @@
             for row in rows:
                 yield row.get_full() if full else row
 
     def check_set_exists(self, value: MULTI_SET):
         """Check if an enforcement set already exists.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
+            value: enforcement set model or str with name or uuid
 
 
         Raises:
             ApiError: if enforcement set already exists
         """
         try:
             existing = self.get_set(value=value)
@@ -198,15 +200,15 @@
         exc.obj = existing
         raise exc
 
     def get_action_type(self, value: MULTI_ACTION_TYPE) -> ActionType:
         """Get an action type.
 
         Args:
-            value (MULTI_ACTION_TYPE): action type model or str with name
+            value: action type model or str with name
 
         Returns:
             ActionType: action type model
 
         Raises:
             ApiError: if value is incorrect type
             NotFoundError: if not found
@@ -249,20 +251,20 @@
         folder: t.Optional[t.Union[str, Folder]] = None,
         create: bool = FolderDefaults.create_action,
         echo: bool = FolderDefaults.echo_action,
     ) -> EnforcementFullModel:
         """Copy an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            name (str): name to assign to copy
-            copy_triggers (bool, optional): copy triggers to new set
-            folder (t.Optional[t.Union[str, Folder]], optional): folder to put object in
-            create (bool, optional): if folder supplied does not exist, create it
-            echo (bool, optional): echo output to console during create/etc
+            value: enforcement set model or str with name or uuid
+            name: name to assign to copy
+            copy_triggers: copy triggers to new set
+            folder: folder to put object in
+            create: if folder supplied does not exist, create it
+            echo: echo output to console during create/etc
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
 
         root: FoldersModel = self.folders.get()
@@ -282,19 +284,18 @@
         folder: t.Union[str, FolderModel],
         create: bool = FolderDefaults.create_action,
         echo: bool = FolderDefaults.echo_action,
     ) -> EnforcementFullModel:
         """Update the name of an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            name (str): name to update
-            folder (t.Optional[t.Union[str, Folder]], optional): folder to put object in
-            create (bool, optional): if folder supplied does not exist, create it
-            echo (bool, optional): echo output to console during create/etc
+            value: enforcement set model or str with name or uuid
+            folder: folder to put object in
+            create: if folder supplied does not exist, create it
+            echo: echo output to console during create/etc
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing: EnforcementFullModel = self.get_set(value=value)
         updated_obj: EnforcementFullModel = existing.move(
             folder=folder,
@@ -304,16 +305,16 @@
         )
         return updated_obj
 
     def update_name(self, value: MULTI_SET, name: str) -> EnforcementFullModel:
         """Update the name of an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            name (str): name to update
+            value: enforcement set model or str with name or uuid
+            name: name to update
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         self.check_set_exists(value=name)
         existing.name = name
@@ -322,34 +323,35 @@
 
     def update_description(
         self, value: MULTI_SET, description: str, append: bool = False
     ) -> EnforcementFullModel:
         """Update the description of an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            description (str): description to update
+            value: enforcement set model or str with name or uuid
+            description: description to update
+            append: append to existing description
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.update_description(value=description, append=append)
         return self.get_set(value=existing)
 
     def update_action_main(
         self, value: MULTI_SET, name: str, action_type: str, config: t.Optional[dict] = None
     ) -> EnforcementFullModel:
         """Update the main action of an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            name (str): name to assign to action
-            action_type (str): action type
-            config (Optional[dict], optional): action configuration
+            value: enforcement set model or str with name or uuid
+            name: name to assign to action
+            action_type: action type
+            config: action configuration
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         action = self.get_set_action(name=name, action_type=action_type, config=config)
         existing.main_action = action
@@ -363,19 +365,19 @@
         name: str,
         action_type: str,
         config: t.Optional[dict] = None,
     ) -> EnforcementFullModel:
         """Add an action to an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            category (Union[ActionCategory, str]): action category to add action to
-            name (str): name of action to add
-            action_type (str): action type
-            config (Optional[dict], optional): action configuration
+            value: enforcement set model or str with name or uuid
+            category: action category to add action to
+            name: name of action to add
+            action_type: action type
+            config: action configuration
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         action = self.get_set_action(name=name, action_type=action_type, config=config)
         existing.add_action(category=category, action=action)
@@ -384,17 +386,17 @@
 
     def update_action_remove(
         self, value: MULTI_SET, category: t.Union[ActionCategory, str], name: str
     ) -> EnforcementFullModel:
         """Remove an action from an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            category (Union[ActionCategory, str]): action category to remove action from
-            name (str): name of action to remove
+            value: enforcement set model or str with name or uuid
+            category: action category to remove action from
+            name: name of action to remove
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.remove_action(category=category, name=name)
         updated = self.update_from_model(value=existing)
@@ -405,76 +407,72 @@
         value: MULTI_SET,
         query_name: str,
         query_type: t.Union[QueryTypes, str] = EnforcementDefaults.query_type,
     ) -> EnforcementFullModel:
         """Update the query of an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            query_name (str): name of saved query
-            query_type (Union[QueryTypes, str], optional): type of saved query
+            value: enforcement set model or str with name or uuid
+            query_name: name of saved query
+            query_type: type of saved query
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         sq = self.get_trigger_view(query_name=query_name, query_type=query_type)
         existing = self.get_set(value=value)
         existing.query_update(query_uuid=sq.uuid, query_type=query_type)
         updated = self.update_from_model(value=existing)
         return self.get_set(value=updated)
 
     def update_query_remove(self, value: MULTI_SET) -> EnforcementFullModel:
         """Remove the query from an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
+            value: enforcement set model or str with name or uuid
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         raise NotAllowedError("Enforcement Sets now require a Saved Query to be defined")
-        existing = self.get_set(value=value)
-        existing.query_remove()
-        updated = self.update_from_model(value=existing)
-        return self.get_set(value=updated)
 
     def update_schedule_never(self, value: MULTI_SET) -> EnforcementFullModel:
         """Set the schedule of an enforcement set to never run.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
+            value: enforcement set model or str with name or uuid
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.set_schedule_never()
         updated = self.update_from_model(value=existing)
         return self.get_set(value=updated)
 
     def update_schedule_discovery(self, value: MULTI_SET) -> EnforcementFullModel:
         """Set the schedule of an enforcement set to run every discovery.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
+            value: enforcement set model or str with name or uuid
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.set_schedule_discovery()
         updated = self.update_from_model(value=existing)
         return self.get_set(value=updated)
 
     def update_schedule_hourly(self, value: MULTI_SET, recurrence: int) -> EnforcementFullModel:
         """Set the schedule of an enforcement set to run hourly.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            recurrence (int): run schedule every N hours (N = 1-24)
+            value: enforcement set model or str with name or uuid
+            recurrence: run schedule every N hours (N = 1-24)
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.set_schedule_hourly(recurrence=recurrence)
         updated = self.update_from_model(value=existing)
@@ -486,18 +484,18 @@
         recurrence: int,
         hour: int = EnforcementDefaults.schedule_hour,
         minute: int = EnforcementDefaults.schedule_minute,
     ) -> EnforcementFullModel:
         """Set the schedule of an enforcement set to run daily.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            recurrence (int): run enforcement every N days (N = 1-~)
-            hour (int, optional): hour of day to run schedule
-            minute (int, optional): minute of hour to run schedule
+            value: enforcement set model or str with name or uuid
+            recurrence: run enforcement every N days (N = 1-~)
+            hour: hour of day to run schedule
+            minute: minute of hour to run schedule
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.set_schedule_daily(recurrence=recurrence, hour=hour, minute=minute)
         updated = self.update_from_model(value=existing)
@@ -509,18 +507,18 @@
         recurrence: t.Union[str, t.List[t.Union[str, int]]],
         hour: int = EnforcementDefaults.schedule_hour,
         minute: int = EnforcementDefaults.schedule_minute,
     ) -> EnforcementFullModel:
         """Set the schedule of an enforcement set to run weekly.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            recurrence (Union[str, t.List[Union[str, int]]]): run enforcement on days of week
-            hour (int, optional): hour of day to run schedule
-            minute (int, optional): minute of hour to run schedule
+            value: enforcement set model or str with name or uuid
+            recurrence: run enforcement on days of week
+            hour: hour of day to run schedule
+            minute: minute of hour to run schedule
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.set_schedule_weekly(recurrence=recurrence, hour=hour, minute=minute)
         updated = self.update_from_model(value=existing)
@@ -532,65 +530,65 @@
         recurrence: t.Union[str, t.List[t.Union[int, str]]],
         hour: int = EnforcementDefaults.schedule_hour,
         minute: int = EnforcementDefaults.schedule_minute,
     ) -> EnforcementFullModel:
         """Set the schedule of an enforcement set to run monthly.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            recurrence (Union[str, t.List[int]]): run enforcement on days of month
-            hour (int, optional): hour of day to run schedule
-            minute (int, optional): minute of hour to run schedule
+            value: enforcement set model or str with name or uuid
+            recurrence: run enforcement on days of month
+            hour: hour of day to run schedule
+            minute: minute of hour to run schedule
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.set_schedule_monthly(recurrence=recurrence, hour=hour, minute=minute)
         updated = self.update_from_model(value=existing)
         return self.get_set(value=updated)
 
     def update_only_new_assets(self, value: MULTI_SET, update: bool) -> EnforcementFullModel:
         """Update enforcement set to only run against newly added assets from last automated run.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            update (bool): False=run against all assets each automated run, True=only run against
+            value: enforcement set model or str with name or uuid
+            update: False=run against all assets each automated run, True=only run against
                 newly added assets from last automated run
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.only_new_assets = update
         updated = self.update_from_model(value=existing)
         return self.get_set(value=updated)
 
     def update_on_count_increased(self, value: MULTI_SET, update: bool) -> EnforcementFullModel:
         """Update enforcement set to only run if asset count increased from last automated run.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            update (bool): False=run regardless if asset count increased, True=only perform
+            value: enforcement set model or str with name or uuid
+            update: False=run regardless if asset count increased, True=only perform
                 automated run if asset count increased
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.on_count_increased = update
         updated = self.update_from_model(value=existing)
         return self.get_set(value=updated)
 
     def update_on_count_decreased(self, value: MULTI_SET, update: bool) -> EnforcementFullModel:
         """Update enforcement set to only run if asset count decreased from last automated run.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            update (bool): False=run regardless if asset count decreased, True=only perform
+            value: enforcement set model or str with name or uuid
+            update: False=run regardless if asset count decreased, True=only perform
                 automated run if asset count decreased
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.on_count_decreased = update
@@ -599,16 +597,16 @@
 
     def update_on_count_above(
         self, value: MULTI_SET, update: t.Optional[int]
     ) -> EnforcementFullModel:
         """Update enforcement set to only run automatically if asset count is above N.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            update (Optional[int]): None to always run automatically regardless of asset count,
+            value: enforcement set model or str with name or uuid
+            update: None to always run automatically regardless of asset count,
                 integer to only run automatically if asset count is above N
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.on_count_above = update
@@ -617,52 +615,53 @@
 
     def update_on_count_below(
         self, value: MULTI_SET, update: t.Optional[int]
     ) -> EnforcementFullModel:
         """Update enforcement set to only run automatically if asset count is below N.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
-            update (Optional[int]): None to always run automatically regardless of asset count,
+            value: enforcement set model or str with name or uuid
+            update: None to always run automatically regardless of asset count,
                 integer to only run automatically if asset count is below N
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         existing = self.get_set(value=value)
         existing.on_count_below = update
         updated = self.update_from_model(value=existing)
         return self.get_set(value=updated)
 
     def update_from_model(self, value: EnforcementFullModel) -> EnforcementFullModel:
         """Update an enforcement set from the values in a model.
 
         Args:
-            value (EnforcementFullModel): enforcement set model to update
+            value: enforcement set model to update
 
         Returns:
             EnforcementFullModel: updated enforcement set
         """
         return self._update(
             uuid=value.uuid,
             name=value.name,
             actions=value.actions,
             triggers=value.triggers,
             folder_id=value.folder_id,
         )
 
+    # noinspection PyDefaultArgument
     def create(
         self,
         name: str,
         main_action_name: str,
         main_action_type: str,
-        main_action_config: t.Optional[dict] = None,
-        description: t.Optional[str] = "",
         query_name: t.Optional[str] = EnforcementDefaults.query_name,
         query_type: str = EnforcementDefaults.query_type,
+        main_action_config: t.Optional[dict] = None,
+        description: t.Optional[str] = "",
         schedule_type: t.Union[EnforcementSchedule, str] = EnforcementDefaults.schedule_type,
         schedule_hour: int = EnforcementDefaults.schedule_hour,
         schedule_minute: int = EnforcementDefaults.schedule_minute,
         schedule_recurrence: t.Optional[
             t.Union[int, t.List[str]]
         ] = EnforcementDefaults.schedule_recurrence,
         only_new_assets: bool = EnforcementDefaults.only_new_assets,
@@ -673,31 +672,34 @@
         folder: t.Optional[t.Union[str, FolderModel]] = None,
         create: bool = FolderDefaults.create_action,
         echo: bool = FolderDefaults.echo_action,
     ) -> EnforcementFullModel:
         """Create an enforcement set.
 
         Args:
-            name (str): Name to assign to enforcement set
-            main_action_name (str): name to assign to main action
-            main_action_type (str): action type to use for main action
-            main_action_config (Optional[dict], optional): action config for main action
-            query_name (Optional[str], optional): Saved Query name to use for trigger
-            query_type (str, optional): Saved Query type
-            schedule_type (Union[EnforcementSchedule, str], optional): EnforcementSchedule type
+            name: Name to assign to enforcement set
+            description: Description to assign to enforcement set
+            query_name: Saved Query name to use for trigger
+            query_type: Saved Query type
+            main_action_name: name to assign to main action
+            main_action_type: action type to use for main action
+            main_action_config: action config for main action
+            schedule_type: EnforcementSchedule type
                 for automation
-            schedule_hour (int, optional): Hour of day to use for schedule_type
-            schedule_minute (int, optional): Minute of hour to use for schedule_type
-            schedule_recurrence (Optional[Union[int, t.List[str]]], optional): recurrence value,
-                type changes based on schedule_type
-            only_new_assets (bool, optional): only run set against assets added since last run
-            on_count_above (Optional[int], optional): only run if asset count above N
-            on_count_below (Optional[int], optional): only run if asset count below N
-            on_count_increased (bool, optional): only run if asset count increased since last run
-            on_count_decreased (bool, optional): only run if asset count decreased since last run
+            schedule_hour: Hour of day to use for schedule_type
+            schedule_minute: Minute of hour to use for schedule_type
+            schedule_recurrence: recurrence value to use for schedule_type
+            only_new_assets: only run set against assets added since last run
+            on_count_above: only run if asset count above N
+            on_count_below: only run if asset count below N
+            on_count_increased: only run if asset count increased since last run
+            on_count_decreased: only run if asset count decreased since last run
+            folder: folder to create set in
+            create: create folder if it doesn't exist
+            echo: echo folder creation
 
         Returns:
             EnforcementFullModel: created enforcement set
         """
         self.check_set_exists(value=name)
 
         root: FoldersModel = self.folders.get()
@@ -737,15 +739,15 @@
         created = self.get_set(value=create_response)
         return created
 
     def delete(self, value: MULTI_SET) -> EnforcementFullModel:
         """Delete an enforcement set.
 
         Args:
-            value (MULTI_SET): enforcement set model or str with name or uuid
+            value: enforcement set model or str with name or uuid
 
         Returns:
             EnforcementFullModel: deleted enforcement set
         """
         obj = self.get_set(value=value)
         self._delete(uuid=obj.uuid)
         return obj
@@ -755,53 +757,58 @@
         name: str,
         action_type: MULTI_ACTION_TYPE,
         config: t.Optional[dict] = None,
     ) -> dict:
         """Get the action dictionary needed to add an action to an enforcement set.
 
         Args:
-            name (str): name to assign to action
-            action_type (MULTI_ACTION_TYPE): action type to use
-            config (Optional[dict], optional): action config
+            name: name to assign to action
+            action_type: action type to use
+            config: action config
 
         Returns:
             dict: action dictionary
         """
         action_type = self.get_action_type(value=action_type)
         config = action_type.check_config(config=config)
 
-        msg = (
-            "CAUTION!!\n"
-            "Adding an action with an incorrect configuration can result in"
-            " an invalid Enforcement Set that can not be deleted!\n"
-            "It is advised to create the appropriate action in the GUI to get an example"
-        )
+        msg = """CAUTION:
+Enforcement Sets will be forced to the /Drafts folder if:
+- Any configuration supplied for any action is invalid
+- No Saved Query is supplied for the trigger
+
+Easiest way to learn the correct configuration for an action:
+- Create an Enforcement Set using the GUI
+- export the Enforcement Set in JSON format using:
+  axonshell enforcements get -v "enforcement name" -xf json
+- Get the 'config' dictionary for the action you want to use from the JSON output
+- Use the 'config' dictionary as appropriate 'config' argument for this method
+"""
         warnings.warn(message=msg, category=ApiWarning)
 
         return EnforcementFullModel.get_action_obj(
             name=name, action_type=action_type, config=config
         )
 
     def get_trigger_view(
         self,
         query_name: t.Optional[MULTI_SQ] = EnforcementDefaults.query_name,
         query_type: t.Union[QueryTypes, str] = EnforcementDefaults.query_type,
     ) -> t.Optional[SavedQuery]:
         """Get the saved query for use in adding a query to an enforcement.
 
         Args:
-            query_name (Optional[MULTI_SQ], optional): Name of Saved Query
-            query_type (Union[QueryTypes, str], optional): Type of Saved Query
+            query_name: Name of Saved Query
+            query_type: Type of Saved Query
 
         Returns:
-            t.Optional[SavedQuery]: None if query_name is not supplied, otherwise saved query model
+            SavedQuery: None if query_name is not supplied, otherwise saved query model
         """
-        query_type = QueryTypes.get_value_by_value(query_type)
-
-        if query_name is not None:
+        if query_name:
+            query_type = QueryTypes.get_value_by_value(query_type)
             return self._triggers_map[query_type].saved_query.get_by_multi(
                 sq=query_name, as_dataclass=True
             )
         return None
 
     def _init(self, **kwargs):
         """Post init method for subclasses to use for extra setup."""
@@ -809,15 +816,15 @@
 
         self.api_devices: Devices = Devices(auth=self.auth, **kwargs)
         """API model for cross reference."""
 
         self.api_users: Users = Users(auth=self.auth, **kwargs)
         """API model for cross reference."""
 
-        self.api_vulnerabilities: Vulnerabilities = Users(auth=self.auth, **kwargs)
+        self.api_vulnerabilities: Vulnerabilities = Vulnerabilities(auth=self.auth, **kwargs)
         """API model for cross reference."""
 
         self.api_instances: Instances = Instances(auth=self.auth, **kwargs)
         """API model for cross reference."""
 
         self.tasks: Tasks = Tasks(auth=self.auth, **kwargs)
         """API model for working with tasks for enforcements."""
@@ -831,46 +838,52 @@
             QueryTypes.vulnerabilities.value: self.api_vulnerabilities,
         }
 
     def _delete(self, uuid: str) -> json_api.generic.Deleted:
         """Delete an enforcement set by UUID.
 
         Args:
-            uuid (str): UUID of set to delete
+            uuid: UUID of set to delete
 
         Returns:
             json_api.generic.Deleted: deleted model
         """
         api_endpoint = ApiEndpoints.enforcements.delete_set
         request_obj = api_endpoint.load_request(value={"ids": [uuid], "include": True})
         response = api_endpoint.perform_request(http=self.auth.http, request_obj=request_obj)
         self.get_sets_cached.cache_clear()
         return response
 
     def _copy(self, uuid: str, name: str, clone_triggers: bool) -> EnforcementFullModel:
         """Copy an enforcement set.
 
         Args:
-            uuid (str): UUID of set to copy
-            name (str): name to give copy
-            clone_triggers (bool): copy triggers into new set
+            uuid: UUID of set to copy
+            name: name to give copy
+            clone_triggers: copy triggers into new set
 
         Returns:
             EnforcementFullModel: copied set
         """
         api_endpoint = ApiEndpoints.enforcements.copy_set
         request_obj = api_endpoint.load_request(
             id=uuid, uuid=uuid, name=name, clone_triggers=clone_triggers
         )
         response = api_endpoint.perform_request(http=self.auth.http, request_obj=request_obj)
         self.get_sets_cached.cache_clear()
         return response
 
-    def _update_description(self, uuid: str, description: str):
-        """Pass."""
+    def _update_description(self, uuid: str, description: str) -> None:
+        """Update the description of an enforcement set.
+
+        Args:
+            uuid: UUID of set to update
+            description: description to set
+
+        """
         api_endpoint = ApiEndpoints.enforcements.update_description
         request_obj = api_endpoint.load_request(description=description)
         response = api_endpoint.perform_request(
             http=self.auth.http, request_obj=request_obj, uuid=uuid
         )
         self.get_sets_cached.cache_clear()
         return response
@@ -882,18 +895,18 @@
         actions: dict,
         folder_id: str = "",
         triggers: t.Optional[t.List[dict]] = None,
     ) -> EnforcementFullModel:
         """Update an enforcement set.
 
         Args:
-            uuid (str): UUID of set to update
-            name (str): name of set
-            actions (dict): actions of set
-            triggers (Optional[t.List[dict]], optional): triggers of set
+            uuid: UUID of set to update
+            name: name of set
+            actions: actions of set
+            triggers: triggers of set
 
         Returns:
             EnforcementFullModel: updated set
         """
         api_endpoint = ApiEndpoints.enforcements.update_set
         request_obj = api_endpoint.load_request(
             uuid=uuid,
@@ -928,20 +941,20 @@
         failure: t.Optional[t.List[dict]] = None,
         post: t.Optional[t.List[dict]] = None,
         triggers: t.Optional[t.List[dict]] = None,
     ) -> EnforcementFullModel:
         """Create an enforcement set.
 
         Args:
-            name (str): name of enforcement to create
-            main (dict): main action
-            success (Optional[t.List[dict]], optional): success actions
-            failure (Optional[t.List[dict]], optional): failure actions
-            post (Optional[t.List[dict]], optional): post actions
-            triggers (Optional[t.List[dict]], optional): saved query trigger
+            name: name of enforcement to create
+            main: main action
+            success: success actions
+            failure: failure actions
+            post: post actions
+            triggers: saved query trigger
 
         Returns:
             EnforcementFullModel: created set
         """
         actions = {
             "main": main,
             "success": success or [],
@@ -956,45 +969,46 @@
             description=description,
             folder_id=folder_id,
         )
         response = api_endpoint.perform_request(http=self.auth.http, request_obj=request_obj)
         self.get_sets_cached.cache_clear()
         return response
 
+    # noinspection PyShadowingBuiltins
     def _get_sets(
         self,
         limit: int = MAX_PAGE_SIZE,
         offset: int = 0,
         sort: t.Optional[str] = None,
         filter: t.Optional[str] = None,
         search: str = "",
     ) -> t.List[EnforcementBasicModel]:
         """Get enforcement sets in basic model.
 
         Args:
-            limit (int, optional): limit to N rows per page
-            offset (int, optional): start at row N
-            sort (Optional[str], optional): sort based on a model attribute
-            filter (Optional[str], optional): AQL filter
-            search (str, optional): search string
+            limit: limit to N rows per page
+            offset: start at row N
+            sort: sort based on a model attribute
+            filter: AQL filter
+            search: search string
 
         Returns:
             t.List[EnforcementBasicModel]: basic models
         """
         api_endpoint = ApiEndpoints.enforcements.get_sets
         request_obj = api_endpoint.load_request(
             page={"limit": limit, "offset": offset}, filter=filter, search=search, sort=sort
         )
         return api_endpoint.perform_request(http=self.auth.http, request_obj=request_obj)
 
     def _get_set(self, uuid: str) -> EnforcementFullModel:
         """Get an enforcement set in full model.
 
         Args:
-            uuid (str): UUID of set to get
+            uuid: UUID of set to get
 
         Returns:
             EnforcementFullModel: full model
         """
         api_endpoint = ApiEndpoints.enforcements.get_set
         return api_endpoint.perform_request(http=self.auth.http, uuid=uuid)
 
@@ -1009,35 +1023,33 @@
 
     def _run_set_against_trigger(
         self, uuid: str, ec_page_run: bool = False, use_conditions: bool = False
     ) -> json_api.generic.Name:
         """Run an enforcement set against its trigger.
 
         Args:
-            uuid (str): UUID of enforcement set to trigger
-            ec_page_run (bool, optional): this was triggered from the EC Page
-                in the GUI
-            use_conditions (bool, optional): use conditions
-                configured on enforcement set to determine execution
+            uuid: UUID of enforcement set to trigger
+            ec_page_run: this was triggered from the EC Page in the GUI
+            use_conditions: use conditions configured on enforcement set to determine execution
 
         """
         api_endpoint = ApiEndpoints.enforcements.run_set_against_trigger
         request_obj = api_endpoint.load_request(
             ec_page_run=ec_page_run, use_conditions=use_conditions
         )
         return api_endpoint.perform_request(http=self.auth.http, request_obj=request_obj, uuid=uuid)
 
     def _move_sets(
-        self, folder_id: str, enforcements_ids: t.List[str]
+        self, folder_id: str, enforcements_ids: t.Union[str, t.List[str]]
     ) -> MoveEnforcementsResponseModel:
-        """Move enforcments to a folder.
+        """Move enforcements to a folder.
 
         Args:
-            folder_id (str): ID of folder to move enforcements to
-            enforcements_ids (t.List[str]): list of uuids to move
+            folder_id: ID of folder to move enforcements to
+            enforcements_ids: list of uuids to move
         """
         api_endpoint = ApiEndpoints.enforcements.move_sets
 
         request_obj: MoveEnforcementsRequestModel = api_endpoint.load_request(
             folder_id=folder_id, enforcements_ids=listify(enforcements_ids)
         )
         response: MoveEnforcementsResponseModel = api_endpoint.perform_request(
@@ -1047,18 +1059,17 @@
 
     def _run_sets_against_trigger(
         self, uuids: t.List[str], include: bool = True, use_conditions: bool = False
     ) -> json_api.generic.ListDictValue:
         """Run enforcement sets against their triggers.
 
         Args:
-            uuids (List[str]): UUIDs of enforcement sets to trigger
-            include (bool, optional): select UUIDs in DB or UUIDs NOT in DB
-            use_conditions (bool, optional): use conditions
-                configured on enforcement set to determine execution
+            uuids: UUIDs of enforcement sets to trigger
+            include: select UUIDs in DB or UUIDs NOT in DB
+            use_conditions: use conditions configured on enforcement set to determine execution
 
         """
         api_endpoint = ApiEndpoints.enforcements.run_sets_against_trigger
         value = {"ids": listify(uuids), "include": include}
         request_obj = api_endpoint.load_request(
             value=value,
             include=include,
@@ -1068,18 +1079,18 @@
 
     def run(
         self, values: t.List[str], use_conditions: bool = False, error: bool = True
     ) -> t.List[EnforcementFullModel]:
         """Run enforcement sets against their triggers.
 
         Args:
-            values (List[str]): names or UUIDs of enforcement sets to trigger
-            use_conditions (bool, optional): use conditions
+            values: names or UUIDs of enforcement sets to trigger
+            use_conditions: use conditions
                 configured on enforcement set to determine execution
-            error (bool, optional): throw error if an enforcement set has no trigger
+            error: throw error if an enforcement set has no trigger
 
         """
         cache = self.get_sets()
         items = [self.get_set(value=x, cache=cache) for x in listify(values)]
         to_run = []
         for item in items:
             if item.check_trigger_exists(msg="run enforcement set", error=error):
```

## axonius_api_client/api/enforcements/tasks.py

```diff
@@ -1,107 +1,371 @@
 # -*- coding: utf-8 -*-
 """API for working with enforcements."""
 import typing as t
+
 from ..api_endpoints import ApiEndpoint, ApiEndpoints
-from ..json_api.paging_state import PagingState
-from ..json_api.tasks import GetTasks, Task, TaskBasic, TaskFull, TaskTypes, TaskFilters
+from ..json_api.count_operator import OperatorTypes
+from ..json_api.generic import IntValue
+from ..json_api.paging_state import Page, PagingState
+from ..json_api.tasks import GetTasks, Task, TaskBasic, TaskFilters, TaskFull, TaskTypes
+from ..json_api.tasks.get_tasks import TypeOperator
 from ..mixins import ModelMixins
-
-# from cachetools import TTLCache, cached
+from ...constants.api import RE_PREFIX, TASK_SLOW_WARNING
+from ...constants.ctypes import (
+    PatternLike,
+    TypeDelta,
+    TypeFloat,
+    TypeInt,
+    TypeMatch,
+    TypeDate,
+    TypeBool,
+)
+from ...constants.general import SPLITTER
+from ...tools import echo_debug, json_dump
 
 
 class Tasks(ModelMixins):
     """API working with tasks for enforcements."""
 
-    # XXX need get enums
+    def count(self, request_obj: t.Optional[GetTasks] = None, **kwargs) -> int:
+        """Get the number of tasks that match the provided filters in request_obj
+
+        Args:
+            request_obj: request object to use
+            **kwargs: passed to build a new request object if one is not provided
+        """
+        request_obj: GetTasks = self.build_get_request(request_obj=request_obj, **kwargs)
+        return self.direct_count(request_obj=request_obj).value
+
     def get_filters(self) -> TaskFilters:
-        """Get all filters for all enforcements."""
+        """Get all filters (enums) for all enforcements."""
         api_endpoint: ApiEndpoint = ApiEndpoints.enforcements.tasks.get_filters
         response: TaskFilters = api_endpoint.perform_request(http=self.auth.http)
         return response
 
     def get(
         self, generator: bool = False, **kwargs
     ) -> t.Union[t.List[TaskTypes], t.Generator[TaskTypes, None, None]]:
-        """Get all tasks for all enforcements."""
+        """Get all tasks for all enforcements.
+
+        Args:
+            generator: return a generator of Tasks, else return a list of Tasks
+            **kwargs: passed to :meth:`get_generator`
+        """
         gen: t.Generator[TaskTypes, None, None] = self.get_generator(**kwargs)
         return gen if generator else list(gen)
 
+    def build_get_request(
+        self,
+        date_from: t.Optional[TypeDate] = None,
+        date_from_add: t.Optional[TypeDelta] = None,
+        date_from_subtract: t.Optional[TypeDelta] = None,
+        date_to: t.Optional[TypeDate] = None,
+        date_to_add: t.Optional[TypeDelta] = None,
+        date_to_subtract: t.Optional[TypeDelta] = None,
+        duration_seconds: t.Optional[TypeFloat] = None,
+        duration_operator: TypeOperator = OperatorTypes.less,
+        task_id: t.Optional[TypeInt] = None,
+        re_prefix: str = RE_PREFIX,
+        split: bool = True,
+        split_max: t.Optional[int] = None,
+        split_sep: t.Optional[PatternLike] = SPLITTER,
+        strip: bool = True,
+        strip_chars: t.Optional[str] = None,
+        action_types: t.Optional[TypeMatch] = None,
+        action_types_error: bool = True,
+        action_types_minimum: t.Optional[int] = None,
+        discovery_uuids: t.Optional[TypeMatch] = None,
+        discovery_uuids_error: bool = True,
+        discovery_uuids_minimum: t.Optional[int] = None,
+        enforcement_names: t.Optional[TypeMatch] = None,
+        enforcement_names_error: bool = True,
+        enforcement_names_minimum: t.Optional[int] = None,
+        statuses: t.Optional[TypeMatch] = None,
+        statuses_error: bool = True,
+        statuses_minimum: t.Optional[int] = None,
+        statuses_result: t.Optional[TypeMatch] = None,
+        statuses_result_error: bool = True,
+        statuses_result_minimum: t.Optional[int] = None,
+        is_refresh: t.Optional[TypeBool] = None,
+        search: t.Optional[str] = None,
+        query: t.Optional[str] = None,
+        sort: t.Optional[str] = None,
+        history: t.Optional[TypeDate] = None,
+        task_filters: t.Optional[TaskFilters] = None,
+        request_obj: t.Optional[GetTasks] = None,
+    ):
+        """Build the request for getting tasks for all enforcements.
+
+        Args:
+            date_from: Only get tasks with creation date >= this date
+            date_from_add: seconds to add to date_from or now if not provided
+            date_from_subtract: seconds to subtract from date_from or now if not provided
+            date_to: Only get tasks with creation date <= this date
+            date_to_add: seconds to add to date_to or now if not provided
+            date_to_subtract: seconds to subtract from date_to or now if not provided
+            duration_seconds: Only get tasks where run time evaluates True against duration_operator
+            duration_operator: evaluate run time against duration_seconds (less, greater, equal)
+            task_id: Only return tasks related to the provided task ID
+            split: Split strings provided to filters by split_sep
+            split_max: Max number of splits to perform on strings provided to action_type,
+            re_prefix: if any values provided start with this, treat them as regex patterns
+            split: split values on split_sep
+            split_max: if value > 0 and split=True, only split on split_sep N times
+            split_sep: if split=True, split values on this pattern or string
+            strip: strip values of strip_chars
+            strip_chars: if strip=True, strip values of these characters from each string
+            action_types: Only get tasks that were ran by types of actions (use re_prefix for regex)
+            action_types_error: Error if any action_types provided are not valid matches
+            action_types_minimum: Error if matches for action_types are < this number
+            discovery_uuids: Only get tasks that were ran by discovery UUIDs (use re_prefix for
+                regex)
+            discovery_uuids_error: Error if any discovery_uuids provided are not valid matches
+            discovery_uuids_minimum: Error if matches for discovery_uuids are < this number
+            enforcement_names: Only get tasks that were ran by  enforcement names (use re_prefix
+                for regex)
+            enforcement_names_error: Error if any enforcement_names provided are not valid matches
+            enforcement_names_minimum: Error if matches for enforcement_names are < this number
+            statuses: Only get tasks that have the provided statuses (use re_prefix for regex)
+            statuses_error: Error if any statuses provided are not valid matches
+            statuses_minimum: Error if matches for statuses are < this number
+            statuses_result: Only get tasks that have the provided result statuses (use re_prefix
+                for regex)
+            statuses_result_error: Error if any result statuses provided are not valid matches
+            statuses_result_minimum: Error if matches for result statuses are < this number
+            is_refresh: logging control, unknown use
+            search: A textual value to search for (unknown use for this endpoint)
+            query: AQL string data filter (other request attributes build this for you)
+            sort: Field name to sort by with direction prefixed (e.g. '-name' or 'name')
+            history: get tasks from a history date snapshot (unknown use for this endpoint)
+            task_filters: TaskFilters object to build query from
+            request_obj: previously built GetTasks object to use instead of building a new one
+        """
+        if not isinstance(request_obj, GetTasks):
+            request_obj = GetTasks()
+            request_obj.HTTP = self.auth.http
+
+            # --> generic request params for objects
+            # unknown if used for this endpoint
+            request_obj.search = search
+
+            # not needed, specific params build this for you
+            request_obj.filter = query
+
+            # sort field
+            request_obj.sort = sort
+
+            # unknown if used for this endpoint
+            request_obj.set_history(value=history)
+
+            # --> endpoint specific request params
+            # start date
+            request_obj.set_date_from(
+                value=date_from, add=date_from_add, subtract=date_from_subtract
+            )
+
+            # end date
+            request_obj.set_date_to(value=date_to, add=date_to_add, subtract=date_to_subtract)
+
+            # logging control, unknown use
+            request_obj.set_is_refresh(value=is_refresh)
+
+            # duration, complex object (for no particular reason) in REST API
+            request_obj.set_duration(operator=duration_operator, seconds=duration_seconds)
+
+            if not isinstance(task_filters, TaskFilters):
+                task_filters = self.get_filters()
+
+            filter_args = dict(
+                re_prefix=re_prefix,
+                split=split,
+                split_max=split_max,
+                split_sep=split_sep,
+                strip=strip,
+                strip_chars=strip_chars,
+                task_filters=task_filters,
+            )
+
+            # filter by specific task "pretty" ID (singular, can not supply multiple task IDs)
+            request_obj.set_task_id(value=task_id, task_filters=task_filters)
+
+            # filter by task status
+            request_obj.set_statuses(
+                values=statuses, error=statuses_error, minimum=statuses_minimum, **filter_args
+            )
+
+            # filter by task result status
+            request_obj.set_statuses_result(
+                values=statuses_result,
+                error=statuses_result_error,
+                minimum=statuses_result_minimum,
+                **filter_args,
+            )
+
+            # filter by action type
+            request_obj.set_action_types(
+                values=action_types,
+                error=action_types_error,
+                minimum=action_types_minimum,
+                **filter_args,
+            )
+
+            # filter by enforcement name
+            request_obj.set_enforcement_names(
+                values=enforcement_names,
+                error=enforcement_names_error,
+                minimum=enforcement_names_minimum,
+                **filter_args,
+            )
+
+            # filter by discovery UUID
+            request_obj.set_discovery_uuids(
+                values=discovery_uuids,
+                error=discovery_uuids_error,
+                minimum=discovery_uuids_minimum,
+                **filter_args,
+            )
+        request_obj.HTTP = self.auth.http
+        return request_obj
+
     def get_generator(
         self,
+        as_full: bool = False,
+        as_basic: bool = False,
         page_sleep: int = PagingState.page_sleep,
         page_size: int = PagingState.page_size,
         row_start: int = PagingState.row_start,
         row_stop: t.Optional[int] = PagingState.row_stop,
         log_level: t.Union[int, str] = PagingState.log_level,
         request_obj: t.Optional[GetTasks] = None,
-        as_full: bool = False,
-        as_task: bool = True,
+        echo: bool = True,
         **kwargs,
     ) -> t.Generator[TaskTypes, None, None]:
-        """Get all tasks for all enforcements in multiple model formats."""
-        request_obj: GetTasks = GetTasks.get_request_if_not_request(
-            request_obj=request_obj, **kwargs
-        )
+        """Get all tasks for all enforcements in multiple model formats.
+
+        Args:
+            as_full: return TaskFull (complicated model from the REST API)
+            as_basic: return TaskBasic (complicated model from the REST API)
+            page_sleep: seconds to sleep between pages
+            page_size: number of rows to get per page
+            row_start: row to start on
+            row_stop: row to stop on
+            log_level: log level to use
+            request_obj: request object to use, will create using above args if not provided
+            echo: echo debug output
+            **kwargs: passed to :meth:`build_get_request`
+        """
+        request_obj: GetTasks = self.build_get_request(request_obj=request_obj, **kwargs)
         for basic in self.direct_get_generator(
             page_sleep=page_sleep,
             page_size=page_size,
             row_start=row_start,
             row_stop=row_stop,
             log_level=log_level,
+            echo=echo,
             request_obj=request_obj,
-            **kwargs,
+            slow_warning=not as_basic,
         ):
-            if as_full or as_task:
+            if as_basic:
+                yield basic
+            else:
+                # only get the full model if we need it
+                # this is because we can only get one full model at a time
+                # which can take quite a long time if there are many tasks
+                # this could be async but that's a problem for future me
                 full: TaskFull = basic.get_full()
-                if as_task:
-                    yield Task.load(basic=basic, full=full, http=self.auth.http)
-                else:
+                if as_full:
                     yield full
-            else:
-                yield basic
+                else:
+                    yield Task.load(basic=basic, full=full, http=self.auth.http)
 
     def direct_get_generator(
         self,
         page_sleep: int = PagingState.page_sleep,
         page_size: int = PagingState.page_size,
         row_start: int = PagingState.row_start,
         row_stop: t.Optional[int] = PagingState.row_stop,
         log_level: t.Union[int, str] = PagingState.log_level,
         request_obj: t.Optional[GetTasks] = None,
+        echo: bool = True,
+        slow_warning: bool = False,
         **kwargs,
     ) -> t.Generator[TaskBasic, None, None]:
-        """Direct API layer to get all tasks for all enforcements in basic model."""
-        request_obj: GetTasks = GetTasks.get_request_if_not_request(
-            request_obj=request_obj, **kwargs
-        )
+        """Direct API method to get all tasks for all enforcements in basic model.
+
+        Args:
+            page_sleep: seconds to sleep between pages
+            page_size: number of rows to get per page
+            row_start: row to start on
+            row_stop: row to stop on
+            log_level: log level to use
+            request_obj: request object to use
+            echo: echo to console
+            slow_warning: echo a warning that the page fetch is quick,
+                but the full model fetch is slow
+            **kwargs: passed to build a new request object if one is not provided
+        """
+        request_obj: GetTasks = self.build_get_request(request_obj=request_obj, **kwargs)
+        count: int = self.count(request_obj=request_obj)
+
+        purpose = [
+            f"Getting {count} tasks in 'basic model' format using request:",
+            f"{json_dump(request_obj)}",
+        ]
+        if slow_warning:
+            purpose += [TASK_SLOW_WARNING]
+
+        purpose = "\n".join(purpose)
+        echo_debug(purpose, do_echo=echo)
         with PagingState(
-            purpose="Get all Tasks for all Enforcement Sets in basic model",
+            purpose=purpose,
             page_sleep=page_sleep,
             page_size=page_size,
             row_start=row_start,
             row_stop=row_stop,
             log_level=log_level,
         ) as state:
             while not state.stop_paging:
-                page = state.page(method=self.direct_get, request_obj=request_obj)
+                page: Page = state.page(method=self.direct_get, request_obj=request_obj)
+                echo_debug(f"Received basic model {page}")
                 yield from page.rows
 
+    def direct_count(self, request_obj: t.Optional[GetTasks] = None, **kwargs) -> IntValue:
+        """Direct API method to get the number of tasks for enforcements.
+
+        Args:
+            request_obj: request object to use
+            **kwargs: passed to build a new request object if one is not provided
+        """
+        api_endpoint: ApiEndpoint = ApiEndpoints.enforcements.tasks.count
+        request_obj: GetTasks = self.build_get_request(request_obj=request_obj, **kwargs)
+        response: IntValue = api_endpoint.perform_request(
+            http=self.auth.http, request_obj=request_obj
+        )
+        return response
+
     def direct_get(self, request_obj: t.Optional[GetTasks] = None, **kwargs) -> t.List[TaskBasic]:
-        """Direct API layer to get all tasks for enforcements in basic model."""
+        """Direct API method to get all tasks for enforcements in basic model.
+
+        Args:
+            request_obj: request object to use
+            **kwargs: passed to build a new request object if one is not provided
+        """
         api_endpoint: ApiEndpoint = ApiEndpoints.enforcements.tasks.get_basic
-        request_obj: GetTasks = GetTasks.get_request_if_not_request(
-            request_obj=request_obj, **kwargs
-        )
+        request_obj: GetTasks = self.build_get_request(request_obj=request_obj, **kwargs)
         response: t.List[TaskBasic] = api_endpoint.perform_request(
             http=self.auth.http, request_obj=request_obj
         )
         return response
 
     def get_full(self, uuid: str) -> TaskFull:
-        """Direct API layer to get a single task for an enforcement in full model."""
+        """Direct API method to get a single task for an enforcement in full model.
+
+        Args:
+            uuid: uuid of the task to get
+        """
         api_endpoint: ApiEndpoint = ApiEndpoints.enforcements.tasks.get_full
         response: TaskFull = api_endpoint.perform_request(
             http=self.auth.http,
             uuid=uuid,
         )
         return response
```

## axonius_api_client/api/folders/folders.py

```diff
@@ -43,15 +43,15 @@
     def get(self) -> FoldersModel:
         """Get the root for this folders object type."""
         root: FoldersModel = self._get()
         return root
 
     @cached(cache=TTLCache(maxsize=1024, ttl=60))
     def get_cached(self) -> t.Union[FoldersModel, FolderModel]:
-        """Get the root for this folders object type using a 60 second cache."""
+        """Get the root for this folders object type using a cache with a TTL of 60."""
         return self.get()
 
     def get_tree(
         self,
         folder: t.Optional[t.Union[str, Folder]] = None,
         maximum_depth: t.Optional[int] = None,
         include_objects: bool = FolderDefaults.include_objects,
@@ -95,15 +95,15 @@
 
         """
         root: FoldersModel = self.get()
         return root.find(folder=folder, create=create, refresh=False, echo=echo)
 
     @cached(cache=TTLCache(maxsize=1024, ttl=60))
     def find_cached(self, *args, **kwargs) -> t.Union[FoldersModel, FolderModel]:
-        """Find a folder for this folders object type using a 60 second cache."""
+        """Find a folder for this folders object type using a cache with a TTL of 60."""
         return self.find(*args, **kwargs)
 
     def search_objects(
         self,
         folder: t.Union[str, Folder],
         searches: t.List[str],
         pattern_prefix: t.Optional[str] = FolderDefaults.pattern_prefix,
```

## axonius_api_client/api/json_api/__init__.py

```diff
@@ -5,23 +5,25 @@
     account,
     adapters,
     assets,
     audit_logs,
     base,
     base2,
     central_core,
+    count_operator,
     custom_fields,
     dashboard_spaces,
     data_scopes,
     duration_operator,
     enforcements,
     folders,
     generic,
     instances,
     lifecycle,
+    nested_access,
     paging_state,
     password_reset,
     remote_support,
     resources,
     saved_queries,
     selection,
     signup,
@@ -61,8 +63,10 @@
     "time_range",
     "paging_state",
     "selection",
     "dashboard_spaces",
     "spaces_export",
     "folders",
     "account",
+    "nested_access",
+    "count_operator",
 )
```

## axonius_api_client/api/json_api/base.py

```diff
@@ -1,19 +1,19 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
 import logging
 import typing as t
 import warnings
 from dataclasses import Field
-from typing import Any, Tuple
 
 import dataclasses_json
 import marshmallow
 import marshmallow_jsonapi
+import marshmallow_jsonapi.fields as mm_fields
 
 from ...constants.ctypes import SimpleLike
 from ...constants.general import RERAISE
 from ...exceptions import (
     ApiAttributeMissingError,
     ApiAttributeTypeError,
     ApiError,
@@ -22,18 +22,18 @@
 )
 from ...http import Http
 from ...logs import get_obj_log
 from ...setup_env import get_env_extra_warn
 from ...tools import coerce_bool, combo_dicts, json_dump, json_load, listify, strip_right
 
 LOGGER = logging.getLogger(__name__)
-WARN_TRACKER: t.Dict["BaseModel", t.Set[str]] = {}
+WARN_TRACKER: t.Dict[t.Type["BaseModel"], t.Set[str]] = {}
 
 
-def get_warn_help(value: Warning) -> t.List[str]:
+def get_warn_help(value: t.Type[Warning]) -> t.List[str]:
     """Pass."""
     name = value.__name__
     ret = [
         "",
         "To silence these warnings please upgrade to latest API client.",
         "If there is not a newer version available yet, you can disable these warnings using:",
         "- from command line, use OS environment variable AX_EXTRA_WARN='no'",
@@ -49,14 +49,15 @@
     """Common methods for all schema and model classes."""
 
     @property
     def logger(self) -> logging.Logger:
         """Pass."""
         return get_obj_log(self)
 
+    # noinspection PyUnusedLocal
     @classmethod
     def _post_load_attrs(
         cls,
         data: t.Union["BaseModel", t.List["BaseModel"]],
         http: t.Optional[Http] = None,
         **kwargs,
     ):
@@ -64,45 +65,46 @@
 
         Args:
             data (t.Union["BaseModel", t.List["BaseModel"]]): Loaded model(s)
             http (Optional[Http], optional): HTTP object to set on loaded model(s)
             **kwargs: n/a
         """
         for item in listify(data):
+            # noinspection PyBroadException
             try:
                 item.HTTP = http
             except Exception:  # pragma: no cover
                 pass
 
     @classmethod
     def _load_schema(
         cls,
         schema: marshmallow.Schema,
         data: t.Union[dict, t.List[dict]],
-        reraise: bool = RERAISE,
         **kwargs,
     ) -> t.Union["BaseModel", t.List["BaseModel"]]:
         """Load data using a marshmallow schema.
 
         Args:
-            schema (marshmallow.Schema): Schema to use to load data
+            schema (marshmallow.Schema): Schema to use to load `data`
             data (t.Union[dict, t.List[dict]]): Data to load
             **kwargs: passed to :meth:`_post_load_attrs`
 
         Returns:
             t.Union["BaseModel", t.List["BaseModel"]]: Loaded model(s)
 
         Raises:
             JsonApiError: when "type" of JSON API data does not match Meta._type of schema
             ValidationError: when marshmallow schema validation fails
         """
+        kwargs["reraise"] = kwargs.get("reraise", RERAISE)
         try:
             loaded = schema.load(data, unknown=marshmallow.INCLUDE)
         except Exception as exc:
-            if reraise:
+            if kwargs["reraise"]:
                 raise
             raise SchemaError(schema=schema, exc=exc, obj=cls, data=data)
 
         cls._post_load_attrs(data=loaded, **kwargs)
         return loaded
 
     @staticmethod
@@ -159,14 +161,15 @@
             exc = ApiError(
                 f"Data to load must be a dictionary or list, not a {type(data).__name__}"
             )
             raise SchemaError(schema=schema, exc=exc, data=data, obj=cls)
 
         return cls._load_schema(**combo_dicts(kwargs, schema=schema, data=data))
 
+    # noinspection PyUnusedLocal
     @marshmallow.post_load
     def post_load_process(self, data: dict, **kwargs) -> t.Union[dict, "BaseModel"]:
         """Marshmallow post_load hook to load validated data into a model class.
 
         Args:
             data (dict): validated data to load
             **kwargs: n/a
@@ -178,25 +181,25 @@
         data = data or {}
 
         if not dataclasses.is_dataclass(model_cls):
             return model_cls(**data) if callable(model_cls) else data
         return model_cls.from_dict(data)
 
     @classmethod
-    def get_model_fields(cls) -> t.List[dataclasses.Field]:
+    def get_model_fields(cls) -> t.List[Field]:
         """Get the fields of the model class that this schema loads into."""
         # noinspection PyProtectedMember
         return cls.get_model_cls()._get_fields()
 
 
 class BaseSchemaJson(BaseSchema, marshmallow_jsonapi.Schema):
     """Schema base class for validating JSON API data."""
 
-    id = marshmallow_jsonapi.fields.Str()
-    document_meta = marshmallow_jsonapi.fields.DocumentMeta()
+    id = mm_fields.Str()
+    document_meta = mm_fields.DocumentMeta()
 
     class Meta:
         """Pass."""
 
         type_ = "base_schema"
 
     @classmethod
@@ -251,23 +254,34 @@
         if check in attrs:
             return value
 
         valids = "\n  " + "\n  ".join(list(attrs))
         raise exc_cls(f"Invalid attribute {value!r}, valids: {valids}")
 
 
+# noinspection PyDataclass
 class BaseModel(dataclasses_json.DataClassJsonMixin, BaseCommon):
     """Model base class for holding data."""
 
+    HTTP: t.ClassVar[Http] = None
+    """HTTP client to use for requests."""
+
     @classmethod
     def get_request_if_not_request(
-        cls, request_obj: t.Optional["BaseModel"] = None, *args, **kwargs
+        cls,
+        http: Http,
+        request_obj: t.Optional["BaseModel"] = None,
+        *args,
+        **kwargs,
     ) -> "BaseModel":
         """If request is not of this type, build one using args and kwargs."""
-        return request_obj if isinstance(request_obj, cls) else cls(*args, **kwargs)
+        # noinspection PyArgumentList
+        data = request_obj if isinstance(request_obj, cls) else cls(*args, **kwargs)
+        cls._post_load_attrs(data, http=http)
+        return data
 
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Get the BaseSchema that should be used to validate the data for this model.
 
         Returns:
             t.Optional[Type[BaseSchema]]: BaseSchema to use to verify data
@@ -319,15 +333,15 @@
         cls, data: t.Union[dict, list, tuple], schema_cls: t.Optional[BaseSchema] = None, **kwargs
     ) -> t.Union["BaseModel", t.List["BaseModel"]]:
         """Load data using this JSON API schema.
 
         Args:
             data (t.Union[dict, list, tuple]): Response data to load using this schema
             schema_cls (Optional[BaseSchema], optional): Schema class to use to validate data
-                will fallback to :meth:`get_schema_cls` or dataclasses_json automatic schema
+                will fall back to :meth:`get_schema_cls` or dataclasses_json automatic schema
             **kwargs: passed to :meth:`BaseCommon._load_schema`
 
         Returns:
             t.Union["BaseModel", t.List["BaseModel"]]: Loaded model(s)
 
         Raises:
             SchemaError: if data is not a dict or list
@@ -363,15 +377,15 @@
         """Convert this model into a dictionary for sending as a request.
 
         This does a bunch of fancy foot work to re-validate the data using marshmallow schema
         if possible.
 
         Args:
             schema_cls (Optional[BaseSchema], optional): Schema class to use to validate data
-                will fallback to :meth:`get_schema_cls` or dataclasses_json automatic schema
+                will fall back to :meth:`get_schema_cls` or dataclasses_json automatic schema
             **kwargs: passed to :meth:`BaseCommon._load_schema` as part of reloading the data
                 to validate it
 
         Returns:
             dict: serialized dict of this model
         """
         schema_cls = schema_cls or self.get_schema_cls() or self.schema
@@ -381,17 +395,17 @@
         if isinstance(schema, BaseSchemaJson):
             if "data" not in dumped:
                 dumped = {"data": {"attributes": dumped, "type": schema.Meta.type_}}
             if "type" not in dumped["data"]:
                 dumped = {"data": {"attributes": dumped, "type": schema.Meta.type_}}
 
         loaded = self._load_schema(**combo_dicts(kwargs, schema=schema, data=dumped))
-        redumped = schema.dump(loaded)
-        redumped.pop("document_meta", None)
-        return redumped
+        re_dumped = schema.dump(loaded)
+        re_dumped.pop("document_meta", None)
+        return re_dumped
 
     def dump_request_params(self, **kwargs) -> dict:
         """Convert this object into a set of GET URL parameters.
 
         Args:
             **kwargs: n/a
 
@@ -432,21 +446,22 @@
         return self._str_join().join(props) if props else super().__str__()
 
     def __repr__(self):
         """Pass."""
         return self.__str__()
 
     @classmethod
-    def from_dict(cls, data: dict):
+    def from_dict(cls, data: dict, **kwargs):
         """Pass."""
-        fields_known = [x.name for x in dataclasses.fields(cls)]
+        fields_known: t.Dict[str, Field] = cls._get_fields_dict()
         extra_attributes = {k: data.pop(k) for k in list(data) if k not in fields_known}
-        obj = super().from_dict(data)
+        obj = super().from_dict(data, **kwargs)
         if extra_attributes:
             if hasattr(obj, "_extra_attributes"):
+                # noinspection PyProtectedMember
                 obj._extra_attributes.update(extra_attributes)
             else:
                 obj.extra_attributes = extra_attributes
         return obj
 
     @staticmethod
     def _human_key(key):  # pragma: no cover
@@ -481,18 +496,61 @@
 
     @classmethod
     def _get_field_names(cls) -> t.List[str]:
         """Get a list of field names defined for this dataclass."""
         return [x.name for x in cls._get_fields()]
 
     @classmethod
-    def _get_fields(cls) -> t.Tuple[Field]:
+    def _get_fields(cls) -> t.Tuple[Field, ...]:
         """Get a list of fields defined for this dataclass."""
         return dataclasses.fields(cls)
 
+    @classmethod
+    def _get_fields_dict(cls) -> t.Dict[str, Field]:
+        """Get a list of fields defined for this dataclass."""
+        return {x.name: x for x in dataclasses.fields(cls)}
+
+    @classmethod
+    def remove_unknown_arguments(
+        cls,
+        remove_unknown_arguments: bool = True,
+        warn_unknown_arguments: bool = True,
+        kwargs: t.Optional[dict] = None,
+    ) -> t.Tuple[dict, dict]:
+        """Remove unknown arguments from the kwargs.
+
+        Args:
+            remove_unknown_arguments (bool, optional): Whether to remove unknown arguments
+                from the kwargs. Defaults to True.
+            warn_unknown_arguments (bool, optional): Whether to warn about unknown arguments.
+                Defaults to True.
+            kwargs (t.Optional[dict], optional): The kwargs to remove unknown arguments from.
+                Defaults to None.
+
+        Returns:
+            t.Tuple[dict, dict]: The kwargs with unknown arguments removed, and the unknown
+                arguments.
+        """
+        if not isinstance(kwargs, dict):
+            kwargs = {}
+
+        known_arguments: t.Dict[str, dataclasses.Field] = cls._get_fields_dict()
+        unknown_arguments: t.Dict[str, t.Any] = {
+            k: v for k, v in kwargs.items() if k not in known_arguments
+        }
+        if unknown_arguments:
+            msg = f"Unknown arguments passed to {cls}: {unknown_arguments}"
+            LOGGER.warning(msg)
+            if warn_unknown_arguments and get_env_extra_warn():
+                warnings.warn(message=msg, category=ExtraAttributeWarning)
+            if remove_unknown_arguments:
+                for k in unknown_arguments:
+                    kwargs.pop(k)
+        return kwargs, unknown_arguments
+
     def __getitem__(self, key):
         """Pass."""
         return self.__dict__[key]
 
     def __setitem__(self, key, value):
         """Pass."""
         self.__dict__[key] = value
@@ -502,17 +560,17 @@
         """Beta concept for our own deserializer."""
         return cls.new_from_dict(obj=kwargs)
 
     @classmethod
     def new_from_dict(cls, obj: dict) -> "BaseModel":  # pragma: no cover
         """Beta concept for our own deserializer."""
 
-        def is_missing(obj) -> bool:
+        def is_missing(check) -> bool:
             """Pass."""
-            return obj == dataclasses.MISSING
+            return check == dataclasses.MISSING
 
         if not isinstance(obj, dict):
             raise ApiError(f"Object to load must be type {dict}, not {type(obj)}")
 
         cls_args = {}
         pre = f"Attribute error for dataclass {cls}"
         fields = cls._get_fields()
@@ -520,24 +578,25 @@
             ftype = field.type
             ftype_args = getattr(ftype, "__args__", ())
             ftype_origin = getattr(ftype, "__origin__", None)
             fname = field.name
 
             required = True
 
+            default_text = ""
             if not is_missing(field.default_factory):
                 default = field.default_factory
                 required = False
+                default_text = f", default={default}"
             if not is_missing(field.default):
                 default = field.default
                 required = False
+                default_text = f", default={default}"
 
-            finfo = f"Attribute name={fname!r}, type={ftype}, required={required}"
-            if not required:
-                finfo += f", default={default!r}"
+            finfo = f"Attribute name={fname!r}, type={ftype}, required={required}{default_text}"
 
             if fname not in obj:
                 if required:
                     raise ApiAttributeMissingError(
                         f"{pre}\n{finfo}\nMissing required attribute {fname!r}"
                     )
 
@@ -592,22 +651,23 @@
 
                 if ftype == bool or bool in ftype_args:
                     cls_args[fname] = coerce_bool(obj=value, errmsg=err)
                     continue
 
                 # int, float, dataclass, etc
 
+        # noinspection PyArgumentList
         ret = cls(**cls_args)
         extra_attributes = {k: v for k, v in obj.items() if k not in [x.name for x in fields]}
         ret._extra_attributes = extra_attributes
         return ret
 
     @property
     def extra_attributes(self) -> dict:
-        """Extra atttributes supplied during deserialization."""
+        """Extra attributes supplied during deserialization."""
         return getattr(self, "_extra_attributes", {})
 
     @extra_attributes.setter
     def extra_attributes(self, value: dict):
         if isinstance(value, dict) and value:
             schema = self.get_schema_cls()
             if self.__class__ not in WARN_TRACKER:
@@ -627,8 +687,9 @@
                     f"{json_dump(value)}",
                 ]
                 msg = "\n".join(msgs)
                 LOGGER.warning(msg)
                 if get_env_extra_warn():
                     warnings.warn(message=msg, category=ExtraAttributeWarning)
 
+        # noinspection PyAttributeOutsideInit
         self._extra_attributes = value
```

## axonius_api_client/api/json_api/base2.py

```diff
@@ -1,26 +1,24 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
+import typing as t
 
 import marshmallow
 
-from . import base
-
-import typing as t
-
 from ...tools import (
-    listify,
-    is_nested_schema,
-    is_subclass,
     get_hint_type,
-    get_mm_field,
     get_mm_description,
+    get_mm_field,
+    is_nested_schema,
+    is_subclass,
+    listify,
     score_prefix,
 )
+from . import base
 
 
 def is_complex_field(value: t.Union[dataclasses.Field, marshmallow.fields.Field]) -> bool:
     """Check if a field is a complex field.
 
     Args:
         value: The field to check.
@@ -64,72 +62,71 @@
         return self._dump(value=self, explode=explode)
 
     @classmethod
     def to_dicts(
         cls,
         values: t.Any,
         explode: bool = False,
-        include_schemas: bool = False,
+        schemas: bool = False,
         as_csv: bool = False,
     ) -> t.Generator[dict, None, None]:
         """Convert a list of models to a list of dicts.
 
         Args:
             values: The values to convert.
             explode: Explode the values.
-            include_schemas: Include the schemas.
+            schemas: Include the schemas.
             as_csv: Return the data as csv.
 
         Yields:
-            dict: if as_csv is True, the headers, then the schemas if include_schemas=True,
+            dict: if as_csv is True, the headers, then the schemas if schemas=True,
                 and then the converted values.
-            dict: if as_csv is False, the schemas if include_schemas=True and then the
+            dict: if as_csv is False, the schemas if schemas=True and then the
                 converted values.
-
         """
         values: t.List[t.Any] = listify(values)
-        schemas: t.Dict[str, dataclasses.Field] = cls._get_fields_explode(explode=explode)
+        fields: t.Dict[str, dataclasses.Field] = cls._get_fields_explode(explode=explode)
 
         if as_csv:
-            yield {k: k for k in schemas}
-            if include_schemas:
-                yield from cls._get_schemas_out_csv(schemas=schemas)
-        else:
-            yield {"schemas": cls._get_schemas_out(schemas=schemas)}
+            yield {k: k for k in fields}
+            if schemas:
+                yield from cls._get_fields_out_csv(fields=fields)
+        elif schemas:
+            yield {"schemas": cls._get_fields_out(fields=fields)}
 
         for value in values:
             if isinstance(value, cls):
                 value_dicts: t.Union[dict, t.List[dict]] = value.to_dict(explode=explode)
                 if isinstance(value_dicts, list):
                     yield from value_dicts
                 else:
                     yield value_dicts
 
     @classmethod
-    def _get_schemas_out(cls, schemas: t.Dict[str, dataclasses.Field]) -> dict:
-        """Get the schemas for the to_dict."""
-        schemas_out: dict = {}
-        for k, v in schemas.items():
+    def _get_fields_out(cls, fields: t.Dict[str, dataclasses.Field]) -> dict:
+        """Get the fields for the to_dict."""
+        fields_out: dict = {}
+        for k, v in fields.items():
             mm_field = get_mm_field(v)
-            schemas_out[k] = {
+            fields_out[k] = {
                 "name": k,
                 "type": str(v.type),
                 "description": get_mm_description(mm_field),
                 "allow_none": mm_field.allow_none,
                 "required": mm_field.required,
                 "default": str(mm_field.load_default),
             }
-        return schemas_out
+        return fields_out
 
     @classmethod
-    def _get_schemas_out_csv(cls, schemas: t.Dict[str, dataclasses.Field]) -> t.List[dict]:
-        """Get the schemas for the to_dict."""
+    def _get_fields_out_csv(cls, fields: t.Dict[str, dataclasses.Field]) -> t.List[dict]:
+        """Get the fields for the to_dict."""
         return [
-            {k: str(v.type) for k, v in schemas.items()},
-            {k: get_mm_description(v) for k, v in schemas.items()},
+            {k: str(v.type) for k, v in fields.items()},
+            {k: get_mm_description(v) for k, v in fields.items()},
         ]
 
     @classmethod
     def _get_fields_explode(
         cls, explode: bool = False, prefix: t.Optional[t.List[str]] = None
     ) -> t.Dict[str, dataclasses.Field]:
         """Get the headers for the CSV.
@@ -137,15 +134,14 @@
         Args:
             explode: Explode the headers for fields that are lists of complex fields.
             prefix: Prefix to add to the headers for complex fields.
 
         Returns:
             t.Dict[str, dataclasses.Field]: The headers.
         """
-
         schemas: t.Dict[str, dataclasses.Field] = {}
         prefix: t.List[str] = listify(prefix)
 
         for field in cls._get_fields():
             if not field.repr:
                 continue
             if is_complex_field(field) and explode:
```

## axonius_api_client/api/json_api/count_operator.py

```diff
@@ -1,37 +1,41 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
 import enum
 import typing as t
 
 import marshmallow
+import marshmallow.validate as mm_validate
+import marshmallow.fields as mm_fields
 
 from .base import BaseModel
 from .custom_fields import get_field_dc_mm
+from ...tools import bytes_to_str
 
 
 class OperatorTypes(enum.Enum):
     """Possible types for defining operators in mongo."""
 
     equal = "$eq"
     greater = "$gt"
     less = "$lt"
 
 
 class CountOperatorSchema(marshmallow.Schema):
     """Pass."""
 
-    type = marshmallow.fields.Str(
+    # noinspection PyTypeChecker
+    type = mm_fields.Str(
         load_default=None,
         dump_default=None,
         allow_none=True,
-        validate=marshmallow.validate.OneOf([None, *[x.name for x in OperatorTypes]]),
+        validate=mm_validate.OneOf([None, *[x.name for x in OperatorTypes]]),
     )
-    count = marshmallow.fields.Integer(dump_default=None, load_default=None, allow_none=True)
+    count = mm_fields.Integer(dump_default=None, load_default=None, allow_none=True)
 
     class Meta:
         """Pass."""
 
         type_ = "count_operator_schema"
 
     @staticmethod
@@ -54,7 +58,25 @@
         mm_field=COUNT_OPERATOR_SCHEMA.fields["count"], default=None
     )
 
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Pass."""
         return CountOperatorSchema
+
+
+TypeOperator: t.TypeVar = t.TypeVar("TypeOperator", str, bytes, OperatorTypes)
+
+
+def coerce_operator(value: TypeOperator = OperatorTypes.less) -> str:
+    """Coerce a value to an operator."""
+    if isinstance(value, OperatorTypes):
+        return value.name
+    value = bytes_to_str(value)
+    if isinstance(value, str):
+        value = value.lower().strip()
+        for operator in OperatorTypes:
+            if value in [operator.value, operator.name]:
+                return operator.name
+    valids = [f"{x.name} ({x.value})" for x in OperatorTypes]
+    valids = "\n" + "\n".join(valids)
+    raise ValueError(f"Invalid duration_operator: {value}, valids:{valids}")
```

## axonius_api_client/api/json_api/custom_fields.py

```diff
@@ -1,20 +1,20 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
 import datetime
 import typing as t
 
-from marshmallow import validate as marshmallow_validate
 import bson
 import dataclasses_json
 import dateutil
 import dateutil.parser
 import dateutil.tz
 import marshmallow
+from marshmallow import validate as marshmallow_validate
 
 from ...tools import coerce_bool, listify
 
 
 class UnionField(marshmallow.fields.Field):
     """Field that deserializes multi-type input data to app-level objects."""
 
@@ -174,14 +174,18 @@
 
 
 def field_from_mm(
     schema: t.Union[t.Type[marshmallow.Schema], marshmallow.Schema], key: str, **kwargs
 ) -> dataclasses.Field:
     """Pass."""
     if isinstance(schema, marshmallow.Schema):
+        if key not in schema.fields:
+            valids = "\n" + "\n".join(f" - {k}: {v}" for k, v in schema.fields.items())
+            raise ValueError(f"Key {key!r} not found in schema {schema}\nValids: {valids}")
+
         mm_field: marshmallow.fields.Field = schema.fields[key]
     else:
         # noinspection PyProtectedMember
         mm_field: marshmallow.fields.Field = schema._declared_fields[key]
     if mm_field.required is not True:
         kwargs = setdefault(mm_field=mm_field, attr="load_default", kwargs=kwargs)
         kwargs = setdefault(mm_field=mm_field, attr="dump_default", kwargs=kwargs)
```

## axonius_api_client/api/json_api/data_scopes.py

```diff
@@ -22,15 +22,15 @@
         """Pass."""
         if not hasattr(self, "_updated_user_obj"):
             self._updated_user_obj = json_load(self.updated_by)
         return self._updated_user_obj
 
     @property
     def updated_user_name(self) -> str:
-        """Get the user name of the user that last updated this object."""
+        """Get the username of the user that last updated this object."""
         return self.updated_user_obj.get("user_name") or ""
 
     @property
     def updated_user_source(self) -> str:
         """Get the source of the user that last updated this object."""
         return self.updated_user_obj.get("source") or ""
```

## axonius_api_client/api/json_api/duration_operator.py

```diff
@@ -1,68 +1,70 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
 import typing as t
 
 import marshmallow
+import marshmallow.validate as mm_validate
+import marshmallow.fields as mm_fields
 
 from .count_operator import OperatorTypes
-from .custom_fields import get_schema_dc
+from .custom_fields import field_from_mm
+from ...tools import combo_dicts
 
 
 class DurationOperatorSchema(marshmallow.Schema):
     """Duration operator schema."""
 
-    type = marshmallow.fields.Str(
+    # noinspection PyTypeChecker
+    type = mm_fields.Str(
         load_default=None,
         dump_default=None,
         description="Duration Operator type",
-        validate=marshmallow.validate.OneOf([None, *[x.name for x in OperatorTypes]]),
+        validate=mm_validate.OneOf([None, *[x.name for x in OperatorTypes]]),
     )
-    seconds = marshmallow.fields.Integer(
+    seconds = mm_fields.Integer(
         load_default=None,
         dump_default=None,
-        description="Amount of seconds",
+        description="Amount of seconds (deprecated)",
         allow_none=True,
     )
-    seconds_float = marshmallow.fields.Number(
+    seconds_float = mm_fields.Number(
         load_default=None,
         dump_default=None,
         description="Amount of seconds as float",
         allow_none=True,
     )
 
     class Meta:
-        """Marshmallow JSONAPI meta class."""
+        """Marshmallow JSONAPI metaclass."""
 
         type_ = "duration_operator_schema"
 
     @staticmethod
     def get_model_cls() -> t.Any:
         """Pass."""
         return DurationOperator
 
 
+SCHEMA = DurationOperatorSchema()
+
+
 @dataclasses.dataclass(repr=False)
 class DurationOperator:
-    """Duration operator dataclass."""
+    """Model for duration operator."""
 
-    type: t.Optional[str] = get_schema_dc(
-        schema=DurationOperatorSchema,
-        key="type",
-        default=None,
-    )
-    seconds: t.Optional[int] = get_schema_dc(
-        schema=DurationOperatorSchema,
-        key="seconds",
-        default=None,
-    )
-    seconds_float: t.Optional[float] = get_schema_dc(
-        schema=DurationOperatorSchema,
-        key="seconds_float",
-        default=None,
-    )
+    type: t.Optional[str] = field_from_mm(SCHEMA, "type")
+    seconds: t.Optional[int] = field_from_mm(SCHEMA, "seconds")
+    seconds_float: t.Optional[float] = field_from_mm(SCHEMA, "seconds_float")
+
+    @classmethod
+    def load_if_needed(cls, value: t.Any) -> t.Any:
+        """Pass through if already an instance of this model, else load from dict."""
+        if isinstance(value, cls):
+            return value
+        return cls(**combo_dicts(value))
 
     @staticmethod
     def get_schema_cls() -> t.Any:
-        """Pass."""
+        """Get the schema for this model."""
         return DurationOperatorSchema
```

## axonius_api_client/api/json_api/enforcements.py

```diff
@@ -546,15 +546,15 @@
 
         Args:
             folder (t.Union[str, FolderBase]): folder to move an object to
             create (bool, optional): create folder if it does not exist
             refresh (Refreshables, optional): refresh the folders before searching
             echo (bool, optional): echo output to console
             root (t.Optional[FolderBase], optional): root folders to use to find folder
-                instead of root folders from self.folder
+                instead of root folders from `self.folder`
         """
         reason: str = f"Move '{self.folder_path}/@{self.name}' to {folder!r}"
         self._check_update_ok(reason=reason)
         if not isinstance(root, FolderBase):
             root: FolderBase = self.folder.root_folders
             root.refresh(value=refresh)
 
@@ -591,15 +591,15 @@
             name (t.Optional[str], optional): if supplied, name to give copy, otherwise use
                 self.name + copy_prefix
             copy_prefix (str, optional): value to prepend to current name if no new name supplied
             create (bool, optional): create folder if it does not exist
             echo (bool, optional): echo output to console
             refresh (Refreshables, optional): refresh the folders before searching
             root (t.Optional[FolderBase], optional): root folders to use to find folder
-                instead of root folders from self.folder
+                instead of root folders from `self.folder`
 
         """
         names: t.List[str] = self.get_names()
         name: str = parse_value_copy(
             default=self.name, value=name, copy_prefix=copy_prefix, existing=names
         )
         if not isinstance(root, FolderBase):
@@ -803,15 +803,15 @@
         """Pass."""
         if not hasattr(self, "_updated_user_obj"):
             self._updated_user_obj = json_load(self.updated_by)
         return self._updated_user_obj
 
     @property
     def updated_user_name(self) -> str:
-        """Get the user name of the user that last updated this set."""
+        """Get the username of the user that last updated this set."""
         return self.updated_user_obj.get("user_name", "")
 
     @property
     def updated_user_source(self) -> str:
         """Get the source of the user that last updated this set."""
         return self.updated_user_obj.get("source", "")
 
@@ -1210,15 +1210,15 @@
     def updated_user_obj(self) -> dict:
         """Pass."""
         return self.get_basic().updated_user_obj
 
     # BASIC MODEL attribute
     @property
     def updated_user_name(self) -> str:
-        """Get the user name of the user that last updated this set."""
+        """Get the username of the user that last updated this set."""
         return self.get_basic().updated_user_name
 
     # BASIC MODEL attribute
     @property
     def updated_user_source(self) -> str:
         """Get the source of the user that last updated this set."""
         return self.get_basic().updated_user_source
```

## axonius_api_client/api/json_api/generic.py

```diff
@@ -1,27 +1,28 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
 import typing as t
 
-import marshmallow_jsonapi
+import marshmallow_jsonapi.fields as mm_fields
 
 from ...http import Http
 from .base import BaseModel, BaseSchemaJson
 from .custom_fields import SchemaBool, get_schema_dc
 
 
 class MetadataSchema(BaseSchemaJson):
     """Pass."""
 
     @staticmethod
     def get_model_cls() -> t.Any:
         """Pass."""
         return Metadata
 
+    # noinspection PyMethodOverriding
     @classmethod
     def load_response(cls, data: dict, http: Http, **kwargs):
         """Pass."""
         # PBUG: Metadata returns None for data
         if data["data"] is None:
             data["data"] = {"type": cls.Meta.type_, "attributes": {}}
 
@@ -73,15 +74,15 @@
         """Pass."""
         return BoolValueSchema
 
 
 class NameSchema(BaseSchemaJson):
     """Pass."""
 
-    name = marshmallow_jsonapi.fields.Str()
+    name = mm_fields.Str()
 
     class Meta:
         """Pass."""
 
         type_ = "name_schema"
 
     @staticmethod
@@ -102,15 +103,15 @@
         """Pass."""
         return NameSchema
 
 
 class IntValueSchema(BaseSchemaJson):
     """Pass."""
 
-    value = marshmallow_jsonapi.fields.Int(required=True)
+    value = mm_fields.Int(required=True)
 
     class Meta:
         """Pass."""
 
         type_ = "int_value_schema"
 
     @staticmethod
@@ -131,15 +132,15 @@
         """Pass."""
         return IntValueSchema
 
 
 class ApiBaseSchema(BaseSchemaJson):
     """Pass."""
 
-    id = marshmallow_jsonapi.fields.Str(required=True)
+    id = mm_fields.Str(required=True)
 
     class Meta:
         """Pass."""
 
         type_ = "base_schema"
 
     @staticmethod
@@ -165,15 +166,15 @@
         """Pass."""
         return {"uuid": self.id, "filename": self.filename}
 
 
 class StrValueSchema(BaseSchemaJson):
     """Pass."""
 
-    value = marshmallow_jsonapi.fields.Str(required=True)
+    value = mm_fields.Str(required=True)
 
     class Meta:
         """Pass."""
 
         type_ = "string_value_schema"
 
     @staticmethod
@@ -194,15 +195,15 @@
         """Pass."""
         return StrValueSchema
 
 
 class ListValueSchema(BaseSchemaJson):
     """Pass."""
 
-    value = marshmallow_jsonapi.fields.List(marshmallow_jsonapi.fields.Str())
+    value = mm_fields.List(mm_fields.Str())
 
     class Meta:
         """Pass."""
 
         type_ = "list_value_schema"
 
     @staticmethod
@@ -223,15 +224,15 @@
         """Pass."""
         return ListValueSchema
 
 
 class ListDictValueSchema(BaseSchemaJson):
     """Pass."""
 
-    value = marshmallow_jsonapi.fields.List(marshmallow_jsonapi.fields.Dict())
+    value = mm_fields.List(mm_fields.Dict())
 
     class Meta:
         """Pass."""
 
         type_ = "list_value_schema"
 
     @staticmethod
@@ -254,15 +255,15 @@
         """Pass."""
         return ListDictValueSchema
 
 
 class DictValueSchema(BaseSchemaJson):
     """Pass."""
 
-    value = marshmallow_jsonapi.fields.Dict()
+    value = mm_fields.Dict()
 
     class Meta:
         """Pass."""
 
         type_ = "dict_value_schema"
 
     @staticmethod
@@ -283,15 +284,15 @@
         """Pass."""
         return DictValueSchema
 
 
 class DeletedSchema(BaseSchemaJson):
     """Pass."""
 
-    deleted = marshmallow_jsonapi.fields.Int()
+    deleted = mm_fields.Int()
 
     class Meta:
         """Pass."""
 
         type_ = "deleted_schema"
 
     @staticmethod
@@ -325,15 +326,15 @@
         """Pass."""
         return PrivateRequestSchema
 
 
 class PrivateRequestSchema(BaseSchemaJson):
     """Pass."""
 
-    private = marshmallow_jsonapi.fields.Bool(load_default=False, dump_default=False)
+    private = mm_fields.Bool(load_default=False, dump_default=False)
 
     class Meta:
         """Pass."""
 
         type_ = "private_schema"
 
     @staticmethod
```

## axonius_api_client/api/json_api/paging_state.py

```diff
@@ -63,18 +63,14 @@
         """Pass."""
         if isinstance(response, (list, tuple)):
             self.state.rows_fetched_total += len(response)
         return response
 
     def __repr__(self):
         """Pass."""
-        return self.__str__()
-
-    def __str__(self) -> t.List[str]:
-        """Pass."""
 
         def getval(prop):
             value = getattr(self, prop, None)
             if value is not None and not isinstance(value, (str, int, float, bool)):
                 value = str(value)
             return repr(value)
 
@@ -85,14 +81,31 @@
             "page_number",
             "row_count",
             "method",
         ]
         vals = ", ".join([f"{p}={getval(p)}" for p in props])
         return f"{self.__class__.__name__}({vals})"
 
+    def __str__(self) -> t.List[str]:
+        """Pass."""
+
+        def getval(prop):
+            value = getattr(self, prop, None)
+            if value is not None and not isinstance(value, (str, int, float, bool)):
+                value = str(value)
+            return repr(value)
+
+        props = [
+            "duration",
+            "page_number",
+            "row_count",
+        ]
+        vals = ", ".join([f"{p}={getval(p)}" for p in props])
+        return f"{self.__class__.__name__}({vals})"
+
     def check_stop(self):
         """Pass."""
         if not self.row_count:
             self.state.stop(reason="No rows returned")
         self.state.check_stop()
 
     def handle_sleep(self):
```

## axonius_api_client/api/json_api/resources.py

```diff
@@ -1,156 +1,165 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
 import typing as t
 
-import marshmallow
-import marshmallow_jsonapi
+import marshmallow_jsonapi.fields as mm_fields
 
 from ...constants.api import MAX_PAGE_SIZE, PAGE_SIZE
-from ...tools import parse_int_min_max
-from .base import BaseModel, BaseSchemaJson
-from .custom_fields import SchemaBool
+from ...tools import combo_dicts, parse_int_min_max
+from .base import BaseModel, BaseSchema, BaseSchemaJson
+from .custom_fields import SchemaBool, field_from_mm
 
 
-class PaginationSchema(marshmallow.Schema):
-    """Pass."""
-
-    offset = marshmallow_jsonapi.fields.Integer(load_default=0, dump_default=0)
-    limit = marshmallow_jsonapi.fields.Integer(
-        load_default=MAX_PAGE_SIZE, dump_default=MAX_PAGE_SIZE
+class PaginationSchema(BaseSchema):
+    """Schema for requests that use pagination."""
+
+    offset = mm_fields.Integer(
+        load_default=0,
+        dump_default=0,
+        description="Row start",
+    )
+    limit = mm_fields.Integer(
+        load_default=MAX_PAGE_SIZE,
+        dump_default=MAX_PAGE_SIZE,
+        description=f"Page size (max: {MAX_PAGE_SIZE})",
     )
 
+    @staticmethod
+    def get_model_cls() -> t.Any:
+        """Get the model for this schema."""
+        return PaginationRequest
+
+
+PAGE_SCHEMA = PaginationSchema()
+
 
 @dataclasses.dataclass
 class PaginationRequest(BaseModel):
-    """Pass."""
+    """Model for requests that use pagination."""
 
-    offset: t.Optional[int] = 0
-    """Row to start from"""
-
-    limit: t.Optional[int] = MAX_PAGE_SIZE
-    """Number of rows to return"""
+    offset: t.Optional[int] = field_from_mm(PAGE_SCHEMA, "offset")
+    limit: t.Optional[int] = field_from_mm(PAGE_SCHEMA, "limit")
 
     @staticmethod
     def get_schema_cls() -> t.Any:
-        """Pass."""
-        return None
+        """Get the schema for this model."""
+        return PaginationSchema
 
-    def __post_init__(self):
-        """Pass."""
+    def set_offset(self, value: t.Optional[int]) -> int:
+        """Set the offset (row start) for this request."""
+        self.offset = parse_int_min_max(value=value, default=0, min_value=0, max_value=None)
+        return self.offset
+
+    def set_limit(self, value: t.Optional[int]) -> int:
+        """Set the limit (page size) for this request."""
         self.limit = parse_int_min_max(
-            value=self.limit, default=PAGE_SIZE, min_value=1, max_value=MAX_PAGE_SIZE
+            value=value, default=PAGE_SIZE, min_value=1, max_value=MAX_PAGE_SIZE
         )
-        self.offset = parse_int_min_max(value=self.offset, default=0, min_value=0, max_value=None)
-
-
-@dataclasses.dataclass
-class PageSortRequest(BaseModel):
-    """Data attributes for pagination and sort."""
-
-    sort: t.Optional[str] = None
-    """Field to sort on and direction to sort.
+        return self.limit
 
-    not used by api client (sort using client side logic)
-
-    examples:
-        for descending: "-field"
-        for ascending: "field"
-    """
-
-    page: t.Optional[PaginationRequest] = dataclasses.field(
-        default=None,
-        metadata={
-            "dataclasses_json": {"mm_field": marshmallow_jsonapi.fields.Nested(PaginationSchema)},
-        },
-    )
-    """Row to start at and number of rows to return.
-
-    examples:
-        in get request: page[offset]=0&page[limit]=2000
-        in post request: {"data": {"attributes": {"page": {"offset": 0, "limit": 2000}}}
-    """
-    # FYI: with out using mm_field metadata for nested schemas for dataclasses_json,
-    # the mm field that dataclasses_json dynamically creates produces warnings
-    # about using deprecated additional_meta args
-
-    get_metadata: bool = True
-    """Return pagination metadata in response."""
+    @classmethod
+    def load_if_needed(cls, value: t.Any) -> t.Any:
+        """Pass through if already an instance of this model, else load from dict."""
+        if isinstance(value, cls):
+            return value
+        return cls(**combo_dicts(value))
 
     def __post_init__(self):
-        """Pass."""
-        self.page = self.page if self.page else PaginationRequest()
-
-    @staticmethod
-    def get_schema_cls() -> t.Any:
-        """Pass."""
-        return None
+        """Dataclass post init."""
+        self.set_offset(self.offset)
+        self.set_limit(self.limit)
 
 
 class ResourcesGetSchema(BaseSchemaJson):
-    """Pass."""
+    """Schema used for getting numerous object types throughout the API."""
 
-    sort = marshmallow_jsonapi.fields.Str(allow_none=True, load_default=None, dump_default=None)
-    page = marshmallow_jsonapi.fields.Nested(PaginationSchema)
-    search = marshmallow_jsonapi.fields.Str(load_default="", dump_default="")
-    filter = marshmallow_jsonapi.fields.Str(allow_none=True, load_default="", dump_default="")
-    get_metadata = SchemaBool(load_default=True, dump_default=True)
+    page = mm_fields.Nested(
+        PaginationSchema(),
+        load_default=PaginationRequest,
+        dump_default=PaginationRequest,
+        allow_none=True,
+        description="Pagination parameters",
+    )
+    sort = mm_fields.Str(
+        load_default=None,
+        dump_default=None,
+        allow_none=True,
+        description="Field to sort by, prefix with '-' for descending",
+    )
+    search = mm_fields.Str(
+        load_default=None,
+        dump_default=None,
+        allow_none=True,
+        description="Search term",
+    )
+    filter = mm_fields.Str(
+        load_default=None,
+        dump_default=None,
+        allow_none=True,
+        description="Filter to limit results, aql-ish",
+    )
+    get_metadata = SchemaBool(
+        load_default=True,
+        dump_default=True,
+        description="Include pagination metadata in response",
+    )
 
     @staticmethod
     def get_model_cls() -> t.Any:
-        """Pass."""
+        """Get the model for this schema."""
         return ResourcesGet
 
     class Meta:
-        """Pass."""
+        """JSONAPI config."""
 
         type_ = "resource_request_schema"
 
 
+RESOURCES_GET_SCHEMA = ResourcesGetSchema()
+
+
 @dataclasses.dataclass
-class ResourcesGet(PageSortRequest):
-    """Request attributes for getting resources."""
+class ResourcesGet(BaseModel):
+    """Model used for getting numerous object types throughout the API."""
 
-    search: str = dataclasses.field(
-        default="",
-        metadata={
-            "dataclasses_json": {
-                "mm_field": marshmallow_jsonapi.fields.Str(load_default="", dump_default="")
-            },
-        },
-    )
-    """AQL search term
-
-    not used by api client (filter using client side logic)
-
-    examples:
-        (name == regex("test", "i"))
-        (name == regex("test", "i")) and tags in ["Linux"]
-    """
-    filter: t.Optional[str] = dataclasses.field(
-        default=None,
-        metadata={
-            "dataclasses_json": {
-                "mm_field": marshmallow_jsonapi.fields.Str(
-                    allow_none=True, load_default="", dump_default=""
-                )
-            },
-        },
-    )
+    page: t.Optional[t.Union[dict, PaginationRequest]] = field_from_mm(RESOURCES_GET_SCHEMA, "page")
+    sort: t.Optional[str] = field_from_mm(RESOURCES_GET_SCHEMA, "sort")
+    search: t.Optional[str] = field_from_mm(RESOURCES_GET_SCHEMA, "search")
+    filter: t.Optional[str] = field_from_mm(RESOURCES_GET_SCHEMA, "filter")
+    get_metadata: bool = field_from_mm(RESOURCES_GET_SCHEMA, "get_metadata")
 
     @staticmethod
     def get_schema_cls() -> t.Any:
-        """Pass."""
+        """Get the schema for this model."""
         return ResourcesGetSchema
 
+    def __post_init__(self):
+        """Dataclass post init."""
+        if isinstance(self.page, dict):
+            self.page = PaginationRequest(**self.page)
+        if not isinstance(self.page, PaginationRequest):
+            self.page = PaginationRequest()
+
+
+class ResourceDeleteSchema(BaseSchemaJson):
+    """Schema used for deleting numerous object types throughout the API."""
+
+    uuid = mm_fields.Str()
+
+    @staticmethod
+    def get_model_cls() -> t.Any:
+        """Get the model for this schema."""
+        return ResourceDelete
+
 
 @dataclasses.dataclass
 class ResourceDelete(BaseModel):
-    """Pass."""
+    """Model used for deleting numerous object types throughout the API."""
 
     uuid: str
 
     @staticmethod
     def get_schema_cls() -> t.Any:
-        """Pass."""
-        return None
+        """Get the schema for this model."""
+        return ResourceDeleteSchema
```

## axonius_api_client/api/json_api/saved_queries.py

```diff
@@ -256,15 +256,15 @@
 
         Args:
             folder (t.Union[str, FolderBase]): folder to move an object to
             create (bool, optional): create folder if it does not exist
             refresh (Refreshables, optional): refresh the folders before searching
             echo (bool, optional): echo output to console
             root (t.Optional[FolderBase], optional): root folders to use to find folder
-                instead of root folders from self.folder
+                instead of root folders from `self.folder`
         """
         reason: str = f"Move '{self.folder.path}/@{self.name}' to {folder!r}/@{self.name}"
         self._check_update_ok(reason=reason)
 
         if not isinstance(root, FolderBase):
             root: FolderBase = self.folder.root_folders
             root.refresh(value=refresh)
@@ -302,15 +302,15 @@
             copy_prefix (str, optional): value to prepend to current name if no new name supplied
             create (bool, optional): create folder if it does not exist
             echo (bool, optional): echo output to console
             refresh (Refreshables, optional): refresh the folders before searching
             private (bool, optional): set copy as private, will change default folder used
             asset_scope (bool, optional): set copy as asset scope, will change default folder used
             root (t.Optional[FolderBase], optional): root folders to use to find folder
-                instead of root folders from self.folder
+                instead of root folders from `self.folder`
 
         """
         private: bool = coerce_bool(private)
         asset_scope: bool = coerce_bool(asset_scope)
         always_cached: bool = coerce_bool(always_cached)
 
         names: t.List[str] = self.get_names()
```

## axonius_api_client/api/json_api/system_roles.py

```diff
@@ -22,15 +22,15 @@
         if measure > lengths[target]:
             lengths[target] = measure
 
     cats = {}
     cat_actions = {}
     lengths = {"categories": 0, "actions": 0, "categories_desc": 0, "actions_desc": 0}
 
-    # first pass, get all of the categories
+    # first pass, get all the categories
     for label, desc in raw.items():
         pre, rest = label.split(".", 1)
         if pre != "permissions":
             continue
 
         split = rest.split(".", 1)
         cat = split.pop(0)
@@ -40,15 +40,15 @@
             assert cat not in cat_actions
             cats[cat] = desc
             set_len(item=desc, target="categories_desc")
             set_len(item=cat, target="categories")
 
             cat_actions[cat] = {}
 
-    # second pass, get all of the actions
+    # second pass, get all the actions
     for label, desc in raw.items():
         pre, rest = label.split(".", 1)
         if pre != "permissions":
             continue
 
         split = rest.split(".", 1)
         cat = split.pop(0)
```

## axonius_api_client/api/json_api/system_users.py

```diff
@@ -33,14 +33,17 @@
     )
     source = marshmallow_jsonapi.fields.Str()
     user_name = marshmallow_jsonapi.fields.Str(required=True)
     uuid = marshmallow_jsonapi.fields.Str(required=True)
     ignore_role_assignment_rules = SchemaBool(
         load_default=False, dump_default=False, allow_none=True
     )
+    allowed_scopes_impersonation = marshmallow_jsonapi.fields.List(
+        marshmallow_jsonapi.fields.Str(), load_default=list, dump_default=list
+    )
 
     @staticmethod
     def get_model_cls() -> t.Any:
         """Pass."""
         return SystemUser
 
     class Meta:
@@ -104,14 +107,15 @@
     password: t.Optional[t.Union[t.List[str], str]] = None
     pic_name: t.Optional[str] = None
     role_name: t.Optional[str] = None
     title: t.Optional[str] = None
     department: t.Optional[str] = None
     source: t.Optional[str] = None
     ignore_role_assignment_rules: bool = False
+    allowed_scopes_impersonation: t.List[str] = dataclasses.field(default_factory=list)
     document_meta: t.Optional[dict] = dataclasses.field(default_factory=dict)
 
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Pass."""
         return SystemUserSchema
```

## axonius_api_client/api/json_api/folders/base.py

```diff
@@ -4,15 +4,15 @@
 import dataclasses
 import datetime
 import enum
 import typing as t
 
 import click
 import marshmallow
-import marshmallow_jsonapi
+import marshmallow_jsonapi.fields as mm_fields
 
 from ....constants.api import FolderDefaults
 from ....constants.ctypes import FolderBase, Refreshables
 from ....data import BaseEnum
 from ....exceptions import (
     AxonTypeError,
     FolderAlreadyExistsError,
@@ -29,42 +29,43 @@
 from ..custom_fields import SchemaDatetime, get_field_dc_mm
 from ..system_users import SystemUser
 
 
 class CreateFolderRequestSchema:
     """Marshmallow schema for request to create a folder."""
 
-    name = marshmallow_jsonapi.fields.Str()
-    parent_id = marshmallow_jsonapi.fields.Str()
+    name = mm_fields.Str()
+    parent_id = mm_fields.Str()
 
 
 class RenameFolderRequestSchema:
     """Marshmallow schema for request to rename a folder."""
 
-    name = marshmallow_jsonapi.fields.Str()
+    name = mm_fields.Str()
 
 
 class MoveFolderRequestSchema:
     """Marshmallow schema for request to move a folder."""
 
-    parent_id = marshmallow_jsonapi.fields.Str()
+    parent_id = mm_fields.Str()
 
 
 class FoldersSchema:
     """Marshmallow schema for response to get folders."""
 
-    folders = marshmallow_jsonapi.fields.List(marshmallow_jsonapi.fields.Dict())
+    folders = mm_fields.List(mm_fields.Dict())
 
 
+# noinspection PyUnresolvedReferences
 class FolderDataMixin:
     """Mixins for Metadata responses with 'folder_data' as dict."""
 
     @property
     def folder_data(self) -> dict:
-        """Get the folder_data dict from the create response metadata."""
+        """Get the folder_data dict from the creation response metadata."""
         return self.document_meta.get("folder_data") or {}
 
     @property
     def id(self) -> t.Optional[str]:
         """Get the ID of the created folder."""
         return self.folder_data.get("_id")
 
@@ -138,14 +139,15 @@
 @dataclasses.dataclass(repr=False)
 class MoveFolderResponseModel(FolderDataMixin):
     """Dataclass model for response to move a folder."""
 
     pass
 
 
+# noinspection PyUnresolvedReferences
 @dataclasses.dataclass(repr=False)
 class DeleteFolderResponseModel:
     """Dataclass model for response to delete a folder."""
 
     @property
     def id(self) -> str:
         """Get the ID of the created folder."""
@@ -177,28 +179,32 @@
 
     def refresh(
         self,
         value: Refreshables = FolderDefaults.refresh,
         force: bool = False,
         root: t.Optional["FoldersModel"] = None,
     ) -> "Folder":
-        """Refresh the root folders data.
+        """Refresh the root folder data.
 
         Args:
             value (Refreshables, optional): only perform if refresh is True or is
                 int/float and elapsed >= refresh.
             force (bool, optional): perform a refresh if force is True
             root (t.Optional["FoldersModel"], optional): dont fetch a new root, use this one yo
 
         """
-        check: bool = parse_refresh(value=value, refresh_elapsed=self.refresh_elapsed)
+        check: bool = self._parse_refresh(value)
         if check is True or force is True:
             return self._refresh(root=root)
         return self
 
+    def _parse_refresh(self, value: Refreshables) -> bool:
+        """Parse the refresh value."""
+        return parse_refresh(value=value, refresh_elapsed=self.refresh_elapsed)
+
     @property
     def refreshed(self) -> datetime.datetime:
         """Get the refreshed status of the root folders."""
         if self.is_model_folders(self):
             return self._refreshed
         return self.root_folders._refreshed
 
@@ -226,66 +232,75 @@
     @property
     def refresh_elapsed(self) -> float:
         """Get the number of seconds since the last time the root folders were fetched."""
         if self.is_model_folders(self):
             return (datetime.datetime.now() - self._refresh_dt).total_seconds()
         return self.root_folders.refresh_elapsed
 
-    @abc.abstractclassmethod
+    @classmethod
+    @abc.abstractmethod
     def get_enum_names(cls) -> BaseEnum:
         """Get the enum containing folder names for this folders object type."""
         raise NotImplementedError()
 
-    @abc.abstractclassmethod
+    @classmethod
+    @abc.abstractmethod
     def get_enum_paths(cls) -> BaseEnum:
         """Get the enum containing folder paths for this folders object type."""
         raise NotImplementedError()
 
-    @abc.abstractclassmethod
+    @classmethod
+    @abc.abstractmethod
     def get_model_folder(cls) -> t.Type["FolderModel"]:
         """Get the folder model for this folders object type."""
         raise NotImplementedError()
 
-    @abc.abstractclassmethod
+    @classmethod
+    @abc.abstractmethod
     def get_model_folders(cls) -> t.Type["FoldersModel"]:
         """Get the folders model for this folders object type."""
         raise NotImplementedError()
 
     @classmethod
-    def get_models_folders(cls) -> t.Tuple[t.Type["Folder"]]:
+    def get_models_folders(
+        cls,
+    ) -> t.Tuple[t.Type[t.Union["Folder", "FoldersModel", "FolderModel"]], ...]:
         """Get all folder models for this folders object type."""
         return (cls.get_model_folder(), cls.get_model_folders())
 
-    @abc.abstractclassmethod
+    @classmethod
+    @abc.abstractmethod
     def get_models_objects(cls) -> t.Tuple[t.Type[BaseModel]]:
         """Get the object models for this folders object type."""
         raise NotImplementedError()
 
-    @abc.abstractclassmethod
+    @classmethod
+    @abc.abstractmethod
     def get_model_create_response(cls) -> t.Type[BaseModel]:
         """Get the folder create model for this folders object type."""
         raise NotImplementedError()
 
-    @abc.abstractproperty
+    @property
+    @abc.abstractmethod
     def api_folders(self):
         """Get the folders API for this type of folders."""
         raise NotImplementedError()
 
     @abc.abstractmethod
     def _get_objects(self, full_objects: bool = FolderDefaults.full_objects):
         """Get the objects for this folders object type."""
         raise NotImplementedError()
 
     @abc.abstractmethod
     def _create_object(self, **kwargs) -> BaseModel:
-        """Create passthru for the object type in question."""
+        """Create pass-thru for the object type in question."""
         raise NotImplementedError()
 
     @classmethod
-    def get_types_findable(cls) -> t.Tuple[type]:
+    def get_types_findable(cls) -> t.Tuple[type, ...]:
         """Get the types that are allowed for `value` in find."""
         return (str, *cls._get_types_findable())
 
     @classmethod
     def _get_types_findable(cls) -> t.Tuple[type]:
         """Get the types that are allowed for `value` in find."""
         return (cls.get_model_create_response(), *cls.get_models_folders())
@@ -344,14 +359,15 @@
                 items += folder.get_tree(
                     maximum_depth=maximum_depth,
                     include_details=include_details,
                     include_objects=include_objects,
                 )
         return "\n".join(items) if as_str else items
 
+    # noinspection PyMethodFirstArgAssignment
     def find(
         self,
         folder: t.Union[str, enum.Enum, "Folder", "CreateFolderResponseModel"],
         create: bool = FolderDefaults.create,
         refresh: Refreshables = FolderDefaults.refresh,
         echo: bool = FolderDefaults.echo,
         minimum_depth: t.Optional[int] = None,
@@ -400,15 +416,15 @@
             full_objects (bool, optional): get objects with their full data
             all_objects (bool, optional): return all objects in system or only objects in this
                 folder directly
             recursive (bool, optional): return all objects under this folder or only objects
                 in this folder directly
             refresh (Refreshables, optional): refresh the object cache if this is True
         """
-        refresh = parse_refresh(value=refresh, refresh_elapsed=self.refresh_elapsed)
+        refresh = self._parse_refresh(value=refresh)
         if refresh is True:
             self._clear_objects_cache()
 
         if all_objects:
             return self._get_objects(full_objects=full_objects)
 
         ret: t.List[BaseModel] = []
@@ -436,15 +452,15 @@
         error_no_matches: bool = FolderDefaults.error_no_matches,
         error_no_objects: bool = FolderDefaults.error_no_objects,
         recursive: bool = FolderDefaults.recursive,
         all_objects: bool = FolderDefaults.all_objects,
         full_objects: bool = FolderDefaults.full_objects_search,
         echo: bool = FolderDefaults.echo,
         refresh: Refreshables = FolderDefaults.refresh,
-    ) -> t.List[BaseModel]:
+    ) -> t.List[object]:
         """Search for objects in this folder.
 
         Args:
             searches (t.List[str]): List of object names to search for
             pattern_prefix (t.Optional[str], optional): Treat any searches that start with this
                 prefix as a regex
             ignore_case (bool, optional): ignore case when building patterns
@@ -487,15 +503,15 @@
             self.spew(objs_txt, echo=echo)
 
         searches: Searches = Searches(
             objects=objs, values=searches, ignore_case=ignore_case, pattern_prefix=pattern_prefix
         )
         self.spew([f"Loaded {searches}"], echo=echo)
 
-        matches: t.List[BaseModel] = searches.matches
+        matches: t.List[object] = searches.matches
         matches_txt: str = f"{searches.str_matches} (error_no_matches={error_no_matches})"
         matches_errs: t.List[str] = [matches_txt, f"{searches}"]
         self.spew(matches_txt, echo=echo, level="info" if matches else "warning")
 
         unmatched: t.List[Search] = searches.unmatched
         unmatched_txts: t.List[str] = [
             f"{searches.str_unmatched} (error_unmatched={error_unmatched})",
@@ -556,15 +572,15 @@
             error_no_objects (bool, optional): Throw a fit if no objects exist in folder
             recursive (bool, optional): search all objects under folder
             all_objects (bool, optional): search all objects in the entire system
             full_objects (bool, optional): return objects with their full data
             echo (bool, optional): echo output to console
             refresh (Refreshables, optional): refresh the folders before searching
         """
-        matches: t.List[BaseModel] = self.search_objects(
+        matches: t.List[object] = self.search_objects(
             searches=searches,
             pattern_prefix=pattern_prefix,
             ignore_case=ignore_case,
             error_unmatched=error_unmatched,
             error_no_matches=error_no_matches,
             error_no_objects=error_no_objects,
             recursive=recursive,
@@ -1343,15 +1359,15 @@
                 self._created_by_user: SystemUser = self.client.system_users.get_cached_single(
                     value=self.created_by
                 )
         return self._created_by_user
 
     @property
     def created_by_user_source(self) -> t.Optional[str]:
-        """Get the user name and user source attributes for self.created_by."""
+        """Get the username and user source attributes for self.created_by."""
         if isinstance(self.created_by_user, SystemUser):
             return self.created_by_user.user_source
         return None
 
     def get_tree_entry(
         self,
         maximum_depth: t.Optional[int] = None,
@@ -1419,15 +1435,15 @@
     def _refetch_root_folders(self) -> "FoldersModel":
         """Refetch the folders data from the API."""
         data: FoldersModel = self.api_folders._get()
         data.refreshed = True
         return data
 
     def _refresh(self, root: t.Optional["FoldersModel"] = None) -> "Folder":
-        """Refresh the root folders data."""
+        """Refresh the root folder data."""
         self._clear_objects_cache()
         if self.is_model_folders(self):
             root: FoldersModel = (
                 root if self.is_model_folders(root) else self._refetch_root_folders()
             )
             # update root folders references in old root folders
             self.__dict__.update(root.__dict__)
@@ -1701,15 +1717,15 @@
 
     _id: str
     depth: int
     name: str
     root_folders: "FoldersModel" = get_field_dc_mm(mm_field=FoldersSchema)
     root_type: t.Optional[str] = None
     predefined: t.Optional[bool] = False
-    path: t.Union[t.List[str], str] = dataclasses.field(default_factory=[])
+    path: t.Union[t.List[str], str] = dataclasses.field(default_factory=list)
     created_at: t.Optional[datetime.datetime] = get_field_dc_mm(
         mm_field=SchemaDatetime(), default=None
     )
     updated_at: t.Optional[datetime.datetime] = get_field_dc_mm(
         mm_field=SchemaDatetime(), default=None
     )
     children_ids: t.List[str] = dataclasses.field(default_factory=list)
```

## axonius_api_client/api/json_api/folders/queries.py

```diff
@@ -16,50 +16,53 @@
 
     public: str = "Shared Queries"
     private: str = "My Private Queries"
     asset_scope: str = "Asset Scope Queries"
     predefined: str = "Predefined Queries"
     untitled: str = "untitled folder"
     archive: str = "Archive"
+    global_scope: str = "Global"
 
 
 class FolderPaths(BaseEnum):
     """Paths of built-in folders used in Axonius."""
 
     public: str = base.Folder.join(FolderNames.public)
     private: str = base.Folder.join(FolderNames.private)
     asset_scope: str = base.Folder.join(FolderNames.asset_scope)
     predefined: str = base.Folder.join(FolderNames.public, FolderNames.predefined)
     archive: str = base.Folder.join(FolderNames.archive)
+    global_scope: str = base.Folder.join(FolderNames.global_scope)
 
 
 class RootTypes(BaseEnum):
     """Types of root folders used in Axonius."""
 
     public: str = "PUBLIC"
     private: str = "PRIVATE"
     asset_scope: str = "ASSET_SCOPE"
     archive: str = "ARCHIVE"
+    current_scope: str = "CURRENT_SCOPE"
 
 
 class Folder(base.Folder):
     """Mixins for folders for queries."""
 
     @classmethod
-    def get_enum_names(cls) -> BaseEnum:
+    def get_enum_names(cls) -> t.Type[BaseEnum]:
         """Pass."""
         return FolderNames
 
     @classmethod
-    def get_enum_paths(cls) -> BaseEnum:
+    def get_enum_paths(cls) -> t.Type[BaseEnum]:
         """Pass."""
         return FolderPaths
 
     @classmethod
-    def get_root_types(cls) -> BaseEnum:
+    def get_root_types(cls) -> t.Type[BaseEnum]:
         """Pass."""
         return RootTypes
 
     @classmethod
     def get_model_folder(cls) -> t.Type[base.FolderModel]:
         """Pass."""
         return FolderModel
@@ -108,29 +111,45 @@
         """Pass."""
         kwargs.setdefault("as_dataclass", True)
         query_type: str = QueryTypes.get_value_by_value(query_type)
         apiobj: object = getattr(self.client, query_type)
         created: SavedQuery = apiobj.saved_query.add(**kwargs)
         return created
 
-    def get_default_folder(self, asset_scope: bool = False, **kwargs) -> "FolderModel":
+    def get_default_folder(
+        self, asset_scope: bool = False, private: bool = False, **kwargs
+    ) -> "FolderModel":
         """Determine folder to use."""
         if asset_scope is True:
             return self.path_asset_scope
-        return super().get_default_folder(**kwargs)
+        if private is True:
+            return self.path_private
+
+        return self.path_public
+
+    @property
+    def path_public(self) -> "FolderModel":
+        """Get the root of the public folders."""
+        # hack to use /Global instead of /Shared Queries when data_scopes feature is enabled
+        try:
+            if self.HTTP.CLIENT.data_scopes.is_feature_enabled:
+                return self.path_global_scope
+        except Exception:
+            pass
+        return super().path_public
 
     def _check_resolved_folder(
         self,
         reason: str,
         fallback: t.Optional["Folder"] = None,
         asset_scope: bool = False,
         **kwargs,
     ) -> "Folder":
         """Check if resolved folder meets object type specific restrictions."""
-        if asset_scope is True and self.path.root_type != str(RootTypes.asset_scope):
+        if asset_scope is True and self.root_type != str(RootTypes.asset_scope):
             msgs: t.List[str] = [
                 f"Supplied path is not an asset scope folder but asset_scope={asset_scope}",
                 f"Supplied path: {self!r}",
                 f"Fallback path: {fallback!r}",
             ]
             if self.is_model_folder(fallback):
                 self.spew(msgs, echo=True)
@@ -150,14 +169,19 @@
         return self.find(folder=self.get_enum_paths().predefined)
 
     @property
     def path_asset_scope(self) -> "FolderModel":
         """Get the root of the asset scope folders."""
         return self.find(folder=self.get_enum_paths().asset_scope)
 
+    @property
+    def path_global_scope(self) -> "FolderModel":
+        """Get the root of the global scope folders."""
+        return self.find(folder=self.get_enum_paths().global_scope)
+
 
 class CreateFolderRequestSchema(BaseSchemaJson, base.CreateFolderRequestSchema):
     """Marshmallow schema for request to create a folder."""
 
     @staticmethod
     def get_model_cls() -> t.Any:
         """Pass."""
```

## axonius_api_client/api/json_api/tasks/__init__.py

```diff
@@ -1,16 +1,17 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import typing as t
+
 from .get_tasks import GetTasks, GetTasksSchema
 from .result import Result
 from .task import Task
 from .task_basic import TaskBasic, TaskBasicSchema
+from .task_filters import TaskFilters, TaskFiltersSchema
 from .task_full import TaskFull, TaskFullSchema
-from .task_filters import TaskFiltersSchema, TaskFilters
 
 TASK_TYPES: tuple = (TaskBasic, TaskFull, Task)
 # noinspection PyTypeHints
 TaskTypes = t.TypeVar("TaskTypes", *TASK_TYPES)
 
 __all__ = (
     "Result",
```

## axonius_api_client/api/json_api/tasks/get_tasks.py

```diff
@@ -1,39 +1,44 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
-import typing as t
 import datetime
+import typing as t
 
 import marshmallow
+import marshmallow_jsonapi.fields as mm_fields
 
 from ..base import BaseModel, BaseSchemaJson
+from ..count_operator import OperatorTypes, TypeOperator, coerce_operator
 from ..custom_fields import SchemaDatetime, field_from_mm
 from ..duration_operator import DurationOperator, DurationOperatorSchema
 from ..resources import PaginationRequest, PaginationSchema
+from .task_filters import TaskFilters
+from ....constants.ctypes import TypeDate, TypeDelta, TypeFloat, TypeMatch, TypeInt
+from ....tools import coerce_date_delta, coerce_seconds, coerce_bool, dt_parse
 
 
 class GetTasksSchema(BaseSchemaJson):
     """Schema for getting enforcement tasks in basic model."""
 
-    search = marshmallow.fields.Str(
+    search = mm_fields.Str(
         data_key="search",
         description="A textual value to search for",
         allow_none=True,
         load_default=None,
         dump_default=None,
     )
-    filter = marshmallow.fields.Str(
+    filter = mm_fields.Str(
         data_key="filter",
-        description="AQL string, representing data filter",
+        description="AQL string data filter",
         allow_none=True,
         load_default=None,
         dump_default=None,
     )
-    sort = marshmallow.fields.Str(
+    sort = mm_fields.Str(
         data_key="sort",
         description="Field name to sort by with direction",
         allow_none=True,
         load_default=None,
         dump_default=None,
     )
     history = SchemaDatetime(
@@ -53,77 +58,77 @@
     date_to = SchemaDatetime(
         data_key="date_to",
         description="The timestamp to filter to",
         allow_none=True,
         load_default=None,
         dump_default=None,
     )
-    task_id = marshmallow.fields.Integer(
+    task_id = mm_fields.Integer(
         data_key="task_id",
         description="A specific task pretty id to filter by",
         allow_none=True,
         load_default=None,
         dump_default=None,
     )
-    statuses_filter = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    statuses_filter = mm_fields.List(
+        mm_fields.Str(),
         data_key="statuses_filter",
         description="List of task status to filter by",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    action_names = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    action_names = mm_fields.List(
+        mm_fields.Str(),
         data_key="action_names",
         description="List of task action names to filter by",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    enforcement_ids = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    enforcement_ids = mm_fields.List(
+        mm_fields.Str(),
         data_key="enforcement_ids",
         description="List of task enforcement ids to filter by",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    aggregated_status = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    aggregated_status = mm_fields.List(
+        mm_fields.Str(),
         data_key="aggregated_status",
         description="List of task results to filter by",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    discovery_cycle = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    discovery_cycle = mm_fields.List(
+        mm_fields.Str(),
         data_key="discovery_cycle",
         description="List of  discovery ids to filter by",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    is_refresh = marshmallow.fields.Bool(
+    is_refresh = mm_fields.Bool(
         data_key="is_refresh",
         description='Whether this request is made for "refresh" and should not be logged',
         allow_none=True,
         load_default=None,
         dump_default=None,
     )
-    duration_filter = marshmallow.fields.Nested(
+    duration_filter = mm_fields.Nested(
         DurationOperatorSchema(),
         data_key="duration_filter",
         description="Duration filter",
         allow_none=True,
         load_default=DurationOperator,
         dump_default=DurationOperator,
     )
-    page = marshmallow.fields.Nested(
+    page = mm_fields.Nested(
         PaginationSchema(),
         data_key="page",
         description="Pagination request",
         allow_none=True,
         load_default=PaginationRequest,
         dump_default=PaginationRequest,
     )
@@ -148,62 +153,233 @@
     """Model for getting enforcement tasks in basic model."""
 
     search: t.Optional[str] = field_from_mm(SCHEMA, "search")
     filter: t.Optional[str] = field_from_mm(SCHEMA, "filter")
     sort: t.Optional[str] = field_from_mm(SCHEMA, "sort")
     history: t.Optional[datetime.datetime] = field_from_mm(SCHEMA, "history")
 
+    task_id: t.Optional[str] = field_from_mm(SCHEMA, "task_id")
+
     date_from: t.Optional[datetime.datetime] = field_from_mm(SCHEMA, "date_from")
     date_to: t.Optional[datetime.datetime] = field_from_mm(SCHEMA, "date_to")
-    task_id: t.Optional[str] = field_from_mm(SCHEMA, "task_id")
+
     statuses_filter: t.Optional[t.List[str]] = field_from_mm(SCHEMA, "statuses_filter")
     action_names: t.Optional[t.List[str]] = field_from_mm(SCHEMA, "action_names")
     enforcement_ids: t.Optional[t.List[str]] = field_from_mm(SCHEMA, "enforcement_ids")
     aggregated_status: t.Optional[t.List[str]] = field_from_mm(SCHEMA, "aggregated_status")
     discovery_cycle: t.Optional[t.List[str]] = field_from_mm(SCHEMA, "discovery_cycle")
+
     is_refresh: t.Optional[bool] = field_from_mm(SCHEMA, "is_refresh")
+
     duration_filter: t.Optional[DurationOperator] = field_from_mm(SCHEMA, "duration_filter")
     page: t.Optional[PaginationRequest] = field_from_mm(SCHEMA, "page")
 
     SCHEMA: t.ClassVar[marshmallow.Schema] = SCHEMA
+    TASK_FILTERS: t.ClassVar[t.Optional[TaskFilters]] = None
 
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Pass."""
         return GetTasksSchema
 
-    def __post_init__(self):
-        """Post init."""
-        if not isinstance(self.search, str):
-            self.search = None
-        if not isinstance(self.filter, str):
-            self.filter = None
-        if not isinstance(self.sort, str):
-            self.sort = None
-
-        if not isinstance(self.history, datetime.datetime):
-            self.history = None
-        if not isinstance(self.date_from, datetime.datetime):
-            self.date_from = None
-        if not isinstance(self.date_to, datetime.datetime):
-            self.date_to = None
-
-        if not isinstance(self.task_id, str):
-            self.task_id = None
-        if not isinstance(self.is_refresh, bool):
-            self.is_refresh = None
-
-        if not isinstance(self.statuses_filter, list):
-            self.statuses_filter = []
-        if not isinstance(self.action_names, list):
-            self.action_names = []
-        if not isinstance(self.enforcement_ids, list):
-            self.enforcement_ids = []
-        if not isinstance(self.aggregated_status, list):
-            self.aggregated_status = []
-        if not isinstance(self.discovery_cycle, list):
-            self.discovery_cycle = []
-
-        if not isinstance(self.page, PaginationRequest):
-            self.page = PaginationRequest()
-        if not isinstance(self.duration_filter, DurationOperator):
-            self.duration_filter = DurationOperator()
+    def get_filters(self, task_filters: t.Optional[TaskFilters] = None) -> TaskFilters:
+        """Pass."""
+        if isinstance(task_filters, TaskFilters):
+            return task_filters
+        if not isinstance(self.TASK_FILTERS, TaskFilters):
+            # noinspection PyUnresolvedReferences
+            self.TASK_FILTERS = self.HTTP.CLIENT.enforcements.tasks.get_filters()
+        return self.TASK_FILTERS
+
+    def set_task_id(
+        self, value: t.Optional[TypeInt] = None, task_filters: t.Optional[TaskFilters] = None
+    ) -> t.Optional[int]:
+        """Set the task id to filter results by.
+
+        Args:
+            value: The task id to filter by.
+            task_filters: The task filters to use to validate the task id.
+        """
+        task_filters = self.get_filters(task_filters=task_filters)
+        self.task_id = task_filters.check_task_id(value=value)
+        return self.task_id
+
+    def set_action_types(
+        self,
+        values: t.Optional[TypeMatch] = None,
+        task_filters: t.Optional[TaskFilters] = None,
+        **kwargs,
+    ) -> t.List[str]:
+        """Set the action types to filter results by.
+
+        Args:
+            values: The action types to filter by.
+            task_filters: The task filters to use to validate the task id.
+            **kwargs: Passed to :meth:`TaskFilters.check_action_types`
+
+        Returns:
+            The parsed list of action types that were set to `action_names`
+        """
+        task_filters = self.get_filters(task_filters=task_filters)
+        self.action_names = task_filters.check_action_types(values=values, **kwargs)
+        return self.action_names
+
+    def set_discovery_uuids(
+        self,
+        values: t.Optional[TypeMatch] = None,
+        task_filters: t.Optional[TaskFilters] = None,
+        **kwargs,
+    ) -> t.List[str]:
+        """Set the discovery uuids to filter results by.
+
+        Args:
+            values: The discovery uuids to filter by (use re_prefix to treat as regex)
+            task_filters: The task filters to use to validate the task id.
+            **kwargs: Passed to :meth:`TaskFilters.check_discovery_uuids`
+
+        Returns:
+            The parsed list of discovery uuids that were set to `discovery_cycle`
+        """
+        task_filters = self.get_filters(task_filters=task_filters)
+        self.discovery_cycle = task_filters.check_discovery_uuids(values=values, **kwargs)
+        return self.discovery_cycle
+
+    def set_enforcement_names(
+        self,
+        values: t.Optional[TypeMatch] = None,
+        task_filters: t.Optional[TaskFilters] = None,
+        **kwargs,
+    ) -> t.List[str]:
+        """Set the task UUIDs to filter results by enforcement names.
+
+        Args:
+            values: The enforcement names to filter by.
+            task_filters: The task filters to use to validate the task id.
+            **kwargs: Passed to :meth:`TaskFilters.check_enforcement_names`
+
+        Returns:
+            The parsed list of task UUIDs that were set to `enforcement_ids`
+        """
+        task_filters = self.get_filters(task_filters=task_filters)
+        self.enforcement_ids = task_filters.check_enforcement_names(values=values, **kwargs)
+        return self.enforcement_ids
+
+    def set_statuses(
+        self,
+        values: t.Optional[TypeMatch] = None,
+        task_filters: t.Optional[TaskFilters] = None,
+        **kwargs,
+    ) -> t.List[str]:
+        """Set the task statuses to filter results by.
+
+        Args:
+            values: The statuses to filter by.
+            task_filters: The task filters to use to validate the task id.
+            **kwargs: Passed to :meth:`TaskFilters.check_statuses`
+
+        Returns:
+            The parsed list of statuses that were set to `statuses_filter`
+        """
+        task_filters = self.get_filters(task_filters=task_filters)
+        self.statuses_filter = task_filters.check_statuses(values=values, **kwargs)
+        return self.statuses_filter
+
+    def set_statuses_result(
+        self,
+        values: t.Optional[TypeMatch] = None,
+        task_filters: t.Optional[TaskFilters] = None,
+        **kwargs,
+    ) -> t.List[str]:
+        """Set the task result statuses to filter results by.
+
+        Args:
+            values: The results statuses to filter by.
+            task_filters: The task filters to use to validate the task id.
+            **kwargs: Passed to :meth:`TaskFilters.check_statuses`
+
+        Returns:
+            The parsed list of results statuses that were set to `aggregated_status`
+        """
+        task_filters = self.get_filters(task_filters=task_filters)
+        self.aggregated_status = task_filters.check_statuses(values=values, **kwargs)
+        return self.aggregated_status
+
+    def set_is_refresh(self, value: t.Optional[bool] = None) -> t.Optional[bool]:
+        """Set the is_refresh to filter results by.
+
+        Args:
+            value: The is_refresh to filter by.
+
+        Returns:
+            The parsed value that was set to `is_refresh`
+        """
+        self.is_refresh = coerce_bool(value, allow_none=True)
+        return self.is_refresh
+
+    def set_date_from(
+        self,
+        value: t.Optional[TypeDate] = None,
+        add: t.Optional[TypeDelta] = None,
+        subtract: t.Optional[TypeDelta] = None,
+    ) -> t.Optional[datetime.datetime]:
+        """Set the date_from to filter results by.
+
+        Args:
+            value: The date_from to filter by.
+            add: Seconds to add to parsed value (if no value, add to now).
+            subtract: Seconds to subtract from parsed value (if no value, subtract from now).
+
+        Returns:
+            The parsed value that was set to `date_from`
+        """
+        self.date_from = coerce_date_delta(value=value, add=add, subtract=subtract)
+        return self.date_from
+
+    def set_date_to(
+        self,
+        value: t.Optional[TypeDate] = None,
+        add: t.Optional[TypeDelta] = None,
+        subtract: t.Optional[TypeDelta] = None,
+    ) -> t.Optional[datetime.datetime]:
+        """Set the date_to to filter results by.
+
+        Args:
+            value: The date_to to filter by.
+            add: Seconds to add to parsed value (if no value, add to now).
+            subtract: Seconds to subtract from parsed value (if no value, subtract from now).
+
+        Returns:
+            The parsed value that was set to `date_to`
+        """
+        self.date_to = coerce_date_delta(value=value, add=add, subtract=subtract)
+        return self.date_to
+
+    def set_history(self, value: t.Optional[TypeDate] = None) -> t.Optional[datetime.datetime]:
+        """Set the history to filter results by.
+
+        Args:
+            value: The history to filter by.
+
+        Returns:
+            The parsed value that was set to `history`
+        """
+        self.history = dt_parse(value, allow_none=True)
+        return self.history
+
+    def set_duration(
+        self,
+        seconds: t.Optional[TypeFloat] = None,
+        operator: t.Union[TypeOperator] = OperatorTypes.less.name,
+    ) -> DurationOperator:
+        """Only get tasks that have a duration that matches the operator and seconds.
+
+        Args:
+            seconds: The duration to filter by.
+            operator: The operator to use to compare the duration.
+
+        Returns:
+            The parsed value that was set to `duration_filter`
+        """
+        seconds_float = coerce_seconds(seconds)
+        operator = coerce_operator(operator)
+        self.duration_filter = DurationOperator(type=operator, seconds_float=seconds_float)
+        return self.duration_filter
```

## axonius_api_client/api/json_api/tasks/result.py

```diff
@@ -2,67 +2,68 @@
 """Models for API requests & responses."""
 import dataclasses
 import datetime
 import types
 import typing as t
 
 import marshmallow
+import marshmallow.fields as mm_fields
 
-from ..custom_fields import SchemaDatetime, SchemaBool, field_from_mm
-from ..base2 import BaseModel, BaseSchema
 from ....constants import enforcements as enums
+from ..base2 import BaseModel, BaseSchema
+from ..custom_fields import SchemaBool, SchemaDatetime, field_from_mm
 
 
 class ResultSchema(BaseSchema):
     """Schema of result for an action in a flow_type of a task for an enforcement."""
 
-    flow_type = marshmallow.fields.Str(description="The type of flow of the result", required=True)
-    flow_position = marshmallow.fields.Int(
+    flow_type = mm_fields.Str(description="The type of flow of the result", required=True)
+    flow_position = mm_fields.Int(
         description="The position of the result in the flow_type", required=True
     )
-    flow_count = marshmallow.fields.Int(
+    flow_count = mm_fields.Int(
         description="The total number of results in the flow_type", required=True
     )
 
-    name = marshmallow.fields.Str(description="The name of the action", required=True)
-    uuid = marshmallow.fields.Str(description="The uuid of the action", required=True)
-    type = marshmallow.fields.Str(description="The type of the action", required=True)
-    category = marshmallow.fields.Str(
+    name = mm_fields.Str(description="The name of the action", required=True)
+    uuid = mm_fields.Str(description="The uuid of the action", required=True)
+    type = mm_fields.Str(description="The type of the action", required=True)
+    category = mm_fields.Str(
         description="The category of the action", allow_none=True, load_default=None
     )
 
-    config = marshmallow.fields.Dict(
+    config = mm_fields.Dict(
         description="The config of the action", allow_none=True, load_default=dict
     )
-    ifttt = marshmallow.fields.Str(
+    ifttt = mm_fields.Str(
         description="The if-this-then-that content for the action",
         allow_none=True,
         load_default=None,
     )
     is_ifttt_enabled = SchemaBool(
         description="Whether if-this-then-that is enabled", allow_none=True, load_default=None
     )
 
     started_at = SchemaDatetime(
         description="The time the action started", allow_none=True, load_default=None
     )
     stopped_at = SchemaDatetime(
         description="The time the action stopped", allow_none=True, load_default=None
     )
-    duration_seconds = marshmallow.fields.Float(
+    duration_seconds = mm_fields.Float(
         description="The duration of the action in seconds", allow_none=True, load_default=None
     )
 
-    total_count = marshmallow.fields.Int(
+    total_count = mm_fields.Int(
         description="The total count of the action", allow_none=True, load_default=None
     )
-    failure_count = marshmallow.fields.Int(
+    failure_count = mm_fields.Int(
         description="The failure count of the action", allow_none=True, load_default=None
     )
-    success_count = marshmallow.fields.Int(
+    success_count = mm_fields.Int(
         description="The success count of the action", allow_none=True, load_default=None
     )
 
     is_started = SchemaBool(
         data_key="is_started",
         description="Whether the action is started",
         allow_none=True,
@@ -101,18 +102,18 @@
     is_pending = SchemaBool(
         data_key="is_pending",
         description="Whether the action is pending",
         allow_none=True,
         load_default=None,
     )
 
-    message = marshmallow.fields.Str(
+    message = mm_fields.Str(
         description="The message of the action", allow_none=True, load_default=None
     )
-    status = marshmallow.fields.Str(
+    status = mm_fields.Str(
         description="The status of the action", allow_none=True, load_default=None
     )
 
     class Meta:
         """Meta."""
 
         type_ = "PROTO_TASK_RESULT"
```

## axonius_api_client/api/json_api/tasks/task.py

```diff
@@ -2,126 +2,122 @@
 """Models for API requests & responses."""
 import dataclasses
 import datetime
 import types
 import typing as t
 
 import marshmallow
+import marshmallow.fields as mm_fields
 
+
+from ....constants import enforcements as enums
 from ....http import Http
 from ....tools import get_diff_seconds, listify
-from ..custom_fields import SchemaBool, SchemaDatetime, field_from_mm, SchemaObjectIDDatetime
 from ..base2 import BaseModel, BaseSchema
+from ..custom_fields import SchemaBool, SchemaDatetime, SchemaObjectIDDatetime, field_from_mm
 from .result import Result, ResultSchema
 from .task_basic import TaskBasic
 from .task_full import TaskFull
-from ....constants import enforcements as enums
 
 
 class TaskSchema(BaseSchema):
     """Schema for human friendly version of tasks for enforcements."""
 
-    id = marshmallow.fields.Int(
-        data_key="id", description="The id of the task in the UI", required=True
-    )
-    uuid = marshmallow.fields.Str(
-        data_key="uuid", description="The uuid of the task", required=True
-    )
-    name = marshmallow.fields.Str(
-        data_key="name", description="The name of the task", required=True
-    )
-    enforcement_name = marshmallow.fields.Str(
+    id = mm_fields.Int(data_key="id", description="The id of the task in the UI", required=True)
+    uuid = mm_fields.Str(data_key="uuid", description="The uuid of the task", required=True)
+    name = mm_fields.Str(data_key="name", description="The name of the task", required=True)
+    enforcement_name = mm_fields.Str(
         data_key="enforcement_name", description="The name of the enforcement", required=True
     )
-    enforcement_uuid = marshmallow.fields.Str(
+    enforcement_uuid = mm_fields.Str(
         data_key="enforcement_uuid", description="The uuid of the enforcement", required=True
     )
 
-    query_type = marshmallow.fields.Str(
+    query_type = mm_fields.Str(
         data_key="query_type",
         description="The type of the query",
         allow_none=True,
         load_default=None,
     )
-    query_name = marshmallow.fields.Str(
+    query_name = mm_fields.Str(
         data_key="query_name",
         description="The name of the query",
         allow_none=True,
         load_default=None,
     )
-    query_uuid = marshmallow.fields.Str(
+    query_uuid = mm_fields.Str(
         data_key="query_uuid",
         description="The uuid of the query",
         allow_none=True,
         load_default=None,
     )
 
-    data_scope_uuid = marshmallow.fields.Str(
+    data_scope_uuid = mm_fields.Str(
         data_key="data_scope_uuid",
         description="The uuid of the data scope of the query, if any",
         allow_none=True,
         load_default=None,
     )
-    discover_uuid = marshmallow.fields.Str(
+    discover_uuid = mm_fields.Str(
         data_key="discover_uuid",
         description="The uuid of the discover cycle that created this task, if any",
         allow_none=True,
         load_default=None,
     )
 
-    schedule_method = marshmallow.fields.Str(
+    schedule_method = mm_fields.Str(
         data_key="schedule_method",
         description="Method of scheduling",
         allow_none=True,
         load_default=None,
     )
-    schedule_conditions = marshmallow.fields.Str(
+    schedule_conditions = mm_fields.Str(
         data_key="schedule_conditions",
         description="The conditions of the schedule",
         allow_none=True,
         load_default=None,
     )
-    schedule_period = marshmallow.fields.Str(
+    schedule_period = mm_fields.Str(
         data_key="schedule_period",
         description="The period of the schedule",
         allow_none=True,
         load_default=None,
     )
-    schedule_recurrence = marshmallow.fields.Int(
+    schedule_recurrence = mm_fields.Int(
         data_key="schedule_recurrence",
         description="The recurrence of the schedule",
         allow_none=True,
         load_default=None,
     )
-    schedule_time = marshmallow.fields.Str(
+    schedule_time = mm_fields.Str(
         data_key="schedule_time",
         description="The time of the schedule",
         allow_none=True,
         load_default=None,
     )
 
     only_against_new = SchemaBool(
         data_key="only_against_new",
         description="Only run against new assets",
         allow_none=True,
         load_default=None,
     )
-    only_against_first_count = marshmallow.fields.Int(
+    only_against_first_count = mm_fields.Int(
         data_key="only_against_first_count",
         description="Only run against the first N assets",
         allow_none=True,
         load_default=None,
     )
-    only_when_above_count = marshmallow.fields.Int(
+    only_when_above_count = mm_fields.Int(
         data_key="only_when_above_count",
         description="Only run when the asset count is above N",
         allow_none=True,
         load_default=None,
     )
-    only_when_below_count = marshmallow.fields.Int(
+    only_when_below_count = mm_fields.Int(
         data_key="only_when_below_count",
         description="Only run when the asset count is below N",
         allow_none=True,
         load_default=None,
     )
     only_when_count_decreases = SchemaBool(
         data_key="only_when_count_decreases",
@@ -132,15 +128,15 @@
     only_when_count_increases = SchemaBool(
         data_key="only_when_count_increases",
         description="Only run when the asset count increases",
         allow_none=True,
         load_default=None,
     )
 
-    previous_count = marshmallow.fields.Int(
+    previous_count = mm_fields.Int(
         data_key="previous_count",
         description="The number of times task has been run",
         allow_none=True,
         load_default=None,
     )
     previous_at = SchemaDatetime(
         data_key="previous_at",
@@ -162,34 +158,34 @@
     )
     stopped_at = SchemaDatetime(
         data_key="stopped_at",
         description="The datetime of the task stop",
         allow_none=True,
         load_default=None,
     )
-    duration_seconds = marshmallow.fields.Float(
+    duration_seconds = mm_fields.Float(
         data_key="duration_seconds",
         description="The duration of the task in seconds",
         allow_none=True,
         load_default=None,
     )
 
-    total_count = marshmallow.fields.Int(
+    total_count = mm_fields.Int(
         data_key="total_count",
         description="The total number of assets",
         allow_none=True,
         load_default=None,
     )
-    success_count = marshmallow.fields.Int(
+    success_count = mm_fields.Int(
         data_key="success_count",
         description="The number of assets that had no errors",
         allow_none=True,
         load_default=None,
     )
-    failure_count = marshmallow.fields.Int(
+    failure_count = mm_fields.Int(
         data_key="failure_count",
         description="The number of assets that had errors",
         allow_none=True,
         load_default=None,
     )
 
     is_started = SchemaBool(
@@ -237,29 +233,29 @@
     is_scheduled = SchemaBool(
         data_key="is_scheduled",
         description="Whether the task is a schedule",
         allow_none=True,
         load_default=None,
     )
 
-    status = marshmallow.fields.Str(
+    status = mm_fields.Str(
         data_key="status",
         description="The status of the task",
         allow_none=True,
         load_default=None,
     )
-    status_results = marshmallow.fields.Str(
+    status_results = mm_fields.Str(
         data_key="status_results",
         description="The aggregate status of the results for the task",
         allow_none=True,
         load_default=None,
     )
 
-    results = marshmallow.fields.List(
-        marshmallow.fields.Nested(ResultSchema()),
+    results = mm_fields.List(
+        mm_fields.Nested(ResultSchema()),
         data_key="results",
         description="The results of the task",
         allow_none=True,
         load_default=list,
     )
 
     @marshmallow.post_dump()
@@ -342,14 +338,41 @@
     status_results: t.Optional[str] = field_from_mm(SCHEMA, "status_results")
 
     results: t.List[Result] = field_from_mm(SCHEMA, "results")
 
     SCHEMA: t.ClassVar[marshmallow.Schema] = SCHEMA
     ENUMS: t.ClassVar[types.ModuleType] = enums
 
+    @property
+    def action_types(self) -> t.List[str]:
+        """Return all action types used by this task."""
+        return [result.type for result in self.results]
+
+    @property
+    def result_main(self) -> t.Optional[Result]:
+        """Return the result for the "main" workflow for this task."""
+        for result in self.results:
+            if result.flow_type == self.ENUMS.FlowTypes.main.value:
+                return result
+
+    @property
+    def results_success(self) -> t.List[Result]:
+        """Return the results for the "success" workflow for this task."""
+        return [x for x in self.results if x.flow_type == self.ENUMS.FlowTypes.success.value]
+
+    @property
+    def results_failure(self) -> t.List[Result]:
+        """Return the results for the "failure" workflow for this task."""
+        return [x for x in self.results if x.flow_type == self.ENUMS.FlowTypes.failure.value]
+
+    @property
+    def results_post(self) -> t.List[Result]:
+        """Return the results for the "post" workflow for this task."""
+        return [x for x in self.results if x.flow_type == self.ENUMS.FlowTypes.post.value]
+
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Get schema class for this model."""
         return TaskSchema
 
     @classmethod
     def load(
@@ -517,15 +540,15 @@
             "flow_count": flow_count,
             "name": basic.get("action_name"),
             "uuid": _action.get("action_id"),
             "type": basic.get("name"),
             "category": _action.get("action_type"),
             "config": _action.get("config") or {},
             "ifttt": _ifttt.get("content"),
-            "is_ifttt_enabled": _ifttt.get("enable"),
+            "is_ifttt_enabled": _ifttt.get("include_output"),
             "started_at": started_at,
             "stopped_at": stopped_at,
             "duration_seconds": duration_seconds,
             "total_count": basic.get("total_affected"),
             "success_count": basic.get("successful_count"),
             "failure_count": failure_count,
             "message": basic.get("status_details"),
@@ -535,7 +558,30 @@
             "is_success": is_success,
             "is_error": is_error,
             "is_terminated": is_terminated,
             "is_pending": is_pending,
             "status": status,
         }
         return result
+
+    @staticmethod
+    def _str_properties() -> t.List[str]:
+        return [
+            "name",
+            "enforcement_name",
+            "query_name",
+            "query_type",
+            "schedule_method",
+            "schedule_conditions",
+            "started_at",
+            "stopped_at",
+            "duration_seconds",
+            "total_count",
+            "success_count",
+            "failure_count",
+            "status",
+            "status_results",
+            "action_types",
+        ]
+
+    def __repr__(self):
+        return self.__str__()
```

## axonius_api_client/api/json_api/tasks/task_basic.py

```diff
@@ -1,122 +1,123 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
+import datetime
 import typing as t
-from datetime import datetime
 
 import marshmallow
+import marshmallow_jsonapi.fields as mm_fields
 
 from ..base import BaseModel, BaseSchemaJson
 from ..custom_fields import SchemaDatetime, field_from_mm
 from .task_full import TaskFull
 
 
 class TaskBasicSchema(BaseSchemaJson):
     """Schema for enforcement task in basic model."""
 
-    id = marshmallow.fields.Str(data_key="id", description="The task id", required=True)
-    uuid = marshmallow.fields.Str(data_key="uuid", description="The task id", required=True)
-    pretty_id = marshmallow.fields.Str(
+    id = mm_fields.Str(data_key="id", description="The task id", required=True)
+    uuid = mm_fields.Str(data_key="uuid", description="The task id", required=True)
+    pretty_id = mm_fields.Str(
         data_key="pretty_id", description="The ID of the task in the UI", required=True
     )
-    date_fetched = marshmallow.fields.Str(
+    date_fetched = mm_fields.Str(
         data_key="date_fetched", description="The date when the task was created", required=True
     )
-    enforcement_name = marshmallow.fields.Str(
+    enforcement_name = mm_fields.Str(
         data_key="enforcement_name",
         description="The enforcement set name",
         required=True,
     )
-    result_main_action_action_name = marshmallow.fields.Str(
+    result_main_action_action_name = mm_fields.Str(
         data_key="result_main_action_action_name",
         description="The action type",
         required=True,
     )
-    result_metadata_task_name = marshmallow.fields.Str(
+    result_metadata_task_name = mm_fields.Str(
         data_key="result_metadata_task_name",
         description="The action name",
         required=True,
     )
-    result_main_name = marshmallow.fields.Str(
+    result_main_name = mm_fields.Str(
         data_key="result_main_name",
         description="The task name",
         required=True,
     )
 
-    affected_assets = marshmallow.fields.Int(
+    affected_assets = mm_fields.Int(
         data_key="affected_assets",
         description="Total amount of affected assets by task",
         allow_none=True,
         load_default=None,
     )
-    success_count = marshmallow.fields.Int(
+    success_count = mm_fields.Int(
         data_key="success_count",
         description="Amount of assets succeeded in the main action",
         allow_none=True,
         load_default=None,
     )
-    failure_count = marshmallow.fields.Int(
+    failure_count = mm_fields.Int(
         data_key="failure_count",
         description="Amount of assets failed in the main " "action",
         allow_none=True,
         load_default=None,
     )
-    result_metadata_successful_total = marshmallow.fields.Str(
+    result_metadata_successful_total = mm_fields.Str(
         data_key="result_metadata_successful_total",
         description="The numbers of successful tasks / all tasks",
         allow_none=True,
         load_default=None,
     )
 
-    module = marshmallow.fields.Str(
+    module = mm_fields.Str(
         data_key="module",
         description="The asset type of the query used",
         allow_none=True,
         load_default=None,
     )
-    result_metadata_trigger_view_name = marshmallow.fields.Str(
+    result_metadata_trigger_view_name = mm_fields.Str(
         data_key="result_metadata_trigger_view_name",
         description="The name of the query defined as the enforcement set trigger",
         allow_none=True,
         load_default=None,
     )
 
-    aggregated_status = marshmallow.fields.Str(
+    aggregated_status = mm_fields.Str(
         data_key="aggregated_status",
         description="The aggregated status of task",
         allow_none=True,
         load_default=None,
     )
-    result_metadata_status = marshmallow.fields.Str(
+    result_metadata_status = mm_fields.Str(
         data_key="result_metadata_status",
         description="The status of the task",
         allow_none=True,
         load_default=None,
     )
 
-    scheduling = marshmallow.fields.Str(
+    scheduling = mm_fields.Str(
         data_key="scheduling",
         description="The task run scheduling",
         allow_none=True,
         load_default=None,
     )
-    duration = marshmallow.fields.Str(
+    duration = mm_fields.Str(
         data_key="duration",
         description="The task run duration, in HH:mm:ss.SS format",
         allow_none=True,
         load_default=None,
     )
-    result_metadata_trigger_condition = marshmallow.fields.Str(
+    result_metadata_trigger_condition = mm_fields.Str(
         data_key="result_metadata_trigger_condition",
         description="The condition which triggered the run",
         allow_none=True,
         load_default=None,
     )
-    discovery_id = marshmallow.fields.Str(
+    discovery_id = mm_fields.Str(
         data_key="discovery_id",
         description="The ID of the discovery cycle that originated the set run record",
         allow_none=True,
         load_default=None,
     )
     finished_at = SchemaDatetime(
         data_key="finished_at",
@@ -127,21 +128,21 @@
     started_at = SchemaDatetime(
         data_key="started_at",
         description="The timestamp when task started running",
         allow_none=True,
         load_default=None,
     )
 
-    action_names = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    action_names = mm_fields.List(
+        mm_fields.Str(),
         data_key="action_names",
         description="List of names for all actions in task.",
         load_default=list,
     )
-    actions_details = marshmallow.fields.Dict(
+    actions_details = mm_fields.Dict(
         data_key="actions_details",
         description="The details of all actions contained in the task.",
         load_default=dict,
     )
 
     class Meta:
         """Marshmallow JSONAPI metaclass."""
@@ -208,24 +209,25 @@
 
     scheduling: t.Optional[str] = field_from_mm(SCHEMA, "scheduling")
     duration: t.Optional[str] = field_from_mm(SCHEMA, "duration")
     result_metadata_trigger_condition: t.Optional[str] = field_from_mm(
         SCHEMA, "result_metadata_trigger_condition"
     )
     discovery_id: t.Optional[str] = field_from_mm(SCHEMA, "discovery_id")
-    finished_at: t.Optional[datetime] = field_from_mm(SCHEMA, "finished_at")
-    started_at: t.Optional[datetime] = field_from_mm(SCHEMA, "started_at")
+    finished_at: t.Optional[datetime.datetime] = field_from_mm(SCHEMA, "finished_at")
+    started_at: t.Optional[datetime.datetime] = field_from_mm(SCHEMA, "started_at")
 
     action_names: t.List[str] = field_from_mm(SCHEMA, "action_names")
     actions_details: dict = field_from_mm(SCHEMA, "actions_details")
 
     SCHEMA: t.ClassVar[marshmallow.Schema] = SCHEMA
 
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Pass."""
         return TaskBasicSchema
 
     def get_full(self) -> "TaskFull":
         """Pass."""
         # TODO: ensure cached!
+        # noinspection PyUnresolvedReferences
         return self.HTTP.CLIENT.enforcements.tasks.get_full(uuid=self.uuid)
```

## axonius_api_client/api/json_api/tasks/task_filters.py

```diff
@@ -1,57 +1,137 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
-import typing as t
 import dataclasses
+import typing as t
 
 import marshmallow
+import marshmallow_jsonapi.fields as mm_fields
+import click
 
+from axonius_api_client.constants.api import RE_PREFIX
+from axonius_api_client.constants.ctypes import PatternLike, TypeMatch
+from axonius_api_client.constants.general import SPLITTER
+from axonius_api_client.exceptions import NotFoundError
+from axonius_api_client.parsers.matcher import Matcher
+from axonius_api_client.tools import coerce_int, listify
 from ..base2 import BaseModel, BaseSchema
 from ..custom_fields import field_from_mm
 
 
+ATTR_MAP = {
+    "action_types": {
+        "TaskFilters": "action_names",
+        "Task": "results[].type",
+        "TaskBasic": "action_names",
+        "TaskFull": None,
+        "GetTasks": "action_names",
+        "enum": "enum_action_types",
+        "desc": "Action types in use by any Enforcement Set",
+    },
+    "discovery_uuids": {
+        "TaskFilters": "discovery_cycle_id",
+        "Task": "discover_uuid",
+        "TaskBasic": "discovery_id",
+        "TaskFull": None,
+        "GetTasks": "discovery_cycle",
+        "enum": "enum_discovery_uuids",
+        "desc": "UUIDs of Discovery Cycles that have launched a task",
+    },
+    "enforcement_names": {
+        "TaskFilters": "enforcement_name",
+        "Task": "uuid",
+        "TaskBasic": "uuid",
+        "TaskFull": "uuid",
+        "GetTasks": "enforcement_ids",
+        "enum": "enum_enforcement_names",
+        "desc": "Names of all Enforcement Sets that have launched a task",
+    },
+    "task_ids": {
+        "TaskFilters": "run",
+        "Task": "id",
+        "TaskBasic": "pretty_id",
+        "TaskFull": "pretty_id",
+        "GetTasks": "task_id",
+        "enum": "enum_task_ids",
+        "desc": "Pretty IDs of all tasks for all Enforcement Sets",
+    },
+    "statuses": {
+        "TaskFilters": "statuses",
+        "Task": "status",
+        "TaskBasic": "result_metadata_status",
+        "TaskFull": None,
+        "GetTasks": "statuses_filter",
+        "enum": "enum_statuses",
+        "desc": "Statuses of all tasks and task results for all Enforcement Sets",
+    },
+}
+
+
+def build_include_options() -> list:
+    """Pass."""
+    options: list = []
+    for arg, details in ATTR_MAP.items():
+        long_form = arg.replace("_", "-")
+        long_form_switch = f"--include-{long_form}/--no-include-{long_form}"
+        short_form = "".join([x[0] for x in arg.split("_")])
+        short_form_switch = f"-i{short_form}/-ni{short_form}"
+        desc = details["desc"]
+        option = click.option(
+            long_form_switch,
+            short_form_switch,
+            arg,
+            help=f"Include {desc} in output",
+            default=True,
+            show_envvar=True,
+            show_default=True,
+        )
+        options.append(option)
+
+    return options
+
+
 class TaskFiltersSchema(BaseSchema):
     """Schema for filters for getting enforcement tasks in basic model."""
 
-    action_names = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    action_names = mm_fields.List(
+        mm_fields.Str(),
         data_key="action_names",
-        description="List of action names to filter by",
+        description="List of all action types in use",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    discover_cycle_id = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    discovery_cycle_id = mm_fields.List(
+        mm_fields.Str(),
         data_key="discover_cycle_id",
-        description="List of discovery cycle IDs to filter by",
+        description="List of all discovery UUIds that launched a task",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    enforcement_name = marshmallow.fields.List(
-        marshmallow.fields.Dict(),
+    enforcement_name = mm_fields.List(
+        mm_fields.Dict(),
         data_key="enforcement_name",
-        description="List of enforcements to filter by",
+        description="List of dict with keys 'text' enforcement set name and 'value' task UUID",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    run = marshmallow.fields.List(
-        marshmallow.fields.Int(),
+    run = mm_fields.List(
+        mm_fields.Int(),
         data_key="run",
-        description="List of pretty IDs to filter by",
+        description="List of pretty IDs of all tasks",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
-    statuses = marshmallow.fields.List(
-        marshmallow.fields.Str(),
+    statuses = mm_fields.List(
+        mm_fields.Str(),
         data_key="statuses",
-        description="List of statuses to filter by",
+        description="List of statuses in use",
         allow_none=True,
         load_default=list,
         dump_default=list,
     )
 
     class Meta:
         """Meta."""
@@ -69,50 +149,213 @@
 
 
 @dataclasses.dataclass
 class TaskFilters(BaseModel):
     """Model for filters for getting enforcement tasks in basic model."""
 
     action_names: t.List[str] = field_from_mm(SCHEMA, "action_names")
-    discover_cycle_id: t.List[str] = field_from_mm(SCHEMA, "discover_cycle_id")
+    discovery_cycle_id: t.List[str] = field_from_mm(SCHEMA, "discovery_cycle_id")
     enforcement_name: t.List[dict] = field_from_mm(SCHEMA, "enforcement_name")
     run: t.List[int] = field_from_mm(SCHEMA, "run")
     statuses: t.List[str] = field_from_mm(SCHEMA, "statuses")
 
     SCHEMA: t.ClassVar[marshmallow.Schema] = SCHEMA
+    ACTION_TYPES: t.ClassVar[str] = ATTR_MAP["action_types"]["desc"]
+    DISCOVERY_UUIDS: t.ClassVar[str] = ATTR_MAP["discovery_uuids"]["desc"]
+    ENFORCEMENT_NAMES: t.ClassVar[str] = ATTR_MAP["enforcement_names"]["desc"]
+    TASK_IDS: t.ClassVar[str] = ATTR_MAP["task_ids"]["desc"]
+    STATUSES: t.ClassVar[str] = ATTR_MAP["statuses"]["desc"]
+
+    def __post_init__(self):
+        """Post init."""
+        self.action_names = listify(self.action_names)
+        self.discovery_cycle_id = listify(self.discovery_cycle_id)
+        self.enforcement_name = listify(self.enforcement_name)
+        self.run = listify(self.run)
+        self.statuses = listify(self.statuses)
+
+    @property
+    def enum_enforcement_names(self) -> t.List[str]:
+        """Get a list of all enforcement names that have a run a task."""
+        return sorted(list(set(list([x["text"] for x in self.enforcement_name]))))
+
+    @property
+    def enum_task_ids(self) -> t.List[int]:
+        """Get a list of all known task pretty_id's."""
+        return sorted(self.run)
+
+    @property
+    def enum_action_types(self) -> t.List[str]:
+        """Get a list of all action types in use."""
+        return sorted(self.action_names)
+
+    @property
+    def enum_discovery_uuids(self) -> t.List[str]:
+        """Get a list of all discovery cycle UUIDs that have launched a task."""
+        return sorted(self.discovery_cycle_id)
+
+    @property
+    def enum_statuses(self) -> t.List[str]:
+        """Get a list of all statuses in use."""
+        return sorted(self.statuses)
+
+    def check_task_id(self, value: t.Optional[t.Union[int, str, bytes]] = None) -> t.Optional[int]:
+        """Check validity of a task id.
+
+        Args:
+            value: task id to check
+        """
+        parsed = coerce_int(
+            obj=value,
+            allow_none=True,
+            max_value=max(self.enum_task_ids, default=0),
+            min_value=min(self.enum_task_ids, default=0),
+            errmsg=f"Invalid task id for {self.TASK_IDS}: {value}",
+        )
+        return parsed
+
+    def check_action_types(self, values: t.Optional[TypeMatch] = None, **kwargs) -> t.List[str]:
+        """Check validity of action types.
+
+        Notes:
+            The REST API names this "action_names", but the actual concept is "action_type".
+            The value stored in action_names is the list of all action_types that are being used
+            by any enforcement set.
+
+        Args:
+            values: strings (starting with re_prefix to treat as a pattern) or patterns
+            kwargs: passed to check_values
+        """
+        return self.check_values(
+            valids=self.enum_action_types, description=self.ACTION_TYPES, values=values, **kwargs
+        )
+
+    def check_discovery_uuids(self, values: t.Optional[TypeMatch] = None, **kwargs) -> t.List[str]:
+        """Check validity of discovery UUIDs.
+
+        Args:
+            values: strings (starting with re_prefix to treat as a pattern) or patterns
+            kwargs: passed to check_values
+        """
+        return self.check_values(
+            valids=self.enum_discovery_uuids,
+            description=self.DISCOVERY_UUIDS,
+            values=values,
+            **kwargs,
+        )
+
+    def check_statuses(self, values: t.Optional[TypeMatch] = None, **kwargs) -> t.List[str]:
+        """Check validity of statuses.
+
+        Args:
+            values: strings (starting with re_prefix to treat as a pattern) or patterns
+            kwargs: passed to check_values
+        """
+        return self.check_values(
+            valids=self.enum_statuses, description=self.STATUSES, values=values, **kwargs
+        )
+
+    def check_enforcement_names(
+        self, values: t.Optional[TypeMatch] = None, as_names: bool = False, **kwargs
+    ) -> t.List[str]:
+        """Check validity of enforcement names and return a list of all associated task UUIDs.
+
+        Args:
+            values: strings (starting with re_prefix to treat as a pattern) or patterns
+            as_names: return a list of enforcement names matches instead of the task UUIDs
+            kwargs: passed to check_values
+        """
+        matched = self.check_values(
+            valids=self.enum_enforcement_names,
+            description=self.ENFORCEMENT_NAMES,
+            values=values,
+            **kwargs,
+        )
+        if not as_names and matched:
+            return [x["value"] for x in self.enforcement_name if x["text"] in matched]
+        return matched
 
-    """
-    delta 22 milliseconds
-    
-    duration_filter: DurationOperator
-    date_from: t.Optional[datetime.datetime]
-        --start-date "2019-01-01T00:00:00Z"
-        --start-date "delta:22ms"
-        (days=0, seconds=0, microseconds=0, milliseconds=0, minutes=0, hours=0, weeks=0)
-                            
-    date_to: t.Optional[datetime.datetime]
-    task_id: t.Optional[str]
-    
-    dateutil.parser
-    enums:
-        statuses_filter: t.List[str]
-            List of task status to filter by
-        action_names: t.List[str]
-            List of action names to filter by
-        enforcement_ids: t.List[str]
-            List of task enforcement ids to filter by
-        aggregated_status: t.List[str]
-            List of task results to filter by
-        discovery_cycle: t.List[str]
-            List of  discovery ids to filter by
-              - the discovery cycle id isa bson.ObjectId,
-                so we can get the datetime of the cycle by:
-    bson.ObjectId(discovery_cycle_id).generation_time.replace(tzinfo=dateutil.tz.tzutc())
-                
-                
-        
-    """
+    @staticmethod
+    def check_values(
+        valids: t.List[str],
+        description: str,
+        values: t.Optional[TypeMatch] = None,
+        error: bool = True,
+        minimum: t.Optional[int] = None,
+        re_prefix: str = RE_PREFIX,
+        split: bool = True,
+        split_max: t.Optional[int] = None,
+        split_sep: t.Optional[PatternLike] = SPLITTER,
+        strip: bool = True,
+        strip_chars: t.Optional[str] = None,
+    ) -> t.List[str]:
+        """Check that a value is valid.
+
+        Args:
+            valids:
+            description: what valids are being checked
+            values: strings (starting with re_prefix to treat as a pattern) or patterns
+            error: raise error if any value supplied fails to match
+            minimum: raise error if matched is less than this number
+            re_prefix: if any values provided start with this, treat them as regex patterns
+            split: split values on split_sep
+            split_max: if value > 0 and split=True, only split on split_sep N times
+            split_sep: if split=True, split values on this pattern or string
+            strip: strip values of strip_chars
+            strip_chars: if strip=True, strip values of these characters
+
+        Returns:
+            list of valids that matches any supplied pattern or string in value
+        """
+        matcher: Matcher = Matcher(
+            values=values,
+            re_prefix=re_prefix,
+            split=split,
+            split_max=split_max,
+            split_sep=split_sep,
+            strip=strip,
+            strip_chars=strip_chars,
+        )
+
+        valids_strs = [f"- {x}" for x in sorted(str(x) for x in valids)]
+        matched, unmatched = matcher.matches(values=valids)
+        matcher_strs: t.List[str] = [
+            f"Searching against valid values of {description}",
+            f"Parsed values {values!r} into {matcher}",
+            f"error: {error}, minimum: {minimum}",
+            f"Matched: {len(matched)}, Unmatched: {len(unmatched)}, Valid: {len(valids)}",
+        ]
+        matched_strs: t.List[str] = [f"- {i}" for i in sorted(str(x) for x in matched)]
+        unmatched_strs: t.List[str] = [f"- {i}" for i in sorted(str(x) for x in unmatched)]
+        valids_strs: t.List[str] = ["", f"Valids ({len(valids)}):", *valids_strs, ""]
+
+        if unmatched and error:
+            err_strs: t.List[str] = [
+                "error is True and some searches did not match any valid values",
+                f"Matched ({len(matched)}):",
+                *matched_strs,
+                f"Unmatched ({len(unmatched)}):",
+                *unmatched_strs,
+            ]
+            raise NotFoundError([*err_strs, *matcher_strs, *valids_strs, *err_strs])
+
+        if isinstance(minimum, int) and minimum > 0 and len(matched) < minimum:
+            err_strs: t.List[str] = [
+                f"minimum={minimum} and matched={len(matched)}",
+                f"Unmatched ({len(unmatched)}):",
+                *unmatched_strs,
+            ]
+            raise NotFoundError([*err_strs, *matcher_strs, *valids_strs, *err_strs])
+        return matched
 
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Pass."""
         return TaskFiltersSchema
+
+    def __str__(self):
+        """Pass."""
+        return f"{self.__class__.__name__}(enums={[x.name for x in dataclasses.fields(self)]})"
+
+    def __repr__(self):
+        """Pass."""
+        return str(self)
```

## axonius_api_client/api/json_api/tasks/task_full.py

```diff
@@ -1,82 +1,59 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
+import datetime
 import typing as t
-from datetime import datetime
 
 import marshmallow
+import marshmallow_jsonapi.fields as mm_fields
 
 from ..base import BaseModel, BaseSchemaJson
 from ..custom_fields import SchemaDatetime, field_from_mm
 
 
 class TaskFullSchema(BaseSchemaJson):
     """Schema for enforcement task in full model."""
 
-    id = marshmallow.fields.Str(
-        data_key="id",
-        description="The task id",
-        required=True,
-    )
-    uuid = marshmallow.fields.Str(
-        data_key="uuid",
-        description="The task id",
-        required=True,
-    )
-    pretty_id = marshmallow.fields.Int(
-        data_key="pretty_id",
-        description="The task ID as it appears in the UI",
-    )
-    date_fetched = marshmallow.fields.Str(
-        data_key="date_fetched",
-        description="The date when the task was created",
-        required=True,
-    )
-    enforcement = marshmallow.fields.Str(
-        data_key="enforcement",
-        description="The enforcement name",
-        required=True,
-    )
-    enforcement_id = marshmallow.fields.Str(
-        data_key="enforcement_id",
-        description="The Enforcement set ID",
-        required=True,
-    )
-    task_name = marshmallow.fields.Str(
-        data_key="task_name",
-        description="The task name",
-        required=True,
-    )
-
-    result = marshmallow.fields.Dict(
-        data_key="result",
-        description="The Task results per Action",
-        load_default=dict,
+    id = mm_fields.Str(data_key="id", description="The task id", required=True)
+    uuid = mm_fields.Str(data_key="uuid", description="The task id", required=True)
+    pretty_id = mm_fields.Int(
+        data_key="pretty_id", description="The task ID as it appears in the UI"
     )
-
-    view = marshmallow.fields.Str(
+    date_fetched = mm_fields.Str(
+        data_key="date_fetched", description="The date when the task was created", required=True
+    )
+    enforcement = mm_fields.Str(
+        data_key="enforcement", description="The enforcement name", required=True
+    )
+    enforcement_id = mm_fields.Str(
+        data_key="enforcement_id", description="The Enforcement set ID", required=True
+    )
+    task_name = mm_fields.Str(data_key="task_name", description="The task name", required=True)
+    result = mm_fields.Dict(
+        data_key="result", description="The Task results per Action", load_default=dict
+    )
+    view = mm_fields.Str(
         data_key="view",
         description="The name of the query defined as the enforcement set trigger",
         allow_none=True,
         load_default=None,
     )
-    period = marshmallow.fields.Str(
+    period = mm_fields.Str(
         data_key="period",
         description="The period scheduled for running the task",
         allow_none=True,
         load_default=None,
     )
-    condition = marshmallow.fields.Str(
+    condition = mm_fields.Str(
         data_key="condition",
         description="The condition which triggered the run",
         allow_none=True,
         load_default=None,
     )
-
     started = SchemaDatetime(
         data_key="started",
         description="The timestamp when the task started running",
         allow_none=True,
         load_default=None,
     )
     finished = SchemaDatetime(
@@ -117,16 +94,16 @@
     task_name: str = field_from_mm(SCHEMA, "task_name")
 
     result: dict = field_from_mm(SCHEMA, "result")
 
     view: t.Optional[str] = field_from_mm(SCHEMA, "view")
     period: t.Optional[str] = field_from_mm(SCHEMA, "period")
     condition: t.Optional[str] = field_from_mm(SCHEMA, "condition")
-    started: t.Optional[datetime] = field_from_mm(SCHEMA, "started")
-    finished: t.Optional[datetime] = field_from_mm(SCHEMA, "finished")
+    started: t.Optional[datetime.datetime] = field_from_mm(SCHEMA, "started")
+    finished: t.Optional[datetime.datetime] = field_from_mm(SCHEMA, "finished")
 
     SCHEMA: t.ClassVar[marshmallow.Schema] = SCHEMA
 
     @staticmethod
     def get_schema_cls() -> t.Any:
         """Pass."""
         return TaskFullSchema
```

## axonius_api_client/api/system/dashboard.py

```diff
@@ -234,15 +234,15 @@
         def get_phase(raw):
             phase = DiscoverPhase(raw=raw)
             phase.status = get_status(phase)
             return phase
 
         return [get_phase(x) for x in self.raw["sub_phases"]]
 
-    def next_run_within_minutes(self, value: int) -> bool:
+    def next_run_within_minutes(self, value: t.Union[int, str]) -> bool:
         """Pass."""
         return coerce_int(obj=value, min_value=0) >= int(self.next_run_starts_in_minutes)
 
     def get_stability(
         self, for_next_minutes: t.Optional[int] = None, start_check: t.Union[int, float] = 0.5
     ) -> t.Tuple[str, bool]:
         """Pass."""
```

## axonius_api_client/api/system/instances.py

```diff
@@ -472,16 +472,16 @@
             Can not run in test suite!
 
         Args:
             key_name: Name of backup file from central core in [bucket_name] to restore to
                 this core
             bucket_name: Name of bucket in S3 to get [key_name] from
                 (Overrides ``Global Settings > Amazon S3 Settings > Amazon S3 bucket name``)
-            access_key_id: AWS Access Key Id to use to access [bucket_name]
-                (Overrides ``Global Settings > Amazon S3 Settings > AWS Access Key Id``)
+            access_key_id: AWS Access Key ID to use to access [bucket_name]
+                (Overrides ``Global Settings > Amazon S3 Settings > AWS Access Key ID``)
             secret_access_key: AWS Secret Access Key to use to access [bucket_name]
                 (Overrides ``Global Settings > Amazon S3 Settings > AWS Secret Access Key``)
             preshared_key: Password to use to decrypt [key_name]
                 (Overrides: ``Global Settings > Amazon S3 Settings > Backup encryption passphrase``)
             allow_re_restore: Restore [key_name] even if it has already been restored
             delete_backups: Delete [key_name] from [bucket_name] after restore has finished
                 (Overrides the current value of :meth:`get_core_delete_mode`)
@@ -560,15 +560,15 @@
         """Upload an admin script from a file or URL and execute it.
 
         Args:
             path: URL or path to file of script to upload and execute
             **kwargs: passed to :meth:`upload_script`
         """
         if is_url(value=path):
-            from ...parsers.url_parser import UrlParser
+            from axonius_api_client.projects.url_parser import UrlParser
 
             parser = UrlParser(url=path)
             path_part = pathlib.Path(parser.parsed.path)
             file_name = path_part.name
             request: dict = {}
             request["url"] = path
             request["verify"] = path_verify
```

## axonius_api_client/api/system/meta.py

```diff
@@ -86,15 +86,16 @@
         Examples:
             Create a ``client`` using :obj:`axonius_api_client.connect.Connect`
 
             >>> client.meta.version
             '3.10'
 
         """
-        return self.about().get("Version", "")
+        about: dict = self.about()
+        return about.get("Version", about.get("Installed Version", ""))
 
     def _about(self) -> dict:
         """Direct API method to get the About page."""
         api_endpoint = ApiEndpoints.system_settings.meta_about
         return api_endpoint.perform_request(http=self.auth.http)
 
     def _about2(self) -> dict:
```

## axonius_api_client/api/system/settings.py

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 """Parent API for working with system settings."""
 import pathlib
 from typing import Any, List, Optional, Tuple, Union
 
-from ... import cert_human
+from axonius_api_client.projects import cert_human
 from ...constants.api import USE_CA_PATH
 from ...exceptions import ApiError, NotFoundError
 from ...parsers.config import config_build, config_unchanged, config_unknown, parse_settings
 from ...parsers.tables import tablize
 from ...tools import path_read
 from ..api_endpoints import ApiEndpoints
 from ..json_api.generic import ApiBase, BoolValue
@@ -60,19 +60,19 @@
                     meta["full_config"] = settings["config"]
                 return meta
 
         err = f"Section Name {section!r} not found in {title}"
         raise NotFoundError(tablize(value=valid_sections, err=err))
 
     def get_sub_section(self, section: str, sub_section: str, full_config: bool = False) -> dict:
-        """Get the current settings for a sub-section of a section of system settings.
+        """Get the current settings for a subsection of a section of system settings.
 
         Args:
             section: name of section
-            sub_section: name of sub section of section
+            sub_section: name of subsection of section
             full_config: return the full configuration
         """
         settings = self.get_section(section=section, full_config=full_config)
         title = settings["settings_title"]
 
         if not settings["sub_sections"]:
             raise ApiError(f"Section Name {section!r} has no sub sections!")
@@ -139,19 +139,19 @@
         full_config[section] = new_config
 
         self._update(new_config=full_config)
 
         return self.get_section(section=section)
 
     def update_sub_section(self, section: str, sub_section: str, **kwargs) -> dict:
-        """Update the current settings for a sub-section of a section of system settings.
+        """Update the current settings for a subsection of a section of system settings.
 
         Args:
             section: name of section
-            sub_section: name of sub section of section
+            sub_section: name of subsection of section
             **kwargs: settings to update
         """
         settings = self.get_sub_section(section=section, sub_section=sub_section, full_config=True)
         title = settings["settings_title"]
         schemas = settings["schemas"]
         source = f"{title} Section Name {section!r} Sub Section Name {sub_section!r}"
         old_config = settings["config"]
@@ -286,15 +286,15 @@
         settings = self.get_section("ssl_trust_settings")
         config = settings.get("config", {})
         config["enabled"] = value
         self._cert_settings(ssl_trust=config)
         return self.ca_get()
 
     @staticmethod
-    def check_cert_is_ca(contents: CONTENT, source: Any = None) -> cert_human.Cert:
+    def check_cert_is_ca(contents: CONTENT, source: Any = None) -> cert_human.Store:
         """Pass."""
         stores = cert_human.Cert.from_content(value=contents, source=source)
         store = stores[0]
         store.check_is_ca()
         store.check_expired()
         return store
```

## axonius_api_client/api/system/signup.py

```diff
@@ -1,49 +1,51 @@
 # -*- coding: utf-8 -*-
 """API for performing initial signup."""
-from ...constants.logs import LOG_LEVEL_API
 from ...exceptions import ApiError
-from ...http import Http
-from ...logs import get_obj_log
 from ...tools import token_parse
 from .. import json_api
 from ..api_endpoints import ApiEndpoints
+from ..mixins import ModelMixins
 
 
-class Signup:
-    """API for performing initial signup.
+class Signup(ModelMixins):
+    """API for performing initial signup and other unauthenticated endpoints.
 
     Examples:
         * Check if initial signup has been done: :meth:`is_signed_up`
         * Perform initial signup: :meth:`signup`
 
     """
 
     @property
     def is_signed_up(self) -> bool:
         """Check if initial signup has been done.
 
         Examples:
-            >>> signup = axonius_api_client.Signup(url="10.20.0.61")
-            >>> signup.is_signed_up
+            >>> import axonius_api_client as axonapi
+            >>> url: str = "10.0.3.2"
+            >>> client: axonapi.Connect = axonapi.Connect(url=url, key="", secret="")
+            >>> client.signup.is_signed_up
             True
         """
         return self._get().value
 
     @property
     def system_status(self) -> json_api.signup.SystemStatus:
         """Pass."""
         return self._status()
 
     def signup(self, password: str, company_name: str, contact_email: str) -> dict:
         """Perform the initial signup and get the API key and API secret of admin user.
 
         Examples:
-            >>> signup = axonius_api_client.Signup(url="10.20.0.61")
-            >>> data = signup.signup(
+            >>> import axonius_api_client as axonapi
+            >>> url: str = "10.0.3.2"
+            >>> client: axonapi.Connect = axonapi.Connect(url=url, key="", secret="")
+            >>> data = client.signup.signup(
             ...     password="demo", company_name="Axonius", contact_email="jim@axonius.com"
             ... )
             >>> data
             {'api_key': 'xxxx', 'api_secret': 'xxxx'}
 
         Args:
             password: password for admin user
@@ -51,15 +53,15 @@
             contact_email: email address of company contact
         """
         return self._perform(
             password=password, company_name=company_name, contact_email=contact_email
         ).to_dict()
 
     def validate_password_reset_token(self, token: str) -> bool:
-        """Pass."""
+        """Validate that a password reset token is legit."""
         token = token_parse(token)
         data = self._token_validate(token=token)
         return data.value
 
     def use_password_reset_token(
         self, token: str, password: str
     ) -> json_api.password_reset.UseResponse:
@@ -81,14 +83,54 @@
         token = token_parse(token)
         if not self.validate_password_reset_token(token=token):
             raise ApiError(f"Password reset token is not valid: {token}")
 
         data = self._token_use(token=token, password=password)
         return data
 
+    @property
+    def is_expired(self) -> bool:
+        """Check if the system has expired."""
+        return self._expired().value
+
+    @property
+    def is_licensed(self) -> bool:
+        """Check if the system is licensed."""
+        return self._license_status().value
+
+    @property
+    def indication_color(self) -> str:
+        """Get the indication color."""
+        return self._get_indication_color().value
+
+    @property
+    def login_options(self) -> dict:
+        """Get the login options."""
+        return self._get_login_options().document_meta
+
+    def _get_login_options(self) -> json_api.generic.Metadata:
+        """Direct API method to get the login options."""
+        api_endpoint = ApiEndpoints.signup.get_login_options
+        return api_endpoint.perform_request(http=self.http)
+
+    def _get_indication_color(self) -> json_api.generic.StrValue:
+        """Direct API method to get the indication color."""
+        api_endpoint = ApiEndpoints.signup.get_indication_color
+        return api_endpoint.perform_request(http=self.http)
+
+    def _license_status(self) -> json_api.generic.BoolValue:
+        """Direct API method to get the license status."""
+        api_endpoint = ApiEndpoints.signup.license_status
+        return api_endpoint.perform_request(http=self.http)
+
+    def _expired(self) -> json_api.generic.BoolValue:
+        """Direct API method to get the expiry status."""
+        api_endpoint = ApiEndpoints.signup.expired
+        return api_endpoint.perform_request(http=self.http)
+
     def _status(self) -> json_api.signup.SystemStatus:
         """Direct API method to get the status of the overall system."""
         api_endpoint = ApiEndpoints.signup.status
         return api_endpoint.perform_request(http=self.http)
 
     def _get(self) -> json_api.generic.BoolValue:
         """Direct API method to get the status of initial signup."""
@@ -128,19 +170,7 @@
             new_password=password,
             confirm_new_password=password,
             contact_email=contact_email,
             user_name="admin",
             api_keys=True,
         )
         return api_endpoint.perform_request(http=self.http, request_obj=request_obj)
-
-    def __init__(self, url, **kwargs):
-        """Provide an API for performing initial signup.
-
-        Args:
-            url: url of instance to perform signup against
-            **kwargs: passed thru to :obj:`axonius_api_client.http.Http`
-        """
-        log_level = kwargs.get("log_level", LOG_LEVEL_API)
-        self.LOG = get_obj_log(obj=self, level=log_level)
-        kwargs.setdefault("certwarn", False)
-        self.HTTP = self.http = Http(url=url, **kwargs)
```

## axonius_api_client/api/system/system_users.py

```diff
@@ -149,15 +149,15 @@
             ...     last="wolf",
             ...     email="badwolf@badwolf.com",
             ... )
             >>> user['uuid']
             '5f7ca7f1e4557d5cba415f12'
 
         Args:
-            name: user name
+            name: username
             password: password
             role_name: role name
             first_name: first name
             last_name: last name
             email: email address
             generate_password_link: create a password reset link
             email_password_link: email the password reset link to a users configured email address
@@ -213,15 +213,15 @@
             Create a ``client`` using :obj:`axonius_api_client.connect.Connect`
 
             >>> user = client.system_users.set_role(name='test1', role_name='Viewer')
             >>> user['role_name']
             'Viewer'
 
         Args:
-            name: user name
+            name: username
             role_name: name of role
         """
         role = self.roles.get_by_name(name=role_name)
         return self._update_user_attr(
             name=name, must_be_internal=True, attr="role_id", value=role["uuid"]
         )
 
@@ -236,15 +236,15 @@
             'Bad'
             >>> user['last_name']
             'Wolf'
             >>> user['full_name']
             'Bad Wolf'
 
         Args:
-            name: user name
+            name: username
             first: first name
             last: last name
         """
         value = {"first_name": first, "last_name": last}
         attr = "first_name and last_name"
         return self._update_user_attr(name=name, must_be_internal=True, attr=attr, value=value)
 
@@ -328,20 +328,20 @@
             Create a ``client`` using :obj:`axonius_api_client.connect.Connect`
 
             >>> link = client.system_users.get_password_reset_link(name='test1')
             >>> link
             'https://10.20.0.61/login?token=LUayznSkfLUjm3A2fIs_zAF-4CzcxRZc7DHOfhOkMRI'
 
         Notes:
-            This link can be provided to the user and they can browse to it, or they can use
-            ``axonshell tools axonshell tools use-password-reset-token`` or use
+            This link can be provided to the user. They can browse to it, or they can use
+            `axonshell tools use-password-reset-token` or use
             :meth:`axonius_api_client.api.system.signup.Signup.use_password_reset_token`
 
         Args:
-            name: user name
+            name: username
 
         """
         user = self.get_by_name(name=name)
         return self._tokens_generate(uuid=user["uuid"], user_name=user["user_name"])
 
     def email_password_reset_link(
         self, name: str, email: t.Optional[str] = None, for_new_user: bool = False, link: str = ""
@@ -376,15 +376,15 @@
             ... )
 
         Notes:
             This returns the password reset link like :meth:`get_password_reset_link` and the email
             address that was used to send the email.
 
         Args:
-            name: user name
+            name: username
             email: use a custom email address to send the email instead of the users defined email
                 address. required if the user has no defined email address.
             for_new_user: use the new user email template instead of the password reset email
                 template
             link: use the specified password reset link vs a newly generated one
         """
         user = self.get_by_name(name=name)
@@ -424,16 +424,16 @@
     ) -> MODEL:
         """Direct API method to add a user.
 
         Args:
             user_name: user name
             password: password
             role_id: role UUID
-            first: first name
-            last: last name
+            first_name: first name
+            last_name: last name
             email: email address
             auto_generated_password: generate a password
         """
         api_endpoint = ApiEndpoints.system_users.create
         request_obj = api_endpoint.load_request(
             user_name=user_name,
             first_name=first_name,
@@ -530,15 +530,15 @@
 
     def _update_user_attr(
         self, name: str, must_be_internal: bool, attr: str, value: t.Union[str, bool, dict]
     ) -> dict:
         """Set an attribute on a user.
 
         Args:
-            name: user name
+            name: username
             must_be_internal: user must be internal or external for this attr to be set
             attr: attribute of user object to set
             value: value to set on attribute
 
         Raises:
             :exc:`ApiError`:
```

## axonius_api_client/api/wizards/wizard.py

```diff
@@ -98,15 +98,15 @@
         """:obj:`axonius_api_client.api.assets.asset_mixin.AssetMixin`: Asset object."""
 
         self.PARSER = WizardParser(apiobj=apiobj)
         """:obj:`axonius_api_client.parsers.wizards.WizardParser`: Value parser."""
 
         self._init()
 
-    def parse(self, entries: List[dict], source: str = Sources.LOD) -> List[dict]:
+    def parse(self, entries: List[dict], source: str = Sources.LOD) -> dict:
         """Parse a list of entries into a query and the associated GUI query wizard expressions.
 
         Args:
             entries: list of entries to parse
             source: where entries came from
         """
         check_type(value=entries, exp=(list, tuple), exp_items=dict)
@@ -150,18 +150,18 @@
 
     def _parse_flags(
         self, entry: dict, idx: int, entries: List[dict], tracker: int, is_open: bool
     ) -> dict:
         """Parse flags from an entry.
 
         Args:
-            entry: entry to parse with Entry.VALUE key
+            entry: entry to parse with `Entry.VALUE` key
             idx: index of this entry
             entries: all entries
-            tracker: tracker for Entry.WEIGHT key
+            tracker: tracker for `Entry.WEIGHT` key
             is_open: parenthesis are currently open
         """
         value_raw = entry[Entry.VALUE]
         flags = listify(entry.get(Entry.FLAGS) or [])
         flags, value = self._split_flags(value_raw=value_raw, flags=flags)
         entry[Entry.VALUE] = value
         entry[Entry.FLAGS] = flags
@@ -311,24 +311,24 @@
             op_comp="",
             children=sub_exprs,
             is_complex=True,
         )
         return expr
 
     def _parse_sub(self, field: dict, value_raw: str, idx: int) -> dict:
-        """Parse an sub expression of an entry of type complex.
+        """Parse sub expression of an entry of type complex.
 
         Args:
             field: complex field schema
             value_raw: the value split from the complex filter line
             idx: index of this entry
 
         Raises:
             :exc:`axonius_api_client.exceptions.WizardError`:
-                if sub field supplied is not a valid sub field of the complex field
+                if sub-field supplied is not a valid sub-field of the complex field
         """
         sub_field, operator, sub_value = self._split_simple(value_raw=value_raw)
 
         field_subs = {x[Fields.NAME]: x for x in field[Fields.SUBS]}
 
         if sub_field not in field_subs:
             fname = field[Fields.NAME]
@@ -435,15 +435,15 @@
             src="OPERATOR",
             patterns=Patterns.OP,
         )
 
         return field, operator, value
 
     def _split_complex(self, value_raw: str) -> Tuple[str, List[str]]:
-        """Split a complex query wizard expression into field and sub field expressions.
+        """Split a complex query wizard expression into field and sub-field expressions.
 
         Args:
             value_raw: the raw unparsed value to parse
         """
         splitter = Entry.CSPLIT
         if splitter not in value_raw:
             raise WizardError(f"No {splitter} found in value '{value_raw}'")
```

## axonius_api_client/api/wizards/wizard_csv.py

```diff
@@ -208,15 +208,15 @@
                         value = default
                 else:
                     value = default
                 entry[key] = value
         return entry
 
     def _process_sqs(self, entries: List[dict]) -> List[dict]:
-        """Process all of the saved queries defined in the CSV.
+        """Process the saved queries defined in the CSV.
 
         Args:
             entries: the entries produced by parsing the rows
         """
         self.SQS = sqs = []
         self.SQ = {}
         self.SQ_ENTRIES = []
```

## axonius_api_client/api/wizards/wizard_text.py

```diff
@@ -70,27 +70,25 @@
         >>> sq['view']['query']['filter'][:80]
         '(specific_data.data.hostname == regex("test", "i")) and (specific_data.data.os.t'
 
     """
 
     DOCS: str = Docs.TEXT
 
-    def parse(self, content: Union[str, List[str]], source: str = Sources.TEXT_STR) -> List[dict]:
+    def parse(self, content: Union[str, List[str]], source: str = Sources.TEXT_STR) -> dict:
         """Parse a CSV string into a query and GUI expressions.
 
         Args:
             content: text string
             source: where content came from
         """
         entries = self._lines_to_entries(content=content, source=source)
         return super().parse(entries=entries, source=source)
 
-    def parse_path(
-        self, path: Union[str, pathlib.Path], source: str = Sources.TEXT_PATH
-    ) -> List[dict]:
+    def parse_path(self, path: Union[str, pathlib.Path], source: str = Sources.TEXT_PATH) -> dict:
         """Parse a text file into a query and GUI expressions.
 
         Args:
             path: text file
             source: where csv file came from
         """
         path, content = path_read(path)
```

## axonius_api_client/auth/__init__.py

```diff
@@ -1,16 +1,20 @@
 # -*- coding: utf-8 -*-
 """Authenticating with Axonius."""
-from . import api_key, credentials, models
-from .api_key import ApiKey
-from .credentials import Credentials
-from .models import Mixins, Model
+from .api_key import AuthApiKey
+from .credentials import AuthCredentials
+from .model import AuthModel
+from .null import AuthNull
+
+# backwards compatibility
+ApiKey = AuthApiKey
+Credentials = AuthCredentials
+
 
 __all__ = (
-    "models",
-    "api_key",
-    "Model",
-    "Mixins",
+    "AuthModel",
+    "AuthApiKey",
+    "AuthCredentials",
+    "AuthNull",
     "ApiKey",
-    "credentials",
     "Credentials",
 )
```

## axonius_api_client/auth/api_key.py

```diff
@@ -1,49 +1,47 @@
 # -*- coding: utf-8 -*-
 """Authentication via API key and API secret."""
-from typing import List
+import typing as t
 
-from ..exceptions import AlreadyLoggedIn
 from ..http import Http
 from ..tools import strip_str
-from .models import Mixins
+from .model import AuthModel
 
 
-class ApiKey(Mixins):
+class AuthApiKey(AuthModel):
     """Authentication method using API key & API secret."""
 
     def __init__(self, http: Http, key: str, secret: str, **kwargs):
         """Authenticate using API key & API secret.
 
         Args:
             http: HTTP client to use to send requests
             key: API key to use in credentials
             secret: API secret to use in credentials
 
         """
-        creds = {"key": strip_str(key), "secret": strip_str(secret)}
-        super().__init__(http=http, creds=creds, **kwargs)
+        creds: t.Dict[str, str] = {"key": strip_str(key), "secret": strip_str(secret)}
+        kwargs["creds"] = creds
+        super().__init__(http=http, **kwargs)
 
-    def login(self):
+    def login(self) -> bool:
         """Login to API."""
-        if self.is_logged_in:
-            raise AlreadyLoggedIn(f"Already logged in on {self}")
-
-        self.http.session.headers["api-key"] = self._creds["key"]
-        self.http.session.headers["api-secret"] = self._creds["secret"]
-        self._validate()
-        self._logged_in = True
-        self.LOG.debug(f"Successfully logged in using {self._cred_fields}")
-
-    def logout(self):
-        """Logout from API."""
-        super().logout()
+        if not self.is_logged_in:
+            self.http.session.headers["api-key"] = self._creds["key"]
+            self.http.session.headers["api-secret"] = self._creds["secret"]
+            self.is_logged_in = self.validate()
+            return True
+        return False
 
     @property
-    def _cred_fields(self) -> List[str]:
+    def fields(self) -> t.List[str]:
         """Credential fields used by this auth model."""
         return ["key", "secret"]
 
-    def _logout(self):
+    def logout(self) -> bool:
         """Logout from API."""
-        self._logged_in = False
-        self.http.session.headers = {}
+        if self.is_logged_in:
+            self.http.session.headers.pop("api-key", None)
+            self.http.session.headers.pop("api-secret", None)
+            self.is_logged_in = False
+            return True
+        return False
```

## axonius_api_client/auth/credentials.py

```diff
@@ -1,70 +1,70 @@
 # -*- coding: utf-8 -*-
-"""Authentication via API key and API secret."""
+"""Authentication via username and password."""
 import typing as t
 
-from ..api.api_endpoints import ApiEndpoint
-from ..api.json_api.account import LoginRequest, LoginResponse
 from ..http import Http
-from .models import Mixins
+from .model import AuthModel
+from ..api.json_api.account import LoginRequest, LoginResponse
+from ..api.api_endpoints import ApiEndpoint, ApiEndpoints
 
 
-class Credentials(Mixins):
+class AuthCredentials(AuthModel):
     """Authentication method using username and password credentials."""
 
     def __init__(
         self,
         http: Http,
-        username: t.Optional[str] = None,
-        password: t.Optional[str] = None,
+        username: str,
+        password: str,
         **kwargs,
     ):
         """Authenticate using username and password.
 
         Args:
-            http (Http): HTTP client to use to send requests
-            username (t.Optional[str], optional): Axonius User Name
-            password (t.Optional[str], optional): Axonius Password
-            prompt (bool, optional): Prompt for credentials that are not non-empty strings
+            http: HTTP client to use to send requests
+            username: Axonius Username
+            password: Axonius Password
         """
         creds: LoginRequest = LoginRequest(user_name=username, password=password, eula_agreed=True)
-        super().__init__(http=http, creds=creds, **kwargs)
+        kwargs["creds"] = creds
+        super().__init__(http=http, **kwargs)
 
     def login(self):
         """Login to API."""
         if not self.is_logged_in:
-            self._creds.check_credentials()
-            self._login_response: LoginResponse = self._login(request_obj=self._creds)
-            headers: dict = {"authorization": self._login_response.authorization}
-            self._api_keys: dict = self._get_api_keys(headers=headers)
-            self.http.session.headers["api-key"] = self._api_keys["api_key"]
-            self.http.session.headers["api-secret"] = self._api_keys["api_secret"]
-            self._validate()
-            self._logged_in = True
-            self.LOG.debug(f"Successfully logged in using {self._cred_fields}")
+            self.LOGIN_RESPONSE: t.ClassVar[LoginResponse] = self._login(request_obj=self._creds)
+            # now that we logged in with username & password
+            # get the API keys and use for the rest of the session
+            api_keys: dict = self._get_api_keys(http_args=self.LOGIN_RESPONSE.http_args)
+            self.http.session.headers["api-key"] = api_keys["api_key"]
+            self.http.session.headers["api-secret"] = api_keys["api_secret"]
+            self.is_logged_in = self.validate()
+            return True
+        return False
 
     def logout(self):
         """Logout from API."""
-        super().logout()
+        if self.is_logged_in:
+            self.http.session.headers.pop("api-key", None)
+            self.http.session.headers.pop("api-secret", None)
+            self.is_logged_in = False
+            return True
+        return False
+
+    @property
+    def fields(self) -> t.List[str]:
+        """Credential fields used by this auth model."""
+        return [f"username={self._creds.user_name!r}", "password"]
 
     def _login(self, request_obj: LoginRequest) -> LoginResponse:
-        """Direct API method to issue a login request.
+        """Direct API method to issue a login request using credentials.
 
         Args:
             request_obj (LoginRequest): Request object to send
 
         Returns:
             LoginResponse: Response object received
         """
-        endpoint: ApiEndpoint = self.endpoints.login
+        endpoint: ApiEndpoint = ApiEndpoints.account.login
         response: LoginResponse = endpoint.perform_request(http=self.http, request_obj=request_obj)
         return response
-
-    @property
-    def _cred_fields(self) -> t.List[str]:
-        """Credential fields used by this auth model."""
-        return [f"username={self._creds.user_name!r}", "password"]
-
-    def _logout(self):
-        """Logout from API."""
-        self._logged_in = False
-        self.http.session.headers = {}
```

## axonius_api_client/cli/__init__.py

```diff
@@ -1,34 +1,27 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 import os
 import sys
+import typing as t
 
 import click
 
-from .. import INIT_DOTENV, version
-from ..constants.api import TIMEOUT_CONNECT, TIMEOUT_RESPONSE
+from ..projects.cf_token import constants as cf_constants
+from .. import INIT_DOTENV, connect, version
 from ..constants.logs import (
-    LOG_FILE_MAX_FILES,
-    LOG_FILE_MAX_MB,
-    LOG_FILE_NAME,
-    LOG_FILE_PATH,
-    LOG_LEVEL_API,
-    LOG_LEVEL_AUTH,
-    LOG_LEVEL_CONSOLE,
-    LOG_LEVEL_ENDPOINTS,
-    LOG_LEVEL_FILE,
-    LOG_LEVEL_HTTP,
-    LOG_LEVEL_PACKAGE,
     LOG_LEVELS_STR,
-    REQUEST_ATTR_MAP,
-    RESPONSE_ATTR_MAP,
+    REQUEST_ATTRS,
+    REQUEST_ATTRS_DEFAULT,
+    RESPONSE_ATTRS,
+    RESPONSE_ATTRS_DEFAULT,
 )
 from ..logs import LOG
 from ..setup_env import DEFAULT_ENV_FILE
+from ..tools import json_dump
 from . import (
     context,
     grp_account,
     grp_adapters,
     grp_assets,
     grp_certs,
     grp_enforcements,
@@ -63,29 +56,30 @@
   - bash: AX_ENV=/path/to/.env axonshell tools shell  # for single commands
   - cmd.exe: SET AX_ENV="c:\\path\\to\\.env"
   - powershell: $AX_ENV = "c:\\path\\to\\.env"
 - Multiple ways to specify AX_COOKIES and AX_HEADERS:
   - As CSV with , as delimiter: AX_COOKIES="key1=value1,key2=value2,key3=value4"
   - As CSV with ; as delimiter: AX_COOKIES="semi:key1=value1;key2=value2;key3=value4"
   - As JSON str: AX_HEADERS='json:{{"key1": "value1", "key2": "value2"}}'
-- Use AX_URL, AX_KEY, AX_SECRET to specify credentials
+- Use AX_URL, AX_KEY, AX_SECRET, AX_CREDENTIALS to specify credentials
 """
+CF = cf_constants.CLIENT_DESC
 
 
 @click.group(
     cls=context.AliasedGroup,
     context_settings=context.CONTEXT_SETTINGS,
     epilog=PROTIPS,
 )
 @click.option(
     "--quiet/--no-quiet",
     "-q/-nq",
     "quiet",
     default=False,
-    help="Silence green text.",
+    help="Silence most green & blue output.",
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--cookie",
     "-cook",
     "cookies",
@@ -96,101 +90,238 @@
     "--header",
     "-head",
     "headers",
     help="Additional headers to supply with every request",
     cls=context.DictOption,
 )
 @click.option(
+    "--cf-url",
+    "-cfu",
+    "cf_url",
+    help=f"{CF}URL to use in cloudflared commands, will fallback to url if not supplied",
+    default=None,
+    envvar=["CF_URL", "AX_URL"],
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-token",
+    "-cft",
+    "cf_token",
+    help=f"{CF}token supplied by user, will be checked for validity if not empty",
+    default=None,
+    envvar="CF_TOKEN",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-run/--no-cf-run",
+    "-cfr/-ncfr",
+    "cf_run",
+    help=(
+        f"{CF}If no token supplied or in OS env vars, try to get token from cloudflared commands"
+    ),
+    default=cf_constants.CLIENT_RUN,
+    envvar="CF_RUN",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-run-access/--no-cf-run-access",
+    "-cfrac/-ncfrac",
+    "cf_run_access",
+    help=f"{CF}If run is True, try to get token from `access token` command",
+    default=cf_constants.FLOW_RUN_ACCESS,
+    envvar="CF_RUN_ACCESS",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-run-login/--no-cf-run-login",
+    "-cfrlc/-ncfrlc",
+    "cf_run_login",
+    help=(
+        f"{CF}If run is True and no token returned from `access token` command, "
+        f"try to get token from `access login` command"
+    ),
+    envvar="CF_RUN_LOGIN",
+    default=cf_constants.FLOW_RUN_LOGIN,
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-path",
+    "-cfp",
+    "cf_path",
+    help=f"{CF}Path to cloudflared binary to run, can be full path or path in OS env var $PATH",
+    default=cf_constants.CF_PATH,
+    envvar="CF_PATH",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-timeout-access",
+    "-cfta",
+    "cf_timeout_access",
+    help=f"{CF}Timeout for `access token` command in seconds",
+    default=cf_constants.TIMEOUT_ACCESS,
+    type=click.INT,
+    envvar="CF_TIMEOUT_ACCESS",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-timeout-login",
+    "-cftl",
+    "cf_timeout_login",
+    help=f"{CF}Timeout for `access login` command in seconds",
+    default=cf_constants.TIMEOUT_LOGIN,
+    type=click.INT,
+    envvar="CF_TIMEOUT_LOGIN",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-error/--no-cf-error",
+    "-cfe/-ncfe",
+    "cf_error",
+    help=f"{CF}Raise error if an invalid token is found or no token can be found",
+    default=cf_constants.CLIENT_ERROR,
+    envvar="CF_ERROR",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-error-access/--no-cf-error-access",
+    "-cfeac/-ncfeac",
+    "cf_error_access",
+    help=f"{CF}Raise exc if `access token` command fails and login is False",
+    default=cf_constants.FLOW_ERROR,
+    envvar="CF_ERROR_ACCESS",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-error-login/--no-cf-error-login",
+    "-cfel/-ncfel",
+    "cf_error_login",
+    help=f"{CF}Raise exc if `access login` command fails",
+    default=cf_constants.FLOW_ERROR,
+    envvar="CF_ERROR_LOGIN",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-echo/--no-cf-echo",
+    "-cfec/-ncfec",
+    "cf_echo",
+    help=f"{CF}Echo commands and results to STDERR",
+    default=cf_constants.FLOW_ECHO,
+    envvar="CF_ECHO",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--cf-echo-verbose/--no-cf-echo-verbose",
+    "-cfev/-ncfev",
+    "cf_echo_verbose",
+    help=f"{CF}Echo more stuff to STDERR",
+    default=cf_constants.FLOW_ECHO_VERBOSE,
+    envvar="CF_ECHO_VERBOSE",
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
     "--log-level-package",
     "-lvlpkg",
     "log_level_package",
-    default=LOG_LEVEL_PACKAGE,
+    default=connect.LOG_LEVEL_PACKAGE,
     help="Logging level to use for entire package.",
     type=click.Choice(LOG_LEVELS_STR),
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-level-http",
     "-lvlhttp",
     "log_level_http",
-    default=LOG_LEVEL_HTTP,
+    default=connect.Http.LOG_LEVEL,
     help="Logging level to use for http client.",
     type=click.Choice(LOG_LEVELS_STR),
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-level-auth",
     "-lvlauth",
     "log_level_auth",
-    default=LOG_LEVEL_AUTH,
+    default=connect.LOG_LEVEL_AUTH,
     help="Logging level to use for auth client.",
     type=click.Choice(LOG_LEVELS_STR),
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-level-api",
     "-lvlapi",
     "log_level_api",
-    default=LOG_LEVEL_API,
+    default=connect.LOG_LEVEL_API,
     help="Logging level to use for API models.",
     type=click.Choice(LOG_LEVELS_STR),
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-level-endpoints",
     "-lvlep",
     "log_level_endpoints",
-    default=LOG_LEVEL_ENDPOINTS,
+    default=connect.LOG_LEVEL_ENDPOINTS,
     help="Logging level to use for API endpoints.",
     type=click.Choice(LOG_LEVELS_STR),
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-level-console",
     "-lvlcon",
     "log_level_console",
-    default=LOG_LEVEL_CONSOLE,
+    default=connect.LOG_LEVEL_CONSOLE,
     help="Logging level to use for console output.",
     type=click.Choice(LOG_LEVELS_STR),
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-level-file",
     "-lvlfile",
     "log_level_file",
-    default=LOG_LEVEL_FILE,
+    default=connect.LOG_LEVEL_FILE,
     help="Logging level to use for file output.",
     type=click.Choice(LOG_LEVELS_STR),
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-request-attrs",
     "-reqattr",
     "log_request_attrs",
     help="Log http client request attributes (multiples)",
-    default=["size", "url"],
+    default=REQUEST_ATTRS_DEFAULT,
     multiple=True,
-    type=click.Choice(list(REQUEST_ATTR_MAP) + ["all"]),
+    type=click.Choice(REQUEST_ATTRS),
     show_envvar=True,
 )
 @click.option(
     "--log-response-attrs",
     "-respattr",
     "log_response_attrs",
-    default=["size", "url", "status", "elapsed"],
+    default=RESPONSE_ATTRS_DEFAULT,
     help="Log http client response attributes (multiples)",
     multiple=True,
-    type=click.Choice(list(RESPONSE_ATTR_MAP) + ["all"]),
+    type=click.Choice(RESPONSE_ATTRS),
     show_envvar=True,
 )
 @click.option(
     "--log-request-body",
     "-reqbody",
     "log_request_body",
     default=False,
@@ -204,14 +335,24 @@
     "log_response_body",
     help="Log http client response body.",
     default=False,
     is_flag=True,
     show_envvar=True,
 )
 @click.option(
+    "--log-body-lines",
+    "-lbl",
+    "log_body_lines",
+    default=connect.Http.LOG_BODY_LINES,
+    help="Number of lines to log from request/response body.",
+    type=click.INT,
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
     "--log-hide-secrets/--no-log-hide-secrets",
     "-lhs/-nlhs",
     "log_hide_secrets",
     default=True,
     help="Enable hiding of secrets in log output",
     is_flag=True,
     show_envvar=True,
@@ -245,49 +386,69 @@
     show_envvar=True,
 )
 @click.option(
     "--log-file-name",
     "-fn",
     "log_file_name",
     metavar="FILENAME",
-    default=LOG_FILE_NAME,
+    default=connect.LOG_FILE_NAME,
     help="Log file to save logs to if -f/--log-file supplied.",
     show_envvar=True,
     show_default=True,
 )
 @click.option(
-    "--log-file-path",
+    "--log-file-token",
     "-fp",
     "log_file_path",
     metavar="PATH",
-    default=LOG_FILE_PATH,
+    default=connect.LOG_FILE_PATH,
     help="Directory to use for -fn/--log-file-name (Defaults to current directory).",
     show_envvar=True,
 )
 @click.option(
     "--log-file-max-mb",
     "-fmb",
     "log_file_max_mb",
-    default=LOG_FILE_MAX_MB,
+    default=connect.LOG_FILE_MAX_MB,
     help="Rollover -fn/--log-file-name at this many megabytes.",
     type=click.INT,
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--log-file-max-files",
     "-fmf",
     "log_file_max_files",
-    default=LOG_FILE_MAX_FILES,
+    default=connect.LOG_FILE_MAX_FILES,
     help="Keep this many rollover logs.",
     type=click.INT,
     show_envvar=True,
     show_default=True,
 )
 @click.option(
+    "--log-hide-secrets/--no-log-hide-secrets",
+    "-lhs/-nlhs",
+    "log_hide_secrets",
+    default=True,
+    help="Enable hiding of secrets in log output",
+    is_flag=True,
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
+    "--log-http-max/--no-log-http-max",
+    "-lmax/-nlmax",
+    "log_http_max",
+    default=connect.Connect.LOG_HTTP_MAX,
+    help=f"Shortcut to include_output http logging - overrides: {connect.Connect.HTTP_MAX_CLI}",
+    is_flag=True,
+    show_envvar=True,
+    show_default=True,
+)
+@click.option(
     "--proxy",
     "-p",
     "proxy",
     default="",
     help="Proxy to use to connect to Axonius.",
     metavar="PROXY",
     show_envvar=True,
@@ -363,69 +524,67 @@
     is_flag=True,
     show_envvar=True,
 )
 @click.option(
     "--timeout-connect",
     "-tc",
     "timeout_connect",
-    default=TIMEOUT_CONNECT,
+    default=connect.Http.CONNECT_TIMEOUT,
     help="Seconds to wait for connections to API",
     type=click.INT,
     show_envvar=True,
     show_default=True,
 )
 @click.option(
     "--timeout-response",
     "-tr",
     "timeout_response",
-    default=TIMEOUT_RESPONSE,
+    default=connect.Http.RESPONSE_TIMEOUT,
     help="Seconds to wait for responses from API",
     type=click.INT,
     show_default=True,
 )
 @click.option(
     "--credentials/--keys",
     "-creds/-keys",
     "credentials",
     default=False,
     help="Treat key as Username and secret as password",
     is_flag=True,
     show_envvar=True,
     show_default=True,
 )
-@click.option(
-    "--log-hide-secrets/--no-log-hide-secrets",
-    "-lhs/-nlhs",
-    "log_hide_secrets",
-    default=True,
-    help="Enable hiding of secrets in log output",
-    is_flag=True,
-    show_envvar=True,
-    show_default=True,
-)
 @click.version_option(version.__version__)
 @context.pass_context
 @click.pass_context
 def cli(click_ctx, ctx, quiet, **kwargs):
     """Command line interface for the Axonius API Client."""
+    # noinspection PyBroadException
     try:
         cli_args = sys.argv
     except Exception:  # pragma: no cover
         cli_args = "No sys.argv!"
-    LOG.debug(f"sys.argv: {cli_args}")
+    LOG.debug(f"sys.argv: {json_dump(cli_args)}")
+    LOG.debug(f"kwargs: {json_dump(kwargs)}")
     ctx._click_ctx = click_ctx
     ctx.QUIET = quiet
+    # noinspection PyProtectedMember
     ctx._connect_args.update(kwargs)
 
 
-cli.add_command(grp_adapters.adapters)
-cli.add_command(grp_assets.devices)
-cli.add_command(grp_assets.users)
-cli.add_command(grp_assets.vulnerabilities)
-cli.add_command(grp_system.system)
-cli.add_command(grp_tools.tools)
-cli.add_command(grp_openapi.openapi)
-cli.add_command(grp_certs.certs)
-cli.add_command(grp_enforcements.enforcements)
-cli.add_command(grp_spaces.spaces)
-cli.add_command(grp_folders.folders)
-cli.add_command(grp_account.account)
+GROUPS: t.List[click.Group] = [
+    grp_adapters.adapters,
+    grp_assets.devices,
+    grp_assets.users,
+    grp_assets.vulnerabilities,
+    grp_system.system,
+    grp_tools.tools,
+    grp_openapi.openapi,
+    grp_certs.certs,
+    grp_enforcements.enforcements,
+    grp_spaces.spaces,
+    grp_folders.folders,
+    grp_account.account,
+]
+
+for grp in GROUPS:
+    cli.add_command(grp)
```

## axonius_api_client/cli/context.py

```diff
@@ -1,109 +1,110 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 import importlib
 import pathlib
+import typing as t
 import warnings
-from typing import List, Optional, Tuple, Type, Union
 
 import click
-import requests
+import urllib3.exceptions
 
 from ..connect import Connect
+from ..constants.ctypes import PathLike
 from ..tools import (
     bom_strip,
     echo_debug,
     echo_error,
     echo_ok,
     echo_warn,
     extract_kvs_auto,
     is_str,
     json_load,
     listify,
     read_stream,
 )
 
-CONTEXT_SETTINGS = {"auto_envvar_prefix": "AX"}
-SSLWARN_CLS = requests.urllib3.exceptions.InsecureRequestWarning
-SSLWARN_MSG = """Unverified HTTPS request!
+CONTEXT_SETTINGS: dict = {"auto_envvar_prefix": "AX"}
+SSL_WARN_CLS: t.Type[Warning] = urllib3.exceptions.InsecureRequestWarning
+SSL_WARN_MSG: str = """Unverified HTTPS request!
 
 To enable certificate validation:
-  * Set the variable: AX_CERTPATH=/path/to/cert_or_ca_bundle
-  * Supply the option: -cp/--cert-path /path/to/cert_or_ca_bundle
+  * axonshell: Set the variable: AX_CERTPATH="/path/to/cert_or_ca_bundle"
+    or supply the option: -cp/--cert-path "/path/to/cert_or_ca_bundle"
+  * library: Connect(url=..., certpath="/path/to/cert_or_ca_bundle")
 
 To silence this message:
-  * Set the variable: AX_CERTWARN=n
-  * Supply the option: -ncw/--no-cert-warn
+  * axonshell: Set the variable: AX_CERTWARN="n" or supply the option: -ncw/--no-cert-warn
+  * library: Connect(url=..., certwarn=False)
 """
 
 
-def load_cmds(path, package, group):
-    """Pass."""
+def load_cmds(path: PathLike, package: str, group: click.Group):
+    """Load the commands for a given path."""
     path = pathlib.Path(path)
 
     for item in path.parent.glob("cmd_*.py"):
         module = importlib.import_module(f".{item.stem}", package=package)
         module_cmd = getattr(module, "cmd", None)
         if callable(module_cmd):
+            # noinspection PyTypeChecker
             group.add_command(module_cmd)
 
 
 class DictParam(click.ParamType):
     """Pass."""
 
     name = "dict_param"
 
     def __init__(self, split_kv: str = "="):
         """Pass."""
         self.split_kv = split_kv
 
-    def convert(self, value, param, ctx) -> Union[dict, Tuple[str, str]]:
+    def convert(self, value, param, ctx) -> t.Union[dict, t.Tuple[str, str]]:
         """Pass."""
         if isinstance(value, (str, bytes)):
             example = f"key1{self.split_kv}value1"
             example = f"\nExample: {example!r}"
             splitit = f"key/value split character {self.split_kv!r}"
             parts = value.split("=", 1)
             if len(parts) != 2:
                 self.fail(f"Missing {splitit} in value {value!r}{example}", param, ctx)
-
-            skey, svalue = parts
-            skey = skey.strip()
-            if not skey:
+            s_key, s_val = parts
+            s_key = s_key.strip()
+            if not s_key:
                 self.fail(f"Missing key before {splitit} in value {value!r}{example}", param, ctx)
-
-            return skey, svalue
+            return s_key, s_val
         return value
 
 
 class DictOption(click.Option):
-    """Pass."""
+    """Custom dictionary type option for Click."""
 
     param_type_name = "dict_option"
     split_kv: str = "="
-    constructor: Type = dict
-    help_post: List[str] = None
+    constructor: t.Type[t.Mapping] = dict
+    help_post: t.List[str] = None
 
     def __init__(self, *args, **kwargs):
         """Pass."""
         self.split_kv = kwargs.pop("split_kv", self.split_kv)
         self.help_post = listify(kwargs.pop("help_post", self.help_post_default))
         self.constructor = kwargs.pop("constructor", self.constructor)
 
-        help = kwargs.get("help", "")
+        help_str = kwargs.get("help", "")
         kwargs["type"] = DictParam(split_kv=self.split_kv)
         kwargs["multiple"] = True
         kwargs["is_flag"] = False
-        kwargs["help"] = "  ".join([x for x in [help, *self.help_post] if is_str(x)])
+        kwargs["help"] = "  ".join([x for x in [help_str, *self.help_post] if is_str(x)])
         kwargs.setdefault("show_envvar", True)
         kwargs.setdefault("show_default", False)
         super().__init__(*args, **kwargs)
 
     @property
-    def help_post_default(self) -> List[str]:
+    def help_post_default(self) -> t.List[str]:
         """Pass."""
         example = f"key1{self.split_kv}value1"
         example = f"Example: {example!r}"
         return [
             f"({example})",
             "(env var parsed as CSV unless starts with 'json:')",
             "(env var CSV delimiter uses ; instead of , if starts with 'semi:')",
@@ -113,25 +114,26 @@
     def to_info_dict(self):
         """Pass."""
         info_dict = super().to_info_dict()
         info_dict["split_kv"] = self.split_kv
         info_dict["constructor"] = self.constructor
         return info_dict
 
-    def type_cast_value(self, ctx, value) -> Optional[dict]:
+    def type_cast_value(self, ctx, value: t.Any) -> t.Optional[t.Mapping]:
         """Pass."""
         if isinstance(value, self.constructor):
             return value
 
         if value is not None:
             typed = [self.type(param=self, ctx=ctx, value=x) for x in value]
+            # noinspection PyArgumentList
             return self.constructor(typed)
         return value
 
-    def get_envvar(self, ctx) -> Optional[str]:
+    def get_envvar(self, ctx) -> t.Optional[str]:
         """Pass."""
         if (
             not self.envvar
             and self.allow_from_autoenv
             and ctx.auto_envvar_prefix is not None
             and self.name is not None
         ):
@@ -230,27 +232,29 @@
             :obj:`str`
 
         """
         return self.__str__()
 
     def echo_ok(self, msg, **kwargs):
         """Pass."""
-        if not getattr(self, "QUIET", False):
+        if not self.QUIET:
             echo_ok(msg=msg, **kwargs)
 
     def echo_debug(self, msg, **kwargs):  # pragma: no cover
         """Pass."""
-        if not getattr(self, "QUIET", False):
+        if not self.QUIET:
             echo_debug(msg=msg, **kwargs)
 
-    def echo_error(self, msg, abort=True, **kwargs):
+    @staticmethod
+    def echo_error(msg, abort=True, **kwargs):
         """Pass."""
         echo_error(msg=msg, abort=abort, **kwargs)
 
-    def echo_warn(self, msg, **kwargs):
+    @staticmethod
+    def echo_warn(msg, **kwargs):
         """Pass."""
         echo_warn(msg=msg, **kwargs)
 
     @property
     def wraperror(self):
         """Pass."""
         return self._connect_args.get("wraperror", True)
@@ -275,69 +279,108 @@
             self.create_client(url=url, key=key, secret=secret, **kwargs)
 
             with self.exc_wrap(wraperror=self.wraperror):
                 with warnings.catch_warnings(record=True) as caught_warnings:
                     self.client.start()
 
                 for caught_warning in caught_warnings:  # pragma: no cover
-                    wmsg = caught_warning.message
-                    is_ssl = isinstance(wmsg, SSLWARN_CLS)
-                    wmsg = SSLWARN_MSG if is_ssl else wmsg
-                    wmsg = format(wmsg)
-                    self.echo_warn(wmsg)
+                    warn_msg = caught_warning.message
+                    is_ssl = isinstance(warn_msg, SSL_WARN_CLS)
+                    warn_msg = SSL_WARN_MSG if is_ssl else warn_msg
+                    warn_msg = format(warn_msg)
+                    self.echo_warn(warn_msg)
 
             # warnings suck.
-            warnings.simplefilter("ignore", SSLWARN_CLS)
+            warnings.simplefilter("ignore", SSL_WARN_CLS)
 
             if echo:
                 self.echo_ok(msg=str(self.client))
 
             self.days_echo(
-                msg="Trial expires in {days} days!!!", days=self.client.instances.trial_days_left
+                msg="Trial expires in {days} days",
+                days=self.client.instances.trial_days_left,
+                days_info=45,
+                days_warn=30,
+                days_error=15,
             )
             self.days_echo(
-                msg="License expires in {days} days!!!",
+                msg="License expires in {days} days",
                 days=self.client.instances.license_days_left,
+                days_info=90,
+                days_warn=60,
+                days_error=30,
+            )
+            self.days_echo(
+                msg="SSL Certificate expires in {days} days",
+                days=self.client.ssl_days_left,
+                days_info=120,
+                days_warn=90,
+                days_error=60,
             )
-
         return self.client
 
-    def days_echo(self, msg, days, info=45, warn=30, error=15):  # pragma: no cover
-        """Pass."""
-        if not isinstance(days, int):
-            return
-
-        if days <= error:
-            self.echo_error(msg.format(days=days), abort=False)
-            return
+    def days_echo(
+        self,
+        msg: str,
+        days: t.Optional[t.Union[int, float]] = None,
+        days_info: t.Optional[t.Union[int, float]] = 45,
+        days_warn: t.Optional[t.Union[int, float]] = 30,
+        days_error: t.Optional[t.Union[int, float]] = 15,
+    ):
+        """Echo a message to console in various colors depending on the days.
 
-        if days <= warn:
-            self.echo_warn(msg.format(days=days))
-            return
+        Args:
+            msg: The message to echo.
+            days: The number of days.
+            days_info: The number of days to use for info color.
+            days_warn: The number of days to use for warn color.
+            days_error: The number of days to use for error color.
+        """
+        try:
+            msg = msg.format(**locals())
+        except KeyError:
+            pass
+
+        bits = [
+            f"error<={days_error}",
+            f"warn<={days_warn}",
+            f"info<={days_info}",
+            f"debug=None or > info",
+        ]
+        bits = ", ".join(bits)
+        msg = f"{msg} ({bits})"
+        method = self.echo_debug
+
+        if isinstance(days, (int, float)):
+            if isinstance(days_error, (int, float)) and days <= days_error:
+                method = self.echo_error
+            elif isinstance(days_warn, (int, float)) and days <= days_warn:
+                method = self.echo_warn
+            elif isinstance(days_info, (int, float)) and days <= days_info:
+                method = self.echo_ok
 
-        if days <= info:
-            self.echo_ok(msg.format(days=days))
-            return
+        method(msg=msg, abort=False)
 
-    def read_stream(self, stream, strip_bom=True):
+    def read_stream(self, stream, strip_bom: bool = True) -> t.Optional[str]:
         """Pass."""
         try:
             content = read_stream(stream=stream)
         except Exception as exc:
             self.echo_error(msg=f"Unable to read from input stream: {exc}", abort=True)
-        if strip_bom:
-            content = bom_strip(content=content)
-        return content
+        else:
+            if strip_bom:
+                content = bom_strip(content=content)
+            return content
 
     def read_stream_json(
         self,
         stream,
-        expect: Optional[type] = None,
-        expect_items: Optional[type] = None,
-        items_min: Optional[int] = None,
+        expect: t.Optional[type] = None,
+        expect_items: t.Optional[type] = None,
+        items_min: t.Optional[int] = None,
     ):
         """Pass."""
         content = self.read_stream(stream=stream)
 
         try:
             content = json_load(obj=content, error=True)
         except Exception as exc:  # pragma: no cover
```

## axonius_api_client/cli/helps.py

```diff
@@ -1,10 +1,11 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 from ..constants.wizards import Docs
+from ..constants.asset_helpers import ASSETS_HELPERS
 
 HELPSTR_AUTH = """
 Detailed help for authentication:
 
 --url: URL of an Axonius instance
 
     This can be an IP address, a hostname, or a fully qualified url (and can
@@ -222,7 +223,8 @@
 HELPSTRS["auth"] = HELPSTR_AUTH
 HELPSTRS["assetexport"] = HELPSTR_EXPORT_ASSET
 HELPSTRS["selectfields"] = HELPSTR_SELECT_FIELDS
 HELPSTRS["query"] = HELPSTR_QUERY
 HELPSTRS["wizard"] = Docs.TEXT
 HELPSTRS["wizard_csv"] = Docs.CSV
 HELPSTRS["multiple_cnx_json"] = HELPSTR_MULTI_CNX_JSON
+HELPSTRS["asset_helper"] = ASSETS_HELPERS.to_str()
```

## axonius_api_client/cli/options.py

```diff
@@ -93,29 +93,29 @@
 AUTH = [
     URL,
     click.option(
         "--key",
         "-k",
         "key",
         required=True,
-        help="API Key of user in an Axonius instance",
+        help="API Key (or username if credentials=True) of user in an Axonius instance",
         metavar="KEY",
-        prompt="API Key of user",
+        prompt="API Key (or username if credentials=True) of user",
         hide_input=True,
         show_envvar=True,
         show_default=True,
     ),
     click.option(
         "--secret",
         "-s",
         "secret",
         required=True,
-        help="API Secret of user in an Axonius instance",
+        help="API Secret (or password if credentials=True) of user in an Axonius instance",
         metavar="SECRET",
-        prompt="API Secret of user",
+        prompt="API Secret (or password if credentials=True) of user",
         hide_input=True,
         show_envvar=True,
         show_default=True,
     ),
 ]
 
 SQ_NAME = click.option(
```

## axonius_api_client/cli/grp_account/__init__.py

```diff
@@ -1,17 +1,25 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
+import typing as t
+
 import click
 
 from ..context import AliasedGroup, load_cmds
 from ..grp_tools import cmd_signup, cmd_use_token_reset_token, cmd_write_config
 
 
 @click.group(cls=AliasedGroup)
 def account():
     """Group: Account commands."""
 
 
 load_cmds(path=__file__, package=__package__, group=account)
-account.add_command(cmd_write_config.cmd)
-account.add_command(cmd_signup.cmd)
-account.add_command(cmd_use_token_reset_token.cmd)
+
+COMMANDS: t.List[t.Any] = [
+    cmd_write_config.cmd,
+    cmd_signup.cmd,
+    cmd_use_token_reset_token.cmd,
+]
+
+for cmd in COMMANDS:
+    account.add_command(cmd)
```

## axonius_api_client/cli/grp_adapters/__init__.py

```diff
@@ -1,15 +1,22 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
+import typing as t
+
 import click
 
 from ..context import AliasedGroup, load_cmds
 from . import grp_cnx
 
 
 @click.group(cls=AliasedGroup)
 def adapters():
     """Group: Work with adapters and adapter connections."""
 
 
 load_cmds(path=__file__, package=__package__, group=adapters)
-adapters.add_command(grp_cnx.cnx)
+
+GROUPS: t.List[t.Any] = [grp_cnx.cnx]
+# Type hint doesn't recognize click.Group because of decorator?
+
+for grp in GROUPS:
+    adapters.add_command(grp)
```

## axonius_api_client/cli/grp_adapters/grp_cnx/parsing.py

```diff
@@ -325,15 +325,15 @@
     @classmethod
     def load_types(cls, schemas: List[dict], **kwargs) -> List["Schema"]:
         """Load a list of schemas into their corresponding Schema objects."""
         return [cls.load_type(schema=x, **kwargs) for x in schemas]
 
     @classmethod
     def get_types(cls) -> Dict[str, Type]:
-        """Get all sub classes mapped by type key."""
+        """Get all subclasses mapped by type key."""
         return {x.for_type(): x for x in get_subcls(cls)}
 
     def __str__(self) -> str:
         """Builtin."""
         return f"Schema {self.name!r}"
 
     def __repr__(self) -> str:
```

## axonius_api_client/cli/grp_assets/cmd_count.py

```diff
@@ -1,18 +1,20 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 from ..context import CONTEXT_SETTINGS, click
 from ..options import AUTH, QUERY, add_options, get_option_help
-from .grp_common import HISTORY, WIZ, load_wiz
+from .grp_common import HISTORY, OPT_USE_CACHE_ENTRY, OPT_USE_HEAVY_FIELDS_COLLECTION, WIZ, load_wiz
 
 OPTIONS = [
     *AUTH,
+    *HISTORY,
+    OPT_USE_CACHE_ENTRY,
+    OPT_USE_HEAVY_FIELDS_COLLECTION,
     *QUERY,
     *WIZ,
-    *HISTORY,
     get_option_help(choices=["auth", "query"]),
 ]
 
 
 @click.command(name="count", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
 @click.pass_context
```

## axonius_api_client/cli/grp_assets/cmd_count_by_saved_query.py

```diff
@@ -1,16 +1,18 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 from ..context import CONTEXT_SETTINGS, click
 from ..options import AUTH, SQ_NAME, add_options, get_option_help
-from .grp_common import HISTORY
+from .grp_common import HISTORY, OPT_USE_CACHE_ENTRY, OPT_USE_HEAVY_FIELDS_COLLECTION
 
 OPTIONS = [
     *AUTH,
     *HISTORY,
+    OPT_USE_CACHE_ENTRY,
+    OPT_USE_HEAVY_FIELDS_COLLECTION,
     SQ_NAME,
     get_option_help(choices=["auth"]),
 ]
 
 
 @click.command(name="count-by-saved-query", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
```

## axonius_api_client/cli/grp_assets/cmd_get.py

```diff
@@ -19,15 +19,17 @@
     *OPTS_EXPORT,
     *GET_EXPORT,
     *FIELDS_SELECT,
     get_option_fields_default(default=True),
     *HISTORY,
     *QUERY,
     *WIZ,
-    get_option_help(choices=["auth", "query", "assetexport", "selectfields", "wizard"]),
+    get_option_help(
+        choices=["auth", "query", "assetexport", "selectfields", "wizard", "asset_helper"]
+    ),
 ]
 
 
 @click.command(name="get", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
 @click.pass_context
 def cmd(ctx, url, key, secret, query_file, wizard_content, whitelist=None, **kwargs):
```

## axonius_api_client/cli/grp_assets/cmd_get_by_saved_query.py

```diff
@@ -1,23 +1,24 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 from ..context import CONTEXT_SETTINGS, click
 from ..options import AUTH, FIELDS_SELECT, PAGING, SQ_NAME, add_options, get_option_help
-from .grp_common import GET_EXPORT, HISTORY, OPTS_EXPORT, load_whitelist
+from .grp_common import GET_EXPORT, HISTORY, OPTS_EXPORT, OPTS_GET_BY_SQ, load_whitelist
 
 METHOD = "get-by-saved-query"
 OPTIONS = [
     *AUTH,
     *PAGING,
     *OPTS_EXPORT,
     *GET_EXPORT,
     *FIELDS_SELECT,
     *HISTORY,
+    *OPTS_GET_BY_SQ,
     SQ_NAME,
-    get_option_help(choices=["auth", "assetexport", "selectfields"]),
+    get_option_help(choices=["auth", "assetexport", "selectfields", "asset_helper"]),
 ]
 
 
 @click.command(name="get-by-saved-query", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
 @click.pass_context
 def cmd(ctx, url, key, secret, whitelist=None, **kwargs):
```

## axonius_api_client/cli/grp_assets/grp_common.py

```diff
@@ -1,11 +1,13 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 from ... import DEFAULT_PATH
 from ...api import asset_callbacks
+from ...api.asset_callbacks.base import ARG_DESCRIPTIONS
+from ...constants.asset_helpers import ASSETS_HELPERS
 from ...constants.wizards import Results, Types
 from ...tools import echo_error, path_read
 from ..context import CONTEXT_SETTINGS, SplitEquals, click
 from ..options import (
     AUTH,
     FIELDS_SELECT,
     OPT_EXPORT_BACKUP,
@@ -35,14 +37,99 @@
     default=DEFAULT_PATH,
     help=f"If --export-file supplied, the directory to write --export_file to {TEMPLATES}",
     type=click.Path(exists=False, resolve_path=True),
     show_envvar=True,
     show_default=True,
 )
 OPTS_EXPORT = [OPT_EXPORT_FILE, OPT_EXPORT_PATH, OPT_EXPORT_OVERWRITE, OPT_EXPORT_BACKUP]
+OPT_INCLUDE_FIELDS = click.option(
+    "--include-fields/--no-include-fields",
+    "-if/-nif",
+    "include_fields",
+    default=True,
+    help=f"Include fields from the saved query {ASSETS_HELPERS.fields.to_str_short()}",
+    show_envvar=True,
+    show_default=True,
+)
+OPT_INCLUDE_EXCLUDED_ADAPTERS = click.option(
+    "--include-excluded-adapters/--no-include-excluded-adapters",
+    "-iea/-niea",
+    "include_exclude_adapters",
+    default=True,
+    help=(
+        "Include column filters for excluded adapters from the saved query"
+        f" {ASSETS_HELPERS.excluded_adapters.to_str_short()}"
+    ),
+    show_envvar=True,
+    show_default=True,
+)
+OPT_INCLUDE_ASSET_EXCLUDED_ADAPTERS = click.option(
+    "--include-asset-excluded-adapters/--no-include-asset-excluded-adapters",
+    "-iaea/-niaea",
+    "include_asset_exclude_adapters",
+    default=True,
+    help=(
+        "Include column filters for asset excluded adapters fields from the saved query"
+        f" {ASSETS_HELPERS.excluded_adapters.to_str_short()}"
+    ),
+    show_envvar=True,
+    show_default=True,
+)
+OPT_INCLUDE_FIELD_FILTERS = click.option(
+    "--include-field-filters/--no-include-field-filters",
+    "-iff/-niff",
+    "include_field_filters",
+    default=True,
+    help=(
+        "Include column filters for field filters from the saved query"
+        f" {ASSETS_HELPERS.field_filters.to_str_short()}"
+    ),
+    show_envvar=True,
+    show_default=True,
+)
+OPT_INCLUDE_ASSET_FILTERS = click.option(
+    "--include-asset-filters/--no-include-asset-filters",
+    "-iaf/-niaf",
+    "include_asset_filters",
+    default=True,
+    help=(
+        "Include column filters for asset filters from the saved query"
+        f" {ASSETS_HELPERS.asset_filters.to_str_short()}"
+    ),
+    show_envvar=True,
+    show_default=True,
+)
+OPT_USE_CACHE_ENTRY = click.option(
+    "--use-cache-entry/--no-use-cache-entry",
+    "-uce/-nuce",
+    "use_cache_entry",
+    default=False,
+    help="Ask the API to use a cache entry for this query, if available",
+    show_envvar=True,
+    show_default=True,
+)
+OPT_USE_HEAVY_FIELDS_COLLECTION = click.option(
+    "--use-heavy-fields-collection/--no-use-heavy-fields-collection",
+    "-uhfc/-nuhfc",
+    "use_heavy_fields_collection",
+    default=False,
+    help="Ask the API to use a heavy fields collection for this query",
+    show_envvar=True,
+    show_default=True,
+)
+
+
+OPTS_GET_BY_SQ = [
+    OPT_INCLUDE_FIELDS,
+    OPT_INCLUDE_EXCLUDED_ADAPTERS,
+    OPT_INCLUDE_ASSET_EXCLUDED_ADAPTERS,
+    OPT_INCLUDE_FIELD_FILTERS,
+    OPT_INCLUDE_ASSET_FILTERS,
+    OPT_USE_CACHE_ENTRY,
+]
 
 HISTORY = [
     click.option(
         "--history-date",
         "-hd",
         "history_date",
         default=None,
@@ -75,41 +162,102 @@
 
 def wiz_callback(ctx, param, value):
     """Pass."""
     contents = []
     if value:
         for i in value:
             etype = i[0].strip().lower()
-            if etype not in Types.CLI:  # pragma: no cover
+            if etype not in Types.CLI:
                 types = ", ".join(Types.CLI)
                 echo_error(msg=f"Invalid expression type {etype!r}, valid types: {types}")
             expr = i[1]
-            if etype == Types.FILE:  # pragma: no cover
+            if etype == Types.FILE:
                 contents += path_read(obj=expr)[1].strip().splitlines()
+            elif etype == Types.LINES:
+                contents += expr.strip().splitlines()
             else:
                 contents.append(f"{etype} {expr}")
     return "\n".join(contents)
 
 
 WIZ = [
     click.option(
         "--wiz",
         "-wz",
         "wizard_content",
-        help="Build a query using an expression (multiples, will override --query)",
+        help=(
+            "Build a query using an expression (multiples, will override --query). "
+            '--wiz "file" "<token>" - Read expressions from a file. '
+            '--wiz "lines" "simple expr1<CR>simple expr2" - Read multiple '
+            "expressions from a string. "
+            '--wiz "simple" "<expr>" - Simple expression. '
+            '--wiz "complex" "<expr>" - Complex expression. '
+        ),
         nargs=2,
         multiple=True,
         default=[],
         show_envvar=True,
         hidden=False,
         callback=wiz_callback,
-        metavar='TYPE "EXPRESSION"',
+        metavar=f'"{"|".join(Types.CLI)}" "EXPRESSION"',
+    ),
+]
+OPTS_GET_API = [
+    # 2023-04-22
+    click.option(
+        "--api-null-for-non-exist/--no-api-null-for-non-exist",
+        "-anfne/-nanfne",
+        "null_for_non_exist",
+        help="Ask the REST API to return null for non existent fields",
+        is_flag=True,
+        default=False,
+        show_envvar=True,
+        show_default=True,
+    ),
+    click.option(
+        "--api-filter-out-non-existing-fields/--no-api-filter-out-non-existing-fields",
+        "-afonef/-nafonef",
+        "filter_out_non_existing_fields",
+        help="Ask the REST API to filter out non existent fields",
+        is_flag=True,
+        default=True,
+        show_envvar=True,
+        show_default=True,
+    ),
+    click.option(
+        "--api-max-field-items",
+        "-amfi",
+        "max_field_items",
+        help="Ask the REST API to limit the number of values returned for a field",
+        default=None,
+        show_envvar=True,
+        show_default=True,
+        type=click.INT,
+    ),
+    click.option(
+        "--api-complex-fields-preview-limit",
+        "-amcfi",
+        "complex_fields_preview_limit",
+        help="Ask the REST API to limit the number of values returned for a complex field",
+        default=None,
+        show_envvar=True,
+        show_default=True,
+        type=click.INT,
+    ),
+    click.option(
+        "--api-use-heavy-fields-collection/--no-api-use-heavy-fields-collection",
+        "-ahfc/-nahfc",
+        "use_heavy_fields_collection",
+        help="Ask the REST API to use the heavy fields collection",
+        is_flag=True,
+        default=False,
+        show_envvar=True,
+        show_default=True,
     ),
 ]
-
 GET_EXPORT = [
     click.option(
         "--echo/--no-echo",
         "do_echo",
         default=True,
         help="Print out details during fetch",
         show_envvar=True,
@@ -379,14 +527,45 @@
         "-sf",
         "sort_field",
         help="Sort assets based on a specific field",
         default=None,
         show_envvar=True,
         show_default=True,
     ),
+    click.option(
+        "--csv-field-flatten/--no-csv-field-flatten",
+        "csv_field_flatten",
+        default=asset_callbacks.Csv.args_map()["csv_field_flatten"],
+        help=ARG_DESCRIPTIONS["csv_field_flatten"],
+        show_envvar=True,
+        show_default=True,
+        is_flag=True,
+        hidden=False,
+    ),
+    click.option(
+        "--csv-field-join/--no-csv-field-join",
+        "csv_field_join",
+        default=asset_callbacks.Csv.args_map()["csv_field_join"],
+        help=ARG_DESCRIPTIONS["csv_field_join"],
+        show_envvar=True,
+        show_default=True,
+        is_flag=True,
+        hidden=False,
+    ),
+    click.option(
+        "--csv-field-null/--no-csv-field-null",
+        "csv_field_null",
+        default=asset_callbacks.Csv.args_map()["csv_field_null"],
+        help=ARG_DESCRIPTIONS["csv_field_null"],
+        show_envvar=True,
+        show_default=True,
+        is_flag=True,
+        hidden=False,
+    ),
+    *OPTS_GET_API,
 ]
 
 GET_BUILDERS = [
     *AUTH,
     *PAGING,
     *OPTS_EXPORT,
     *GET_EXPORT,
@@ -513,17 +692,21 @@
         with ctx.obj.exc_wrap(wraperror=ctx.obj.wraperror):
             apimethod(**kwargs)
 
     cmd.__doc__ = doc
     return cmd
 
 
-def load_wiz(apiobj, wizard_content, kwargs, exprs=False):
+def load_wiz(apiobj, wizard_content, kwargs, wizard_file=None, exprs=False):
     """Pass."""
+    result = None
+    if wizard_file:
+        result = apiobj.wizard_file.parse_path(path=wizard_file)
     if wizard_content:
         result = apiobj.wizard_text.parse(content=wizard_content)
+    if result:
         query = result[Results.QUERY]
         click.secho(f"Wizard built a query: {query}", err=True, fg="green")
         kwargs["query"] = query
         if exprs:
             kwargs["expressions"] = result[Results.EXPRS]
     return kwargs
```

## axonius_api_client/cli/grp_certs/grp_common.py

```diff
@@ -1,13 +1,13 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 import click
 
-from ... import INIT_DOTENV, cert_human
-from ...parsers.url_parser import UrlParser
+from ... import INIT_DOTENV
+from ...projects import cert_human, url_parser
 from ...setup_env import KEY_CERTPATH, set_env
 from ...tools import echo_ok, echo_warn, get_path, json_dump, path_write
 
 
 def export_json(data, **kwargs):
     """Pass."""
 
@@ -279,15 +279,15 @@
         echo_ok(f"{intm_cert}")
     click.secho("")
     return leaf_cert, intm_certs
 
 
 def from_url(url, split=True, ca_only=False):
     """Pass."""
-    url_parsed = UrlParser(url=url, default_scheme="https")
+    url_parsed = url_parser.UrlParser(url=url, default_scheme="https")
     echo_ok(f"Parsed {url} to {url_parsed}")
     chain = cert_human.Cert.from_requests_chain(url=url_parsed.url)
     leaf_cert, intm_certs = split_leaf(chain=chain)
     if ca_only:
         return [x for x in chain if x.is_certificate_authority]
     if split:
         return leaf_cert, intm_certs
```

## axonius_api_client/cli/grp_enforcements/__init__.py

```diff
@@ -1,13 +1,18 @@
 # -*- coding: utf-8 -*-
 """Command line interface for Axonius API Client."""
 import click
+import typing as t
 
 from ..context import AliasedGroup, load_cmds
+from . import grp_tasks
 
 
 @click.group(cls=AliasedGroup)
 def enforcements():
     """Group: Work with the Enforcement Center."""
 
 
 load_cmds(path=__file__, package=__package__, group=enforcements)
+GROUPS: t.List[t.Any] = [grp_tasks.tasks]
+for grp in GROUPS:
+    enforcements.add_command(grp)
```

## axonius_api_client/cli/grp_enforcements/cmd_create.py

```diff
@@ -3,16 +3,14 @@
 from ..context import CONTEXT_SETTINGS, click
 from ..grp_folders.grp_options import OPTS_OBJECT_CREATE
 from ..options import AUTH, add_options
 from .grp_common import EXPORT_FORMATS, OPT_EXPORT_FORMAT, OPTS_CREATE
 
 OPTIONS = [*AUTH, OPT_EXPORT_FORMAT, *OPTS_CREATE, *OPTS_OBJECT_CREATE]
 
-# XXX test
-
 
 @click.command(name="create", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
 @click.pass_context
 def cmd(ctx, url, key, secret, export_format, main_action_config, **kwargs):
     """Create an Enforcement Set."""
     client = ctx.obj.start_client(url=url, key=key, secret=secret)
```

## axonius_api_client/cli/grp_enforcements/grp_common.py

```diff
@@ -274,16 +274,15 @@
         show_default=True,
     ),
     click.option(
         "--query-name",
         "-qn",
         "query_name",
         help="Name of Saved Query to use for trigger",
-        default=EnforcementDefaults.query_name,
-        required=False,
+        required=True,
         show_envvar=True,
         show_default=True,
     ),
     click.option(
         "--query-type",
         "-qt",
         "query_type",
```

## axonius_api_client/cli/grp_system/grp_discover/cmd_is_running.py

```diff
@@ -7,15 +7,15 @@
 OPTIONS = [*AUTH, *OPTS_EXPORT]
 
 
 @click.command(name="is-running", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
 @click.pass_context
 def cmd(ctx, url, key, secret, export_format, phases, progress):
-    """Return exit code 0 if discover is running, 1 if it is not."""
+    """Return exit code 0 if discovery is running, 1 if it is not."""
     client = ctx.obj.start_client(url=url, key=key, secret=secret)
 
     with ctx.obj.exc_wrap(wraperror=ctx.obj.wraperror):
         data = client.dashboard.get()
 
     click.secho(EXPORT_FORMATS[export_format](data=data, phases=phases, progress=progress))
     ctx.obj.echo_ok(f"Is running: {data.is_running}")
```

## axonius_api_client/cli/grp_system/grp_settings/cmd_get_subsection.py

```diff
@@ -8,15 +8,15 @@
 OPTIONS = [*AUTH, OPT_EXPORT_FORMAT, OPT_SECTION, OPT_SUB_SECTION]
 
 
 @click.command(name="get-sub-section", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
 @click.pass_context
 def cmd(ctx, url, key, secret, section, sub_section, export_format, **kwargs):
-    """Get settings for a sub-section."""
+    """Get settings for a subsection."""
     client = ctx.obj.start_client(url=url, key=key, secret=secret)
 
     apiname = ctx.parent.command.name.replace("-", "_")
     apiobj = getattr(client, apiname)
 
     with ctx.obj.exc_wrap(wraperror=ctx.obj.wraperror):
         settings = apiobj.get_sub_section(section=section, sub_section=sub_section)
```

## axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection.py

```diff
@@ -17,15 +17,15 @@
     key,
     secret,
     config,
     section,
     sub_section,
     export_format,
 ):
-    """Update a sub-section from arguments."""
+    """Update a subsection from arguments."""
     client = ctx.obj.start_client(url=url, key=key, secret=secret)
     config = dict(config)
 
     apiname = ctx.parent.command.name.replace("-", "_")
     apiobj = getattr(client, apiname)
 
     with ctx.obj.exc_wrap(wraperror=ctx.obj.wraperror):
```

## axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection_from_json.py

```diff
@@ -18,15 +18,15 @@
     secret,
     input_file,
     section,
     sub_section,
     export_format,
     **kwargs,
 ):
-    """Update a sub-section from a JSON file."""
+    """Update a subsection from a JSON file."""
     client = ctx.obj.start_client(url=url, key=key, secret=secret)
     new_config = ctx.obj.read_stream_json(stream=input_file, expect=dict)
 
     apiname = ctx.parent.command.name.replace("-", "_")
     apiobj = getattr(client, apiname)
 
     with ctx.obj.exc_wrap(wraperror=ctx.obj.wraperror):
```

## axonius_api_client/cli/grp_system/grp_users/cmd_get_by_name.py

```diff
@@ -23,14 +23,14 @@
 ]
 
 
 @click.command(name="get-by-name", context_settings=CONTEXT_SETTINGS)
 @add_options(OPTIONS)
 @click.pass_context
 def cmd(ctx, url, key, secret, name, **kwargs):
-    """Get a specific user by user name."""
+    """Get a specific user by username."""
     client = ctx.obj.start_client(url=url, key=key, secret=secret)
 
     with ctx.obj.exc_wrap(wraperror=ctx.obj.wraperror):
         data = client.system_users.get_by_name(name=name)
 
     handle_export(ctx=ctx, data=data, **kwargs)
```

## axonius_api_client/cli/grp_tools/cmd_shell.py

```diff
@@ -8,91 +8,67 @@
 import axonius_api_client as axonapi
 
 from ...constants.general import PY36
 from ...tools import echo_error, json_reload, pathlib
 from ..context import CONTEXT_SETTINGS
 from ..options import AUTH, add_options
 
-SHELL_BANNER = """Welcome human. We have some refreshments available for you:
-
+HELP: str = """
+Local variables available:
     - axonapi: API client package itself
     - client/c: API Client connection object
     - ctx: Click context object
     - jdump/j: Helper function to pretty print python objects
+    - j(HELP): this message
 
-API Objects:
+Local variables as shortcuts from client properties:
     - activity_logs/al: Work with activity logs
     - adapters/a: Work with adapters and adapter connections
     - dashboard/db: Work with discovery cycle
     - dashboard_spaces/dbs: Work with dashboard spaces
     - data_scopes/ds: Work with data scopes
     - devices/d: Work with device assets
     - folders/f: Work with folders
     - instances/i: Work with instances
     - meta/m: Work with instance metadata
     - remote_support/rs: Work with configuring system remote support
     - settings_global/sgl: Work with Global system settings
     - settings_gui/sgu: Work with GUI system settings
-    - settings_lifecycle/sl: Work with Lifecyle system settings
+    - settings_lifecycle/sl: Work with lifecycle system settings
     - settings_ip/sip: Work with Identity Provider system settings
+    - enforcements/e: Work with enforcements   
     - system_roles/sr: Work with system roles
     - system_users/su: Work with system users
     - users/u: Work with user assets
     - openapi/oas: Work with the OpenAPI specification file
     - vulnerabilities/v: Work with vulnerability assets
-
 """
 
+SHELL_BANNER = f"Welcome human. We have some refreshments available for you:\n{HELP}"
+
 SHELL_EXIT = """Goodbye human. We hope you enjoyed your stay."""
 
 HISTPATH = os.path.expanduser("~")
 
 HISTFILE = ".python_history"
 
 
 def jdump(data):
     """Pass."""
     print(json_reload(data))
 
 
-@click.command(name="shell", context_settings=CONTEXT_SETTINGS)
+@click.command(name="shell", context_settings=CONTEXT_SETTINGS, epilog=f"\b\n\n{HELP}")
 @add_options(AUTH)
 @click.pass_context
 def cmd(ctx, url, key, secret):  # noqa: D301
-    """Start an interactive python shell.
+    f"""Start an interactive python shell with Axonius API Client loaded.
 
     The shell will authenticate to Axonius, setup autocompletion, enable history,
     and create the following objects:
-
-    \b
-        - axonapi: API Client package itself
-        - client/c: API Client connection object
-        - ctx: Click context object
-        - jdump/j: Helper function to pretty print python objects
-
-        - activity_logs/al: Work with activity logs
-        - adapters/a: Work with adapters and adapter connections
-        - dashboard/db: Work with discovery cycle
-        - dashboard_spaces/dbs: Work with dashboard spaces
-        - data_scopes/ds: Work with data scopes
-        - devices/d: Work with device assets
-        - folders/f: Work with folders
-        - instances/i: Work with instances
-        - meta/m: Work with instance metadata
-        - remote_support/rs: Work with configuring system remote support
-        - settings_global/sgl: Work with Global system settings
-        - settings_gui/sgu: Work with GUI system settings
-        - settings_lifecycle/sl: Work with Lifecyle system settings
-        - settings_ip/sip: Work with Identity Provider system settings
-        - system_roles/sr: Work with system roles
-        - system_users/su: Work with system users
-        - users/u: Work with user assets
-        - openapi/oas: Work with the OpenAPI specification file
-        - vulnerabilities/v: Work with vulnerability assets
-
     """
     client = ctx.obj.start_client(url=url, key=key, secret=secret, save_history=True)
 
     client.HTTP.save_history = True
 
     shellvars = {
         "activity_logs": client.activity_logs,
@@ -137,55 +113,57 @@
         "sgu": client.settings_gui,
         "sip": client.settings_ip,
         "sl": client.settings_lifecycle,
         "sr": client.system_roles,
         "su": client.system_users,
         "u": client.users,
         "v": client.vulnerabilities,
+        "HELP": HELP,
     }
 
     spawn_shell(shellvars)
 
 
 def write_hist_file():
     """Pass."""
     try:
         import readline
 
-        histpath = pathlib.Path(HISTPATH)
-        histfile = histpath / HISTFILE
+        hist_path = pathlib.Path(HISTPATH)
+        hist_file = hist_path / HISTFILE
 
-        histpath.mkdir(mode=0o700, exist_ok=True)
-        histfile.touch(mode=0o600, exist_ok=True)
+        hist_path.mkdir(mode=0o700, exist_ok=True)
+        hist_file.touch(mode=0o600, exist_ok=True)
 
-        readline.write_history_file(format(histfile))
+        readline.write_history_file(format(hist_file))
     except Exception as exc:  # pragma: no cover
         msg = f"Unable to import readline! {exc}"
         echo_error(msg, abort=False)
 
 
 def register_readline(shellvars=None):
     """Pass."""
     shellvars = shellvars or {}
 
-    histpath = pathlib.Path(HISTPATH)
-    histfile = histpath / HISTFILE
+    hist_path = pathlib.Path(HISTPATH)
+    hist_file = hist_path / HISTFILE
 
-    histpath.mkdir(mode=0o700, exist_ok=True)
-    histfile.touch(mode=0o600, exist_ok=True)
+    hist_path.mkdir(mode=0o700, exist_ok=True)
+    hist_file.touch(mode=0o600, exist_ok=True)
 
     try:
         try:
             import readline
-        except Exception:  # pragma: no cover
+        except ImportError:  # pragma: no cover
+            # noinspection PyUnresolvedReferences
             import pyreadline as readline
 
         import rlcompleter
 
-        readline.read_history_file(format(histfile))
+        readline.read_history_file(format(hist_file))
         atexit.register(write_hist_file)
 
         readline.set_completer(rlcompleter.Completer(shellvars).complete)
 
         readline_doc = getattr(readline, "__doc__", "")
         is_libedit = readline_doc and "libedit" in readline_doc
```

## axonius_api_client/constants/__init__.py

```diff
@@ -1,16 +1,17 @@
 # -*- coding: utf-8 -*-
 """Constants."""
 from ..setup_env import load_dotenv
-from . import adapters, api, ctypes, fields, general, logs, tables, wizards
+from . import adapters, api, asset_helpers, ctypes, fields, general, logs, tables, wizards
 
 __all__ = (
     "adapters",
     "api",
     "fields",
     "general",
     "logs",
     "wizards",
     "load_dotenv",
     "tables",
     "ctypes",
+    "asset_helpers",
 )
```

## axonius_api_client/constants/adapters.py

```diff
@@ -63,35 +63,35 @@
 #     ("id", "ID", 0),
 #     ("uuid", "UUID", 0),
 #     ("working", "Working", 0),
 #     ("error", "Error", 20),
 #     ("connection_label", "Connection Label", 0),
 #     ("schemas", None, 0),
 # ]
-# """Tablize map of field name to user friendly title for adapter connections."""
+# """Tablize map of field name to user-friendly title for adapter connections."""
 
 # KEY_MAP_ADAPTER: List[Tuple[str, Optional[str], int]] = [
 #     ("name", "Name", 0),
 #     ("node_name", "Node", 0),
 #     ("cnx_count_total", "Connections", 0),
 #     ("cnx_count_broken", "Broken", 0),
 #     ("cnx_count_working", "Working", 0),
 #     ("cnx_count_inactive", "Inactive", 0),
 # ]
-# """Tablize map of field name to user friendly title for adapters."""
+# """Tablize map of field name to user-friendly title for adapters."""
 
 # KEY_MAP_SCHEMA: List[Tuple[str, Optional[str], int]] = [
 #     ("name", "Name", 0),
 #     ("title", "Title", 30),
 #     ("type", "Type", 0),
 #     ("required", "Required", 0),
 #     ("default", "Default", 0),
 #     ("description", "Description", 20),
 #     ("format", "Format", 0),
 # ]
-# """Tablize map of field name to user friendly title for config schemas."""
+# """Tablize map of field name to user-friendly title for config schemas."""
 
 CNX_GONE: str = "Server is already gone, please try again after refreshing the page"
 """Message to print when an adapter connection disappears"""
 
 CNX_RETRY: int = 15
 """Number of times to retry fetching a connection"""
```

## axonius_api_client/constants/api.py

```diff
@@ -56,14 +56,24 @@
 AS_DATACLASS: bool = False
 """Global default for returning objects as dataclass instead of dict."""
 
 BARRIER: str = "-" * 15
 ASSET_TMPL: str = "{k}: {v}"
 # ALL_ID: str = "all"
 REFRESH: bool = 60
+RE_PREFIX: str = "~"
+
+TASK_SLOW_WARNING = """
+
+Notice:
+  Fetching a page of tasks as "basic models" is fast, but fetching each task
+  individually to get the "full models" is quite slow.
+  Use as many filters as possible to minimize the number of "full models" that must be fetched.
+
+"""
 
 
 class FolderDefaults:
     """Pass."""
 
     all_objects: bool = False
     confirm: bool = False
@@ -79,15 +89,15 @@
     full_objects: bool = False
     full_objects_search: bool = True
     ignore_case: bool = True
     include_details: bool = False
     include_objects: bool = False
     include_subfolders: bool = False
     copy_prefix: str = "Copy of"
-    pattern_prefix: str = "~"
+    pattern_prefix: str = RE_PREFIX
     prompt: bool = False
     prompt_default: bool = False
     prompt_shell: bool = True
     query_type: str = "devices"
     recursive: bool = False
     refresh: int = REFRESH
     refresh_action: bool = True
```

## axonius_api_client/constants/ctypes.py

```diff
@@ -1,18 +1,32 @@
 # -*- coding: utf-8 -*-
 """Custom types."""
+import datetime
 import pathlib
 import typing as t
 
 PathLike: t.TypeVar = t.TypeVar("PathLike", pathlib.Path, str, bytes)
 PatternLike: t.TypeVar = t.TypeVar("PatternLike", t.Pattern, str, bytes)
-PatternLikeListy: t.Type = t.Union[PatternLike, t.List[PatternLike]]
+PatternLikeListy: t.Type = t.Union[PatternLike, t.Iterable[PatternLike]]
 ComplexLike: t.Tuple[t.Type, ...] = (dict, list, tuple)
 SimpleLike: t.Tuple[t.Type, ...] = (str, int, bool, float)
 Refreshables: t.Type = t.Optional[t.Union[str, bytes, int, float, bool]]
+TypeDate: t.TypeVar = t.TypeVar("TypeDate", str, bytes, datetime.datetime, datetime.timedelta)
+TypeDelta: t.TypeVar = t.TypeVar("TypeDelta", str, bytes, float, int, datetime.timedelta)
+TypeFloat: t.TypeVar = t.TypeVar("TypeFloat", float, int, str, bytes)
+TypeMatch: t.TypeVar = t.TypeVar(
+    "TypeMatch",
+    str,
+    bytes,
+    t.Pattern,
+    t.List[t.Union[str, bytes, t.Pattern]],
+    t.Tuple[t.Union[str, bytes, t.Pattern]],
+)
+TypeInt: t.TypeVar = t.TypeVar("TypeInt", int, str, bytes)
+TypeBool: t.TypeVar = t.TypeVar("TypeBool", bool, str, bytes, int, float)
 
 
 class FolderBase:
     """baseclass for all folder types."""
 
 
 # STR_RE = t.Union[str, t.Pattern]
```

## axonius_api_client/constants/fields.py

```diff
@@ -209,15 +209,15 @@
             "name_base": "current_date",
             "name_qual": "current_date",
             "title": "Current Date",
             "type": "string",
             "type_norm": "string_datetime",
             "format": "date-time",
             "is_custom": True,
-        }
+        },
     },
 }
 """custom schemas for reports in asset callbacks"""
 
 
 class Parsers(BaseEnum):
     """Names of value parsers."""
```

## axonius_api_client/constants/general.py

```diff
@@ -38,18 +38,20 @@
 """python version is 3.7 or higher"""
 
 JSON_TYPES = t.Union[int, str, float, bool, dict, list, tuple, None]
 
 EMPTY: t.List[t.Union[str, list, dict, tuple]] = [None, "", [], {}, ()]
 """Values that should be considered as empty"""
 
-YES: t.List[t.Union[bool, int, str]] = [True, 1, "1", "true", "t", "yes", "y", "on"]
+YES_STR: t.Tuple[str, ...] = ("1", "true", "t", "yes", "y", "on")
+YES: t.List[t.Union[bool, int, str]] = [True, 1, *YES_STR]
 """Values that should be considered as truthy"""
 
-NO: t.List[t.Union[bool, int, str]] = [False, 0, "0", "false", "f", "no", "n", "off"]
+NO_STR: t.Tuple[str, ...] = ("0", "false", "f", "no", "n", "off")
+NO: t.List[t.Union[bool, int, str]] = [False, 0, *NO_STR]
 """Values that should be considered as falsey"""
 
 IS_WINDOWS: bool = sys.platform == "win32"
 """Running on a windows platform"""
 
 IS_LINUX: bool = sys.platform == "linux"
 """Running on a linux platform"""
```

## axonius_api_client/constants/logs.py

```diff
@@ -74,15 +74,15 @@
 LOG_LEVELS_INT_CSV: str = ", ".join([str(x) for x in LOG_LEVELS_INT])
 """csv of valid logging level ints"""
 
 LOG_FILE_PATH: str = DEFAULT_PATH
 """default path for log files"""
 
 LOG_FILE_PATH_MODE = 0o700
-""":obj:`oct` default permisisons to use when creating directories"""
+""":obj:`oct` default permissions to use when creating directories"""
 
 LOG_FILE_NAME: str = f"{PACKAGE_ROOT}.log"
 """default log file name to use"""
 
 LOG_FILE_MAX_MB: int = 5
 """default rollover trigger in MB"""
 
@@ -117,7 +117,19 @@
     "url": "{url!r}",
     "size": "{body_size}",
     "method": "{method!r}",
     "headers": "{headers}",
     "cookies": "{cookies}",
 }
 """Mapping of request attributes to log to their formatting strings."""
+
+RESPONSE_ATTRS: t.List[str] = list(RESPONSE_ATTR_MAP) + ["all"]
+"""List of valid response attributes to log."""
+
+REQUEST_ATTRS: t.List[str] = list(REQUEST_ATTR_MAP) + ["all"]
+"""List of valid request attributes to log."""
+
+RESPONSE_ATTRS_DEFAULT: t.Tuple[str, ...] = ("url", "status", "reason", "elapsed")
+"""Default response attributes to log."""
+
+REQUEST_ATTRS_DEFAULT: t.Tuple[str, ...] = ("url", "size")
+"""Default request attributes to log."""
```

## axonius_api_client/constants/wizards.py

```diff
@@ -202,24 +202,27 @@
 
     SAVED_QUERY: str = "saved_query"
     """saved query type, used in CSV wizard"""
 
     FILE: str = "file"
     """file type, used in CLI wizards"""
 
+    LINES: str = "lines"
+    """lines type, used in CLI wizards"""
+
     DICT: List[str] = [SIMPLE, COMPLEX]
     """valid types for the base Wizard class."""
 
     TEXT: List[str] = [*DICT]
     """valid types for the WizardText class."""
 
     SQ: List[str] = [*DICT, SAVED_QUERY]
     """valid types for the WizardCsv class."""
 
-    CLI: List[str] = [*DICT, FILE]
+    CLI: List[str] = [*DICT, FILE, LINES]
     """valid types for the CLI."""
 
 
 class Docs:
     """Documentation strings for wizards."""
 
     SUB_OPT: str = f"[{Entry.CSPLIT} ...]"
```

## axonius_api_client/examples/adapter_fetch_history_addin.py

```diff
@@ -18,15 +18,15 @@
 from axonius_api_client.api.mixins import ModelMixins
 
 # from axonius_api_client.api.models import (ApiEndpoint, Model, BaseModel,
 #                                            BaseSchema)
 from axonius_api_client.cli import cli, grp_adapters
 from axonius_api_client.cli.context import CONTEXT_SETTINGS
 from axonius_api_client.cli.options import AUTH, add_options
-from axonius_api_client.constants.tables import TABLE_FMT
+from axonius_api_client.constants.api import TABLE_FORMAT
 from axonius_api_client.exceptions import NotFoundError
 from axonius_api_client.parsers.tables import tablize
 from axonius_api_client.tools import coerce_bool, coerce_int, dt_now, dt_parse, json_dump, listify
 
 CLI_MODE: bool = True
 """This controls whether to use the click cli or run as a standard program."""
 
@@ -105,15 +105,15 @@
     def to_tablize(self) -> dict:
         """Pass."""
         props = getattr(self, "_table_properties", self._str_properties)()
         return {self._human_key(k): getattr(self, k, None) for k in props}
 
 
 class FetchHistory(ModelMixins):
-    """Handle all of the interactions."""
+    """Handle all the interactions."""
 
     def get(
         self, generator: bool = False, **kwargs
     ) -> Union[Generator[AdapterFetchHistory, None, None], List[AdapterFetchHistory]]:
         """Get all adapter history.
 
         Args:
@@ -390,15 +390,15 @@
         default="table",
         show_envvar=True,
         show_default=True,
     )
     @click.option(
         "--table-format",
         "table_format",
-        default=TABLE_FMT,
+        default=TABLE_FORMAT,
         help="Base format to use for --export-format=table",
         type=click.Choice(tabulate.tabulate_formats),
         show_envvar=True,
         show_default=True,
     )
     @click.pass_context
     def get_fetch_history(
@@ -410,15 +410,15 @@
         date_to: Optional[str] = None,
         past_minutes: Optional[int] = None,
         adapters: Optional[List[str]] = None,
         statuses: Optional[List[str]] = None,
         clients: Optional[List[str]] = None,
         exclude_realtime: bool = False,
         export_format: str = "table",
-        table_format: str = TABLE_FMT,
+        table_format: str = TABLE_FORMAT,
     ):
         """Process business_units assets using regex searches against fields."""
         client = ctx.obj.start_client(url=url, key=key, secret=secret)
         client.adapters.fetch_history = FetchHistory(auth=client.AUTH)
 
         with ctx.obj.exc_wrap(wraperror=ctx.obj.wraperror):
             data = client.adapters.fetch_history.get(
```

## axonius_api_client/examples/example_os_count_magic.py

```diff
@@ -1,10 +1,10 @@
 #!/usr/bin/env python -i
 # -*- coding: utf-8 -*-
-"""Example of getting all OS types and post processing for missing build versions."""
+"""Example of getting all OS types and post-processing for missing build versions."""
 import csv
 import json  # noqa: F401
 import os
 import pathlib
 import re
 
 import axonius_api_client as axonapi  # noqa: F401
```

## axonius_api_client/examples/script_base.py

```diff
@@ -30,15 +30,14 @@
 # client.users.saved_queries    # CRUD for saved queries for user assets
 # client.openapi                # get the OpenAPI specification file
 
 client_args = {}
 
 # --- get the URL, API key, API secret, & certwarn from the default ".env" file
 client_args.update(axonapi.get_env_connect())
-
 # --- OR override OS env vars with the values from a custom .env file
 # client_args.update(axonapi.get_env_connect(ax_env="/path/to/envfile", override=True))
 
 # --- OR supply them here in the script
 # client_args["url"] = "10.20.0.94"
 # client_args["key"] = ""
 # client_args["secret"] = ""
```

## axonius_api_client/examples/wip_roles.py

```diff
@@ -19,11 +19,11 @@
 users = client.users  # work with user assets
 adapters = client.adapters  # work with adapters and adapter connections
 enforcements = client.enforcements  # work with enforcements
 instances = client.instances  # work with instances
 dashboard = client.dashboard  # work with dashboards and discovery cycles
 system_users = client.system_users  # work with system users
 system_roles = client.system_roles  # work with system roles
-meta = client.meta  # work with instance meta data
+meta = client.meta  # work with instance metadata
 settings_global = client.settings_global  # work with core system settings
 settings_gui = client.settings_gui  # work with gui system settings
 settings_lifecycle = client.settings_lifecycle  # work with lifecycle system settings
```

## axonius_api_client/parsers/__init__.py

```diff
@@ -1,15 +1,15 @@
 # -*- coding: utf-8 -*-
 """Parsers for API models."""
-from . import config, fields, grabber, matcher, searchers, tables, url_parser, wizards, dt_delta
+from . import config, fields, grabber, matcher, searchers, tables, wizards
+from ..projects import url_parser
 
 __all__ = (
     "config",
     "fields",
     "grabber",
     "tables",
     "url_parser",
     "wizards",
     "matcher",
     "searchers",
-    "dt_delta",
 )
```

## axonius_api_client/parsers/config.py

```diff
@@ -321,15 +321,15 @@
         schemas: schemas to validate against key value pairs in new_config
         old_config: previous configuration
         new_config: new configuration supplied by user
         source: identifier of where new_config came from
         callbacks: callback operations to use for schemas
 
     Raises:
-        :exc:`ConfigUnchanged`: if the supplied new config is empty or is not different than old
+        :exc:`ConfigUnchanged`: if the supplied new config is empty or is not different from old
             config
     """
     if new_config == old_config or not new_config:
         err = f"No changes supplied for {source}"
         raise ConfigUnchanged(tablize_schemas(schemas=schemas, config=old_config, err=err))
     return new_config
 
@@ -379,15 +379,15 @@
         schemas: schemas to validate against key value pairs in new_config
         new_config: new configuration supplied by user
         source: identifier of where new_config came from
         ignores: schema keys to ignore
         callbacks: callback operations to use for schemas
 
     Raises:
-        :exc:`ConfigRequired`: if the supplied new config is empty or is not different than old
+        :exc:`ConfigRequired`: if the supplied new config is empty or is not different from old
             config
     """
     missing = []
     ignores = ignores or []
     for name, schema in schemas.items():
         if schema["required"] and name not in new_config and name not in ignores:
             missing.append(schema)
@@ -407,15 +407,15 @@
         schemas: schemas to validate against key value pairs in new_config
         new_config: new configuration supplied by user
         source: identifier of where new_config came from
         ignores: schema keys to ignore
         callbacks: callback operations to use for schemas
 
     Raises:
-        :exc:`ConfigRequired`: if the supplied new config is empty or is not different than old
+        :exc:`ConfigRequired`: if the supplied new config is empty or is not different from old
             config
     """
     if not new_config:
         err = f"No configuration supplied for {source}"
         raise ConfigRequired(tablize_schemas(schemas=schemas, err=err))
     return new_config
 
@@ -453,15 +453,15 @@
         values = [uuid, filename]
         if all(values) and all([isinstance(x, str) for x in values]):
             return True, check
     return False, value
 
 
 def parse_schema(raw: dict) -> dict:
-    """Parse a field, adapter, or config schema into a more user friendly format.
+    """Parse a field, adapter, or config schema into a more user-friendly format.
 
     Args:
         raw: original schema
     """
     if not raw:  # pragma: no cover
         return {}
 
@@ -477,28 +477,28 @@
         )
         parsed[name] = schema
 
     return parsed
 
 
 def parse_schema_enum(schema: dict):
-    """Parse a field, adapter, or config schema into a more user friendly format.
+    """Parse a field, adapter, or config schema into a more user-friendly format.
 
     Args:
         raw: original schema
     """
     # core settings: password_brute_force_protection: conditional
     # has a list of dict enums, so turn it into a lookup map
     if schema.get("enum") and isinstance(schema["enum"][0], dict):
         schema["enum"] = {x["name"]: x for x in schema["enum"]}
 
 
 # TBD: re-tool to return cleaned up 'raw'
 def parse_section(raw: dict, raw_config: dict, parent: dict, settings: dict) -> dict:
-    """Parse a section of system settings into a more user friendly format."""
+    """Parse a section of system settings into a more user-friendly format."""
     # FYI has no title:
     #   settings_gui::saml_login_settings::configure_authncc
     title = raw.get("title", raw["name"].replace("_", " ").title())
     config = raw_config.get(raw["name"], {})
     section_name = raw["name"]
     schemas = raw["items"]
     # FYI core_settings::tunnel_email_recipients is missing 'required' as of 3.6!
@@ -533,15 +533,15 @@
                 parent=parsed,
                 settings=settings,
             )
             schema.pop("items")
 
         # non sub_sections:
         #   no items key in schema
-        #   {"items": {"type": ""} "type": "array"}
+        #   {{"items": {"type": ""} "type": "array"}
         # FYI some things have a required key already that is a bool 3.6
         if not isinstance(schema.get("required", []), bool):
             schema["required"] = schema_name in required
 
         parsed["schemas"][schema_name] = schema
 
         # FYI does not follow schema for defaults:
```

## axonius_api_client/parsers/fields.py

```diff
@@ -149,18 +149,18 @@
     raw_fields: List[dict],
     agg_base_names: Optional[List[str]] = None,
 ) -> List[dict]:
     """Parse field schemas for an adapter.
 
     Args:
         adapter_name_raw: raw name of current adapter (aws_adapter)
-        adapter_name: user friendly name of current adapter (aws)
+        adapter_name: user-friendly name of current adapter (aws)
         adapter_prefix: fully qualified prefix of adapter (specific_data.data or
             adapters_data.aws_adapter)
-        adapter_title: user friendly title of adapter
+        adapter_title: user-friendly title of adapter
         all_field: name to use for all field schema
         raw_fields: raw unparsed fields for current adapter
         agg_base_names: used to determine if a field is aggregated or not
     """
     agg_base_names = agg_base_names or []
     is_agg = adapter_name == AGG_ADAPTER_NAME
```

## axonius_api_client/parsers/grabber.py

```diff
@@ -3,20 +3,30 @@
 import dataclasses
 import logging
 import pathlib
 import types
 import typing as t
 import warnings
 
-from ..cert_human.paths import PathLike, pathify
 from ..constants.fields import AXID
+from ..constants.ctypes import PathLike
 from ..data import BaseData
 from ..exceptions import GrabberError, GrabberWarning
 from ..logs import get_echoer, get_obj_log
-from ..tools import add_source, csv_able, csv_load, json_load, jsonl_load, listify, text_load, tlens
+from ..tools import (
+    add_source,
+    csv_able,
+    csv_load,
+    json_load,
+    jsonl_load,
+    listify,
+    text_load,
+    tlens,
+    pathify,
+)
 
 
 class Mixins:
     """Pass."""
 
     _exc_cls: t.ClassVar[Exception] = GrabberError
     _warn_cls: t.ClassVar[Warning] = GrabberWarning
```

## axonius_api_client/parsers/matcher.py

```diff
@@ -1,20 +1,23 @@
 # -*- coding: utf-8 -*-
 """Parsers for API models."""
 
 import typing as t
 
 import cachetools
 
+from ..constants.api import RE_PREFIX
 from ..constants.ctypes import PatternLike, PatternLikeListy
 from ..constants.general import HIDDEN, SPLITTER
 from ..tools import bytes_to_str, coerce_str_re, is_pattern, is_str, listify
 
 CACHE_SIZE: int = 1024
-MatcherLoad: t.TypeVar = t.TypeVar("MatcherLoad", PatternLikeListy, "Matcher")
+MatcherLoad: t.TypeVar = t.TypeVar(
+    "MatcherLoad", "Matcher", str, t.Pattern, t.Iterable[t.Union[str, t.Pattern]]
+)
 
 
 class Matcher:
     """Caching pattern matcher."""
 
     def _listify(self, value: t.Any) -> t.List[t.Any]:
         """Pass."""
@@ -26,29 +29,29 @@
             strip=self.strip,
             strip_chars=self.strip_chars,
         )
 
     def __init__(
         self,
         values: t.Optional[PatternLikeListy] = None,
-        re_prefix: str = "~",
+        re_prefix: str = RE_PREFIX,
         split: bool = True,
         split_max: int = -1,
         split_sep: t.Optional[PatternLike] = SPLITTER,
         strip: bool = True,
         strip_chars: t.Optional[str] = None,
         hidden: t.Optional[str] = HIDDEN,
     ) -> None:
         """Pass."""
         self.values_orig: t.Optional[PatternLikeListy] = values
         self.re_prefix: str = re_prefix
         self.split: bool = split
         self.split_sep: t.Optional[PatternLike] = split_sep
         self.split_max: int = split_max
-        self.strip: t.Optional[PatternLike] = strip
+        self.strip: bool = strip
         self.strip_chars: t.Optional[str] = strip_chars
         self.hidden: t.Optional[str] = hidden
         self.strings: t.List[str] = []
         self.patterns: t.List[t.Pattern] = []
         self.values: t.List[t.Union[str, t.Pattern]] = self._listify(values)
 
         for value in self.values:
@@ -76,15 +79,21 @@
         strip_chars: t.Optional[str] = None,
         hidden: t.Optional[str] = HIDDEN,
     ) -> "Matcher":
         """Load matcher values, return values as is if already Matcher obj.
 
         Args:
             values (t.Optional[MATCHER]): Matcher or values to parse into Matcher
-            **kwargs: passed to Matcher instantiation
+            re_prefix (str, optional): prefix to use for regexes
+            split (bool, optional): split values on split_sep
+            split_max (int, optional): max number of splits to perform
+            split_sep (t.Optional[PatternLike], optional): separator to split values on
+            strip (bool, optional): strip values
+            strip_chars (t.Optional[str], optional): chars to strip from values
+            hidden (t.Optional[str], optional): token to use for hidden values
 
         Returns:
             Matcher: Matcher object
         """
         cls_args = dict(
             re_prefix=re_prefix,
             split=split,
```

## axonius_api_client/tests/conftest.py

```diff
@@ -1,25 +1,53 @@
 # -*- coding: utf-8 -*-
 """Conf for py.test."""
 import os
 import pathlib
-
 import pytest
 
+from axonius_api_client.tools import coerce_bool
+from axonius_api_client.http import Http
+
 from .meta import CSV_FILECONTENT_STR, CSV_FILENAME, USER_NAME
-from .utils import check_apiobj_children, check_apiobj_xref, get_url
+from .utils import (
+    check_apiobj_children,
+    check_apiobj_xref,
+    get_arg_credentials,
+    get_arg_key,
+    get_arg_secret,
+    get_arg_url,
+    get_http,
+    get_connect,
+    get_arg_cf_token,
+    get_arg_cf_error,
+    get_arg_cf_run,
+)
 
 AX_URL = os.environ.get("AX_URL", None) or None
 AX_KEY = os.environ.get("AX_KEY", None) or None
 AX_SECRET = os.environ.get("AX_SECRET", None) or None
+CF_TOKEN = os.environ.get("CF_TOKEN", None) or None
+CF_RUN_RAW = os.environ.get("CF_RUN", True)
+CF_RUN = coerce_bool(obj=CF_RUN_RAW, errmsg="CF_RUN must be a bool", allow_none=True, as_none=False)
+CF_ERROR_RAW = os.environ.get("CF_ERROR", True)
+CF_ERROR = coerce_bool(
+    obj=CF_ERROR_RAW, errmsg="CF_ERROR must be a bool", allow_none=True, as_none=False
+)
+
+
+AX_CREDENTIALS_RAW = os.environ.get("AX_CREDENTIALS", False)
+AX_CREDENTIALS = coerce_bool(
+    obj=AX_CREDENTIALS_RAW, errmsg="AX_CREDENTIALS must be a bool", allow_none=True, as_none=False
+)
+
 ARTIFACTS = pathlib.Path(__file__).parent.parent.parent / "artifacts"
 os.environ.setdefault("AX_LOG_FILE_PATH", str(ARTIFACTS))
 
 
-def pytest_addoption(parser):
+def pytest_addoption(parser: pytest.Parser) -> None:
     """Add API connection options."""
     parser.addoption(
         "--ax-url",
         action="store",
         default=AX_URL,
         required=not bool(AX_URL),
         help="URL of Axonius API",
@@ -34,18 +62,46 @@
     parser.addoption(
         "--ax-secret",
         action="store",
         default=AX_SECRET,
         required=not bool(AX_SECRET),
         help="API secret for Axonius API",
     )
+    parser.addoption(
+        "--ax-credentials",
+        action="store_true",
+        default=AX_CREDENTIALS,
+        required=False,
+        help="Treat key and secret as username and password",
+    )
+    parser.addoption(
+        "--cf-token",
+        action="store",
+        default=CF_TOKEN,
+        required=not bool(CF_TOKEN),
+        help="Token for CloudFlare access",
+    )
+    parser.addoption(
+        "--cf-run",
+        action="store_true",
+        default=CF_RUN,
+        required=False,
+        help="Run cloudflared to get cloudflare token if not supplied",
+    )
+    parser.addoption(
+        "--cf-error",
+        action="store_true",
+        default=CF_ERROR,
+        required=False,
+        help="Error if a token can not be obtained from cloudflare",
+    )
 
 
 def load_asset_api(obj):
-    """Pass."""
+    """Check an asset API model and load datasets to it."""
     from axonius_api_client.api import Adapters, DataScopes, Wizard, WizardCsv, WizardText
     from axonius_api_client.api.assets import Fields, Labels, SavedQuery
 
     assert isinstance(obj.fields_default, list)
 
     check_apiobj_children(
         apiobj=obj,
@@ -58,35 +114,28 @@
     assert isinstance(obj.wizard, Wizard)
     assert isinstance(obj.wizard_text, WizardText)
     assert isinstance(obj.wizard_csv, WizardCsv)
     assert isinstance(obj.data_scopes, DataScopes)
     obj.ORIGINAL_ROWS = obj.get(max_rows=5)
     obj.IDS = [x["internal_axon_id"] for x in obj.ORIGINAL_ROWS]
 
+    # noinspection PyBroadException
     try:
         obj.COMPLEX_ROWS = obj.get(
             max_rows=5, fields=obj.FIELD_COMPLEX, wiz_entries=f"simple {obj.FIELD_COMPLEX} exists"
         )
     except Exception:
         obj.COMPLEX_ROWS = []
     return obj
 
 
 @pytest.fixture(scope="session")
 def api_client(request):
     """Test utility."""
-    from axonius_api_client.connect import Connect
-
-    url = request.config.getoption("--ax-url")
-    key = request.config.getoption("--ax-key")
-    secret = request.config.getoption("--ax-secret")
-    if isinstance(url, str):
-        url = url.rstrip("/")
-
-    client = Connect(url=url, key=key, secret=secret, certwarn=False, save_history=True)
+    client = get_connect(request=request)
     client.start()
     return client
 
 
 @pytest.fixture(scope="session")
 def api_openapi(api_client):
     """Test utility."""
@@ -144,14 +193,22 @@
 @pytest.fixture(scope="session")
 def api_instances(api_client):
     """Test utility."""
     return api_client.instances
 
 
 @pytest.fixture(scope="session")
+def device_sq_predefined(api_devices):
+    """Test utility."""
+    sqs = api_devices.saved_query.get(as_dataclass=True)
+    sqs = sorted([x for x in sqs if x.predefined], key=lambda x: len(x.name))
+    return sqs[0]
+
+
+@pytest.fixture(scope="session")
 def api_system_roles(api_client):
     """Test utility."""
     return api_client.system_roles
 
 
 @pytest.fixture(scope="session")
 def api_data_scopes(api_client):
@@ -195,16 +252,19 @@
     return api_client.settings_ip
 
 
 @pytest.fixture(scope="session")
 def api_signup(request):
     """Test utility."""
     from axonius_api_client.api import Signup
+    from axonius_api_client.auth import AuthNull
 
-    obj = Signup(url=get_url(request))
+    http = get_http(request)
+    auth = AuthNull(http=http)
+    obj = Signup(auth=auth)
     return obj
 
 
 @pytest.fixture(scope="session")
 def api_remote_support(api_client):
     """Test utility."""
     return api_client.remote_support
@@ -248,38 +308,43 @@
     assert data["uuid"]
     assert data["filename"]
     return data
 
 
 @pytest.fixture()
 def smtp_setup(api_settings_global):
-    """Pass."""
+    """Configure Axonius with SMTP server for tests that require it."""
 
+    # noinspection PyBroadException
     def setup():
+        """Setup smtp server for email tests."""
         try:
             api_settings_global.update_section(
                 section="email_settings", enabled=True, smtpHost="10.0.2.110", smtpPort=25
             )
         except Exception:
             pass
 
+    # noinspection PyBroadException
     def teardown():
+        """Teardown smtp server for email tests."""
         try:
             api_settings_global.update_section(
                 section="email_settings", enabled=False, smtpHost=None, smtpPort=None
             )
         except Exception:
             pass
 
     return setup, teardown
 
 
+# noinspection PyProtectedMember,PyBroadException
 @pytest.fixture(scope="function")
 def temp_user(api_system_users):
-    """Pass."""
+    """Fixture to create a temporary user."""
     roles = api_system_users.roles._get()
     users = api_system_users._get()
     for user in users:
         if user.user_name == USER_NAME:
             api_system_users._delete(uuid=user.uuid)
 
     role_id = roles[0].uuid
@@ -292,17 +357,18 @@
         api_system_users._delete(uuid=tuser.uuid)
     except Exception:
         pass
 
 
 @pytest.fixture
 def datafiles(request):
-    """Pass."""
+    """Fixture to read in datafiles."""
 
     def get_datafile(filename):
+        """Get a datafile from the datafiles directory."""
         path = pathlib.Path(__file__).parent / "datafiles" / filename
         if not path.is_file():
             msg = f"datafile {filename!r} not found at path {path}"
             if skip_if_missing:
                 pytest.skip(msg)
             else:
                 raise Exception(msg)
@@ -324,24 +390,79 @@
 
     paths = [get_datafile(x) for x in filenames]
     return paths
 
 
 @pytest.fixture(scope="session")
 def core_node(api_instances):
-    """Pass."""
+    """Get the core Axonius instance."""
     return api_instances.get_core()
 
 
 @pytest.fixture(scope="session")
 def tunnel_feature_check(api_instances):
-    """Pass."""
+    """Fixture to check if saas is enabled and skip the test if not."""
     if not api_instances.has_saas_enabled:
         pytest.skip("saas_enabled=False, can not test for tunnels")
 
 
 @pytest.fixture(scope="session")
 def tunnel_count_check(api_instances, tunnel_feature_check):
-    """Pass."""
+    """Fixture to check if any tunnels exist and skip the test if not."""
     tunnels = api_instances.get_tunnels()
     if not tunnels:
         pytest.skip("No tunnels configured, can not test for tunnels")
+
+
+@pytest.fixture()
+def arg_key(request: pytest.FixtureRequest) -> str:
+    """Get credentials from command line args."""
+    return get_arg_key(request)
+
+
+@pytest.fixture()
+def arg_secret(request: pytest.FixtureRequest) -> str:
+    """Get credentials from command line args."""
+    return get_arg_secret(request)
+
+
+@pytest.fixture()
+def arg_credentials(request: pytest.FixtureRequest) -> bool:
+    """Get credentials from command line args."""
+    return get_arg_credentials(request)
+
+
+@pytest.fixture()
+def arg_url(request: pytest.FixtureRequest) -> str:
+    """Get credentials from command line args."""
+    return get_arg_url(request)
+
+
+@pytest.fixture()
+def arg_cf_token(request: pytest.FixtureRequest) -> str:
+    """Get credentials from command line args."""
+    return get_arg_cf_token(request)
+
+
+@pytest.fixture()
+def arg_cf_error(request: pytest.FixtureRequest) -> str:
+    """Get credentials from command line args."""
+    return get_arg_cf_error(request)
+
+
+@pytest.fixture()
+def arg_cf_run(request: pytest.FixtureRequest) -> str:
+    """Get credentials from command line args."""
+    return get_arg_cf_run(request)
+
+
+@pytest.fixture()
+def arg_url_http(arg_url: str, arg_cf_run: bool, arg_cf_error: bool, arg_cf_token: str) -> Http:
+    """Get a Http client using url from command line args."""
+    return Http(
+        url=arg_url,
+        certwarn=False,
+        save_history=True,
+        cf_run=arg_cf_run,
+        cf_error=arg_cf_error,
+        cf_token=arg_cf_token,
+    )
```

## axonius_api_client/tests/utils.py

```diff
@@ -14,24 +14,26 @@
 
 import pytest
 import requests
 from cachetools import TTLCache, cached
 from click.testing import CliRunner
 from flaky import flaky
 
+
 IS_WINDOWS = sys.platform == "win32"
 IS_LINUX = sys.platform == "linux"
 IS_MAC = sys.platform == "darwin"
 SALT_SIZE = 32  # In Bytes
 SOURCE: str = string.ascii_lowercase + string.digits
 
 
 CACHE: TTLCache = TTLCache(maxsize=1024, ttl=600)
 
 
+# noinspection PyUnusedLocal
 def flaky_filter(err, *args):
     """Pass."""
     if issubclass(err[0], requests.exceptions.ReadTimeout):
         time.sleep(30)
     return True
 
 
@@ -93,27 +95,27 @@
     try:
         schema = apiobj.fields.get_field_schema(
             value=field,
             schemas=schemas,
         )
     except NotFoundError as exc:
         pytest.skip(f"field {field} not found, exc:\n{exc}")
-
-    return schema[key] if key else schema
+    else:
+        return schema[key] if key else schema
 
 
 def random_string(length: int = 32, source: str = SOURCE):
     """Test utility."""
-    result_str = "".join(random.choice(source) for i in range(length))
+    result_str = "".join(random.choice(source) for _ in range(length))
     return result_str
 
 
 def random_strs(num: int = 1, length: int = 32, source: str = SOURCE) -> t.List[str]:
     """Test utility."""
-    return [random_string(length=length, source=source) for i in range(num)]
+    return [random_string(length=length, source=source) for _ in range(num)]
 
 
 def random_string_salt(length: int, source: str = SOURCE) -> str:
     """Generate a random string with length, seed it time in milliseconds and salt."""
     result = ""
     for i in range(0, length):
         result += secrets.choice(source)
@@ -126,18 +128,18 @@
     from axonius_api_client.exceptions import NotFoundError
 
     query = exists_query(apiobj=apiobj, fields=fields, not_exist=not_exist)
     try:
         rows = apiobj.get(fields=fields, max_rows=max_rows, query=query, **kwargs)
     except NotFoundError as exc:
         pytest.skip(f"fields {fields} not found, exc:\n{exc}")
-
-    if not rows:
-        pytest.skip(f"No {apiobj} assets with fields {fields}")
-    return rows[0] if max_rows == 1 else rows
+    else:
+        if not rows:
+            pytest.skip(f"No {apiobj} assets with fields {fields}")
+        return rows[0] if max_rows == 1 else rows
 
 
 @cached(cache=CACHE)
 def get_schemas(apiobj, adapter=None):
     """Test utility."""
     from axonius_api_client.constants.fields import AGG_ADAPTER_NAME
 
@@ -158,15 +160,14 @@
         else:
             if any(re.search(entry, m) for m in msgs):
                 error = "Found unexpected entry in log: {!r}\nAll entries:\n{}"
                 error = error.format(entry, "\n".join(msgs))
                 raise Exception(error)
 
 
-#
 def get_cnx_existing(apiobj, name=None, reqkeys=None):
     """Test utility."""
     found = get_cnx(apiobj=apiobj, name=name, reqkeys=reqkeys)
 
     if not found:
         pytest.skip("No connections found for any adapter!")
     return found
@@ -193,14 +194,15 @@
     found = get_cnx(apiobj=apiobj, cntkey="error_count", name=name, reqkeys=reqkeys)
 
     if not found:
         pytest.skip("No broken connections found for any adapter!")
     return found
 
 
+# noinspection PyProtectedMember
 def get_cnx(apiobj, cntkey="total_count", name=None, reqkeys=None, problems=None):
     """Pass."""
     adapters = apiobj._get(get_clients=False)
     reqkeys = reqkeys or []
     problems = problems or []
 
     for adapter in adapters:
@@ -226,71 +228,154 @@
                 if cntkey == "success_count" and cnx.working:
                     return cnx.to_dict_old()
                 if cntkey == "error_count" and not cnx.working:
                     return cnx.to_dict_old()
     return None
 
 
-def get_url(request):
+def get_arg_url(request):
     """Test utility."""
-    return request.config.getoption("--ax-url").rstrip("/")
+    value = request.config.getoption("--ax-url").rstrip("/")
+
+    if isinstance(value, str):
+        value = value.rstrip("/")
+    return value
+
+
+get_url = get_arg_url
 
 
 def get_key_creds(request):
     """Test utility."""
-    key = request.config.getoption("--ax-key")
-    secret = request.config.getoption("--ax-secret")
-    return {"key": key, "secret": secret}
+    key = get_arg_key(request)
+    secret = get_arg_secret(request)
+    creds = get_arg_credentials(request)
+    return {"username": key, "password": secret} if creds else {"keys": key, "secret": secret}
 
 
-def get_auth(request):
+def get_http(request, **kwargs):
     """Test utility."""
-    from axonius_api_client import auth
     from axonius_api_client.http import Http
 
-    http = Http(url=get_url(request), certwarn=False, save_history=True)
+    kwargs.setdefault("url", get_url(request))
+    kwargs.setdefault("cf_run", get_arg_cf_run(request))
+    kwargs.setdefault("cf_error", get_arg_cf_error(request))
+    kwargs.setdefault("cf_token", get_arg_cf_token(request))
+    kwargs.setdefault("certwarn", False)
+
+    http = Http(**kwargs)
+    return http
+
+
+def get_connect(request, **kwargs):
+    """Test utility."""
+    from axonius_api_client.connect import Connect
+
+    kwargs.setdefault("url", get_url(request))
+    kwargs.setdefault("key", get_arg_key(request))
+    kwargs.setdefault("secret", get_arg_secret(request))
+    kwargs.setdefault("credentials", get_arg_credentials(request))
+    kwargs.setdefault("cf_run", get_arg_cf_run(request))
+    kwargs.setdefault("cf_error", get_arg_cf_error(request))
+    kwargs.setdefault("cf_token", get_arg_cf_token(request))
+    kwargs.setdefault("certwarn", False)
+
+    connect = Connect(**kwargs)
+    return connect
+
+
+def get_arg_key(request):
+    """Test utility."""
+    return request.config.getoption("--ax-key")
+
+
+def get_arg_secret(request):
+    """Test utility."""
+    return request.config.getoption("--ax-secret")
+
+
+def get_arg_cf_token(request):
+    """Test utility."""
+    return request.config.getoption("--cf-token")
+
+
+def get_arg_cf_run(request):
+    """Test utility."""
+    return request.config.getoption("--cf-run")
+
+
+def get_arg_cf_error(request):
+    """Test utility."""
+    return request.config.getoption("--cf-error")
+
 
-    obj = auth.ApiKey(http=http, **get_key_creds(request))
+def get_arg_credentials(request):
+    """Test utility."""
+    value = request.config.getoption("--ax-credentials")
+    return value
+
+
+def get_auth_obj(request):
+    """Test utility."""
+    from axonius_api_client.auth import AuthApiKey, AuthCredentials
+
+    arg_credentials: bool = get_arg_credentials(request)
+    arg_key: str = get_arg_key(request)
+    arg_secret: str = get_arg_secret(request)
+    http = get_http(request)
+    if arg_credentials:
+        obj = AuthCredentials(http=http, username=arg_key, password=arg_secret)
+    else:
+        obj = AuthApiKey(http=http, key=arg_key, secret=arg_secret)
+    return obj
+
+
+def get_auth(request):
+    """Test utility."""
+    obj = get_auth_obj(request)
     obj.login()
     return obj
 
 
 def check_apiobj(authobj, apiobj):
     """Test utility."""
     from axonius_api_client import auth
     from axonius_api_client.http import Http
 
-    url = authobj._http.url
-    authclsname = format(authobj.__class__.__name__)
-    assert authclsname in format(apiobj)
-    assert authclsname in repr(apiobj)
-    assert url in format(apiobj)
-    assert url in repr(apiobj)
+    url = authobj.http.url
+    name = authobj.__class__.__name__
+
+    obj_str = str(apiobj)
+    obj_repr = repr(apiobj)
+    assert name in obj_str
+    assert url in obj_str
+    assert name in obj_repr
+    assert url in obj_repr
 
-    assert isinstance(apiobj.auth, auth.Model)
+    assert isinstance(apiobj.auth, auth.AuthModel)
     assert isinstance(apiobj.http, Http)
 
 
 def check_apiobj_children(apiobj, **kwargs):
     """Test utility."""
     from axonius_api_client import api, auth
     from axonius_api_client.http import Http
 
     for k, v in kwargs.items():
         attr = getattr(apiobj, k)
-        attrclsname = format(attr.__class__.__name__)
+        name = format(attr.__class__.__name__)
 
         assert isinstance(attr, api.mixins.ChildMixins)
         assert isinstance(attr, v)
 
-        assert isinstance(attr.auth, auth.Model)
+        assert isinstance(attr.auth, auth.AuthModel)
         assert isinstance(attr.http, Http)
         assert isinstance(attr.parent, api.mixins.Model)
-        assert attrclsname in format(attr)
-        assert attrclsname in repr(attr)
+        assert name in str(attr)
+        assert name in repr(attr)
 
 
 def check_apiobj_xref(apiobj, **kwargs):
     """Test utility."""
     from axonius_api_client import api
 
     for k, v in kwargs.items():
@@ -326,14 +411,15 @@
     """Test utility."""
 
 
 class MockCtx:
     """Test utility."""
 
 
+# noinspection PyUnusedLocal
 def mock_failure(*args, **kwargs):
     """Test utility."""
     raise MockError("badwolf")
 
 
 def get_mockctx():
     """Test utility."""
@@ -342,18 +428,18 @@
     ctx = MockCtx()
     ctx.obj = Context()
     return ctx
 
 
 def check_csv_cols(content, cols):
     """Test utility."""
-    QUOTING = csv.QUOTE_NONNUMERIC
+    quoting = csv.QUOTE_NONNUMERIC
     fh = StringIO()
     fh.write(content)
     fh.seek(0)
-    reader = csv.DictReader(fh, quoting=QUOTING)
+    reader = csv.DictReader(fh, quoting=quoting)
     rows = []
     for row in reader:
         rows.append(row)
         for x in cols:
             assert x in row, "column {!r} not in {}".format(x, list(row))
     return rows
```

## axonius_api_client/tests/tests_api/test_api_endpoints.py

```diff
@@ -1,15 +1,21 @@
+"""Tests."""
 import dataclasses
 from typing import List, Type
 
 import pytest
 import requests
 
 from axonius_api_client.api import json_api
-from axonius_api_client.api.api_endpoints import ApiEndpoint, ApiEndpointGroup, ApiEndpoints
+from axonius_api_client.api.api_endpoints import (
+    ApiEndpoint,
+    ApiEndpointGroup,
+    ApiEndpoints,
+    ApiEndpointsGroups,
+)
 from axonius_api_client.exceptions import (
     InvalidCredentials,
     JsonInvalidError,
     RequestFormatObjectError,
     RequestFormatPathError,
     RequestLoadObjectError,
     RequestMissingArgsError,
@@ -19,69 +25,80 @@
 )
 from axonius_api_client.http import Http
 from axonius_api_client.tools import get_subcls
 
 from ..utils import get_auth, get_url
 
 MODELS_EXCLUDE = [
-    json_api.base.BaseModel,
-    json_api.resources.PaginationRequest,
-    json_api.resources.PageSortRequest,
-    json_api.assets.AssetTypeHistoryDates,
-    json_api.adapters.Cnx,
-    json_api.system_settings.SystemSettingsUpdate,
-    json_api.assets.AssetTypeHistoryDate,
-    json_api.adapters.AdapterNodeCnx,
     json_api.adapters.AdapterClientsCount,
     json_api.adapters.AdapterNode,
-    json_api.folders.queries.FolderModel,
-    json_api.folders.enforcements.FolderModel,
+    json_api.adapters.AdapterNodeCnx,
+    json_api.adapters.Cnx,
+    json_api.assets.AssetTypeHistoryDate,
+    json_api.assets.AssetTypeHistoryDates,
+    json_api.base.BaseModel,
+    json_api.base2.BaseModel,
+    json_api.count_operator.CountOperator,
+    json_api.dashboard_spaces.Chart,
+    json_api.dashboard_spaces.ChartQuery,
+    json_api.dashboard_spaces.Size,
     json_api.data_scopes.DataScope,
-    json_api.time_range.TimeRange,
+    json_api.folders.enforcements.FolderModel,
+    json_api.folders.queries.FolderModel,
+    json_api.nested_access.Access,
     json_api.paging_state.Page,
     json_api.paging_state.PagingState,
+    json_api.resources.PaginationRequest,
     json_api.selection.IdSelection,
-    json_api.nested_access.Access,
-    json_api.dashboard_spaces.Size,
-    json_api.dashboard_spaces.ChartQuery,
-    json_api.dashboard_spaces.Chart,
+    json_api.system_settings.SystemSettingsUpdate,
+    json_api.tasks.result.Result,
+    json_api.tasks.task.Task,
+    json_api.time_range.TimeRange,
 ]
 
 SCHEMAS_EXCLUDE = [
     json_api.base.BaseSchema,
     json_api.base.BaseSchemaJson,
+    json_api.base2.BaseSchema,
     json_api.central_core.AdditionalDataAws,
-    json_api.system_settings.SystemSettingsUpdateSchema,
-    json_api.selection.IdSelectionSchema,
-    json_api.nested_access.AccessSchema,
-    json_api.dashboard_spaces.SizeSchema,
+    json_api.count_operator.CountOperatorSchema,
     json_api.dashboard_spaces.ChartSchema,
+    json_api.dashboard_spaces.SizeSchema,
+    json_api.nested_access.AccessSchema,
     json_api.nested_access.AccessSchemaJson,
+    json_api.selection.IdSelectionSchema,
+    json_api.system_settings.SystemSettingsUpdateSchema,
+    json_api.tasks.result.ResultSchema,
+    json_api.tasks.task.TaskSchema,
+    json_api.resources.PaginationSchema,
 ]
 
 
 def get_model_classes() -> List[Type[json_api.base.BaseModel]]:
+    """Get all model classes in the package."""
     return [
         x
         for x in get_subcls(json_api.base.BaseModel)
         if x not in MODELS_EXCLUDE and "tests." not in str(x)
     ]
 
 
 def get_schema_classes() -> List[Type[json_api.base.BaseSchema]]:
+    """Get all schema classes in the package."""
     return [
         x
         for x in get_subcls(json_api.base.BaseSchema)
         if x not in SCHEMAS_EXCLUDE and "tests." not in str(x)
     ]
 
 
 GROUPS_SUBCLS: List[Type[ApiEndpointGroup]] = get_subcls(ApiEndpointGroup)
 GROUPS_SUBCLS_USED: List[Type[ApiEndpointGroup]] = [
-    x.__class__ for x in ApiEndpoints.get_groups().values()
+    ApiEndpointsGroups,
+    *[x.__class__ for x in ApiEndpoints.get_subgroups(recursive=True).values()],
 ]
 
 
 class TestApiEndpoint:
     def test_response_as_text(self, request):
         endpoint = ApiEndpoint(
             method="get",
@@ -92,156 +109,158 @@
             response_model_cls=None,
             response_as_text=True,
         )
 
         ax_url = get_url(request)
         http = Http(url=ax_url)
         response = endpoint.perform_request_raw(http=http)
-        ret = endpoint.handle_response(http=None, response=response)
+        ret = endpoint.handle_response(http=http, response=response)
         assert ret == response.text
 
     def test_wrong_model_cls(self):
         with pytest.raises(ValueError):
+            # noinspection PyTypeChecker
             ApiEndpoint(
                 method="get",
                 path="",
                 request_schema_cls=None,
                 request_model_cls=pytest,
                 response_schema_cls=None,
                 response_model_cls=None,
             )
 
     def test_wrong_schema_cls(self):
         with pytest.raises(ValueError):
+            # noinspection PyTypeChecker
             ApiEndpoint(
                 method="get",
                 path="",
                 request_schema_cls=pytest,
                 request_model_cls=None,
                 response_schema_cls=None,
                 response_model_cls=None,
             )
 
-    def test_dump_path_from_obj(self, request):
+    def test_dump_path_from_obj(self):
         endpoint = ApiEndpoint(
             method="get",
             path="test/{value}",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = json_api.generic.BoolValue(value=False)
         exp = "test/False"
         ret = endpoint.dump_path(request_obj=request_obj)
         assert ret == exp
 
-    def test_dump_path_from_kwargs(self, request):
+    def test_dump_path_from_kwargs(self):
         endpoint = ApiEndpoint(
             method="get",
             path="test/{value}/{arg1}",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = json_api.generic.BoolValue(value=False)
         exp = "test/False/moo"
         ret = endpoint.dump_path(request_obj=request_obj, arg1="moo")
         assert ret == exp
 
-    def test_dump_path_empty(self, request):
+    def test_dump_path_empty(self):
         endpoint = ApiEndpoint(
             method="get",
             path="test/value/arg1",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = json_api.generic.BoolValue(value=False)
         exp = "test/value/arg1"
         ret = endpoint.dump_path(request_obj=request_obj, arg1="moo")
         assert ret == exp
 
-    def test_dump_path_no_obj(self, request):
+    def test_dump_path_no_obj(self):
         endpoint = ApiEndpoint(
             method="get",
             path="test/value/{arg1}",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         exp = "test/value/moo"
         ret = endpoint.dump_path(arg1="moo")
         assert ret == exp
 
-    def test_dump_path_err(self, request):
+    def test_dump_path_err(self):
         endpoint = ApiEndpoint(
             method="get",
             path="test/value/{arg1}",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         with pytest.raises(RequestFormatPathError):
             endpoint.dump_path()
 
-    def test_check_request_obj(self, request):
+    def test_check_request_obj(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = json_api.generic.BoolValue(value=False)
         endpoint.check_request_obj(request_obj=request_obj)
 
-    def test_check_request_obj_fail(self, request):
+    def test_check_request_obj_fail(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = json_api.generic.IntValue(value=2)
         with pytest.raises(RequestObjectTypeError):
             endpoint.check_request_obj(request_obj=request_obj)
 
-    def test_check_missing_args(self, request):
+    def test_check_missing_args(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=None,
             response_schema_cls=None,
             response_model_cls=None,
             http_args_required=[],
         )
         endpoint.check_missing_args(args={})
 
-    def test_check_missing_args_success(self, request):
+    def test_check_missing_args_success(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=None,
             response_schema_cls=None,
             response_model_cls=None,
             http_args_required=["wah"],
         )
         endpoint.check_missing_args(args={"wah": 2})
 
-    def test_check_missing_args_fail(self, request):
+    def test_check_missing_args_fail(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=None,
             response_schema_cls=None,
             response_model_cls=None,
@@ -262,83 +281,81 @@
 
         ax_url = get_url(request)
         http = Http(url=ax_url)
         response = endpoint.perform_request_raw(http=http)
         with pytest.raises(JsonInvalidError):
             endpoint.get_response_json(response=response)
 
-    def test_dump_object_request_as_none(self, request, monkeypatch):
+    def test_dump_object_request_as_none(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
             request_as_none=True,
         )
         request_obj = json_api.generic.BoolValue(value=False)
         exp = {}
         ret = endpoint.dump_object(request_obj=request_obj)
-        assert isinstance(ret, dict)
         assert ret == exp
 
-    def test_dump_object_post(self, request, monkeypatch):
+    def test_dump_object_post(self):
         endpoint = ApiEndpoint(
             method="post",
             path="",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = json_api.generic.BoolValue(value=False)
         exp = {"json": {"data": {"type": "bool_value_schema", "attributes": {"value": False}}}}
         ret = endpoint.dump_object(request_obj=request_obj)
-        assert isinstance(ret, dict)
         assert ret == exp
 
-    def test_dump_object_get(self, request, monkeypatch):
+    def test_dump_object_get(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = json_api.generic.BoolValue(value=False)
         exp = {"params": {"value": False}}
         ret = endpoint.dump_object(request_obj=request_obj)
-        assert isinstance(ret, dict)
         assert ret == exp
 
-    def test_dump_object_fail_not_model(self, request, monkeypatch):
+    def test_dump_object_fail_not_model(self):
         endpoint = ApiEndpoint(
             method="get",
             path="",
             request_schema_cls=None,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
         request_obj = {"i am not a": "model"}
         with pytest.raises(RequestFormatObjectError):
             endpoint.dump_object(request_obj=request_obj)
 
-    def test_dump_object_fail_serialization(self, request, monkeypatch):
+    def test_dump_object_fail_serialization(self):
         endpoint = ApiEndpoint(
             method="post",
             path="",
             request_schema_cls=json_api.generic.BoolValueSchema,
             request_model_cls=json_api.generic.BoolValue,
             response_schema_cls=None,
             response_model_cls=None,
         )
-        request_obj = json_api.generic.BoolValue(value="MANANANANANA")
+        # noinspection PyTypeChecker
+        request_obj = json_api.generic.BoolValue(value="BADWOLF")
         with pytest.raises(RequestFormatObjectError):
             endpoint.dump_object(request_obj=request_obj)
 
     def test_get_response_json(self, request):
         endpoint = ApiEndpoints.system_settings.feature_flags_get
         auth = get_auth(request)
 
@@ -370,54 +387,66 @@
         response = endpoint.perform_request_raw(http=auth.http)
         monkeypatch.setattr(response, "status_code", 500)
         with pytest.raises(ResponseNotOk):
             endpoint.check_response_status(http=auth.http, response=response)
 
     def test_check_response_json_hook_status_skip(self, request, monkeypatch):
         def hook(http, response, **kwargs):
+            """Fake hook that returns True to skip status check."""
             assert isinstance(http, Http)
             assert isinstance(response, requests.Response)
+            kwargs["other"] = 2
+
             return True
 
         endpoint = ApiEndpoints.system_settings.feature_flags_get
         auth = get_auth(request)
 
-        response = endpoint.perform_request_raw(http=auth.http)
-        monkeypatch.setattr(response, "status_code", 500)
+        _response = endpoint.perform_request_raw(http=auth.http)
+        monkeypatch.setattr(_response, "status_code", 500)
 
-        endpoint.check_response_status(http=auth.http, response=response, response_status_hook=hook)
+        endpoint.check_response_status(
+            http=auth.http, response=_response, response_status_hook=hook
+        )
 
-    def test_check_response_json_hook(self, request, monkeypatch):
+    def test_check_response_json_hook(self, request):
         def hook(http, response, **kwargs):
+            """Fake hook that does not return True to skip status check."""
             assert isinstance(http, Http)
             assert isinstance(response, requests.Response)
+            kwargs["other"] = 2
 
         endpoint = ApiEndpoints.system_settings.feature_flags_get
         auth = get_auth(request)
 
-        response = endpoint.perform_request_raw(http=auth.http)
-        endpoint.check_response_status(http=auth.http, response=response, response_status_hook=hook)
+        _response = endpoint.perform_request_raw(http=auth.http)
+        endpoint.check_response_status(
+            http=auth.http, response=_response, response_status_hook=hook
+        )
 
-    def test_check_response_json_hook_fail(self, request, monkeypatch):
+    def test_check_response_json_hook_fail(self, request):
         class Failure(Exception):
+            """Fake exception to raise."""
+
             pass
 
         def hook(http, response, **kwargs):
+            """Fake hook that raises an exception."""
             assert isinstance(http, Http)
             assert isinstance(response, requests.Response)
             assert kwargs["other"] == 2
             raise Failure
 
         endpoint = ApiEndpoints.system_settings.feature_flags_get
         auth = get_auth(request)
 
-        response = endpoint.perform_request_raw(http=auth.http)
+        _response = endpoint.perform_request_raw(http=auth.http)
         with pytest.raises(Failure):
             endpoint.check_response_status(
-                http=auth.http, response=response, response_status_hook=hook, other=2
+                http=auth.http, response=_response, response_status_hook=hook, other=2
             )
 
     def test_load_response_unloaded(self, request):
         endpoint = ApiEndpoints.system_settings.feature_flags_get
         auth = get_auth(request)
         response = endpoint.perform_request_raw(http=auth.http)
         data = response.json()
@@ -428,15 +457,15 @@
         endpoint = ApiEndpoints.system_settings.feature_flags_get
         auth = get_auth(request)
         response = endpoint.perform_request_raw(http=auth.http)
         data = response.json()
         ret = endpoint.load_response(data=data, http=auth.http)
         assert isinstance(ret, endpoint.response_model_cls)
 
-    def test_load_response_fail(self, request):
+    def test_load_response_fail(self):
         endpoint = ApiEndpoints.system_settings.feature_flags_get
         data = {"wah": 2}
         with pytest.raises(ResponseLoadObjectError):
             endpoint.load_response(data=data, http=None)
 
     def test_attr_request_load_cls(self):
         endpoint = ApiEndpoint(
@@ -549,15 +578,15 @@
         exp = SomeDataModel(test=1)
         ret = endpoint.load_request(test=1)
         assert ret == exp
 
 
 class TestApiEndpointGroups:
     def test_strs(self):
-        for group_name, group in ApiEndpoints.get_groups().items():
+        for group_name, group in ApiEndpoints.get_subgroups().items():
             assert group_name in str(ApiEndpoints)
             for endpoint_name, endpoint in group.get_endpoints().items():
                 assert endpoint_name in str(group)
 
     def test_groups_used(self):
         for group in GROUPS_SUBCLS:
             err = f"{group} not in use in {GROUPS_SUBCLS_USED}"
```

## axonius_api_client/tests/tests_api/test_signup.py

```diff
@@ -5,67 +5,81 @@
 from axonius_api_client.api import json_api
 from axonius_api_client.exceptions import ApiError, ResponseNotOk
 
 from ..meta import EMAIL
 from ..utils import random_string
 
 
-class TestSignup:
-    @pytest.fixture(scope="class")
-    def apiobj(self, api_signup):
-        return api_signup
-
-
-class TestSignupPrivate(TestSignup):
-    def test_signup_get(self, apiobj):
-        data = apiobj._get()
+class TestSignupPrivate:
+    def test_signup_get(self, api_signup):
+        data = api_signup._get()
         assert isinstance(data, json_api.generic.BoolValue)
         assert data.value is True
 
-    def test_signup(self, apiobj):
+    def test_signup(self, api_signup):
         with pytest.raises(ResponseNotOk) as exc:
-            apiobj._perform(password="x", company_name="x", contact_email=EMAIL)
+            api_signup._perform(password="x", company_name="x", contact_email=EMAIL)
 
         assert "Signup already completed" in str(exc.value)
 
-    @pytest.mark.skip("broken")
-    def test_status(self, apiobj):
-        ret = apiobj._status()
+    def test_status(self, api_signup):
+        ret = api_signup._status()
         assert isinstance(ret, json_api.signup.SystemStatus)
         assert str(ret)
 
 
-class TestSignupPublic(TestSignup):
-    def test_is_signed_up(self, apiobj):
-        data = apiobj.is_signed_up
-        assert isinstance(data, bool) and data
-
-    @pytest.mark.skip("broken")
-    def test_system_status(self, apiobj):
-        ret = apiobj.system_status
+class TestSignupPublic:
+    def test_is_signed_up(self, api_signup):
+        data = api_signup.is_signed_up
+        assert isinstance(data, bool)
+
+    def test_is_licensed(self, api_signup):
+        data = api_signup.is_licensed
+        assert isinstance(data, bool)
+
+    def test_is_expired(self, api_signup):
+        data = api_signup.is_expired
+        assert isinstance(data, bool)
+
+    def test_indication_color(self, api_signup):
+        data = api_signup.indication_color
+        assert isinstance(data, str)
+
+    def test_login_options(self, api_signup):
+        data = api_signup.login_options
+        assert isinstance(data, dict) and data
+
+    def test_system_status(self, api_signup):
+        ret = api_signup.system_status
         assert isinstance(ret, json_api.signup.SystemStatus)
         assert str(ret)
 
-    def test_signup(self, apiobj):
-        with pytest.raises(ResponseNotOk) as exc:
-            apiobj.signup(password="x", company_name="x", contact_email=EMAIL)
-        assert "Signup already completed" in str(exc.value)
+    def test_signup(self, api_signup):
+        if api_signup.is_signed_up:
+            with pytest.raises(ResponseNotOk) as exc:
+                api_signup.signup(password="x", company_name="x", contact_email=EMAIL)
+            assert "Signup already completed" in str(exc.value)
 
-    def test_use_password_reset_token(self, apiobj, api_system_users, temp_user):
+    def test_use_password_reset_token(self, api_signup, api_system_users, temp_user):
         token = api_system_users.get_password_reset_link(name=temp_user.user_name)
+
+        val = api_signup.validate_password_reset_token(token=token)
+        assert val is True
+
         password = random_string(12)
 
-        user = apiobj.use_password_reset_token(token=token, password=password)
+        user = api_signup.use_password_reset_token(token=token, password=password)
         assert user.user_name == temp_user.user_name
 
-        val = apiobj.validate_password_reset_token(token=token)
+        val = api_signup.validate_password_reset_token(token=token)
         assert val is False
 
         token2 = api_system_users.get_password_reset_link(name=temp_user.user_name)
-
-        with pytest.raises(ApiError) as exc:
-            apiobj.use_password_reset_token(token="ZZZZZZZZZZZZ", password=password)
-
         with pytest.raises(ResponseNotOk) as exc:
-            apiobj.use_password_reset_token(token=token2, password=password)
+            api_signup.use_password_reset_token(token=token2, password=password)
 
         assert "password must be different" in str(exc.value)
+
+    def test_use_password_reset_token_invalid(self, api_signup):
+        with pytest.raises(ApiError) as exc:
+            api_signup.use_password_reset_token(token="BAD WOLF", password=random_string(12))
+        assert "Password reset token is not valid" in str(exc.value)
```

## axonius_api_client/tests/tests_api/tests_adapters/test_cnx.py

```diff
@@ -15,123 +15,29 @@
 )
 from axonius_api_client.tools import combo_dicts
 
 from ...meta import CSV_FILECONTENT_STR, CsvData, CsvKeys, TanData, TanKeys
 from ...utils import get_cnx_existing, get_cnx_working
 
 
+# noinspection PyProtectedMember
 def skip_if_no_adapter(api_adapters, adapter):
+    """Pass."""
     if not adapter.endswith("_adapter"):
         adapter = f"{adapter}_adapter"
 
     adapters = api_adapters._get()
     name_map = {x.id: x for x in adapters}
     found = name_map.get(adapter)
     if not found:
         pytest.skip(f"Adapter {adapter!r} not found in {list(name_map)}")
     return found
 
 
-"""
-cnx errors
- adapters.cnx._delete(
-    uuid='x',
-    adapter_name='tanium_adapter',
-    instance_id=core['id'],
-    instance_name=core['name'],
-)
-{
-    "status": "error",
-    "type": "InvalidId",
-    "message": "An error occurred. Please contact Axonius support. AX-ID: ",
-}
-
-
- adapters.cnx._delete(
-    uuid='61eae13dc44696465eacb18a',
-    adapter_name='tanium_adapter',
-    instance_id=core['id'],
-    instance_name=core['name'],
-)
-{
-    "errors": [
-        {
-            "additional_data": None,
-            "detail": "Server is already gone, please try again after refreshing the page",
-        }
-    ]
-}
-
- adapters.cnx._delete(
-    uuid='61eae13dc44696465eacb18a',
-    adapter_name='x',
-    instance_id=core['id'],
-    instance_name=core['name'],
-)
-{
-    "errors": [
-        {
-            "additional_data": None,
-            "detail": "Adapter tanium_adapter with Instance None or x not found",
-        }
-    ]
-}
-
-adapters.cnx._test(
-    adapter_name='tanium_adapter',
-    instance=instances.get_core()["id"],
-    connection={}
-)
-{
-    "errors": [
-        {
-            "additional_data": None,
-            "detail": "Adapter name and connection data are required",
-        },
-    ]
-}
-adapters.cnx._test(
-    adapter_name='tanium_adapter',
-    instance=instances.get_core()["id"],
-    connection={"d": "x"}
-)
-
-adapters.cnx._test(
-    adapter_name='tanium_adapter',
-    instance=instances.get_core()["id"],
-    connection={"username": "x"}
-)
-{
-    "status": "error",
-    "type": "JSONDecodeError",
-    "message": "An error occurred. Please contact Axonius support. AX-ID: ",
-}
-
-
-
-adapters.cnx._test(
-    adapter_name="tanium_adapter",
-    instance="53ccf92e6a1940ed9493c277312bbc6b",
-    connection={"domain": "dumdum", "username": "dumdum", "password": "dumdum"},
-)
-{
-    "errors": [
-        {
-            "additional_data": {
-                "additional_data": None,
-                "message": "Client is not reachable.",
-                "status": "error",
-            },
-            "detail": "Client is not reachable.",
-        }
-    ]
-}
-"""
-
-
+# noinspection PyMissingOrEmptyDocstring,PyProtectedMember,PyBroadException
 class TestCnxBase:
     @pytest.fixture(scope="class")
     def apiobj(self, api_adapters):
         return api_adapters
 
     @pytest.fixture(scope="class")
     def adapter(self, apiobj):
@@ -154,23 +60,30 @@
         core_instance = apiobj.instances.get_core()
         added = apiobj.cnx._add(
             connection=config,
             instance_id=core_instance["id"],
             instance_name=core_instance["name"],
             adapter_name=CsvData.adapter_name_raw,
         )
-        fetched = [
-            x
-            for x in apiobj.cnx._get(adapter_name=CsvData.adapter_name_raw).cnxs
-            if x.uuid == added.id
-        ][0]
-        yield fetched
 
-        cnxs = apiobj.cnx._get(adapter_name=CsvData.adapter_name_raw).cnxs
-        for cnx in cnxs:
+        data = apiobj.cnx._get(adapter_name=CsvData.adapter_name_raw)
+
+        for cnx in data.cnxs:
+            if cnx.uuid == added.id:
+                yield cnx
+                break
+        # fetched = [
+        #     x
+        #     for x in apiobj.cnx._get(adapter_name=CsvData.adapter_name_raw).cnxs
+        #     if x.uuid == added.id
+        # ][0]
+        # yield fetched
+
+        data = apiobj.cnx._get(adapter_name=CsvData.adapter_name_raw)
+        for cnx in data.cnxs:
             user_id = cnx.client_config.get(CsvKeys.user_id) or ""
             if user_id == CsvData.user_id:
                 try:
                     apiobj.cnx._delete(
                         uuid=cnx.uuid,
                         adapter_name=cnx.adapter_name_raw,
                         instance_id=cnx.node_id,
@@ -222,14 +135,15 @@
                         instance_name=cnx.node_name,
                     )
                 except Exception:
                     print(f"Unable to delete connection: {cnx}")
                 break
 
 
+# noinspection PyMissingOrEmptyDocstring
 class TestCnxPrivate(TestCnxBase):
     pass
 
 
 @pytest.mark.tunneltests
 class TestCnxTunnel(TestCnxBase):
     def test_get_by_adapter_tunnel(self, apiobj, csv_cnx_tunnel):
@@ -264,14 +178,15 @@
 
         cnx_deleted = apiobj.cnx.delete_by_id(
             cnx_id=cnx_new["id"], adapter_name=TanData.adapter_name
         )
         assert cnx_deleted.client_id == cnx_new["id"]
 
 
+# noinspection PyMissingOrEmptyDocstring,PyUnusedLocal
 class TestCnxPublic(TestCnxBase):
     def test_add_update_fail(self, apiobj):
         skip_if_no_adapter(apiobj, TanData.adapter_name)
         with pytest.raises(CnxAddError) as exc:
             apiobj.cnx.add(adapter_name=TanData.adapter_name, **TanData.config_bad)
 
         cnx_new = exc.value.cnx_new
@@ -354,15 +269,15 @@
             assert isinstance(cnx, dict)
             assert isinstance(cnx["schemas"], dict)
 
     def test_get_by_adapter_badname(self, apiobj):
         with pytest.raises(NotFoundError):
             apiobj.cnx.get_by_adapter(adapter_name="badwolf")
 
-    def test_get_by_adapter_badnode(self, apiobj):
+    def test_get_by_adapter_bad_node(self, apiobj):
         skip_if_no_adapter(apiobj, "csv")
         with pytest.raises(NotFoundError):
             apiobj.cnx.get_by_adapter(adapter_name=CSV_ADAPTER, adapter_node="badwolf")
 
     def test_get_by_uuid(self, apiobj):
         cnx = get_cnx_existing(apiobj)
         found = apiobj.cnx.get_by_uuid(
@@ -446,14 +361,15 @@
                 callbacks={"adapter_name": "badwolf", "adapter_node": "badwolf"},
                 source="badwolf",
             )
 
     def test_cb_file_upload_dict(self, apiobj, csv_file_path, monkeypatch):
         mock_return = {"filename": "badwolf", "uuid": "badwolf"}
 
+        # noinspection PyUnusedLocal
         def mock_file_upload(name, field_name, file_name, file_content, node):
             return mock_return
 
         monkeypatch.setattr(apiobj, "file_upload", mock_file_upload)
         result = apiobj.cnx.cb_file_upload(
             value=csv_file_path,
             schema={"name": "badwolf"},
@@ -505,15 +421,15 @@
                 value=file_path,
                 schema={"name": "badwolf"},
                 callbacks={"adapter_name": "badwolf", "adapter_node": "badwolf"},
                 source="badwolf",
             )
 
     def test_cb_file_upload_str(self, apiobj, tmp_path, monkeypatch):
-        file_path = tmp_path / "testcsv"
+        file_path = tmp_path / "test_csv"
         file_path.write_text(CSV_FILECONTENT_STR)
 
         def mock_file_upload(name, field_name, file_name, file_content, node):
             return {"filename": file_name, "uuid": "badwolf"}
 
         monkeypatch.setattr(apiobj, "file_upload", mock_file_upload)
         result = apiobj.cnx.cb_file_upload(
@@ -521,15 +437,15 @@
             schema={"name": "badwolf"},
             callbacks={"adapter_name": "badwolf", "adapter_node": "badwolf"},
             source="badwolf",
         )
         assert result == {"filename": str(file_path.name), "uuid": "badwolf"}
 
     def test_cb_file_upload_str_fail(self, apiobj, tmp_path, monkeypatch):
-        file_path = tmp_path / "testfail.csv"
+        file_path = tmp_path / "test_fail.csv"
 
         def mock_file_upload(name, field_name, file_name, file_content, node):
             return {"filename": file_name, "uuid": "badwolf"}
 
         monkeypatch.setattr(apiobj, "file_upload", mock_file_upload)
         with pytest.raises(ConfigInvalidValue):
             apiobj.cnx.cb_file_upload(
@@ -588,17 +504,17 @@
 
         with pytest.raises(NotFoundError):
             apiobj.cnx.get_by_id(
                 cnx_id=client_id,
                 adapter_name=CSV_ADAPTER,
             )
 
-    def test_add_remove_path_notexists(self, apiobj, tmp_path):
+    def test_add_remove_path_not_exists(self, apiobj, tmp_path):
         skip_if_no_adapter(apiobj, "csv")
-        file_path = tmp_path / "badtest.csv"
+        file_path = tmp_path / "bad_test.csv"
         config = {
             "user_id": "badwolf",
             "file_path": file_path,
         }
         with pytest.raises(ConfigInvalidValue):
             apiobj.cnx.add(adapter_name=CSV_ADAPTER, **config)
```

## axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks.py

```diff
@@ -1,33 +1,37 @@
 # -*- coding: utf-8 -*-
 """Test suite for assets."""
 import copy
 import io
 import logging
 import sys
+import typing as t
 
 import pytest
 
-from axonius_api_client.api.asset_callbacks import get_callbacks_cls
+from axonius_api_client.api.asset_callbacks import Base, ExportMixins, get_callbacks_cls
 from axonius_api_client.constants.api import FIELD_TRIM_LEN
 from axonius_api_client.constants.fields import SCHEMAS_CUSTOM
 from axonius_api_client.exceptions import ApiError
 
 from ...utils import get_rows_exist, get_schema, log_check, random_string
 
 
-def get_cbobj_main(apiobj, cbexport, getargs=None, state=None, store=None):
+def get_cbobj_main(
+    apiobj, cbexport, getargs=None, state=None, store=None
+) -> t.Union[Base, ExportMixins]:
+    """Get a callback object."""
     state = state or {}
     getargs = getargs or {}
     store = store or {}
 
-    cbcls = get_callbacks_cls(export=cbexport)
-    assert cbcls.CB_NAME == cbexport
+    cb_cls = get_callbacks_cls(export=cbexport)
+    assert cb_cls.CB_NAME == cbexport
 
-    cbobj = cbcls(apiobj=apiobj, getargs=getargs, state=state, store=store)
+    cbobj = cb_cls(apiobj=apiobj, getargs=getargs, state=state, store=store)
     assert cbobj.CB_NAME == cbexport
     assert cbobj.APIOBJ == apiobj
     assert cbobj.STORE == store
     assert cbobj.STATE == state
 
     assert isinstance(cbobj.ALL_SCHEMAS, dict) and cbobj.ALL_SCHEMAS
     assert isinstance(cbobj.args_map(), dict)
@@ -57,21 +61,27 @@
 
     return cbobj
 
 
 @pytest.mark.slow
 @pytest.mark.trylast
 class Callbacks:
-    def get_cbobj(self, apiobj, cbexport, getargs=None, state=None, store=None):
+    """Base class for all tests for all callbacks."""
+
+    @staticmethod
+    def get_cbobj(apiobj, cbexport, getargs=None, state=None, store=None):
+        """Get a callback object for testing."""
         return get_cbobj_main(
             apiobj=apiobj, cbexport=cbexport, getargs=getargs, state=state, store=store
         )
 
 
 class CallbacksFull(Callbacks):
+    """All tests for all callbacks."""
+
     def test_start_stop(self, cbexport, apiobj, caplog):
         getargs = {}
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs=getargs)
 
         cbobj.start()
         log_check(caplog=caplog, entries=["Starting"], exists=True)
 
@@ -109,37 +119,33 @@
 
         assert isinstance(cbobj.custom_schemas, list)
         for field, schema in SCHEMAS_CUSTOM["report_adapters_missing"].items():
             assert schema["name_qual"] in test_row
             assert schema in cbobj.custom_schemas
             assert schema in cbobj.final_schemas
 
-    def test_include_dates_false(self, cbexport, apiobj, caplog):
+    def test_include_dates_false(self, cbexport, apiobj):
         original_row = copy.deepcopy(apiobj.ORIGINAL_ROWS[0])
         test_row = copy.deepcopy(original_row)
 
-        cbobj = self.get_cbobj(
-            apiobj=apiobj, cbexport=cbexport, getargs={"include_dates": False}
-        )
+        cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"include_dates": False})
 
         rows = cbobj.add_include_dates(rows=test_row)
         assert isinstance(rows, list)
         assert len(rows) == 1
         assert rows[0] == original_row
 
         assert isinstance(cbobj.custom_schemas, list)
         assert not cbobj.custom_schemas
 
-    def test_include_dates_true(self, cbexport, apiobj, caplog):
+    def test_include_dates_true(self, cbexport, apiobj):
         original_row = copy.deepcopy(apiobj.ORIGINAL_ROWS[0])
         test_row = copy.deepcopy(original_row)
 
-        cbobj = self.get_cbobj(
-            apiobj=apiobj, cbexport=cbexport, getargs={"include_dates": True}
-        )
+        cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"include_dates": True})
 
         rows = cbobj.add_include_dates(rows=test_row)
         assert isinstance(rows, list)
         assert len(rows) == 1
         assert original_row != rows[0]
 
         assert isinstance(cbobj.custom_schemas, list)
@@ -251,15 +257,17 @@
         rows = cbobj.do_add_null_values(rows=test_row)
         assert isinstance(rows, list)
         assert len(rows) == 1
 
         assert apiobj.FIELD_ADAPTERS not in rows[0]
 
     def test_do_custom_cb(self, cbexport, apiobj):
+        # noinspection PyShadowingNames
         def cb1(self, rows):
+            """Custom callback that modifies the rows."""
             for row in rows:
                 self._row_idx = getattr(self, "_row_idx", 0)
                 row["idcaps"] = row[apiobj.FIELD_AXON_ID].upper()
                 row["idx"] = self._row_idx
                 self._row_idx += 1
             return rows
 
@@ -275,16 +283,18 @@
         assert len(rows) == 1
 
         for idx, row in enumerate(rows):
             assert idx == row["idx"]
             assert row["idcaps"] == row[apiobj.FIELD_AXON_ID].upper()
 
     def test_do_custom_cb_fail(self, cbexport, apiobj):
+        # noinspection PyShadowingNames
         def cb1(self, rows):
-            raise ValueError("boom")
+            """Fake custom callback that raises an exception."""
+            raise ValueError(f"boom {self} {rows}")
 
         original_row = copy.deepcopy(apiobj.ORIGINAL_ROWS[0])
 
         cbobj = self.get_cbobj(
             apiobj=apiobj,
             cbexport=cbexport,
             getargs={"custom_cbs": [cb1]},
@@ -568,15 +578,15 @@
         for field, value in original_row.items():
             if not isinstance(value, list) or not value:
                 continue
 
             if not isinstance(value[0], dict):
                 continue
 
-            # make sure the orginal complex field got removed from the new row
+            # make sure the original complex field got removed from the new row
             assert field not in test_row
 
             for sub_value in value:
                 row_sub_fields.update(list(sub_value))
 
         for field, value in test_row.items():
             assert not isinstance(value, dict)
@@ -584,15 +594,15 @@
             if not isinstance(value, list):
                 continue
 
             # assert no list items are complex
             assert not any([isinstance(i, (list, dict)) for i in value])
 
             if field not in original_row:
-                # assert subfields from complex added to new row are fully qual'd
+                # assert subfields from complex added to new row are fully qualified
                 assert "specific_data.data." in field
 
             if field in original_row:
                 continue
 
             if not any([field.endswith(x) for x in row_sub_fields]):
                 assert all([x is None for x in value])
@@ -672,15 +682,15 @@
         for field, value in original_row.items():
             if not isinstance(value, list) or not value:
                 continue
 
             if not isinstance(value[0], dict):
                 continue
 
-            # make sure the orginal complex field got removed from the new row
+            # make sure the original complex field got removed from the new row
             assert field not in test_row
 
             for sub_value in value:
                 row_sub_fields.update(list(sub_value))
 
         for field, value in test_row.items():
             assert not isinstance(value, dict)
@@ -688,15 +698,15 @@
             if not isinstance(value, list):
                 continue
 
             # assert no list items are complex
             assert not any([isinstance(i, (list, dict)) for i in value])
 
             if field not in original_row:
-                # assert subfields from complex added to new row are fully qual'd
+                # assert subfields from complex added to new row are fully qualified
                 assert "specific_data.data." in field
 
             if field in original_row:
                 continue
 
             if not any([field.endswith(x) for x in row_sub_fields]):
                 assert all([x == "badwolf" for x in value])
@@ -774,106 +784,108 @@
         assert test_row == rows[0]
 
     def test_schema_to_explode_error(self, cbexport, apiobj):
         cbobj = self.get_cbobj(
             apiobj=apiobj, cbexport=cbexport, getargs={"field_explode": "badwolf"}
         )
         with pytest.raises(ApiError):
+            # noinspection PyStatementEffect
             cbobj.schema_to_explode
 
     def test_schema_to_explode_success(self, cbexport, apiobj):
         field = "adapters"
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"field_explode": field})
         schema = cbobj.schema_to_explode
         assert schema["name_qual"] == field
 
     def test_echo_ok_doecho_yes(self, cbexport, apiobj, capsys, caplog):
-        entry = "xxxxxxx"
+        entry = "badwolf"
 
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"do_echo": True})
         cbobj.echo(msg=entry)
         capture = capsys.readouterr()
         assert f"{entry}\n" in capture.err
         assert not capture.out
         log_check(caplog=caplog, entries=[entry], exists=True)
 
     def test_echo_debug_doecho_yes(self, cbexport, apiobj, capsys, caplog):
-        entry = "xxxxxxx"
+        entry = "badwolf"
 
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"do_echo": True})
         cbobj.echo(msg=entry, debug=True)
         capture = capsys.readouterr()
         assert f"{entry}\n" in capture.err
         assert not capture.out
         log_check(caplog=caplog, entries=[entry], exists=True)
 
     def test_echo_warning_doecho_yes(self, cbexport, apiobj, capsys, caplog):
-        entry = "xxxxxxx"
+        entry = "badwolf"
 
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"do_echo": True})
         cbobj.echo(msg=entry, warning=True)
         capture = capsys.readouterr()
         assert f"{entry}\n" in capture.err
         assert not capture.out
         log_check(caplog=caplog, entries=[entry], exists=True)
 
     def test_echo_error_doecho_yes(self, cbexport, apiobj, capsys, caplog):
-        entry = "xxxxxxx"
+        entry = "badwolf"
 
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"do_echo": True})
         with pytest.raises(SystemExit):
             cbobj.echo(msg=entry, error=ApiError)
 
         capture = capsys.readouterr()
         assert f"{entry}\n" in capture.err
         assert not capture.out
         log_check(caplog=caplog, entries=[entry], exists=True)
 
     def test_echo_ok_doecho_no(self, cbexport, apiobj, capsys, caplog):
-        entry = "xxxxxxx"
+        entry = "badwolf"
 
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"do_echo": False})
         cbobj.echo(msg=entry)
         capture = capsys.readouterr()
         if sys.version_info >= (3, 8, 0):
             assert not capture.err
         assert not capture.out
         log_check(caplog=caplog, entries=[entry], exists=True)
 
     def test_echo_error_doecho_no(self, cbexport, apiobj, capsys, caplog):
-        entry = "xxxxxxx"
+        entry = "badwolf"
 
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"do_echo": False})
         with pytest.raises(ApiError):
             cbobj.echo(msg=entry, error=ApiError)
 
         capture = capsys.readouterr()
         if capture.err:
             lines = capture.err.splitlines()
             for line in lines:
                 assert line.startswith("ResourceWarning") or line.startswith("Exception ignored in")
         assert not capture.out
         log_check(caplog=caplog, entries=[entry], exists=True)
 
     def test_echo_error_doecho_yes_abort_no(self, cbexport, apiobj, capsys, caplog):
-        entry = "xxxxxxx"
+        entry = "badwolf"
 
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport, getargs={"do_echo": True})
         cbobj.echo(msg=entry, error=ApiError, abort=False)
 
         capture = capsys.readouterr()
         assert f"{entry}\n" in capture.err
         assert not capture.out
         log_check(caplog=caplog, entries=[entry], exists=True)
 
+    # noinspection PyMethodMayBeStatic
     def test_get_callbacks_cls_error(self):
         with pytest.raises(ApiError):
             get_callbacks_cls(export="badwolf")
 
-    def test_sw_whitelist_fail_no_sw_field(self, cbexport, apiobj, caplog):
+    def test_sw_whitelist_fail_no_sw_field(self, cbexport, apiobj):
         whitelist = ["chrome"]
         rows = copy.deepcopy(apiobj.ORIGINAL_ROWS)
 
         cbobj = self.get_cbobj(
             apiobj=apiobj,
             cbexport=cbexport,
             getargs={"report_software_whitelist": whitelist},
@@ -1014,15 +1026,17 @@
         assert len(rows) == 1
 
         for row in rows:
             for key in row:
                 assert "." not in key
 
 
-class Exports:
+class Exports(Callbacks):
+    """Tests for the exports class."""
+
     def test_fd_stdout_open_no_close(self, cbexport, apiobj):
         cbobj = self.get_cbobj(apiobj=apiobj, cbexport=cbexport)
 
         fd = cbobj.open_fd()
 
         assert fd == sys.stdout
         assert cbobj._fd == sys.stdout
```

## axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_csv.py

```diff
@@ -2,18 +2,18 @@
 """Test suite for assets."""
 import copy
 import io
 
 import pytest
 
 from ...utils import get_schema
-from .test_callbacks import Callbacks, Exports
+from .test_callbacks import Exports
 
 
-class TestCallbacksCsv(Callbacks, Exports):
+class TestCallbacksCsv(Exports):
     @pytest.fixture(params=["api_devices"], scope="class")
     def apiobj(self, request):
         return request.getfixturevalue(request.param)
 
     @pytest.fixture(scope="class")
     def cbexport(self):
         return "csv"
```

## axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json.py

```diff
@@ -2,18 +2,18 @@
 """Test suite for assets."""
 import copy
 import io
 import json
 
 import pytest
 
-from .test_callbacks import Callbacks, Exports
+from .test_callbacks import Exports
 
 
-class TestCallbacksJson(Callbacks, Exports):
+class TestCallbacksJson(Exports):
     @pytest.fixture(params=["api_devices"], scope="class")
     def apiobj(self, request):
         return request.getfixturevalue(request.param)
 
     @pytest.fixture(scope="class")
     def cbexport(self):
         return "json"
```

## axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json_to_csv.py

```diff
@@ -1,18 +1,18 @@
 # -*- coding: utf-8 -*-
 """Test suite for assets."""
 import copy
 import io
 
 import pytest
 
-from .test_callbacks import Callbacks, Exports
+from .test_callbacks import Exports
 
 
-class TestCallbacksJsonToCsv(Callbacks, Exports):
+class TestCallbacksJsonToCsv(Exports):
     @pytest.fixture(params=["api_devices"], scope="class")
     def apiobj(self, request):
         return request.getfixturevalue(request.param)
 
     @pytest.fixture(scope="class")
     def cbexport(self):
         return "json_to_csv"
```

## axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_table.py

```diff
@@ -5,18 +5,18 @@
 import io
 
 import pytest
 
 from axonius_api_client.exceptions import ApiError, StopFetch
 
 from ...utils import get_schema
-from .test_callbacks import Callbacks, Exports
+from .test_callbacks import Exports
 
 
-class TestCallbacksTable(Callbacks, Exports):
+class TestCallbacksTable(Exports):
     @pytest.fixture(params=["api_devices"], scope="class")
     def apiobj(self, request):
         return request.getfixturevalue(request.param)
 
     @pytest.fixture(scope="class")
     def cbexport(self):
         return "table"
```

## axonius_api_client/tests/tests_api/tests_assets/test_assets.py

```diff
@@ -1,25 +1,26 @@
 # -*- coding: utf-8 -*-
 """Test suite for assets."""
 import datetime
 from typing import Any, List
 
 import pytest
 
-from axonius_api_client.api import json_api, mixins
+from axonius_api_client.api import json_api, mixins, AssetMixin
 
-# from axonius_api_client.constants.api import MAX_PAGE_SIZE
-from axonius_api_client.exceptions import ApiError, NotFoundError, StopFetch
+from axonius_api_client.exceptions import ApiError, NotFoundError, StopFetch, ToolsError
 from axonius_api_client.tools import listify
 
 from ...meta import QUERIES
 from ...utils import FLAKY, check_asset, check_assets
 
 
 class WizData:
+    """Pass."""
+
     wiz_str: str = "simple active_directory:id exists"
     wiz_dict: dict = {"type": "simple", "value": "active_directory:id exists"}
     exp: dict = {
         "expressions": [
             {
                 "bracketWeight": 0,
                 "children": [
@@ -51,34 +52,42 @@
         '({"$exists":true,"$ne":""})))',
     }
     invalids: List[Any] = [1, [[]]]
     nones: List[Any] = [[], None, {}, ""]
 
 
 class ModelMixinsBase:
-    def test_model_child(self, apiobj):
+    """Pass."""
+
+    @staticmethod
+    def test_model_child(apiobj):
         child = mixins.ChildMixins(parent=apiobj)
         assert str(apiobj) in str(child)
         assert repr(apiobj) in repr(child)
 
 
 @pytest.mark.slow
 @pytest.mark.trylast
 class TestAssetsPrivate(ModelMixinsBase):
+    """Pass."""
+
     @pytest.fixture(params=["api_devices"], scope="class")
     def apiobj(self, request):
+        """Pass."""
         return request.getfixturevalue(request.param)
 
     def test_history_dates(self, apiobj):
         def check_asset_type_history_date(obj):
+            """Pass."""
             assert isinstance(obj, json_api.assets.AssetTypeHistoryDate)
             assert str(obj)
             assert repr(obj)
 
         def check_asset_type_history_dates(obj):
+            """Pass."""
             assert isinstance(obj, json_api.assets.AssetTypeHistoryDates)
             assert str(obj)
             assert repr(obj)
             assert isinstance(obj.dates, list)
             assert obj.get_date() is None
 
             if not obj.dates:
@@ -89,15 +98,15 @@
 
             for days, date in obj.dates_by_days_ago.items():
                 assert isinstance(days, int)
                 assert isinstance(date, json_api.assets.AssetTypeHistoryDate)
 
             date_oldest = sorted(obj.dates, key=lambda x: x.date)[0]
 
-            with pytest.raises(ApiError):
+            with pytest.raises(ToolsError):
                 obj.get_date_by_days_ago(value="xxx")
 
             date = obj.get_date_by_days_ago(value="0")
             assert isinstance(date, str) and date
 
             date = obj.get_date_by_days_ago(value=0)
             assert isinstance(date, str) and date
@@ -135,14 +144,15 @@
         for asset_type, dates in data.parsed.items():
             assert isinstance(asset_type, str)
             assert asset_type in data.value
             check_asset_type_history_dates(dates)
 
     def test_get_asset_page(self, apiobj, monkeypatch):
         def check_page(page):
+            """Pass."""
             state = page.create_state()
             assert isinstance(state, dict)
             assert isinstance(page, json_api.assets.AssetsPage)
             assert isinstance(page.assets, list)
             if page.asset_count_total:
                 assert len(page.assets) == 1
                 assert page.asset_count_page == 1
@@ -261,16 +271,29 @@
         query = apiobj._build_query(inner=inner, pre=pre, post=post, not_flag=True)
         assert query == f"{pre} (not ({inner})) {post}"
 
 
 class TestAssetsPublic(ModelMixinsBase):
     @pytest.fixture(params=["api_devices"], scope="class")
     def apiobj(self, request):
+        """Pass."""
         return request.getfixturevalue(request.param)
 
+    def test_cls_asset_modules(self, apiobj):
+        data = apiobj.asset_modules()
+        assert isinstance(data, list) and data
+        for item in data:
+            assert issubclass(item, AssetMixin)
+
+    def test_cls_asset_types(self, apiobj):
+        data = apiobj.asset_types()
+        assert isinstance(data, list) and data
+        for item in data:
+            assert isinstance(item, str)
+
     @FLAKY()
     def test_sort(self, apiobj):
         apiobj.get(max_rows=1, sort_field=apiobj.FIELD_MAIN, http_args={"response_timeout": 30})
 
     def test_sort_bad(self, apiobj):
         with pytest.raises(NotFoundError):
             apiobj.get(max_rows=1, sort_field="badwolf")
@@ -296,62 +319,44 @@
         assert isinstance(data, int)
 
     def test_count_by_saved_query(self, apiobj):
         sq_name = apiobj.saved_query.get()[0]["name"]
         data = apiobj.count_by_saved_query(name=sq_name)
         assert isinstance(data, int)
 
-    # def test_get_no_dups(self, apiobj):
-    #     rows = apiobj.get(generator=True)
-    #     ids = {}
-    #     for idx, row in enumerate(rows):
-    #         id = row["internal_axon_id"]
-    #         if id in ids:
-    #             raise Exception(f"Duplicate id {id} at row {idx}")
-
     @FLAKY()
     def test_get_agg_raw_data(self, apiobj):
         rows = apiobj.get(max_rows=1, fields=["agg:raw_data"], http_args={"response_timeout": 30})
         for row in rows:
             self.validate_raw_data(apiobj=apiobj, row=row, field="specific_data.data.raw_data")
 
-    def validate_raw_data(self, apiobj, row, field):
+    @staticmethod
+    def validate_raw_data(apiobj, row, field):
+        """Pass."""
         adapters = row["adapters"]
-        raw_datas = row[field]
+        raw_datas = row.get(field)
         assert isinstance(raw_datas, (dict, list)) and raw_datas
 
         raw_datas = listify(raw_datas)
         for raw_data in raw_datas:
             assert isinstance(raw_data["data"], dict)
             assert isinstance(raw_data["plugin_name"], str) and raw_data["plugin_name"]
             assert isinstance(raw_data["client_used"], str) and raw_data["client_used"]
             assert raw_data["plugin_name"] in adapters
 
     @FLAKY()
     def test_get_adapter_raw_data(self, apiobj):
-        rows = apiobj.get(
-            max_rows=1,
-            fields=["active_directory:raw_data"],
-            wiz_entries="simple active_directory:id exists",
-            http_args={"response_timeout": 30},
-        )
+        field = "adapters_data.active_directory_adapter.raw_data"
+        fields = ["active_directory:raw_data"]
+        wiz = "simple active_directory:id exists"
+        http_args = {"response_timeout": 30}
+
+        rows = apiobj.get(max_rows=1, fields=fields, wiz_entries=wiz, http_args=http_args)
         for row in rows:
-            self.validate_raw_data(
-                apiobj=apiobj, row=row, field="adapters_data.active_directory_adapter.raw_data"
-            )
-
-    # def test_get_page_size_over_max(self, apiobj):
-    #     rows = apiobj.get(page_size=3000, max_pages=1)
-    #     check_assets(rows)
-    #     assert len(rows) <= MAX_PAGE_SIZE
-
-    # def test_get_maxpages(self, apiobj):
-    #     rows = apiobj.get(page_size=2, max_pages=1)
-    #     check_assets(rows)
-    #     assert len(rows) == 2
+            self.validate_raw_data(apiobj=apiobj, row=row, field=field)
 
     @FLAKY()
     def test_get_all_agg(self, apiobj):
         rows = apiobj.get(fields="agg:all", max_rows=1, http_args={"response_timeout": 30})
         for row in rows:
             assert "specific_data" in row
             all_datas = row["specific_data"]
@@ -384,19 +389,19 @@
                 assert (
                     isinstance(all_data["accurate_for_datetime"], str)
                     and all_data["accurate_for_datetime"]
                 )
 
     @FLAKY()
     def test_get_id(self, apiobj):
-        axid = apiobj.ORIGINAL_ROWS[0]["internal_axon_id"]
+        axon_id = apiobj.ORIGINAL_ROWS[0]["internal_axon_id"]
 
-        row = apiobj.get_by_id(id=axid)
+        row = apiobj.get_by_id(id=axon_id)
         check_asset(row)
-        assert row["internal_axon_id"] == axid
+        assert row["internal_axon_id"] == axon_id
 
     def test_get_id_error(self, apiobj):
         with pytest.raises(NotFoundError):
             apiobj.get_by_id(id="badwolf")
 
     @FLAKY()
     def test_get_by_saved_query(self, apiobj):
```

## axonius_api_client/tests/tests_api/tests_assets/test_labels.py

```diff
@@ -32,15 +32,15 @@
 
         # re-get the asset and check that it has the label
         wiz_entries = f"simple labels in {labels_str}"
         assets_added = apiobj.get(wiz_entries=wiz_entries, fields="labels", max_rows=1)
         assets_added_ids = [x["internal_axon_id"] for x in assets_added]
         assert asset_id in assets_added_ids
 
-        # check the each label has been added
+        # check each label has been added
         for x in assets_added:
             for label in labels:
                 assert label in x["labels"]
 
         # check that the label is in all the labels on the system
         all_labels_post_add = apiobj.labels._get()
         assert isinstance(all_labels_post_add, list)
@@ -57,15 +57,15 @@
         assert isinstance(remove_label_result, json_api.generic.IntValue)
         assert remove_label_result.value >= 1
 
         # re-get the asset and check that it has the label
         wiz_entries = f"simple internal_axon_id equals {asset_id}"
         assets_removed = apiobj.get(wiz_entries=wiz_entries, fields="labels", max_rows=1)
 
-        # check the each label has been removed
+        # check each label has been removed
         for x in assets_removed:
             for label in labels:
                 assert label not in x.get("labels", [])
 
         # check that the label is not in all the labels on the system
         all_labels_post_remove = apiobj.labels._get()
         assert isinstance(all_labels_post_remove, list)
@@ -95,15 +95,15 @@
 
         # re-get the asset and check that it has the label
         wiz_entries = f"simple labels in {labels_str}"
         assets_added = apiobj.get(wiz_entries=wiz_entries, fields="labels", max_rows=1)
         assets_added_ids = [x["internal_axon_id"] for x in assets_added]
         assert asset_id in assets_added_ids
 
-        # check the each label has been added
+        # check each label has been added
         for x in assets_added:
             for label in labels:
                 assert label in x["labels"]
 
         # check that the label is in all the labels on the system
         all_labels_post_add = apiobj.labels.get()
         assert isinstance(all_labels_post_add, list)
@@ -117,15 +117,15 @@
         # remove the label from an asset
         remove_label_result = apiobj.labels.remove(labels=labels, rows=assets_added)
         assert remove_label_result >= 1
 
         # re-get the asset and check that it has the label
         wiz_entries = f"simple internal_axon_id equals {asset_id}"
         assets_removed = apiobj.get(wiz_entries=wiz_entries, fields="labels", max_rows=1)
-        # check the each label has been removed
+        # check each label has been removed
         for x in assets_removed:
             for label in labels:
                 assert label not in x.get("labels", [])
 
         # check that the label is not in all the labels on the system
         all_labels_post_remove = apiobj.labels.get()
         assert isinstance(all_labels_post_remove, list)
```

## axonius_api_client/tests/tests_api/tests_assets/test_saved_query.py

```diff
@@ -20,47 +20,54 @@
     SavedQueryNotFoundError,
 )
 
 from ...utils import get_schema, random_string
 
 
 class FixtureData:
-    name = "badwolf torked"
+    """Pass."""
+
+    name = "badwolf forked"
     name_asset_scope = "badwolf asset scope"
     fields = [
         "adapters",
         "last_seen",
         "id",
     ]
     sort_desc = False
     gui_page_size = GUI_PAGE_SIZES[-1]
     tags = ["badwolf1", "badwolf2"]
-    description = "badwolf torked"
+    description = "badwolf forked"
     query = 'not ("specific_data.data.last_seen" >= date("NOW - 1d"))'
-    wiz_name = "badwolf torked wiz"
+    wiz_name = "badwolf forked wiz"
     wiz_entries = "simple !last_seen last_days 1"
 
 
 class SavedQueryBase:
+    """Pass."""
+
     @pytest.fixture(params=["api_devices"], scope="class")
     def apiobj(self, request):
+        """Pass."""
         return request.getfixturevalue(request.param)
 
 
+# noinspection PyBroadException
 class TestSavedQueryPrivate(SavedQueryBase):
     def test_get(self, apiobj):
         request_obj = json_api.saved_queries.SavedQueryGet()
         result = apiobj.saved_query._get_model(request_obj=request_obj)
         assert isinstance(result, list)
         for item in result:
             assert isinstance(item, json_api.saved_queries.SavedQuery)
             validate_sq(item.to_dict())
 
+    # noinspection PyTypeChecker
     def test_add(self, apiobj):
-        name = "badwolfvvv"
+        name = "badwolf why why why"
         view = {
             "colExcludedAdapters": [],
             "colFilters": [],
             "query": {
                 "filter": "",
                 "expressions": [],
                 "search": None,
@@ -175,14 +182,15 @@
     def test_set_name_term_empty(self, apiobj):
         request_obj = json_api.saved_queries.QueryHistoryRequest()
         value = None
         exp = value
         ret = request_obj.set_name_term(value=value)
         assert ret == exp
 
+    # noinspection PyTypeChecker
     def test_set_date_no_start_date(self, apiobj):
         request_obj = json_api.saved_queries.QueryHistoryRequest()
         with pytest.raises(ApiError):
             request_obj.set_date(date_end="2020-02-20")
 
     def test_set_date_valid(self, apiobj):
         request_obj = json_api.saved_queries.QueryHistoryRequest()
@@ -917,14 +925,18 @@
             "title",
             "description",
         ]
         for updated_str_key in updated_str_keys_opt:
             val = updated_by.pop(updated_str_key, None)
             assert isinstance(val, (str, int, float)) or val is None
 
+        # 20230420
+        allowed_scopes_impersonation = updated_by.pop("allowed_scopes_impersonation", [])
+        assert isinstance(allowed_scopes_impersonation, list)
+
         assert not updated_by
 
     tags = asset.pop("tags", [])
     assert isinstance(tags, list)
     for tag in tags:
         assert isinstance(tag, str)
 
@@ -945,27 +957,27 @@
 
     page = view.pop("page", 0)
     assert isinstance(page, int)
 
     pagesize = view.pop("pageSize", 0)
     assert isinstance(pagesize, int)
 
-    # in 4.6 SQ 'New devices seen in the last day' has no sort key
+    # in 4.6 SQ 'New devices seen in the last day' has no sort keys
     if "sort" in view:
         sort = view.pop("sort")
         assert isinstance(sort, dict)
 
         sort_desc = sort.pop("desc", False)
         assert isinstance(sort_desc, bool)
 
         sort_field = sort.pop("field", "")
         assert isinstance(sort_field, str)
         assert not sort
 
-    # in 4.6 SQ 'New devices seen in the last day' has no fields key
+    # in 4.6 SQ 'New devices seen in the last day' has no fields keys
     if "fields" in view:
         fields = view.pop("fields")
         assert isinstance(fields, list)
 
         for x in fields:
             assert isinstance(x, str)
 
@@ -1075,12 +1087,16 @@
     assetExcludeAdapters = view.pop("assetExcludeAdapters", [])
     assert isinstance(assetExcludeAdapters, list) or assetExcludeAdapters is None
 
     # 4.6: 2022/04/19
     queryStrings = view.pop("queryStrings", {})
     assert isinstance(queryStrings, dict)
 
+    # 4.9: 2023/04/11
+    folder_id = query.pop("folder_id", None)
+    assert folder_id is None or isinstance(folder_id, str)
+
     assert not query, list(query)
     assert not view, list(view)
 
     document_meta = asset.pop("document_meta", {})
     assert isinstance(document_meta, dict)
```

## axonius_api_client/tests/tests_api/tests_enforcements/test_enforcements.py

```diff
@@ -67,26 +67,26 @@
 
         yield created_set
 
         deleted = self.cleanup(apiobj=apiobj, value=created_set)
         assert isinstance(deleted, EnforcementFullModel)
 
     @pytest.fixture(scope="class")
-    def created_set_trigger(self, apiobj):
+    def created_set_trigger(self, apiobj, device_sq_predefined):
         try:
             created_set = apiobj.get_set(value=Meta.name_trigger)
         except NotFoundError:
             with pytest.warns(ApiWarning):
                 created_set = apiobj.create(
                     name=Meta.name_trigger,
                     main_action_type=Meta.action_type,
                     main_action_name=Meta.action_name,
                     main_action_config=Meta.action_config,
-                    query_name=Meta.trigger_name,
-                    query_type=Meta.trigger_type,
+                    query_name=device_sq_predefined.name,
+                    query_type=device_sq_predefined.module,
                 )
 
         assert isinstance(created_set, EnforcementFullModel)
 
         yield created_set
 
         deleted = self.cleanup(apiobj=apiobj, value=created_set)
```

## axonius_api_client/tests/tests_api/tests_folders/test_folders.py

```diff
@@ -1,12 +1,13 @@
 # -*- coding: utf-8 -*-
 """Test suite."""
 import datetime
 
 import pytest
+
 from axonius_api_client.api.json_api import folders
 from axonius_api_client.data import BaseEnum
 from axonius_api_client.exceptions import (  # SearchZeroObjectsError,
     AlreadyExists,
     ApiWarning,
     AxonTypeError,
     ConfirmNotTrue,
@@ -99,15 +100,16 @@
                 existing_object.delete(confirm=True)
                 created_obj = root.create_object(**create_object_args)
 
         yield created_obj
 
         with pytest.raises(ResponseNotOk) as exc:
             created_obj.delete(confirm=True)
-        assert "was not found" in str(exc.value)
+        # 2023-04-21 error changed from "...was not found" to "No enforcement was deleted"
+        assert "was not found" in str(exc.value) or "No enforcement was deleted" in str(exc.value)
 
 
 class FolderBase:
     def test_search_objects(self, apiobj, created_obj):
         root = apiobj.get()
         api_name = root.api_folders.__class__.__name__
         folder_path = root.path_public.join_under(f"badwolf search objects for {api_name}")
@@ -660,16 +662,18 @@
         private_name: str = str(root.get_enum_names().private)
         assert isinstance(root.path_private, jsonapi_module.FolderModel)
         assert root.path_private.name == private_name
         assert root.path_private.depth == 1
 
     def test_property_path_public(self, apiobj, jsonapi_module):
         root = apiobj.get()
+        enum_names = root.get_enum_names()
         assert isinstance(root.path_public, jsonapi_module.FolderModel)
-        assert root.path_public.name == root.get_enum_names().public.value
+        expected_name = str(enum_names.public)
+        assert root.path_public.name == expected_name
         assert root.path_public.depth == 1
 
     def test_property_count_total(self, apiobj):
         root = apiobj.get()
         assert root.count_total >= 2
 
     def test_property_count_subfolders(self, apiobj):
@@ -1072,14 +1076,27 @@
     #     root = apiobj.get()
 
     #     archive_name: str = str(root.get_enum_names().archive)
     #     assert isinstance(root.path_archive, jsonapi_module.FolderModel)
     #     assert root.path_archive.name == archive_name
     #     assert root.path_archive.depth == 1
 
+    def test_property_path_public(self, api_client, apiobj, jsonapi_module):
+        root = apiobj.get()
+        enum_names = root.get_enum_names()
+        assert isinstance(root.path_public, jsonapi_module.FolderModel)
+
+        if api_client.data_scopes.is_feature_enabled:
+            expected_name = str(enum_names.global_scope)
+        else:
+            expected_name = str(enum_names.public)
+
+        assert root.path_public.name == expected_name
+        assert root.path_public.depth == 1
+
     def test_property_path_predefined(self, apiobj, jsonapi_module):
         root = apiobj.get()
 
         predefined_name: str = str(root.get_enum_names().predefined)
         assert isinstance(root.path_predefined, jsonapi_module.FolderModel)
         assert root.path_predefined.name == predefined_name
         assert root.path_predefined.depth == 2
```

## axonius_api_client/tests/tests_api/tests_json_api/test_json_api.py

```diff
@@ -220,10 +220,10 @@
 
         exp = {"test": 1}
         ret = data.dump_request(schema_cls=data.schema)
         assert ret == exp
 
     def test_dump_request_params(self):
         data = json_api.resources.ResourcesGet(page={"limit": 20, "offset": 3})
-        exp = {"page[limit]": 20, "page[offset]": 3, "get_metadata": True, "search": ""}
+        exp = {"page[limit]": 20, "page[offset]": 3, "get_metadata": True}
         ret = data.dump_request_params()
         assert ret == exp
```

## axonius_api_client/tests/tests_api/tests_system/test_dashboard.py

```diff
@@ -5,14 +5,16 @@
 import pytest
 
 from axonius_api_client.api import json_api
 from axonius_api_client.api.system.dashboard import DiscoverData, DiscoverPhase
 
 
 class DashboardBase:
+    """Pass."""
+
     @pytest.fixture(scope="class")
     def apiobj(self, api_dashboard):
         return api_dashboard
 
 
 class TestDashboardPrivate(DashboardBase):
     def test_private_lifecycle(self, apiobj):
```

## axonius_api_client/tests/tests_api/tests_system/test_meta.py

```diff
@@ -50,15 +50,17 @@
         assert isinstance(sws, dict) or sws is None
         # 2022-12-20
         tickets = entity_sizes.pop("Tickets", None)  # 4.7
         assert isinstance(tickets, dict) or tickets is None
 
         generic_assets = entity_sizes.pop("GenericAssets", None)  # 4.8
         assert isinstance(generic_assets, dict) or generic_assets is None
-        assert not entity_sizes
+
+        # 2023-04-22 many more types added to entity_sizes, not going to check them all
+        # assert not entity_sizes
         assert not data
 
     def val_about(self, data):
         """Pass."""
         if data:
             data = copy.deepcopy(data)
             build_date = data.pop("Build Date")
```

## axonius_api_client/tests/tests_api/tests_system/test_settings.py

```diff
@@ -2,15 +2,15 @@
 """Test suite."""
 
 # import os
 # import subprocess
 
 import pytest
 
-from axonius_api_client import cert_human
+from axonius_api_client.projects import cert_human
 from axonius_api_client.api.json_api.generic import ApiBase
 from axonius_api_client.api.json_api.system_settings import CertificateDetails
 from axonius_api_client.exceptions import ApiError, NotFoundError
 
 GUI_SECTION_WITH_SUBS = "system_settings"
 GUI_SECTION_NO_SUBS = "mutual_tls_settings"
 GUI_NON_SUB_SECTION = "exactSearch"
@@ -27,15 +27,15 @@
 
     def val_sub_section(self, name, meta, settings):
         assert isinstance(meta, dict) and meta
         assert meta["settings_title"] == settings["settings_title"]
         assert meta["name"] == name
         assert isinstance(meta["title"], str)
         assert isinstance(meta["schemas"], dict) and meta["schemas"]
-        # pre-4.3 sub section "login_settings" of "system_settings" (GUI Settings)
+        # pre-4.3 sub section "login_settings" of "system_settings" (GUI SettingsUpdate)
         # has its own sub section ldap_login
         assert isinstance(meta["sub_sections"], dict)  # and not meta["sub_sections"]
         assert isinstance(meta["parent_name"], str) and meta["parent_name"]
         assert isinstance(meta["parent_title"], str) and meta["parent_title"]
 
         assert isinstance(meta["config"], dict)
 
@@ -294,17 +294,17 @@
             "enable_destroy": False,
             "enable_factory_reset": False,
             "enabled": False,
             "minimize_output": False,
         }
         """
         {
-            "settings_title": "Global Settings",
+            "settings_title": "Global SettingsUpdate",
             "name": "api_settings",
-            "title": "API Settings",
+            "title": "API SettingsUpdate",
             "schemas": {
                 "enabled": {
                     "name": "enabled",
                     "title": "Enable advanced API settings",
                     "type": "bool",
                     "required": True,
                 },
```

## axonius_api_client/tests/tests_auth/test_auth.py

```diff
@@ -1,96 +1,132 @@
 # -*- coding: utf-8 -*-
 """Test suite for axonius_api_client.tools."""
+import typing as t
 import pytest
 
-# from axonius_api_client.api.routers import API_VERSION
-from axonius_api_client.auth import ApiKey
-from axonius_api_client.exceptions import (
-    AlreadyLoggedIn,
-    AuthError,
-    InvalidCredentials,
-    NotLoggedIn,
-)
+from axonius_api_client.auth import AuthApiKey, AuthCredentials, AuthModel, AuthNull
+from axonius_api_client.exceptions import InvalidCredentials, NotLoggedIn
 from axonius_api_client.http import Http
+from ..utils import get_connect
 
-from ..utils import get_key_creds, get_url
+BAD_CREDS = "bad wolf"
 
 
-class TestApiKey:
-    """Test axonius_api_client.auth."""
+# noinspection PyMethodMayBeStatic
+class AuthBase:
+    """Base class for auth tests."""
 
-    def test_valid_creds(self, request):
-        """Test str/repr has URL."""
-        http = Http(url=get_url(request), certwarn=False)
-
-        auth = ApiKey(http=http, **get_key_creds(request))
-
-        auth.login()
-
-        assert auth.is_logged_in
-        assert "url" in format(auth)
+    def test_str(self, auth_valid: dict, auth_cls: t.Type[AuthModel]):
+        auth: AuthModel = auth_cls(**auth_valid)
+        assert isinstance(auth.fields, list)
+        assert "url" in str(auth)
         assert "url" in repr(auth)
 
-    def test_logout(self, request):
-        """Test no exc when logout() after login()."""
-        http = Http(url=get_url(request), certwarn=False)
-
-        auth = ApiKey(http=http, **get_key_creds(request))
-
-        auth.login()
+    def test_logout(self, auth_valid: dict, auth_cls: t.Type[AuthModel]):
+        auth: AuthModel = auth_cls(**auth_valid)
 
-        assert auth.is_logged_in
+        action = auth.logout()
+        assert action is False
+        assert auth.is_logged_in is False
 
-        auth.logout()
+        action = auth.login()
+        assert action is True
+        assert auth.is_logged_in is True
 
-        assert not auth.is_logged_in
+        action = auth.logout()
+        assert action is True
+        assert auth.is_logged_in is False
 
-    # def test_old_version(self, request, monkeypatch):
-    #     """Test exc thrown when login() and login() already called."""
-    #     monkeypatch.setattr(ApiKey, "_validate_path", "api/badwolf")
+        action = auth.logout()
+        assert action is False
 
-    #     http = Http(url=get_url(request), certwarn=False)
-    #     auth = ApiKey(http=http, **get_key_creds(request))
-    #     with pytest.raises(AuthError):
-    #         auth.login()
+    def test_login_already_logged_in(self, auth_valid: dict, auth_cls: t.Type[AuthModel]):
+        auth: AuthModel = auth_cls(**auth_valid)
 
-    def test_login_already_logged_in(self, request):
-        """Test exc thrown when login() and login() already called."""
-        http = Http(url=get_url(request), certwarn=False)
-
-        auth = ApiKey(http=http, **get_key_creds(request))
-
-        auth.login()
-
-        with pytest.raises(AlreadyLoggedIn):
-            auth.login()
+        action = auth.login()
+        assert action is True
+        assert auth.is_logged_in is True
 
-    def test_logout_not_logged_in(self, request):
-        """Test exc thrown when logout() but login() not called."""
-        http = Http(url=get_url(request), certwarn=False)
-
-        auth = ApiKey(http=http, **get_key_creds(request))
-
-        with pytest.raises(NotLoggedIn):
-            auth.logout()
-
-    def test_invalid_creds(self, request):
-        """Test str/repr has URL."""
-        http = Http(url=get_url(request), certwarn=False)
-
-        bad = "badwolf"
-
-        auth = ApiKey(http=http, key=bad, secret=bad)
+        action = auth.login()
+        assert action is False
+        assert auth.is_logged_in is True
 
+    def test_login_invalid_credentials(self, auth_invalid: dict, auth_cls: t.Type[AuthModel]):
+        auth: AuthModel = auth_cls(**auth_invalid)
         with pytest.raises(InvalidCredentials):
             auth.login()
 
-    def test_http_lock_fail(self, request):
-        """Test using an http client from another authmethod throws exc."""
-        http = Http(url=get_url(request), certwarn=False)
-
-        auth = ApiKey(http=http, **get_key_creds(request))
+    def test_not_logged_in(self, auth_invalid: dict, auth_cls: t.Type[AuthModel]):
+        auth: AuthModel = auth_cls(**auth_invalid)
+        with pytest.raises(NotLoggedIn):
+            auth.check_login()
 
-        assert auth.http._auth_lock
 
-        with pytest.raises(AuthError):
-            auth = ApiKey(http=http, **get_key_creds(request))
+class TestNull(AuthBase):
+    @pytest.fixture()
+    def auth_valid(self, arg_url_http: Http) -> dict:
+        """Get credentials from command line args."""
+        return {"http": arg_url_http}
+
+    @pytest.fixture()
+    def auth_invalid(self, arg_url_http: Http) -> dict:
+        """Get credentials from command line args."""
+        return {"http": arg_url_http}
+
+    @pytest.fixture()
+    def auth_cls(self) -> t.Type[AuthModel]:
+        """Get the class to run tests against."""
+        return AuthNull
+
+    def test_login_invalid_credentials(self, auth_invalid: dict, auth_cls: t.Type[AuthModel]):
+        auth: AuthModel = auth_cls(**auth_invalid)
+        action = auth.login()
+        assert action is True
+        assert auth.is_logged_in is True
+
+
+class TestCredentials(AuthBase):
+    """Test suite for axonius_api_client.auth.AuthCredentials."""
+
+    @pytest.fixture()
+    def auth_valid(
+        self, arg_url_http: Http, arg_key: str, arg_secret: str, arg_credentials: bool
+    ) -> dict:
+        """Get credentials from command line args."""
+        if not arg_credentials:
+            pytest.skip("credentials=False, skipping test")
+        return {"http": arg_url_http, "username": arg_key, "password": arg_secret}
+
+    @pytest.fixture()
+    def auth_invalid(self, arg_url_http: Http) -> dict:
+        """Get credentials from command line args."""
+        return {"http": arg_url_http, "username": BAD_CREDS, "password": BAD_CREDS}
+
+    @pytest.fixture()
+    def auth_cls(self) -> t.Type[AuthModel]:
+        """Get the class to run tests against."""
+        return AuthCredentials
+
+
+class TestApiKey(AuthBase):
+    @pytest.fixture()
+    def auth_valid(
+        self, request, arg_url_http: Http, arg_key: str, arg_secret: str, arg_credentials: bool
+    ) -> dict:
+        """Get credentials from command line args."""
+        if arg_credentials:
+            client = get_connect(request)
+            client.start()
+            arg_key = client.api_keys["api_key"]
+            arg_secret = client.api_keys["api_secret"]
+
+        return {"http": arg_url_http, "key": arg_key, "secret": arg_secret}
+
+    @pytest.fixture()
+    def auth_invalid(self, arg_url_http: Http) -> dict:
+        """Get credentials from command line args."""
+        return {"http": arg_url_http, "key": BAD_CREDS, "secret": BAD_CREDS}
+
+    @pytest.fixture()
+    def auth_cls(self) -> t.Type[AuthModel]:
+        """Get the class to run tests against."""
+        return AuthApiKey
```

## axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add_multiple_from_json.py

```diff
@@ -22,22 +22,18 @@
         "'config' with value None type <class 'NoneType'> is not a <class 'dict'> or is empty",
     ],
     '[{"adapter_name": "xx"}]': [
         "Connection item #1/1 initial parsing errors:",
         "'config' key is required but was not supplied",
         "'config' with value None type <class 'NoneType'> is not a <class 'dict'> or is empty",
     ],
-    '[{"adapter": 111, "config": [], "save_and_fetch": "doot", "active": "foot"}]': [
+    '[{"adapter": 111, "config": [], "save_and_fetch": "bad", "active": "foot"}]': [
         "Connection item #1/1 initial parsing errors:",
-        "'active' is not a valid bool: Supplied value 'foot' of type str must be one of:",
-        "  For True: True, 1, '1', 'true', 't', 'yes', 'y', 'on'",
-        "  For False: False, 0, '0', 'false', 'f', 'no', 'n', 'off'",
-        "'save_and_fetch' is not a valid bool: Supplied value 'doot' of type str must be one of:",
-        "  For True: True, 1, '1', 'true', 't', 'yes', 'y', 'on'",
-        "  For False: False, 0, '0', 'false', 'f', 'no', 'n', 'off'",
+        "'active' is not a valid bool",
+        "'save_and_fetch' is not a valid bool",
         "'adapter_name' with value 111 type <class 'int'> is not a <class 'str'> or is empty",
         "'config' with value [] type <class 'list'> is not a <class 'dict'> or is empty",
     ],
     '[{"adapter_name": "banzarbar", "config": {"x": "y"}}]': [
         "No adapter named 'banzarbar' found on instance",
     ],
 }
@@ -165,15 +161,15 @@
 
             for exp in exps:
                 assert exp in result.stderr
 
     def test_errors_no_abort_add_error(self, api_adapters, request, monkeypatch):
         runner = load_clirunner(request, monkeypatch)
         with runner.isolated_filesystem():
-            content = '[{"adapter_name": "csv", "config": {"user_id": "shanananannanana"}}]'
+            content = '[{"adapter_name": "csv", "config": {"user_id": "badwolf"}}]'
             exps = [
                 "Connection item #1/1 Connection added with error:",
                 # "Error: Error - No way to find the resource from config.",
                 "Added 1 out of 1 connections (error count: 1)",
             ]
             args = [
                 "adapters",
@@ -201,15 +197,15 @@
             file_path = """CONTENT:"name","mac_address","extra_field"
 "why","01:37:53:9E:82:7C","foo1"
 "cuz","01:37:53:9E:82:8C","foo2"
 """
             content_obj = [
                 {
                     "adapter_name": "csv",
-                    "config": {"user_id": "alllll from str", "file_path": file_path},
+                    "config": {"user_id": "badwolf from str", "file_path": file_path},
                 }
             ]
             content = json_dump(content_obj)
             exps = [
                 "Schema 'file_path': Resolved value to PosixPath",
                 "Connection item #1/1 Connection added with no errors",
                 "Added 1 out of 1 connections (error count: 0)",
```

## axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_copy.py

```diff
@@ -9,37 +9,38 @@
 from axonius_api_client.tools import json_load
 
 from ...tests_api.tests_enforcements.test_enforcements import EnforcementsBase, Meta
 from ...utils import load_clirunner
 
 
 class TestGrpEnforcementsCmdCopyUpdate(EnforcementsBase):
-    def test_multi(self, request, monkeypatch, apiobj, created_set):
+    def test_multi(self, request, monkeypatch, apiobj, created_set_trigger):
         self.cleanup(apiobj=apiobj, value=Meta.name_copy)
         self.cleanup(apiobj=apiobj, value=Meta.name_rename_cli)
 
         runner = load_clirunner(request, monkeypatch)
         with runner.isolated_filesystem():
             # COPY
             args = [
                 "enforcements",
                 "copy",
                 "--value",
-                created_set.name,
+                created_set_trigger.name,
                 "--name",
                 Meta.name_copy,
                 "--export-format",
                 "json",
             ]
             result = runner.invoke(cli=cli, args=args)
 
             assert result.stdout
             assert result.stderr
             assert result.exit_code == 0
-            assert created_set.name in result.stdout
+            assert Meta.name_copy in result.stdout
+            assert created_set_trigger.name not in result.stdout
 
             data = json_load(result.stdout)
             assert isinstance(data, dict) and data
 
             # RENAME
             args = [
                 "enforcements",
@@ -53,15 +54,16 @@
             ]
             result = runner.invoke(cli=cli, args=args)
 
             assert result.stdout
             assert result.stderr
             assert result.exit_code == 0
             assert Meta.name_rename_cli in result.stdout
-            assert Meta.name not in result.stdout
+            assert Meta.name_copy not in result.stdout
+            assert created_set_trigger.name not in result.stdout
 
             data = json_load(result.stdout)
             assert isinstance(data, dict) and data
 
             # update main action
             args = [
                 "enforcements",
```

## axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_create.py

```diff
@@ -9,23 +9,25 @@
 from axonius_api_client.tools import json_load
 
 from ...tests_api.tests_enforcements.test_enforcements import EnforcementsBase, Meta
 from ...utils import load_clirunner
 
 
 class TestGrpEnforcementsCmdCreateDelete(EnforcementsBase):
-    def test_config_stdin(self, request, monkeypatch, apiobj):
+    def test_config_stdin(self, request, monkeypatch, apiobj, device_sq_predefined):
         self.cleanup(apiobj=apiobj, value=Meta.name_cli)
         runner = load_clirunner(request, monkeypatch)
         with runner.isolated_filesystem():
             args = [
                 "enforcements",
                 "create",
                 "--name",
                 Meta.name_cli,
+                "--query-name",
+                device_sq_predefined.name,
                 "--main-action-name",
                 Meta.action_name,
                 "--main-action-type",
                 Meta.action_type,
                 "--export-format",
                 "json",
             ]
@@ -59,27 +61,29 @@
             assert Meta.name_cli in result.stdout
 
             data = json_load(result.stdout)
             assert isinstance(data, dict) and data
 
         self.cleanup(apiobj=apiobj, value=Meta.name_cli)
 
-    def test_config_file(self, request, monkeypatch, apiobj):
+    def test_config_file(self, request, monkeypatch, apiobj, device_sq_predefined):
         self.cleanup(apiobj=apiobj, value=Meta.name_cli)
         runner = load_clirunner(request, monkeypatch)
         with runner.isolated_filesystem():
             config = "config.json"
             with open(config, "w") as fh:
                 json.dump(Meta.action_config, fh)
 
             args = [
                 "enforcements",
                 "create",
                 "--name",
                 Meta.name_cli,
+                "--query-name",
+                device_sq_predefined.name,
                 "--main-action-name",
                 Meta.action_name,
                 "--main-action-type",
                 Meta.action_type,
                 "--main-action-config",
                 config,
                 "--export-format",
```

## axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_shell.py

```diff
@@ -48,16 +48,16 @@
         assert stdout1
         assert stderr1
         assert exit_code1 == 0
 
         outlines1 = stdout1.splitlines()
 
         assert outlines1[0] == "URL: {}".format(url)
-        assert outlines1[1] == "API Key of user: "
-        assert outlines1[2] == "API Secret of user: "
+        assert outlines1[1].startswith("API Key")
+        assert outlines1[2].startswith("API Secret")
 
     def test_no_prompt(self, request, monkeypatch):
         try:
             import readline  # noqa
         except Exception:
             pytest.skip("No readline support")
```

## axonius_api_client/tests/tests_pkg/test_connect.py

```diff
@@ -1,128 +1,196 @@
 # -*- coding: utf-8 -*-
 """Test suite for axonius_api_client.tools."""
+import axonius_api_client as axonapi
 import logging
 
 import pytest
 
 from axonius_api_client.connect import Connect
 from axonius_api_client.exceptions import ConnectError, InvalidCredentials
 from axonius_api_client.http import requests
+from axonius_api_client.api import json_api
 
-from ..utils import IS_LINUX, get_key_creds, get_url
+from ..utils import IS_LINUX, get_url, get_connect
 
 BAD_CRED = "tardis"
 
 
 class TestConnect:
-    def test_no_start(self, request):
+    def test_log_http_max(self, request):
+        client = get_connect(request, log_http_max=True)
+        assert client.LOG_HTTP_MAX is True
+        assert client.ARGS_HTTP["log_request_body"] is True
+        assert client.ARGS_HTTP["log_response_body"] is True
+        assert client.ARGS_HTTP["log_request_attrs"] == "all"
+        assert client.ARGS_HTTP["log_response_attrs"] == "all"
+        assert client.ARGS_HTTP["log_body_lines"] >= 10000
+
+    def test_check_binding(self, test_no_start, request):
         ax_url = get_url(request)
 
-        c = Connect(url=ax_url, key=BAD_CRED, secret=BAD_CRED)
+        with pytest.raises(ConnectError):
+            Connect(url=ax_url, key=BAD_CRED, secret=BAD_CRED, http=test_no_start.http)
 
-        assert "Not connected" in format(c)
-        assert "Not connected" in repr(c)
-        assert c.HANDLER_FILE is None
-        assert c.HANDLER_CON is None
+    def test_set_log_level_connect(self, test_start):
+        test_start.set_log_level_connect("info")
+        assert test_start.LOG.level == logging.INFO
 
-    def test_no_start_logs(self, request):
-        ax_url = get_url(request)
+    def test_set_log_level_http(self, test_start):
+        test_start.set_log_level_http("warning")
+        assert test_start.http.LOG.level == logging.WARNING
 
-        c = Connect(url=ax_url, key=BAD_CRED, secret=BAD_CRED, log_console=True, log_file=True)
+    def test_set_log_level_auth(self, test_start):
+        test_start.set_log_level_auth("error")
+        assert test_start.auth.LOG.level == logging.ERROR
 
-        assert "Not connected" in format(c)
-        assert "Not connected" in repr(c)
-        assert isinstance(c.HANDLER_FILE, logging.Handler)
-        assert isinstance(c.HANDLER_CON, logging.Handler)
+    def test_set_log_level_package(self, test_start):
+        test_start.set_log_level_package("debug")
+        assert axonapi.LOG.level == logging.DEBUG
 
-    def test_start(self, request):
-        ax_url = get_url(request)
+    def test_set_log_level_api(self, test_start):
+        str(test_start.settings_gui)
+        test_start.set_log_level_api("error")
+        assert test_start.API_LOG_LEVEL == "ERROR"
+        assert test_start.settings_ip.LOG.level == logging.ERROR
+        assert test_start.settings_gui.LOG.level == logging.ERROR
 
-        c = Connect(url=ax_url, certwarn=False, **get_key_creds(request))
+    def test_control_log_console(self, request):
+        client = get_connect(request)
 
-        c.start()
+        result = client.control_log_console(True)
+        assert result is True
+        assert client.HANDLER_CON
 
-        assert "Connected" in format(c)
-        assert "Connected" in repr(c)
+        client.set_log_level_console("critical")
+        assert client.HANDLER_CON.level == logging.CRITICAL
 
-        props = [
-            "activity_logs",
-            "adapters",
-            "dashboard",
-            "devices",
-            "enforcements",
-            "instances",
-            "meta",
-            "remote_support",
-            "settings_global",
-            "settings_gui",
-            "settings_ip",
-            "settings_lifecycle",
-            "signup",
-            "system_roles",
-            "system_users",
-            "users",
-        ]
-        for prop in props:
-            prop_attr = getattr(c, prop)
-            assert format(prop_attr)
-            assert repr(prop_attr)
+        result = client.control_log_console(True)
+        assert result is False
+        assert client.HANDLER_CON
 
-    def test_invalid_creds(self, request):
-        ax_url = get_url(request)
+        result = client.control_log_console(False)
+        assert result is True
+        assert client.HANDLER_CON is None
 
-        c = Connect(url=ax_url, key=BAD_CRED, secret=BAD_CRED, certwarn=False)
+        result = client.control_log_console(False)
+        assert result is False
+        assert client.HANDLER_CON is None
 
-        c.HTTP.CONNECT_TIMEOUT = 1
+    def test_control_log_file(self, request, tmp_path):
+        log_file_name = tmp_path / "test.log"
+        client = get_connect(request, log_file_name=log_file_name)
 
-        with pytest.raises(ConnectError) as exc:
-            c.start()
+        result = client.control_log_file(True)
+        assert result is True
+        assert client.HANDLER_FILE
 
-        assert isinstance(exc.value.exc, InvalidCredentials)
+        client.set_log_level_file("info")
+        assert client.HANDLER_FILE.level == logging.INFO
 
-    def test_connect_timeout(self):
-        c = Connect(url="127.0.0.99", key=BAD_CRED, secret=BAD_CRED, certwarn=False)
+        result = client.control_log_file(True, rotate=True)
+        assert result is False
+        assert client.HANDLER_FILE
 
-        c.HTTP.CONNECT_TIMEOUT = 1
+        result = client.control_log_file(False)
+        assert result is True
+        assert client.HANDLER_FILE is None
 
-        with pytest.raises(ConnectError) as exc:
-            c.start()
+        result = client.control_log_file(False)
+        assert result is False
+        assert client.HANDLER_FILE is None
 
-        if IS_LINUX:
-            assert isinstance(exc.value.exc, requests.ConnectionError)
-        else:
-            assert isinstance(exc.value.exc, requests.ConnectTimeout)
+    def test_ssl_days_left(self, test_no_start):
+        assert test_no_start.ssl_days_left > 0
 
-    def test_connect_error(self):
-        c = Connect(url="https://127.0.0.1:3919", key=BAD_CRED, secret=BAD_CRED, certwarn=False)
+    def test_get_api_keys(self, test_start):
+        assert isinstance(test_start.api_keys, dict) and test_start.api_keys
 
-        c.HTTP.CONNECT_TIMEOUT = 1
+    @pytest.fixture(scope="class")
+    def test_no_start(self, request):
+        client = get_connect(request, key=BAD_CRED, secret=BAD_CRED, timeout_connect=1)
+        assert "Not connected" in format(client)
+        assert "Not connected" in repr(client)
+        assert client.HANDLER_FILE is None
+        assert client.HANDLER_CON is None
+        yield client
 
-        with pytest.raises(ConnectError) as exc:
-            c.start()
-        assert isinstance(exc.value.exc, requests.ConnectionError)
+    def test_jdump(self, test_no_start):
+        test_no_start.jdump(test_no_start)
 
-    def test_invalid_creds_nowrap(self, request):
-        ax_url = get_url(request)
+    def test_no_start_logs(self, request):
+        client = get_connect(request, log_console=True, log_file=True)
+        assert "Not connected" in format(client)
+        assert "Not connected" in repr(client)
+        assert isinstance(client.HANDLER_FILE, logging.Handler)
+        assert isinstance(client.HANDLER_CON, logging.Handler)
 
-        c = Connect(url=ax_url, key=BAD_CRED, secret=BAD_CRED, certwarn=False, wraperror=False)
+    @pytest.fixture(scope="class")
+    def test_start(self, request):
+        client = get_connect(request)
+        client.start()
+        assert "Connected" in format(client)
+        assert "Connected" in repr(client)
+        for attr in client.API_ATTRS:
+            value = getattr(client, attr)
+            assert format(value)
+            assert repr(value)
+        yield client
+
+    def test_current_user(self, test_start):
+        user = test_start.current_user
+        assert isinstance(user, json_api.account.CurrentUser)
 
-        c.HTTP.CONNECT_TIMEOUT = 1
+    def test_invalid_creds(self, request):
+        client = get_connect(request, key=BAD_CRED, secret=BAD_CRED, timeout_connect=1)
+        with pytest.raises(ConnectError) as exc:
+            client.start()
+        assert isinstance(exc.value.exc, InvalidCredentials)
 
-        with pytest.raises(InvalidCredentials):
-            c.start()
+    def test_connect_timeout(self):
+        client = Connect(
+            url="127.0.0.99",
+            key=BAD_CRED,
+            secret=BAD_CRED,
+            certwarn=False,
+            timeout_connect=1,
+        )
+        with pytest.raises(ConnectError) as exc:
+            client.start()
 
-    def test_other_exc(self, request):
-        c = Connect(url="127.0.0.1", key=BAD_CRED, secret=BAD_CRED, certwarn=False)
+        exp_exc = requests.ConnectionError if IS_LINUX else requests.ConnectTimeout
+        assert isinstance(exc.value.exc, exp_exc)
 
-        c.HTTP.CONNECT_TIMEOUT = 1
-        c.AUTH._creds = None
+    def test_connect_error(self):
+        client = Connect(
+            url="https://127.0.0.1:3919",
+            key=BAD_CRED,
+            secret=BAD_CRED,
+            certwarn=False,
+            timeout_connect=1,
+        )
+        with pytest.raises(ConnectError) as exc:
+            client.start()
+        assert isinstance(exc.value.exc, requests.ConnectionError)
+
+    def test_invalid_creds_nowrap(self, request):
+        client = get_connect(
+            request, key=BAD_CRED, secret=BAD_CRED, timeout_connect=1, wraperror=False
+        )
+        with pytest.raises(InvalidCredentials):
+            client.start()
 
+    def test_other_exc(self):
+        client = Connect(
+            url="127.0.0.1",
+            certwarn=False,
+            key=BAD_CRED,
+            secret=BAD_CRED,
+            timeout_connect=1,
+        )
         with pytest.raises(ConnectError):
-            c.start()
+            client.start()
 
     def test_reason(self):
         exc = Exception("badwolf")
-
         reason = Connect._get_exc_reason(exc)
-
         assert format(reason) == "badwolf"
```

## axonius_api_client/tests/tests_pkg/test_http.py

```diff
@@ -1,138 +1,117 @@
 # -*- coding: utf-8 -*-
 """Test suite for axonius_api_client.http."""
 import logging
 
 import pytest
 import requests
 
+import urllib3.exceptions
+
 from axonius_api_client.exceptions import HttpError
 from axonius_api_client.http import Http
-from axonius_api_client.parsers.url_parser import UrlParser
+from axonius_api_client.projects.url_parser import UrlParser
+from axonius_api_client.projects import cert_human
 from axonius_api_client.version import __version__
 
 from ..meta import TEST_CLIENT_CERT, TEST_CLIENT_CERT_NAME, TEST_CLIENT_KEY, TEST_CLIENT_KEY_NAME
-from ..utils import get_url, log_check
+from ..utils import (
+    get_arg_url,
+    log_check,
+    get_http,
+)
 
-InsecureRequestWarning = requests.urllib3.exceptions.InsecureRequestWarning
+InsecureRequestWarning = urllib3.exceptions.InsecureRequestWarning
 
 
-@pytest.mark.skip("httpbin_secure issues")
 class TestHttp:
     """Test Http."""
 
     def test_str_repr(self, request):
         """Test str/repr has URL."""
-        ax_url = get_url(request)
-
+        ax_url = get_arg_url(request)
         http = Http(url=ax_url)
-
         assert ax_url in format(http)
         assert ax_url in repr(http)
 
     def test_parsed_url(self, request):
         """Test url=UrlParser() works."""
-        ax_url = get_url(request)
-
+        ax_url = get_arg_url(request)
         parsed_url = UrlParser(url=ax_url, default_scheme="https")
-
         http = Http(url=parsed_url)
-
         assert ax_url in format(http)
         assert ax_url in repr(http)
 
     def test_user_agent(self, request):
         """Test user_agent has version in it."""
-        ax_url = get_url(request)
-
+        ax_url = get_arg_url(request)
         http = Http(url=ax_url)
         assert __version__ in http.user_agent
 
-    def test_certwarn_true(self, request, httpbin_secure):
-        """Test quiet_urllib=False shows warning from urllib3."""
+    def test_certwarn_true(self, httpbin_secure):
         url = httpbin_secure.url
         http = Http(url=url, certwarn=True, save_history=True)
-
         with pytest.warns(InsecureRequestWarning):
             http()
 
-    # def test_certwarn_false(self, request, httpbin_secure):
-    #     """Test quiet_urllib=False shows warning from urllib3."""
-    #     url = httpbin_secure.url
-    #     http = Http(url=url, certwarn=False)
-
-    #     with pytest.warns() as record:
-    #         response = http()
-    #     assert response
-    #     assert record
-
-    # @pytest.mark.skipif(sys.version_info < (3, 6), reason="requires python3.6 or higher")
-    # def test_verify_ca_bundle(self, request, httpbin_secure, httpbin_ca_bundle):
-    #     Test quiet_urllib=False no warning from urllib3 when using ca bundle.
-    #     url = httpbin_secure.url
-    #     http = Http(url=url, certwarn=False)
-    #     with pytest.warns() as record:
-    #         response = http()
-    #     assert response.status_code == 200
-    #     assert record
+    def test_certwarn_false(self, httpbin_secure):
+        url = httpbin_secure.url
+        http = Http(url=url, certwarn=False)
+        with pytest.warns() as record:
+            response = http()
+        assert response
+        assert record
 
     def test_save_last_true(self, request):
         """Test last req/resp with save_last=True."""
-        ax_url = get_url(request)
-
+        ax_url = get_arg_url(request)
         http = Http(url=ax_url, save_last=True, certwarn=False)
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
         assert response == http.LAST_RESPONSE
-        assert response.request == http.LAST_REQUEST
+        # request can be different due to redirects
+        # assert response.request == http.LAST_REQUEST
 
     def test_save_last_false(self, request):
         """Test last req/resp with save_last=False."""
-        ax_url = get_url(request)
-
+        ax_url = get_arg_url(request)
         http = Http(url=ax_url, save_last=False, certwarn=False)
-
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
         assert not http.LAST_RESPONSE
         assert not http.LAST_REQUEST
 
     def test_save_history(self, request):
         """Test last resp added to history with save_history=True."""
-        ax_url = get_url(request)
-
+        ax_url = get_arg_url(request)
         http = Http(url=ax_url, save_history=True, certwarn=False)
-
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
         assert response in http.HISTORY
 
     def test_client_cert_missing_one(self, request, tmp_path):
         """Test cert or key supplied, but not the other."""
-        ax_url = get_url(request)
+        ax_url = get_arg_url(request)
         test_path = tmp_path / TEST_CLIENT_CERT_NAME
         test_path.write_text(TEST_CLIENT_CERT)
-
         with pytest.raises(HttpError):
             Http(url=ax_url, cert_client_cert=test_path, certwarn=False)
 
         with pytest.raises(HttpError):
             Http(url=ax_url, cert_client_key=test_path, certwarn=False)
 
-    def test_client_cert_seperate(self, request, tmp_path):
+    def test_client_cert_separate(self, request, tmp_path):
         """Test cert or key supplied, but not the other."""
-        ax_url = get_url(request)
+        ax_url = get_arg_url(request)
         cert_path = tmp_path / TEST_CLIENT_CERT_NAME
         cert_path.write_text(TEST_CLIENT_CERT)
         key_path = tmp_path / TEST_CLIENT_KEY_NAME
         key_path.write_text(TEST_CLIENT_KEY)
 
         http = Http(
             url=ax_url,
@@ -144,171 +123,145 @@
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
 
     def test_client_cert_both(self, request, tmp_path):
         """Test cert or key supplied, but not the other."""
-        ax_url = get_url(request)
+        ax_url = get_arg_url(request)
         both_path = tmp_path / TEST_CLIENT_CERT_NAME
         both_cert = "{}\n{}\n".format(TEST_CLIENT_CERT, TEST_CLIENT_KEY)
         both_path.write_text(both_cert)
-
         http = Http(url=ax_url, cert_client_both=both_path, certwarn=False)
-
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
 
     def test_log_req_attrs_true(self, request, caplog):
         """Test verbose logging of request attrs when log_request_attrs=True."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(
-            url=ax_url,
-            log_request_attrs=["url", "headers"],
-            certwarn=False,
-            log_level="debug",
-        )
-
+        http = get_http(request, log_request_attrs=["url", "headers"], log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
         entries = ["REQUEST ATTRS:.*{}.*headers".format(http.url)]
         log_check(caplog, entries)
 
     def test_log_req_attrs_none(self, request, caplog):
         """Test no logging of request attrs when log_request_attrs=None."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(url=ax_url, log_request_attrs=None, certwarn=False, log_level="debug")
-
+        http = get_http(request, log_request_attrs=None, log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
         entries = ["REQUEST ATTRS:"]
         log_check(caplog, entries, exists=False)
 
     def test_log_resp_attrs_true(self, request, caplog):
         """Test verbose logging of response attrs when log_response_attrs=True."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(
-            url=ax_url,
-            log_response_attrs=["url", "headers"],
-            certwarn=False,
-            log_level="debug",
-        )
-
+        http = get_http(request, log_response_attrs=["url", "headers"], log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
-        entries = ["RESPONSE ATTRS:.*{}.*headers".format(http.url)]
+        entries = ["RESPONSE ATTRS:.*url.*headers"]
         log_check(caplog, entries)
 
+    def test_get_cert(self, request):
+        """Test get_cert."""
+        http = get_http(request)
+        cert = http.get_cert()
+        assert isinstance(cert, cert_human.Cert)
+
+    def test_get_cert_chain(self, request):
+        """Test get_cert_chain."""
+        http = get_http(request)
+        chain = http.get_cert_chain()
+        assert (
+            isinstance(chain, list) and chain and all(isinstance(c, cert_human.Cert) for c in chain)
+        )
+
+    def test_safe_request(self, request):
+        """Test safe request."""
+        http = get_http(request)
+        data = http.safe_request(path="/force_error", method="POST")
+        assert isinstance(data, requests.Response) or data is None
+
     def test_log_response_attrs_all(self, request, caplog):
         """Test no logging of response attrs when log_response_attrs=None."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(url=ax_url, log_response_attrs="all", certwarn=False, log_level="debug")
-
+        http = get_http(request, log_response_attrs="all", log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
         entries = ["RESPONSE ATTRS:"]
         log_check(caplog, entries)
 
     def test_log_response_attrs_none(self, request, caplog):
         """Test no logging of response attrs when log_response_attrs=None."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(url=ax_url, log_response_attrs=None, certwarn=False, log_level="debug")
-
+        http = get_http(request, log_response_attrs=None, log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
         entries = ["RESPONSE ATTRS:.*"]
         log_check(caplog, entries, exists=False)
 
     def test_log_resp_body_true(self, request, caplog):
         """Test logging of response body when log_response_body=True."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(url=ax_url, log_response_body=True, certwarn=False, log_level="debug")
-
+        http = get_http(request, log_response_body=True, log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
-        entries = ["RESPONSE BODY:.*"]
+        entries = ["RESPONSE BODY.*"]
         log_check(caplog, entries)
 
     def test_log_resp_body_false(self, request, caplog):
         """Test no logging of response body when log_response_body=False."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(url=ax_url, log_response_body=False, certwarn=False, log_level="debug")
-
+        http = get_http(request, log_response_body=False, log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
-        entries = ["RESPONSE BODY:.*"]
+        entries = ["RESPONSE BODY.*"]
         log_check(caplog, entries, exists=False)
 
     def test_log_req_body_true(self, request, caplog):
         """Test logging of request body when log_request_body=True."""
         caplog.set_level(logging.DEBUG)
-
-        ax_url = get_url(request)
-
-        http = Http(url=ax_url, log_request_body=True, certwarn=False, log_level="debug")
-
+        http = get_http(request, log_request_body=True, log_level="debug")
         with pytest.warns() as record:
             response = http()
         assert response
         assert record
-
-        entries = ["REQUEST BODY:.*"]
+        entries = ["REQUEST BODY.*"]
         log_check(caplog, entries)
 
-    # def test_log_req_body_false(self, request, caplog):
-    #     """Test no logging of request body when log_request_body=False."""
-    #     caplog.set_level(logging.DEBUG)
-
-    #     ax_url = get_url(request)
+    def test_log_req_body_false(self, request, caplog):
+        """Test no logging of request body when log_request_body=False."""
+        caplog.set_level(logging.DEBUG)
+        http = get_http(request, log_request_body=False, log_level="debug")
+        with pytest.warns() as record:
+            response = http()
+        assert response
+        assert record
+        entries = ["REQUEST BODY:.*"]
+        log_check(caplog, entries, exists=False)
 
-    #     http = Http(url=ax_url, log_request_body=False, certwarn=False, log_level="debug")
+    # TBD
+    # def test_verify_ca_bundle(self, request, httpbin_secure, httpbin_ca_bundle):
+    #     Test quiet_urllib=False no warning from urllib3 when using ca bundle.
+    #     url = httpbin_secure.url
+    #     http = Http(url=url, certwarn=False)
     #     with pytest.warns() as record:
     #         response = http()
-    #     assert response
+    #     assert response.status_code == 200
     #     assert record
-
-    #     entries = ["REQUEST BODY:.*"]
-    #     log_check(caplog, entries, exists=False)
```

## axonius_api_client/tests/tests_pkg/test_setup_env.py

```diff
@@ -1,38 +1,52 @@
 # -*- coding: utf-8 -*-
 """Test suite for axonius_api_client."""
+import logging
 import os
 import pathlib
 
 import pytest
 
 from axonius_api_client import PACKAGE_ROOT
 from axonius_api_client.setup_env import (
-    KEY_CERTWARN,
     KEY_DEFAULT_PATH,
     KEY_ENV_FILE,
     KEY_ENV_PATH,
+    LOADED,
+    MSG,
+    Results,
+    CONNECT_SCHEMAS,
     KEY_FEATURES,
-    KEY_KEY,
-    KEY_OVERRIDE,
-    KEY_SECRET,
-    KEY_URL,
     NO,
     YES,
     find_dotenv,
     get_env_ax,
     get_env_bool,
     get_env_connect,
+    load_dotenv,
     get_env_csv,
+    get_env_extra_warn,
     get_env_features,
     get_env_path,
     get_env_str,
+    get_env_user_agent,
+    set_env,
 )
 
 
+class TestSetEnv:
+    def test_ax_env_set(self, monkeypatch, tmp_path):
+        ax_env = tmp_path / ".env"
+        exp = (True, "AX_TEST", "abc")
+        ret = set_env(key="AX_TEST", value="abc", ax_env=ax_env)
+        assert ret == exp
+        contents = ax_env.read_text()
+        assert contents == "AX_TEST='abc'\n"
+
+
 class TestGetEnvCsv:
     def test_set(self, monkeypatch):
         monkeypatch.setenv("AX_TEST", "abc,def,ghi")
         ret = get_env_csv("AX_TEST")
         assert ret == ["abc", "def", "ghi"]
 
 
@@ -44,67 +58,67 @@
 
 
 class TestFindDotEnv:
     def test_supplied(self, monkeypatch, tmp_path):
         path = tmp_path / ".env"
         path.touch()
         src, ret = find_dotenv(ax_env=tmp_path)
-        assert src == "supplied"
+        assert src == Results.supplied.name
         assert ret == str(path)
 
     def test_env_path_key(self, monkeypatch, tmp_path):
         monkeypatch.setenv(KEY_ENV_PATH, str(tmp_path))
         path = tmp_path / ".env"
         path.touch()
         src, ret = find_dotenv()
-        assert src == "env_path"
+        assert src == Results.env_path.name
         assert ret == str(path)
 
     def test_default_env_path_key(self, monkeypatch, tmp_path):
         monkeypatch.delenv(KEY_ENV_PATH, raising=False)
         monkeypatch.delenv(KEY_DEFAULT_PATH, raising=False)
         monkeypatch.setenv(KEY_DEFAULT_PATH, str(tmp_path))
         path = tmp_path / ".env"
         path.touch()
         src, ret = find_dotenv()
-        assert src == "default_path"
+        assert src == Results.default_path.name
         assert ret == str(path)
 
     def test_find_dotenv_not_found(self, monkeypatch, tmp_path):
         monkeypatch.delenv(KEY_ENV_PATH, raising=False)
         monkeypatch.delenv(KEY_DEFAULT_PATH, raising=False)
-        monkeypatch.setenv(KEY_ENV_FILE, "moofile")
+        monkeypatch.setenv(KEY_ENV_PATH, "badwolf")
         old_path = os.getcwd()
         os.chdir(tmp_path)
-        src, ret = find_dotenv(default=None)
+        src, ret = find_dotenv(default=None, check_walk_cwd=False, check_walk_script=False)
         os.chdir(old_path)
-        assert src == "not_found"
+        assert src == Results.not_found.name
         assert ret == ""
 
     def test_find_dotenv_cwd(self, monkeypatch, tmp_path):
         monkeypatch.delenv(KEY_ENV_PATH, raising=False)
         monkeypatch.delenv(KEY_DEFAULT_PATH, raising=False)
         path = tmp_path / ".env"
         path.touch()
         old_path = os.getcwd()
         os.chdir(tmp_path)
         src, ret = find_dotenv(default=None)
         os.chdir(old_path)
-        assert src == "find_dotenv_cwd"
+        assert src == Results.find_dotenv_cwd.name
         assert ret == str(path)
 
     def test_find_dotenv_pkg(self, monkeypatch, tmp_path):
         monkeypatch.delenv(KEY_ENV_PATH, raising=False)
         monkeypatch.delenv(KEY_DEFAULT_PATH, raising=False)
         monkeypatch.setenv(KEY_ENV_FILE, "test.env")
         path = pathlib.Path(PACKAGE_ROOT) / "test.env"
         path.touch()
         src, ret = find_dotenv(default=None)
         path.unlink()
-        assert src == "find_dotenv_pkg"
+        assert src == Results.find_dotenv_script.name
         assert ret.endswith("test.env")
 
 
 class TestGetEnvStr:
     def test_default(self, monkeypatch):
         ret = get_env_str(key="boom", default="abc")
         assert ret == "abc"
@@ -115,14 +129,41 @@
         assert ret == "abc"
 
     def test_invalid(self, monkeypatch):
         monkeypatch.delenv("AX_TEST", raising=False)
         with pytest.raises(ValueError):
             get_env_str(key="AX_TEST")
 
+    def test_bytes(self, monkeypatch):
+        monkeypatch.delenv("AX_TEST", raising=False)
+        ret = get_env_str(key="AX_TEST", default=b"abc", lower=True)
+        assert ret == "abc"
+
+
+class TestGetEnvUserAgent:
+    def test_default(self, monkeypatch):
+        ret = get_env_user_agent()
+        assert ret == ""
+
+    def test_default_set(self, monkeypatch):
+        monkeypatch.setenv("AX_USER_AGENT", "abc")
+        ret = get_env_user_agent()
+        assert ret == "abc"
+
+
+class TestGetEnvExtraWarn:
+    def test_default(self, monkeypatch):
+        ret = get_env_extra_warn()
+        assert ret is True
+
+    def test_default_set(self, monkeypatch):
+        monkeypatch.setenv("AX_EXTRA_WARN", "n")
+        ret = get_env_extra_warn()
+        assert ret is False
+
 
 class TestGetEnvPath:
     def test_none(self, monkeypatch):
         ret = get_env_path(key="boom")
         assert ret == ""
 
     def test_default(self, monkeypatch):
@@ -131,15 +172,15 @@
 
     def test_default_get_dir(self, monkeypatch, tmp_path):
         path = tmp_path / "file.test"
         path.touch()
         ret = get_env_path(key="boom", default=str(path))
         assert ret == tmp_path
 
-    def test_default_noget_dir(self, monkeypatch, tmp_path):
+    def test_default_no_get_dir(self, monkeypatch, tmp_path):
         path = tmp_path / "file.test"
         path.touch()
         ret = get_env_path(key="boom", default=str(path), get_dir=False)
         assert ret == path
 
 
 class TestGetEnvBool:
@@ -167,41 +208,69 @@
 
     def test_default2(self, monkeypatch):
         monkeypatch.delenv("AX_TEST", raising=False)
         ret = get_env_bool("AX_TEST", default="yes")
         assert ret is True
 
 
+def del_connect_envs(monkeypatch):
+    """Clear all connect envs."""
+    for k, v in CONNECT_SCHEMAS.items():
+        monkeypatch.delenv(name=v["env"], raising=False)
+
+
+class TestLoadDotEnv:
+    def test_not_found(self, caplog, tmp_path):
+        ax_env = tmp_path / ".env"
+        caplog.set_level(logging.DEBUG)
+        load_dotenv(
+            ax_env=ax_env,
+            check_ax_env=False,
+            check_default=False,
+            check_walk_cwd=False,
+            check_walk_script=False,
+        )
+        assert MSG.not_found in caplog.text
+
+    def test_found(self, caplog, monkeypatch, tmp_path):
+        ax_env = tmp_path / ".env"
+        ax_env.touch()
+        caplog.set_level(logging.DEBUG)
+        ax_env_loaded = load_dotenv(ax_env)
+        assert MSG.loading in caplog.text
+        assert ax_env_loaded in LOADED
+
+        load_dotenv(ax_env, override=False)
+        assert MSG.already_loaded in caplog.text
+
+
 class TestGetEnvConnect:
     def test_no_override(self, monkeypatch):
-        URL = "a"
-        KEY = "b"
-        SEC = "c"
-        WARN = "yes"
-        exp = {"url": URL, "key": KEY, "secret": SEC, "certwarn": True}
-        monkeypatch.setenv(KEY_URL, URL)
-        monkeypatch.setenv(KEY_KEY, KEY)
-        monkeypatch.setenv(KEY_SECRET, SEC)
-        monkeypatch.setenv(KEY_CERTWARN, WARN)
-        monkeypatch.setenv(KEY_OVERRIDE, "no")
-        ret = get_env_connect()
-        assert ret == exp
-
-    def test_override(self, monkeypatch):
-        URL = "a"
-        KEY = "b"
-        SEC = "c"
-        monkeypatch.setenv(KEY_URL, URL)
-        monkeypatch.setenv(KEY_KEY, KEY)
-        monkeypatch.setenv(KEY_SECRET, SEC)
-        monkeypatch.setenv(KEY_OVERRIDE, "yes")
-        ret = get_env_connect()
-        assert ret["url"]
-        assert ret["key"]
-        assert ret["secret"]
+        mock_envs = {
+            "url": "abc",
+            "key": "def",
+            "secret": "ghi",
+            "certwarn": "yes",
+            "credentials": "n",
+        }
+        exp_envs = {
+            "key": "def",
+            "secret": "ghi",
+            "certwarn": True,
+            "credentials": False,
+        }
+
+        del_connect_envs(monkeypatch)
+
+        for k, v in mock_envs.items():
+            monkeypatch.setenv(CONNECT_SCHEMAS[k]["env"], v)
+
+        ret = get_env_connect(load_override=False)
+        for k, v in exp_envs.items():
+            assert ret[k] == v
 
 
 class TestGetEnvAx:
     def test_valid(self, monkeypatch):
         monkeypatch.setenv("AX_TEST", "boom")
         ret = get_env_ax()
         assert ret["AX_TEST"] == "boom"
```

## axonius_api_client/tests/tests_pkg/test_tools.py

```diff
@@ -1,12 +1,13 @@
 # -*- coding: utf-8 -*-
 """Test suite for axonius_api_client."""
 import codecs
 import io
 import tempfile
+from datetime import timezone
 
 import dateutil.tz
 import pytest
 
 from axonius_api_client.api.json_api.generic import IntValue
 from axonius_api_client.constants.api import GUI_PAGE_SIZES
 from axonius_api_client.constants.general import IS_WINDOWS
@@ -68,15 +69,14 @@
     prettify_obj,
     read_stream,
     split_str,
     strip_left,
     strip_right,
     strip_str,
     sysinfo,
-    timedelta,
     token_parse,
 )
 
 BOM_BYTES = codecs.BOM_UTF8
 BOM_STR = BOM_BYTES.decode()
 
 
@@ -122,15 +122,15 @@
         exp = "1\n2\nTrimmed 3 lines down to 2"
         ret = coerce_str(value=value, trim=2, trim_lines=2)
         assert ret == exp
 
 
 class TestDtDaysLeft:
     def test_valid(self):
-        assert dt_days_left(datetime.utcnow() + timedelta(days=100)) == 100
+        assert dt_days_left(datetime.datetime.utcnow() + datetime.timedelta(days=100)) == 100
 
 
 class TestKvDump:
     def test_valid(self):
         assert kv_dump({"k": "v", "a": "b"}) == "\n  k: v\n  a: b"
 
 
@@ -1191,15 +1191,15 @@
         """Simple test."""
         x = b"xxx"
         y = json_dump(obj=x)
         assert y == '"xxx"'
 
     def test_serial(self):
         dc = IntValue(value=1111)
-        now = datetime.utcnow()
+        now = datetime.datetime.utcnow()
         obj = {"foo": json_dump, "now": now, "dc": dc}
 
         exp = [
             "{",
             f'  "foo": "{json_dump}",',
             f'  "now": "{now.isoformat()}",',
             '  "dc": {',
@@ -1391,35 +1391,35 @@
 
 
 class TestDtMinAgo:
     """Test dt_*."""
 
     def test_min_ago_utc_str(self):
         """Simple test."""
-        then = format(dt_now() - timedelta(minutes=1))
+        then = format(dt_now() - datetime.timedelta(minutes=1))
         assert dt_min_ago(obj=then) == 1
 
     def test_min_ago_utc_dt(self):
         """Simple test."""
-        then = dt_now() - timedelta(minutes=1)
+        then = dt_now() - datetime.timedelta(minutes=1)
         assert dt_min_ago(obj=then) == 1
 
     def test_min_ago_utc_dt_naive(self):
         """Simple test."""
-        then = dt_now(None) - timedelta(minutes=1)
+        then = dt_now(None) - datetime.timedelta(minutes=1)
         assert dt_min_ago(obj=then) == 1
 
     def test_min_ago_utc_dtdelta(self):
         """Simple test."""
-        then = timedelta(minutes=3)
+        then = datetime.timedelta(minutes=3)
         assert dt_min_ago(obj=then) == 3
 
     def test_min_ago_naive(self):
         """Simple test."""
-        then = datetime.now() - timedelta(minutes=1)
+        then = datetime.datetime.now() - datetime.timedelta(minutes=1)
         assert dt_min_ago(obj=format(then)) == 1
 
 
 class TestDtNow:
     """Test dt_*."""
 
     def test_now(self):
@@ -1427,54 +1427,54 @@
         assert now.tzinfo
 
     def test_now_notz(self):
         now = dt_now(tz=None)
         assert not now.tzinfo
 
     def test_now_delta(self):
-        then = dt_now(delta=timedelta(minutes=5))
+        then = dt_now(delta=datetime.timedelta(minutes=5))
         assert dt_min_ago(then) == 5
 
 
 class TestDtParse:
     """Test dt_*."""
 
     @pytest.mark.parametrize(
         "val",
-        [format(dt_now()), dt_now(), timedelta(minutes=1)],
+        [format(dt_now()), dt_now(), datetime.timedelta(minutes=1)],
         scope="class",
     )
     def test_val(self, val):
         now = dt_parse(obj=val)
-        assert isinstance(now, datetime)
+        assert isinstance(now, datetime.datetime)
 
     def test_list(self):
         now = [format(dt_now())]
         now = dt_parse(obj=now)
         assert isinstance(now, list)
-        assert [isinstance(x, datetime) for x in now]
+        assert [isinstance(x, datetime.datetime) for x in now]
 
     def test_default_tz(self):
-        now = datetime.now()
+        now = datetime.datetime.now()
         assert not now.tzinfo
         ret = dt_parse(obj=now, default_tz_utc=True)
-        assert ret.tzinfo == dateutil.tz.tzutc()
+        assert ret.tzinfo in [dateutil.tz.tzutc(), timezone.utc]
 
 
 class TestDtWithinMin:
     """Test dt_*."""
 
     @pytest.mark.parametrize("val", [None, "x", False, True, {}, [], 6, "8", b"9"], scope="class")
     def test_bad(self, val):
-        then = dt_now(delta=timedelta(minutes=5))
+        then = dt_now(delta=datetime.timedelta(minutes=5))
         assert dt_within_min(obj=then, n=val) is False
 
     @pytest.mark.parametrize("val", [0, 4, "1", b"2"], scope="class")
     def test_ok(self, val):
-        then = dt_now(delta=timedelta(minutes=5))
+        then = dt_now(delta=datetime.timedelta(minutes=5))
         assert dt_within_min(obj=then, n=val) is True
 
 
 class TestGetPathsFormat:
     def test_basic(self):
         exp = pathlib.Path("/x")
         ret = get_paths_format("/x")
```

## axonius_api_client/tests/tests_pkg/test_url_parser.py

```diff
@@ -1,90 +1,90 @@
 # -*- coding: utf-8 -*-
 """Test suite for axonius_api_client.http."""
 
 import pytest
 
-from axonius_api_client.exceptions import HttpError
-from axonius_api_client.parsers.url_parser import UrlParser
+from axonius_api_client.projects.url_parser import UrlParser
 
 
 class TestUrlParser:
     """Test UrlParser."""
 
-    def test_schemehostport443(self):
+    def test_scheme_host_port_443(self):
         """Test a proper URL gets parsed the same."""
-        u = UrlParser("https://host:443/blah")
-        assert u.hostname == "host"
-        assert u.port == 443
-        assert u.scheme == "https"
-        assert u.parsed.path == "/blah"
-        assert u.url_full == "https://host:443/blah"
-        assert u.url == "https://host:443"
+        parser = UrlParser("https://host:443/blah")
+        assert parser.hostname == "host"
+        assert parser.port == 443
+        assert parser.scheme == "https"
+        assert parser.parsed.path == "/blah"
+        assert parser.url_full == "https://host:443/blah"
+        assert parser.url == "https://host:443"
 
     def test_str_repr(self):
         """Test str/repr has URL path."""
-        u = UrlParser("https://host:443/blah")
-        assert u.parsed.path in format(u)
-        assert u.parsed.path in repr(u)
+        parser = UrlParser("https://host:443/blah")
+        assert parser.parsed.path in str(parser)
+        assert parser.parsed.path in repr(parser)
 
-    def test_schemehost_noport443(self):
+    def test_scheme_host_no_port_443(self):
         """Test port gets added for https scheme."""
-        u = UrlParser("https://host")
-        assert u.hostname == "host"
-        assert u.port == 443
-        assert u.scheme == "https"
+        parser = UrlParser("https://host")
+        assert parser.hostname == "host"
+        assert parser.port == 443
+        assert parser.scheme == "https"
 
-    def test_justhost(self):
+    def test_just_host(self):
         """Test schema added for just host."""
-        u = UrlParser("host")
-        assert u.hostname == "host"
-        assert u.port == 443
-        assert u.scheme == "https"
+        parser = UrlParser("host")
+        assert parser.hostname == "host"
+        assert parser.port == 443
+        assert parser.scheme == "https"
 
-    def test_justhostport(self):
+    def test_just_host_port(self):
         """Test schema added for just host and port."""
-        u = UrlParser("host:443")
-        assert u.hostname == "host"
-        assert u.port == 443
-        assert u.scheme == "https"
+        parser = UrlParser("host:443")
+        assert parser.hostname == "host"
+        assert parser.port == 443
+        assert parser.scheme == "https"
 
-    def test_host_noschemeport(self):
+    def test_host_no_scheme_port(self):
         """Test exc when no port or scheme in URL."""
-        exc = HttpError
+        exc = ValueError
         match = "no.*'port'"
         with pytest.raises(exc, match=match):
             UrlParser("host", default_scheme="")
 
-    def test_unknownschemehost_noport(self):
+    def test_unknown_scheme_host_no_port(self):
         """Test exc when no port and non http/https scheme."""
-        exc = HttpError
+        exc = ValueError
         match = "no.*'port'"
         with pytest.raises(exc, match=match):
             UrlParser("httpx://host")
 
-    def test_hostport443_withslash(self):
+    def test_hostport443_with_slash(self):
         """Test scheme added with port 443 and no scheme in URL."""
-        u = UrlParser("host:443/")
-        assert u.hostname == "host"
-        assert u.port == 443
-        assert u.scheme == "https"
+        parser = UrlParser("host:443/")
+        assert parser.hostname == "host"
+        assert parser.port == 443
+        assert parser.scheme == "https"
 
-    def test_hostport443_noscheme(self):
+    def test_hostport443_no_scheme(self):
         """Test scheme added with port 443 and no scheme in URL."""
-        u = UrlParser("host:443", default_scheme="")
-        assert u.hostname == "host"
-        assert u.port == 443
-        assert u.scheme == "https"
+        parser = UrlParser("host:443", default_scheme="")
+        assert parser.hostname == "host"
+        assert parser.port == 443
+        assert parser.scheme == "https"
 
-    def test_hostport80_noscheme(self):
+    def test_hostport80_no_scheme(self):
         """Test scheme added with port 80 and no scheme in URL."""
-        u = UrlParser("host:80", default_scheme="")
-        assert u.hostname == "host"
-        assert u.port == 80
-        assert u.scheme == "http"
+        parser = UrlParser("host:80", default_scheme="")
+        assert parser.hostname == "host"
+        assert parser.port == 80
+        assert parser.scheme == "http"
 
-    def test_schemehost_noport80(self):
+    def test_scheme_host_no_port80(self):
         """Test port added with no port and http scheme in URL."""
-        u = UrlParser("http://host")
-        assert u.hostname == "host"
-        assert u.port == 80
-        assert u.scheme == "http"
+        # noinspection HttpUrlsUsage
+        parser = UrlParser("http://host")
+        assert parser.hostname == "host"
+        assert parser.port == 80
+        assert parser.scheme == "http"
```

## Comparing `axonius_api_client/api/json_api/audit_logs.py` & `axonius_api_client/api/json_api/audit_logs/audit_log_response.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,101 +1,72 @@
 # -*- coding: utf-8 -*-
 """Models for API requests & responses."""
 import dataclasses
 import datetime
 import re
 import typing as t
 
-import marshmallow_jsonapi
+import marshmallow_jsonapi.fields as mm_fields
 
-from ...exceptions import ApiError
-from ...tools import coerce_int_float, dt_now, dt_parse, listify, trim_float
-from .base import BaseModel, BaseSchemaJson
-from .custom_fields import SchemaDatetime, get_field_dc_mm
-from .resources import ResourcesGet, ResourcesGetSchema
-
-
-class AuditLogRequestSchema(ResourcesGetSchema):
-    """Pass."""
-
-    date_from = SchemaDatetime(allow_none=True)
-    date_to = SchemaDatetime(allow_none=True)
-
-    class Meta:
-        """Pass."""
-
-        type_ = "audit_request_schema"
-
-    @classmethod
-    def get_model_cls(cls) -> t.Any:
-        """Pass."""
-        return AuditLogRequest
-
-
-@dataclasses.dataclass
-class AuditLogRequest(ResourcesGet):
-    """Pass."""
-
-    date_from: t.Optional[datetime.datetime] = get_field_dc_mm(
-        mm_field=SchemaDatetime(allow_none=True), default=None
-    )
-    date_to: t.Optional[datetime.datetime] = get_field_dc_mm(
-        mm_field=SchemaDatetime(allow_none=True), default=None
-    )
-
-    @classmethod
-    def get_schema_cls(cls) -> t.Any:
-        """Pass."""
-        return AuditLogRequestSchema
+from ....exceptions import ApiError
+from ....tools import coerce_int_float, dt_now, dt_parse, listify, trim_float
+from ..base import BaseModel, BaseSchemaJson
+from ..custom_fields import SchemaDatetime, field_from_mm
 
 
 class AuditLogSchema(BaseSchemaJson):
-    """Pass."""
+    """Schema for response to get audit logs."""
 
-    action = marshmallow_jsonapi.fields.Str()
-    category = marshmallow_jsonapi.fields.Str()
+    action = mm_fields.Str()
+    category = mm_fields.Str()
     date = SchemaDatetime()
-    message = marshmallow_jsonapi.fields.Str()
-    type = marshmallow_jsonapi.fields.Str()
-    user = marshmallow_jsonapi.fields.Str()
-    role = marshmallow_jsonapi.fields.Str(allow_none=True)
-    impersonator_user = marshmallow_jsonapi.fields.Str(allow_none=True)
-    data_scope = marshmallow_jsonapi.fields.Str(allow_none=True)
-    discovery_cycle_id = marshmallow_jsonapi.fields.Str(allow_none=True)
+    message = mm_fields.Str()
+    type = mm_fields.Str()
+    user = mm_fields.Str()
+    role = mm_fields.Str(allow_none=True, load_default=None, dump_default=None)
+    impersonator_user = mm_fields.Str(allow_none=True, load_default=None, dump_default=None)
+    data_scope = mm_fields.Str(allow_none=True, load_default=None, dump_default=None)
+    discovery_cycle_id = mm_fields.Str(allow_none=True, load_default=None, dump_default=None)
 
     class Meta:
-        """Pass."""
+        """JSONAPI config."""
 
         type_ = "audit_schema"
 
     @classmethod
     def get_model_cls(cls) -> t.Any:
-        """Pass."""
+        """Get the model for this schema."""
         return AuditLog
 
 
+SCHEMA = AuditLogSchema()
+
+
 @dataclasses.dataclass
 class AuditLog(BaseModel):
-    """Pass."""
+    """Response to get audit logs."""
+
+    action: str = field_from_mm(SCHEMA, "action")
+    category: str = field_from_mm(SCHEMA, "category")
+    date: datetime.datetime = field_from_mm(SCHEMA, "date")
+    message: str = field_from_mm(SCHEMA, "message")
+    type: str = field_from_mm(SCHEMA, "type")
+    user: str = field_from_mm(SCHEMA, "user")
+    role: t.Optional[str] = field_from_mm(SCHEMA, "role")
+    impersonator_user: t.Optional[str] = field_from_mm(SCHEMA, "impersonator_user")
+    data_scope: t.Optional[str] = field_from_mm(SCHEMA, "data_scope")
+    discovery_cycle_id: t.Optional[str] = field_from_mm(SCHEMA, "discovery_cycle_id")
 
-    action: str
-    category: str
-    date: datetime.datetime = get_field_dc_mm(mm_field=SchemaDatetime())
-    message: str
-    type: str
-    user: str
-    role: t.Optional[str] = None
-    impersonator_user: t.Optional[str] = None
-    data_scope: t.Optional[str] = None
-    discovery_cycle_id: t.Optional[str] = None
     document_meta: t.Optional[dict] = dataclasses.field(default_factory=dict)
 
+    SCHEMA: t.ClassVar[AuditLogSchema] = SCHEMA
+
     @classmethod
     def get_schema_cls(cls) -> t.Any:
-        """Pass."""
+        """Get the schema for this model."""
         return AuditLogSchema
 
     @staticmethod
     def _search_properties() -> t.List[str]:
         return [
             "action",
             "category",
@@ -123,15 +94,15 @@
 
     @staticmethod
     def _str_join() -> str:
         """Pass."""
         return ", "
 
     @property
-    def hours_ago(self) -> str:
+    def hours_ago(self) -> float:
         """Pass."""
         return trim_float(value=(dt_now() - self.date).total_seconds() / 60 / 60)
 
     def within_last_hours(self, hours: t.Optional[t.Union[int, float]] = None) -> bool:
         """Pass."""
         return coerce_int_float(value=hours) >= self.hours_ago if hours else True
 
@@ -165,10 +136,10 @@
         all_searches = []
         hits = []
         for prop, searches in kwargs.items():
             searches = listify(searches)
             all_searches += searches
             value = getattr(self, prop) or ""
             if any([re.search(x, value, re.I) for x in searches]):
-                hits.append({"propery": prop, "value": value})
+                hits.append({"property": prop, "token": value})
 
         return True if (hits or not all_searches) else False
```

## Comparing `axonius_api_client/cert_human/all_logs_list.json` & `axonius_api_client/projects/cert_human/all_logs_list.json`

 * *Files 0% similar despite different names*

### Pretty-printed

 * *Similarity: 0.7499964055670577%*

 * *Differences: {"'log_list_timestamp'": "'2023-04-10T12:55:02Z'",*

 * * "'operators'": "{2: {'logs': {0: {'state': {replace: OrderedDict([('rejected', "*

 * *                "OrderedDict([('timestamp', '2023-03-02T18:00:00Z')]))])}}}}}",*

 * * "'version'": "'20.35'"}*

```diff
@@ -1,10 +1,10 @@
 {
     "is_all_logs": true,
-    "log_list_timestamp": "2023-02-13T12:56:28Z",
+    "log_list_timestamp": "2023-04-10T12:55:02Z",
     "operators": [
         {
             "email": [
                 "google-ct-logs@googlegroups.com"
             ],
             "logs": [
                 {
@@ -572,16 +572,16 @@
             "logs": [
                 {
                     "description": "DigiCert Log Server",
                     "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAkbFvhu7gkAW6MHSrBlpE1n4+HCFRkC5OLAjgqhkTH+/uzSfSl8ois8ZxAD2NgaTZe1M9akhYlrYkes4JECs6A==",
                     "log_id": "VhQGmi/XwuzT9eG9RLI+x0Z2ubyZEVzA75SYVdaJ0N0=",
                     "mmd": 86400,
                     "state": {
-                        "retired": {
-                            "timestamp": "2022-01-24T00:00:00Z"
+                        "rejected": {
+                            "timestamp": "2023-03-02T18:00:00Z"
                         }
                     },
                     "url": "https://ct1.digicert-ct.com/log/"
                 },
                 {
                     "description": "DigiCert Log Server 2",
                     "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEzF05L2a4TH/BLgOhNKPoioYCrkoRxvcmajeb8Dj4XQmNY+gxa4Zmz3mzJTwe33i0qMVp+rfwgnliQ/bM/oFmhA==",
@@ -1800,9 +1800,9 @@
                     },
                     "url": "https://ct.browser.360.cn/v1/2023/"
                 }
             ],
             "name": "Qihoo 360"
         }
     ],
-    "version": "19.10"
+    "version": "20.35"
 }
```

## Comparing `axonius_api_client/cert_human/all_logs_list.py` & `axonius_api_client/projects/cert_human/all_logs_list.py`

 * *Files 4% similar despite different names*

```diff
@@ -13,15 +13,15 @@
       "email": [
         "google-ct-logs@googlegroups.com"
       ],
       "logs": [
         {
           "description": "Google 'Argon2017' log",
           "log_id": "+tTJfMSe4vishcXqXOoJ0CINu/TknGtQZi/4aPhrjCg=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEVG18id3qnfC6X/RtYHo3TwIlvxz2b4WurxXfaW7t26maKZfymXYe5jNGHif0vnDdWde6z/7Qco6wVw+dN4liow==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEVG18id3qnfC6X/RtYHo3TwIlvxz2b4WurxXfaW7t26maKZfymXYe5jNGHif0vnDdWde6z/7Qco6wVw+dN4liow==",
           "url": "https://ct.googleapis.com/logs/argon2017/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2018-02-27T00:00:00Z"
             }
           },
@@ -29,15 +29,15 @@
             "start_inclusive": "2017-01-01T00:00:00Z",
             "end_exclusive": "2018-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Argon2018' log",
           "log_id": "pFASaQVaFVReYhGrN7wQP2KuVXakXksXFEU+GyIQaiU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0gBVBa3VR7QZu82V+ynXWD14JM3ORp37MtRxTmACJV5ZPtfUA7htQ2hofuigZQs+bnFZkje+qejxoyvk2Q1VaA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0gBVBa3VR7QZu82V+ynXWD14JM3ORp37MtRxTmACJV5ZPtfUA7htQ2hofuigZQs+bnFZkje+qejxoyvk2Q1VaA==",
           "url": "https://ct.googleapis.com/logs/argon2018/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2019-05-07T00:00:00Z"
             }
           },
@@ -45,15 +45,15 @@
             "start_inclusive": "2018-01-01T00:00:00Z",
             "end_exclusive": "2019-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Argon2019' log",
           "log_id": "Y/Lbzeg7zCzPC3KEJ1drM6SNYXePvXWmOLHHaFRL2I0=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEI3MQm+HzXvaYa2mVlhB4zknbtAT8cSxakmBoJcBKGqGwYS0bhxSpuvABM1kdBTDpQhXnVdcq+LSiukXJRpGHVg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEI3MQm+HzXvaYa2mVlhB4zknbtAT8cSxakmBoJcBKGqGwYS0bhxSpuvABM1kdBTDpQhXnVdcq+LSiukXJRpGHVg==",
           "url": "https://ct.googleapis.com/logs/argon2019/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-02-07T00:00:00Z"
             }
           },
@@ -61,15 +61,15 @@
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Argon2020' log",
           "log_id": "sh4FzIuizYogTodm+Su5iiUgZ2va+nDnsklTLe+LkF4=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE6Tx2p1yKY4015NyIYvdrk36es0uAc1zA4PQ+TGRY+3ZjUTIYY9Wyu+3q/147JG4vNVKLtDWarZwVqGkg6lAYzA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE6Tx2p1yKY4015NyIYvdrk36es0uAc1zA4PQ+TGRY+3ZjUTIYY9Wyu+3q/147JG4vNVKLtDWarZwVqGkg6lAYzA==",
           "url": "https://ct.googleapis.com/logs/argon2020/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2021-04-21T00:00:00Z"
             }
           },
@@ -77,15 +77,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Argon2021' log",
           "log_id": "9lyUL9F3MCIUVBgIMJRWjuNNExkzv98MLyALzE7xZOM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAETeBmZOrzZKo4xYktx9gI2chEce3cw/tbr5xkoQlmhB18aKfsxD+MnILgGNl0FOm0eYGilFVi85wLRIOhK8lxKw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAETeBmZOrzZKo4xYktx9gI2chEce3cw/tbr5xkoQlmhB18aKfsxD+MnILgGNl0FOm0eYGilFVi85wLRIOhK8lxKw==",
           "url": "https://ct.googleapis.com/logs/argon2021/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2018-06-15T02:30:13Z"
             }
           },
@@ -93,15 +93,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Argon2022' log",
           "log_id": "KXm+8J45OSHwVnOfY6V35b5XfZxgCvj5TV0mXCVdx4Q=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEeIPc6fGmuBg6AJkv/z7NFckmHvf/OqmjchZJ6wm2qN200keRDg352dWpi7CHnSV51BpQYAj1CQY5JuRAwrrDwg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEeIPc6fGmuBg6AJkv/z7NFckmHvf/OqmjchZJ6wm2qN200keRDg352dWpi7CHnSV51BpQYAj1CQY5JuRAwrrDwg==",
           "url": "https://ct.googleapis.com/logs/argon2022/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-12-17T18:38:01Z"
             }
           },
@@ -109,15 +109,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Argon2023' log",
           "log_id": "6D7Q2j71BjUy51covIlryQPTy9ERa+zraeF3fW0GvW4=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0JCPZFJOQqyEti5M8j13ALN3CAVHqkVM4yyOcKWCu2yye5yYeqDpEXYoALIgtM3TmHtNlifmt+4iatGwLpF3eA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE0JCPZFJOQqyEti5M8j13ALN3CAVHqkVM4yyOcKWCu2yye5yYeqDpEXYoALIgtM3TmHtNlifmt+4iatGwLpF3eA==",
           "url": "https://ct.googleapis.com/logs/argon2023/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-12-17T18:38:01Z"
             }
           },
@@ -125,26 +125,26 @@
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Argon2024' log",
           "log_id": "7s3QZNXbGs7FXLedtM0TojKHRny87N7DUUhZRnEftZs=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEHblsqctplMVc5ramA7vSuNxUQxcomQwGAVAdnWTAWUYr3MgDHQW0LagJ95lB7QT75Ve6JgT2EVLOFGU7L3YrwA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEHblsqctplMVc5ramA7vSuNxUQxcomQwGAVAdnWTAWUYr3MgDHQW0LagJ95lB7QT75Ve6JgT2EVLOFGU7L3YrwA==",
           "url": "https://ct.googleapis.com/logs/argon2024/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2024-01-01T00:00:00Z",
             "end_exclusive": "2025-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Xenon2018' log",
           "log_id": "sQzVWabWeEaBH335pRUyc5rEjXA76gMj2l04dVvArU4=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1syJvwQdrv0a8dM2VAnK/SmHJNw/+FxC+CncFcnXMX2jNH9Xs7Q56FiV3taG5G2CokMsizhpcm7xXzuR3IHmag==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1syJvwQdrv0a8dM2VAnK/SmHJNw/+FxC+CncFcnXMX2jNH9Xs7Q56FiV3taG5G2CokMsizhpcm7xXzuR3IHmag==",
           "url": "https://ct.googleapis.com/logs/xenon2018/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2019-02-21T00:00:00Z"
             }
           },
@@ -152,15 +152,15 @@
             "start_inclusive": "2018-01-01T00:00:00Z",
             "end_exclusive": "2019-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Xenon2019' log",
           "log_id": "CEEUmABxUywWGQRgvPxH/cJlOvopLHKzf/hjrinMyfA=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE/XyDwqzXL9i2GTjMYkqaEyiRL0Dy9sHq/BTebFdshbvCaXXEh6mjUK0Yy+AsDcI4MpzF1l7Kded2MD5zi420gA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE/XyDwqzXL9i2GTjMYkqaEyiRL0Dy9sHq/BTebFdshbvCaXXEh6mjUK0Yy+AsDcI4MpzF1l7Kded2MD5zi420gA==",
           "url": "https://ct.googleapis.com/logs/xenon2019/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-02-07T00:00:00Z"
             }
           },
@@ -168,15 +168,15 @@
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Xenon2020' log",
           "log_id": "B7dcG+V9aP/xsMYdIxXHuuZXfFeUt2ruvGE6GmnTohw=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEZU75VqjyzSTgFZKAnWg1QeYfFFIRZTMK7q3kWWZsmHhQdrBYnHRZ3OA4kUeUx0JN+xX+dSgt1ruqUhhl7jOvmw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEZU75VqjyzSTgFZKAnWg1QeYfFFIRZTMK7q3kWWZsmHhQdrBYnHRZ3OA4kUeUx0JN+xX+dSgt1ruqUhhl7jOvmw==",
           "url": "https://ct.googleapis.com/logs/xenon2020/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2021-04-21T00:00:00Z"
             }
           },
@@ -184,15 +184,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Xenon2021' log",
           "log_id": "fT7y+I//iFVoJMLAyp5SiXkrxQ54CX8uapdomX4i8Nc=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAER+1MInu8Q39BwDZ5Rp9TwXhwm3ktvgJzpk/r7dDgGk7ZacMm3ljfcoIvP1E72T8jvyLT1bvdapylajZcTH6W5g==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAER+1MInu8Q39BwDZ5Rp9TwXhwm3ktvgJzpk/r7dDgGk7ZacMm3ljfcoIvP1E72T8jvyLT1bvdapylajZcTH6W5g==",
           "url": "https://ct.googleapis.com/logs/xenon2021/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-06-17T21:23:01Z"
             }
           },
@@ -200,15 +200,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Xenon2022' log",
           "log_id": "RqVV63X6kSAwtaKJafTzfREsQXS+/Um4havy/HD+bUc=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE+WS9FSxAYlCVEzg8xyGwOrmPonoV14nWjjETAIdZvLvukPzIWBMKv6tDNlQjpIHNrUcUt1igRPpqoKDXw2MeKw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE+WS9FSxAYlCVEzg8xyGwOrmPonoV14nWjjETAIdZvLvukPzIWBMKv6tDNlQjpIHNrUcUt1igRPpqoKDXw2MeKw==",
           "url": "https://ct.googleapis.com/logs/xenon2022/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-06-17T21:23:01Z"
             }
           },
@@ -216,15 +216,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Xenon2023' log",
           "log_id": "rfe++nz/EMiLnT2cHj4YarRnKV3PsQwkyoWGNOvcgoo=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEchY+C+/vzj5g3ZXLY3q5qY1Kb2zcYYCmRV4vg6yU84WI0KV00HuO/8XuQqLwLZPjwtCymeLhQunSxgAnaXSuzg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEchY+C+/vzj5g3ZXLY3q5qY1Kb2zcYYCmRV4vg6yU84WI0KV00HuO/8XuQqLwLZPjwtCymeLhQunSxgAnaXSuzg==",
           "url": "https://ct.googleapis.com/logs/xenon2023/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-12-17T18:38:01Z"
             }
           },
@@ -232,26 +232,26 @@
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Xenon2024' log",
           "log_id": "dv+IPwq2+5VRwmHM9Ye6NLSkzbsp3GhCCp/mZ0xaOnQ=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEuWDgNB415GUAk0+QCb1a7ETdjA/O7RE+KllGmjG2x5n33O89zY+GwjWlPtwpurvyVOKoDIMIUQbeIW02UI44TQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEuWDgNB415GUAk0+QCb1a7ETdjA/O7RE+KllGmjG2x5n33O89zY+GwjWlPtwpurvyVOKoDIMIUQbeIW02UI44TQ==",
           "url": "https://ct.googleapis.com/logs/xenon2024/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2024-01-01T00:00:00Z",
             "end_exclusive": "2025-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Aviator' log",
           "log_id": "aPaY+B9kgr46jO65KB1M/HFRXWeT1ETRCmesu09P+8Q=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1/TMabLkDpCjiupacAlP7xNi0I1JYP8bQFAHDG1xhtolSY1l4QgNRzRrvSe8liE+NPWHdjGxfx3JhTsN9x8/6Q==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1/TMabLkDpCjiupacAlP7xNi0I1JYP8bQFAHDG1xhtolSY1l4QgNRzRrvSe8liE+NPWHdjGxfx3JhTsN9x8/6Q==",
           "url": "https://ct.googleapis.com/aviator/",
           "mmd": 86400,
           "state": {
             "readonly": {
               "timestamp": "2016-11-30T13:24:18Z",
               "final_tree_head": {
                 "sha256_root_hash": "LcGcZRsm+LGYmrlyC5LXhV1T6OD8iH5dNlb0sEJl9bA=",
@@ -259,146 +259,146 @@
               }
             }
           }
         },
         {
           "description": "Google 'Icarus' log",
           "log_id": "KTxRllTIOWW6qlD8WAfUt2+/WHopctykwwz05UVH9Hg=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAETtK8v7MICve56qTHHDhhBOuV4IlUaESxZryCfk9QbG9co/CqPvTsgPDbCpp6oFtyAHwlDhnvr7JijXRD9Cb2FA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAETtK8v7MICve56qTHHDhhBOuV4IlUaESxZryCfk9QbG9co/CqPvTsgPDbCpp6oFtyAHwlDhnvr7JijXRD9Cb2FA==",
           "url": "https://ct.googleapis.com/icarus/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2017-03-06T19:35:01Z"
             }
           }
         },
         {
           "description": "Google 'Pilot' log",
           "log_id": "pLkJkLQYWBSHuxOizGdwCjw1mAT5G9+443fNDsgN3BA=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEfahLEimAoz2t01p3uMziiLOl/fHTDM0YDOhBRuiBARsV4UvxG2LdNgoIGLrtCzWE0J5APC2em4JlvR8EEEFMoA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEfahLEimAoz2t01p3uMziiLOl/fHTDM0YDOhBRuiBARsV4UvxG2LdNgoIGLrtCzWE0J5APC2em4JlvR8EEEFMoA==",
           "url": "https://ct.googleapis.com/pilot/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2014-09-02T20:41:44Z"
             }
           }
         },
         {
           "description": "Google 'Rocketeer' log",
           "log_id": "7ku9t3XOYLrhQmkfq+GeZqMPfl+wctiDAMR7iXqo/cs=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEIFsYyDzBi7MxCAC/oJBXK7dHjG+1aLCOkHjpoHPqTyghLpzA9BYbqvnV16mAw04vUjyYASVGJCUoI3ctBcJAeg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEIFsYyDzBi7MxCAC/oJBXK7dHjG+1aLCOkHjpoHPqTyghLpzA9BYbqvnV16mAw04vUjyYASVGJCUoI3ctBcJAeg==",
           "url": "https://ct.googleapis.com/rocketeer/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2015-08-04T19:00:05Z"
             }
           }
         },
         {
           "description": "Google 'Skydiver' log",
           "log_id": "u9nfvB+KcbWTlCOXqpJ7RzhXlQqrUugakJZkNo4e0YU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEmyGDvYXsRJsNyXSrYc9DjHsIa2xzb4UR7ZxVoV6mrc9iZB7xjI6+NrOiwH+P/xxkRmOFG6Jel20q37hTh58rA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEmyGDvYXsRJsNyXSrYc9DjHsIa2xzb4UR7ZxVoV6mrc9iZB7xjI6+NrOiwH+P/xxkRmOFG6Jel20q37hTh58rA==",
           "url": "https://ct.googleapis.com/skydiver/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2017-03-06T19:35:01Z"
             }
           }
         },
         {
           "description": "Google 'Submariner' log",
           "log_id": "qJnYeAySkKr0YvMYgMz71SRR6XDQ+/WR73Ww2ZtkVoE=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEOfifIGLUV1Voou9JLfA5LZreRLSUMOCeeic8q3Dw0fpRkGMWV0Gtq20fgHQweQJeLVmEByQj9p81uIW4QkWkTw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEOfifIGLUV1Voou9JLfA5LZreRLSUMOCeeic8q3Dw0fpRkGMWV0Gtq20fgHQweQJeLVmEByQj9p81uIW4QkWkTw==",
           "url": "https://ct.googleapis.com/submariner/",
           "mmd": 86400
         },
         {
           "description": "Google 'Daedalus' log",
           "log_id": "HQJLjrFJizRN/YfqPvwJlvdQbyNdHUlwYaR3PEOcJfs=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEbgwcuu4rakGFYB17fqsILPwMCqUIsz7VcCTRbR0ttrfzizbcI02VYxK75IaNzOnR7qFAot8LowYKMMqNrKQpVg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEbgwcuu4rakGFYB17fqsILPwMCqUIsz7VcCTRbR0ttrfzizbcI02VYxK75IaNzOnR7qFAot8LowYKMMqNrKQpVg==",
           "url": "https://ct.googleapis.com/daedalus/",
           "mmd": 604800
         },
         {
           "description": "Google 'Testtube' log",
           "log_id": "sMyD5aX5fWuvfAnMKEkEhyrH6IsTLGNQt8b9JuFsbHc=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEw8i8S7qiGEs9NXv0ZJFh6uuOmR2Q7dPprzk9XNNGkUXjzqx2SDvRfiwKYwBljfWujozHESVPQyydGaHhkaSz/g==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEw8i8S7qiGEs9NXv0ZJFh6uuOmR2Q7dPprzk9XNNGkUXjzqx2SDvRfiwKYwBljfWujozHESVPQyydGaHhkaSz/g==",
           "url": "https://ct.googleapis.com/testtube/",
           "mmd": 86400
         },
         {
           "description": "Google 'Crucible' log",
           "log_id": "w78Dp+HKiEHGB7rj/0Jw/KXsRbGG675OLPP8d4Yw9fY=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEKATl2B3SAbxyzGOfNRB+AytNTGvdF/FFY6HzWb+/HPE4lJ37vx2nEm99KYUy9SoNzF5VyTwCQG5nL/c5Q77yQQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEKATl2B3SAbxyzGOfNRB+AytNTGvdF/FFY6HzWb+/HPE4lJ37vx2nEm99KYUy9SoNzF5VyTwCQG5nL/c5Q77yQQ==",
           "url": "https://ct.googleapis.com/logs/crucible/",
           "mmd": 86400
         },
         {
           "description": "Google 'Solera2018' log",
           "log_id": "UutLIl7IlpdIUGdfI+Q7wdAh4yFM5S7NX6h8IDzfygM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEuFqn5cy1nACARlWIUjeJaRDKl0mcf9gvFZXpPhHsyykizXvULF5GZNGfucWIyUccBRfmYJZTTrXqw0mVts7hA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEuFqn5cy1nACARlWIUjeJaRDKl0mcf9gvFZXpPhHsyykizXvULF5GZNGfucWIyUccBRfmYJZTTrXqw0mVts7hA==",
           "url": "https://ct.googleapis.com/logs/solera2018/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2018-01-01T00:00:00Z",
             "end_exclusive": "2019-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Solera2019' log",
           "log_id": "C3YOmouaaC+ImFsV6UdQGlZEa7qIMHhcOEKZQ4ZFDAA=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEJUwGinXUWVNaBiK2Vl/rdyMkxKaWJHR8dj9yD5AlZEtEbfvAMQQ8o7DQyXVm7TX+eAA9wL2Vtt6DpoMEL0q/rw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEJUwGinXUWVNaBiK2Vl/rdyMkxKaWJHR8dj9yD5AlZEtEbfvAMQQ8o7DQyXVm7TX+eAA9wL2Vtt6DpoMEL0q/rw==",
           "url": "https://ct.googleapis.com/logs/solera2019/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Solera2020' log",
           "log_id": "H8cs5aG3mfQAw1m/+WyjkTVI6GRCIGEJUum6F3T3usc=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEiKfWtuoWCPMEzSKySjMjXpo38WOdZr6Yq0WYa2JQOv1uVMxkqHywf9Gz1kGeRLq/Rz3tVVvXgqb4jQ1UqKVKnw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEiKfWtuoWCPMEzSKySjMjXpo38WOdZr6Yq0WYa2JQOv1uVMxkqHywf9Gz1kGeRLq/Rz3tVVvXgqb4jQ1UqKVKnw==",
           "url": "https://ct.googleapis.com/logs/solera2020/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Solera2021' log",
           "log_id": "o8mYRegKt84AFXs3Qt8CB90nKytgLs+Y7iwS25xa5+c=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1glwxqXsw2VqlAbHSeWbTthMGNIuACVn8Jj/jrnY2iN2uVUrEEwLj5VUCb+WF2XY44+mfUVYY7R/d8TIZ4olnw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1glwxqXsw2VqlAbHSeWbTthMGNIuACVn8Jj/jrnY2iN2uVUrEEwLj5VUCb+WF2XY44+mfUVYY7R/d8TIZ4olnw==",
           "url": "https://ct.googleapis.com/logs/solera2021/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Solera2022' log",
           "log_id": "aXqvyhprU2+uISBQRt661+Dq6hPSQy5unY+zefK5qvM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEFWj6UQDxzHWmgzQtQQ7REDC0nxnU9mpOmA0lv5trA0t7IRzSkh4DOznPe+nkxmaC8iS1capCtKjyYhUNRrvWqA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEFWj6UQDxzHWmgzQtQQ7REDC0nxnU9mpOmA0lv5trA0t7IRzSkh4DOznPe+nkxmaC8iS1capCtKjyYhUNRrvWqA==",
           "url": "https://ct.googleapis.com/logs/solera2022/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "Google 'Solera2023' log",
           "log_id": "+X6XuNM+96FZAqU6GeF5kOXcQGoDGCW6rZPpj5ucacs=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEK9Y56IP6DGQy2d9moYGJChZPoktXoYwaG0MBN/4X5MSFmBaYJfNm3mCwzLVefkjh2wz8Q6q2S75hS/OeHGiZUg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEK9Y56IP6DGQy2d9moYGJChZPoktXoYwaG0MBN/4X5MSFmBaYJfNm3mCwzLVefkjh2wz8Q6q2S75hS/OeHGiZUg==",
           "url": "https://ct.googleapis.com/logs/solera2023/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-01T00:00:00Z"
           }
         }
@@ -414,15 +414,15 @@
         "zi@cloudflare.com",
         "ivan@cloudflare.com"
       ],
       "logs": [
         {
           "description": "Cloudflare 'Nimbus2017' Log",
           "log_id": "H7w24ALt6X9AGZ6Gs1c7ikIX2AGHdGrQ2gOgYFTSDfQ=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE15ypB40iQe6ToFJB2vSA8CW86/rzPNJ+kdg/LNpRvcjuKnLj/xhW5DoiDyI8xtUws5toLqtWwkFf1mRXFLFarw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE15ypB40iQe6ToFJB2vSA8CW86/rzPNJ+kdg/LNpRvcjuKnLj/xhW5DoiDyI8xtUws5toLqtWwkFf1mRXFLFarw==",
           "url": "https://ct.cloudflare.com/logs/nimbus2017/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2017-11-13T19:42:25Z"
             }
           },
@@ -430,15 +430,15 @@
             "start_inclusive": "2017-01-01T00:00:00Z",
             "end_exclusive": "2018-01-01T00:00:00Z"
           }
         },
         {
           "description": "Cloudflare 'Nimbus2018' Log",
           "log_id": "23Sv7ssp7LH+yj5xbSzluaq7NveEcYPHXZ1PN7Yfv2Q=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAsVpWvrH3Ke0VRaMg9ZQoQjb5g/xh1z3DDa6IuxY5DyPsk6brlvrUNXZzoIg0DcvFiAn2kd6xmu4Obk5XA/nRg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAsVpWvrH3Ke0VRaMg9ZQoQjb5g/xh1z3DDa6IuxY5DyPsk6brlvrUNXZzoIg0DcvFiAn2kd6xmu4Obk5XA/nRg==",
           "url": "https://ct.cloudflare.com/logs/nimbus2018/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2019-05-07T00:00:00Z"
             }
           },
@@ -446,15 +446,15 @@
             "start_inclusive": "2018-01-01T00:00:00Z",
             "end_exclusive": "2019-01-01T00:00:00Z"
           }
         },
         {
           "description": "Cloudflare 'Nimbus2019' Log",
           "log_id": "dH7agzGtMxCRIZzOJU9CcMK//V5CIAjGNzV55hB7zFY=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEkZHz1v5r8a9LmXSMegYZAg4UW+Ug56GtNfJTDNFZuubEJYgWf4FcC5D+ZkYwttXTDSo4OkanG9b3AI4swIQ28g==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEkZHz1v5r8a9LmXSMegYZAg4UW+Ug56GtNfJTDNFZuubEJYgWf4FcC5D+ZkYwttXTDSo4OkanG9b3AI4swIQ28g==",
           "url": "https://ct.cloudflare.com/logs/nimbus2019/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-02-07T00:00:00Z"
             }
           },
@@ -462,15 +462,15 @@
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-01T00:00:00Z"
           }
         },
         {
           "description": "Cloudflare 'Nimbus2020' Log",
           "log_id": "Xqdz+d9WwOe1Nkh90EngMnqRmgyEoRIShBh1loFxRVg=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE01EAhx4o0zPQrXTcYjgCt4MVFsT0Pwjzb1RwrM0lhWDlxAYPP6/gyMCXNkOn/7KFsjL7rwk78tHMpY8rXn8AYg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE01EAhx4o0zPQrXTcYjgCt4MVFsT0Pwjzb1RwrM0lhWDlxAYPP6/gyMCXNkOn/7KFsjL7rwk78tHMpY8rXn8AYg==",
           "url": "https://ct.cloudflare.com/logs/nimbus2020/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2021-04-21T00:00:00Z"
             }
           },
@@ -478,15 +478,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "Cloudflare 'Nimbus2021' Log",
           "log_id": "RJRlLrDuzq/EQAfYqP4owNrmgr7YyzG1P9MzlrW2gag=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAExpon7ipsqehIeU1bmpog9TFo4Pk8+9oN8OYHl1Q2JGVXnkVFnuuvPgSo2Ep+6vLffNLcmEbxOucz03sFiematg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAExpon7ipsqehIeU1bmpog9TFo4Pk8+9oN8OYHl1Q2JGVXnkVFnuuvPgSo2Ep+6vLffNLcmEbxOucz03sFiematg==",
           "url": "https://ct.cloudflare.com/logs/nimbus2021/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2018-06-15T02:30:13Z"
             }
           },
@@ -494,15 +494,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Cloudflare 'Nimbus2022' Log",
           "log_id": "QcjKsd8iRkoQxqE6CUKHXk4xixsD6+tLx2jwkGKWBvY=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESLJHTlAycmJKDQxIv60pZG8g33lSYxYpCi5gteI6HLevWbFVCdtZx+m9b+0LrwWWl/87mkNN6xE0M4rnrIPA/w==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESLJHTlAycmJKDQxIv60pZG8g33lSYxYpCi5gteI6HLevWbFVCdtZx+m9b+0LrwWWl/87mkNN6xE0M4rnrIPA/w==",
           "url": "https://ct.cloudflare.com/logs/nimbus2022/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-10-31T19:22:00Z"
             }
           },
@@ -510,15 +510,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "Cloudflare 'Nimbus2023' Log",
           "log_id": "ejKMVNi3LbYg6jjgUh7phBZwMhOFTTvSK8E6V6NS61I=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEi/8tkhjLRp0SXrlZdTzNkTd6HqmcmXiDJz3fAdWLgOhjmv4mohvRhwXul9bgW0ODgRwC9UGAgH/vpGHPvIS1qA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEi/8tkhjLRp0SXrlZdTzNkTd6HqmcmXiDJz3fAdWLgOhjmv4mohvRhwXul9bgW0ODgRwC9UGAgH/vpGHPvIS1qA==",
           "url": "https://ct.cloudflare.com/logs/nimbus2023/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-10-31T19:22:00Z"
             }
           },
@@ -534,39 +534,39 @@
       "email": [
         "ctops@digicert.com"
       ],
       "logs": [
         {
           "description": "DigiCert Log Server",
           "log_id": "VhQGmi/XwuzT9eG9RLI+x0Z2ubyZEVzA75SYVdaJ0N0=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAkbFvhu7gkAW6MHSrBlpE1n4+HCFRkC5OLAjgqhkTH+/uzSfSl8ois8ZxAD2NgaTZe1M9akhYlrYkes4JECs6A==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAkbFvhu7gkAW6MHSrBlpE1n4+HCFRkC5OLAjgqhkTH+/uzSfSl8ois8ZxAD2NgaTZe1M9akhYlrYkes4JECs6A==",
           "url": "https://ct1.digicert-ct.com/log/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2022-01-24T00:00:00Z"
             }
           }
         },
         {
           "description": "DigiCert Log Server 2",
           "log_id": "h3W/51l8+IxDmV+9827/Vo1HVjb/SrVgwbTq/16ggw8=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEzF05L2a4TH/BLgOhNKPoioYCrkoRxvcmajeb8Dj4XQmNY+gxa4Zmz3mzJTwe33i0qMVp+rfwgnliQ/bM/oFmhA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEzF05L2a4TH/BLgOhNKPoioYCrkoRxvcmajeb8Dj4XQmNY+gxa4Zmz3mzJTwe33i0qMVp+rfwgnliQ/bM/oFmhA==",
           "url": "https://ct2.digicert-ct.com/log/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2020-05-04T00:00:40Z"
             }
           }
         },
         {
           "description": "DigiCert Yeti2018 Log",
           "log_id": "wRZK4Kdy0tQ5LcgKwQdw1PDEm96ZGkhAwfoHUWT2M2A=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESYlKFDLLFmA9JScaiaNnqlU8oWDytxIYMfswHy9Esg0aiX+WnP/yj4O0ViEHtLwbmOQeSWBGkIu9YK9CLeer+g==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESYlKFDLLFmA9JScaiaNnqlU8oWDytxIYMfswHy9Esg0aiX+WnP/yj4O0ViEHtLwbmOQeSWBGkIu9YK9CLeer+g==",
           "url": "https://yeti2018.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2019-05-07T00:00:00Z"
             }
           },
@@ -574,15 +574,15 @@
             "start_inclusive": "2017-12-12T00:00:00Z",
             "end_exclusive": "2019-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Yeti2019 Log",
           "log_id": "4mlLribo6UAJ6IYbtjuD1D7n/nSI+6SPKJMBnd3x2/4=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEkZd/ow8X+FSVWAVSf8xzkFohcPph/x6pS1JHh7g1wnCZ5y/8Hk6jzJxs6t3YMAWz2CPd4VkCdxwKexGhcFxD9A==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEkZd/ow8X+FSVWAVSf8xzkFohcPph/x6pS1JHh7g1wnCZ5y/8Hk6jzJxs6t3YMAWz2CPd4VkCdxwKexGhcFxD9A==",
           "url": "https://yeti2019.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-02-07T00:00:00Z"
             }
           },
@@ -590,15 +590,15 @@
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Yeti2020 Log",
           "log_id": "8JWkWfIA0YJAEC0vk4iOrUv+HUfjmeHQNKawqKqOsnM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEURAG+Zo0ac3n37ifZKUhBFEV6jfcCzGIRz3tsq8Ca9BP/5XUHy6ZiqsPaAEbVM0uI3Tm9U24RVBHR9JxDElPmg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEURAG+Zo0ac3n37ifZKUhBFEV6jfcCzGIRz3tsq8Ca9BP/5XUHy6ZiqsPaAEbVM0uI3Tm9U24RVBHR9JxDElPmg==",
           "url": "https://yeti2020.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2021-04-21T00:00:00Z"
             }
           },
@@ -606,15 +606,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Yeti2021 Log",
           "log_id": "XNxDkv7mq0VEsV6a1FbmEDf71fpH3KFzlLJe5vbHDso=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE6J4EbcpIAl1+AkSRsbhoY5oRTj3VoFfaf1DlQkfi7Rbe/HcjfVtrwN8jaC+tQDGjF+dqvKhWJAQ6Q6ev6q9Mew==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE6J4EbcpIAl1+AkSRsbhoY5oRTj3VoFfaf1DlQkfi7Rbe/HcjfVtrwN8jaC+tQDGjF+dqvKhWJAQ6Q6ev6q9Mew==",
           "url": "https://yeti2021.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2018-08-24T00:53:07Z"
             }
           },
@@ -622,15 +622,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Yeti2022 Log",
           "log_id": "IkVFB1lVJFaWP6Ev8fdthuAjJmOtwEt/XcaDXG7iDwI=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEn/jYHd77W1G1+131td5mEbCdX/1v/KiYW5hPLcOROvv+xA8Nw2BDjB7y+RGyutD2vKXStp/5XIeiffzUfdYTJg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEn/jYHd77W1G1+131td5mEbCdX/1v/KiYW5hPLcOROvv+xA8Nw2BDjB7y+RGyutD2vKXStp/5XIeiffzUfdYTJg==",
           "url": "https://yeti2022.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2021-07-21T00:00:00Z"
             }
           },
@@ -638,15 +638,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Yeti2022-2 Log",
           "log_id": "BZwB0yDgB4QTlYBJjRF8kDJmr69yULWvO0akPhGEDUo=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEHWlePwrycXfNnV3DNEkA7mB34XJ2dKh8XH0J8jIdBX4u/lsx1Tr9czRuSRROUFiWWsTH9L4FZKT31+WxbTMMww==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEHWlePwrycXfNnV3DNEkA7mB34XJ2dKh8XH0J8jIdBX4u/lsx1Tr9czRuSRROUFiWWsTH9L4FZKT31+WxbTMMww==",
           "url": "https://yeti2022-2.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "pending": {
               "timestamp": "2021-07-20T07:41:00Z"
             }
           },
@@ -654,15 +654,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Yeti2023 Log",
           "log_id": "Nc8ZG7+xbFe/D61MbULLu7YnICZR6j/hKu+oA8M71kw=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEfQ0DsdWYitzwFTvG3F4Nbj8Nv5XIVYzQpkyWsU4nuSYlmcwrAp6m092fsdXEw6w1BAeHlzaqrSgNfyvZaJ9y0Q==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEfQ0DsdWYitzwFTvG3F4Nbj8Nv5XIVYzQpkyWsU4nuSYlmcwrAp6m092fsdXEw6w1BAeHlzaqrSgNfyvZaJ9y0Q==",
           "url": "https://yeti2023.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-10-31T19:22:00Z"
             }
           },
@@ -670,15 +670,15 @@
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Nessie2018 Log",
           "log_id": "b/FBtWR+QiL37wUs7658If1gjifSr1pun0uKN9ZjPuU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEVqpLa2W+Rz1XDZPBIyKJO+KKFOYZTj9MpJWnZeFUqzc5aivOiWEVhs8Gy2AlH3irWPFjIZPZMs3Dv7M+0LbPyQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEVqpLa2W+Rz1XDZPBIyKJO+KKFOYZTj9MpJWnZeFUqzc5aivOiWEVhs8Gy2AlH3irWPFjIZPZMs3Dv7M+0LbPyQ==",
           "url": "https://nessie2018.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2019-05-07T00:00:00Z"
             }
           },
@@ -686,15 +686,15 @@
             "start_inclusive": "2017-12-12T00:00:00Z",
             "end_exclusive": "2019-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Nessie2019 Log",
           "log_id": "/kRhCLHQGreKYsz+q2qysrq/86va2ApNizDfLQAIgww=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEX+0nudCKImd7QCtelhMrDW0OXni5RE10tiiClZesmrwUk2iHLCoTHHVV+yg5D4n/rxCRVyRhikPpVDOLMLxJaA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEX+0nudCKImd7QCtelhMrDW0OXni5RE10tiiClZesmrwUk2iHLCoTHHVV+yg5D4n/rxCRVyRhikPpVDOLMLxJaA==",
           "url": "https://nessie2019.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-02-07T00:00:00Z"
             }
           },
@@ -702,15 +702,15 @@
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Nessie2020 Log",
           "log_id": "xlKg7EjOs/yrFwmSxDqHQTMJ6ABlomJSQBujNioXxWU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE4hHIyMVIrR9oShgbQMYEk8WX1lmkfFKB448Gn93KbsZnnwljDHY6MQqEnWfKGgMOq0gh3QK48c5ZB3UKSIFZ4g==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE4hHIyMVIrR9oShgbQMYEk8WX1lmkfFKB448Gn93KbsZnnwljDHY6MQqEnWfKGgMOq0gh3QK48c5ZB3UKSIFZ4g==",
           "url": "https://nessie2020.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2021-04-21T00:00:00Z"
             }
           },
@@ -718,15 +718,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Nessie2021 Log",
           "log_id": "7sCV7o1yZA+S48O5G8cSo2lqCXtLahoUOOZHssvtxfk=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE9o7AiwrbGBIX6Lnc47I6OfLMdZnRzKoP5u072nBi6vpIOEooktTi1gNwlRPzGC2ySGfuc1xLDeaA/wSFGgpYFg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE9o7AiwrbGBIX6Lnc47I6OfLMdZnRzKoP5u072nBi6vpIOEooktTi1gNwlRPzGC2ySGfuc1xLDeaA/wSFGgpYFg==",
           "url": "https://nessie2021.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-05-09T22:11:02Z"
             }
           },
@@ -734,15 +734,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Nessie2022 Log",
           "log_id": "UaOw9f0BeZxWbbg3eI8MpHrMGyfL956IQpoN/tSLBeU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEJyTdaAMoy/5jvg4RR019F2ihEV1McclBKMe2okuX7MCv/C87v+nxsfz1Af+p+0lADGMkmNd5LqZVqxbGvlHYcQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEJyTdaAMoy/5jvg4RR019F2ihEV1McclBKMe2okuX7MCv/C87v+nxsfz1Af+p+0lADGMkmNd5LqZVqxbGvlHYcQ==",
           "url": "https://nessie2022.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-05-09T22:11:02Z"
             }
           },
@@ -750,15 +750,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "DigiCert Nessie2023 Log",
           "log_id": "s3N3B+GEUPhjhtYFqdwRCUp5LbFnDAuH3PADDnk2pZo=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEXu8iQwSCRSf2CbITGpUpBtFVt8+I0IU0d1C36Lfe1+fbwdaI0Z5FktfM2fBoI1bXBd18k2ggKGYGgdZBgLKTg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEXu8iQwSCRSf2CbITGpUpBtFVt8+I0IU0d1C36Lfe1+fbwdaI0Z5FktfM2fBoI1bXBd18k2ggKGYGgdZBgLKTg==",
           "url": "https://nessie2023.ct.digicert.com/log/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2019-10-31T19:22:00Z"
             }
           },
@@ -766,46 +766,46 @@
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-01T00:00:00Z"
           }
         },
         {
           "description": "Symantec log",
           "log_id": "3esdK3oNT6Ygi4GtgWhwfi6OnQHVXIiNPRHEzbbsvsw=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEluqsHEYMG1XcDfy1lCdGV0JwOmkY4r87xNuroPS2bMBTP01CEDPwWJePa75y9CrsHEKqAy8afig1dpkIPSEUhg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEluqsHEYMG1XcDfy1lCdGV0JwOmkY4r87xNuroPS2bMBTP01CEDPwWJePa75y9CrsHEKqAy8afig1dpkIPSEUhg==",
           "url": "https://ct.ws.symantec.com/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2019-02-16T00:00:00Z"
             }
           }
         },
         {
           "description": "Symantec 'Vega' log",
           "log_id": "vHjh38X2PGhGSTNNoQ+hXwl5aSAJwIG08/aRfz7ZuKU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE6pWeAv/u8TNtS4e8zf0ZF2L/lNPQWQc/Ai0ckP7IRzA78d0NuBEMXR2G3avTK0Zm+25ltzv9WWis36b4ztIYTQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE6pWeAv/u8TNtS4e8zf0ZF2L/lNPQWQc/Ai0ckP7IRzA78d0NuBEMXR2G3avTK0Zm+25ltzv9WWis36b4ztIYTQ==",
           "url": "https://vega.ws.symantec.com/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2019-02-16T00:00:00Z"
             }
           }
         },
         {
           "description": "Symantec Deneb",
           "log_id": "p85KTmIH4K3e5f2qSx+GdodntdACpV1HMQ5+ZwqV6rI=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEloIeo806gIQel7i3BxmudhoO+FV2nRIzTpGI5NBIUFzBn2py1gH1FNbQOG7hMrxnDTfouiIQ0XKGeSiW+RcemA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEloIeo806gIQel7i3BxmudhoO+FV2nRIzTpGI5NBIUFzBn2py1gH1FNbQOG7hMrxnDTfouiIQ0XKGeSiW+RcemA==",
           "url": "https://deneb.ws.symantec.com/",
           "mmd": 86400
         },
         {
           "description": "Symantec 'Sirius' log",
           "log_id": "FZcEiNe5l6Bb61JRKt7o0ui0oxZSZBIan6v71fha2T8=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEowJkhCK7JewN47zCyYl93UXQ7uYVhY/Z5xcbE4Dq7bKFN61qxdglnfr0tPNuFiglN+qjN2Syxwv9UeXBBfQOtQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEowJkhCK7JewN47zCyYl93UXQ7uYVhY/Z5xcbE4Dq7bKFN61qxdglnfr0tPNuFiglN+qjN2Syxwv9UeXBBfQOtQ==",
           "url": "https://sirius.ws.symantec.com/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2019-02-16T00:00:00Z"
             }
           }
@@ -817,15 +817,15 @@
       "email": [
         "ian@certly.io"
       ],
       "logs": [
         {
           "description": "Certly.IO log",
           "log_id": "zbUXm3/BwEb+6jETaj+PAC5hgvr4iW/syLL1tatgSQA=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAECyPLhWKYYUgEc+tUXfPQB4wtGS2MNvXrjwFCCnyYJifBtd2Sk7Cu+Js9DNhMTh35FftHaHu6ZrclnNBKwmbbSA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAECyPLhWKYYUgEc+tUXfPQB4wtGS2MNvXrjwFCCnyYJifBtd2Sk7Cu+Js9DNhMTh35FftHaHu6ZrclnNBKwmbbSA==",
           "url": "https://log.certly.io/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2016-04-15T00:00:00Z"
             }
           }
@@ -837,27 +837,27 @@
       "email": [
         "atecnica@izenpe.net"
       ],
       "logs": [
         {
           "description": "Izenpe log",
           "log_id": "dGG0oJz7PUHXUVlXWy52SaRFqNJ3CbDMVkpkgrfrQaM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEJ2Q5DC3cUBj4IQCiDu0s6j51up+TZAkAEcQRF6tczw90rLWXkJMAW7jr9yc92bIKgV8vDXU4lDeZHvYHduDuvg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEJ2Q5DC3cUBj4IQCiDu0s6j51up+TZAkAEcQRF6tczw90rLWXkJMAW7jr9yc92bIKgV8vDXU4lDeZHvYHduDuvg==",
           "url": "https://ct.izenpe.com/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2016-05-30T00:00:00Z"
             }
           }
         },
         {
           "description": "Izenpe 'Argi' log",
           "log_id": "iUFEnHB0Lga5/JznsRa6ACSqNtWa9E8CBEBPAPfqhWY=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE18gOIz6eAjyauAdKKgX/SkuI1IpNOc73xfK2N+mj7eT1RQkOZxT9UyTVOpTy6rUT2R2LXKfD82vYPy07ZXJY1g==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE18gOIz6eAjyauAdKKgX/SkuI1IpNOc73xfK2N+mj7eT1RQkOZxT9UyTVOpTy6rUT2R2LXKfD82vYPy07ZXJY1g==",
           "url": "https://ct.izenpe.eus/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2016-08-25T11:36:00Z"
             }
           }
@@ -869,39 +869,39 @@
       "email": [
         "ctlog@wosign.com"
       ],
       "logs": [
         {
           "description": "WoSign CT log #1",
           "log_id": "nk/3PcPOIgtpIXyJnkaAdqv414Y21cz8haMadWKLqIs=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1+wvK3VPN7yjQ7qLZWY8fWrlDCqmwuUm/gx9TnzwOrzi0yLcAdAfbkOcXG6DrZwV9sSNYLUdu6NiaX7rp6oBmw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE1+wvK3VPN7yjQ7qLZWY8fWrlDCqmwuUm/gx9TnzwOrzi0yLcAdAfbkOcXG6DrZwV9sSNYLUdu6NiaX7rp6oBmw==",
           "url": "https://ct.wosign.com/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2016-04-02T01:27:00Z"
             }
           }
         },
         {
           "description": "WoSign log",
           "log_id": "QbLcLonmPOSvG6e7Kb9oxt7m+fHMBH4w3/rjs7olkmM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEzBGIey1my66PTTBmJxklIpMhRrQvAdPG+SvVyLpzmwai8IoCnNBrRhgwhbrpJIsO0VtwKAx+8TpFf1rzgkJgMQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEzBGIey1my66PTTBmJxklIpMhRrQvAdPG+SvVyLpzmwai8IoCnNBrRhgwhbrpJIsO0VtwKAx+8TpFf1rzgkJgMQ==",
           "url": "https://ctlog.wosign.com/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2018-02-12T23:59:59Z"
             }
           }
         },
         {
           "description": "WoSign log 2",
           "log_id": "Y9AAYCbd4QuwYB9FJEaWXuK26izU+8layGalUK+Qdbc=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEpYzoNS6O5Wp1rVxLMWEpnTBXjgITX+nKu1KoQwVgvw1zV3eyBdhn9vAzyflE3rZTc6oMVcKDCkvOXhrHFx2zzQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEpYzoNS6O5Wp1rVxLMWEpnTBXjgITX+nKu1KoQwVgvw1zV3eyBdhn9vAzyflE3rZTc6oMVcKDCkvOXhrHFx2zzQ==",
           "url": "https://ctlog2.wosign.com/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2017-09-04T13:30:58Z"
             }
           }
@@ -913,15 +913,15 @@
       "email": [
         "capoc@gdca.com.cn"
       ],
       "logs": [
         {
           "description": "GDCA CT log #1",
           "log_id": "yc+JCiEQnGZswXo+0GXJMNDgE1qf66ha8UIQuAckIao=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAErQ8wrZ55pDiJJlSGq0FykG/7yhemrO7Gn30CBexBqMdBnTJJrbA5vTqHPnzuaGxg0Ucqk67hQPQLyDU8HQ9l0w==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAErQ8wrZ55pDiJJlSGq0FykG/7yhemrO7Gn30CBexBqMdBnTJJrbA5vTqHPnzuaGxg0Ucqk67hQPQLyDU8HQ9l0w==",
           "url": "https://ct.gdca.com.cn/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2016-07-12T13:03:00Z"
             }
           }
@@ -933,27 +933,27 @@
       "email": [
         "capoc@gdca.com.cn"
       ],
       "logs": [
         {
           "description": "GDCA CT log #2",
           "log_id": "kkow+Qkzb/Q11pk6EKx1osZBco5/wtZZrmGI/61AzgE=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEW0rHAbd0VLpAnEN1lD+s77NxVrjT4nuuobE+U6qXM6GCu19dHAv6hQ289+Wg4CLwoInZCn9fJpTTJOOZLuQVjQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEW0rHAbd0VLpAnEN1lD+s77NxVrjT4nuuobE+U6qXM6GCu19dHAv6hQ289+Wg4CLwoInZCn9fJpTTJOOZLuQVjQ==",
           "url": "https://ctlog.gdca.com.cn/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2017-01-11T10:46:00Z"
             }
           }
         },
         {
           "description": "GDCA Log 1",
           "log_id": "cX6nQgl1voSicjVT8Xd8Jt1Rr04QIUQJTZAZtGL7Zmg=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEzVH/qe+rgr/Mo+owQ00I6aegDSjHttgqQBmg+hBdTXXLgJT/+8LdSgjfY/8lOBtfivndJzQlTNQ9Le1cU6wXNQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEzVH/qe+rgr/Mo+owQ00I6aegDSjHttgqQBmg+hBdTXXLgJT/+8LdSgjfY/8lOBtfivndJzQlTNQ9Le1cU6wXNQ==",
           "url": "https://log.gdca.com.cn/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2018-11-05T22:18:14Z"
             }
           },
@@ -961,15 +961,15 @@
             "start_inclusive": "2018-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "GDCA Log 2",
           "log_id": "FDCNkMzQMBNQBcAcpSbYHoTodiTjm2JI4I9ySuo7tCo=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEx6zC8wCbMs3fpQWnSCeo2RvG827AEssGkMa+5RVIpRF0SDO4hFsn2Ph1l8marSCuwVLhv0JIN3arSzbUieX6HA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEx6zC8wCbMs3fpQWnSCeo2RvG827AEssGkMa+5RVIpRF0SDO4hFsn2Ph1l8marSCuwVLhv0JIN3arSzbUieX6HA==",
           "url": "https://log2.gdca.com.cn/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2018-11-08T05:47:34Z"
             }
           },
@@ -985,34 +985,34 @@
       "email": [
         "ctops@sectigo.com"
       ],
       "logs": [
         {
           "description": "Sectigo 'Dodo' CT log",
           "log_id": "23b9raxl59CVCIhuIVm9i5A1L1/q0+PcXiLrNQrMe5g=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELPXCMfVjQ2oWSgrewu4fIW4Sfh3lco90CwKZ061pvAI1eflh6c8ACE90pKM0muBDHCN+j0HV7scco4KKQPqq4A==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELPXCMfVjQ2oWSgrewu4fIW4Sfh3lco90CwKZ061pvAI1eflh6c8ACE90pKM0muBDHCN+j0HV7scco4KKQPqq4A==",
           "url": "https://dodo.ct.comodo.com/",
           "mmd": 86400
         },
         {
           "description": "Sectigo 'Sabre' CT log",
           "log_id": "VYHUwhaQNgFK6gubVzxT8MDkOHhwJQgXL6OqHQcT0ww=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8m/SiQ8/xfiHHqtls9m7FyOMBg4JVZY9CgiixXGz0akvKD6DEL8S0ERmFe9U4ZiA0M4kbT5nmuk3I85Sk4bagA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8m/SiQ8/xfiHHqtls9m7FyOMBg4JVZY9CgiixXGz0akvKD6DEL8S0ERmFe9U4ZiA0M4kbT5nmuk3I85Sk4bagA==",
           "url": "https://sabre.ct.comodo.com/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2017-10-10T00:38:10Z"
             }
           }
         },
         {
           "description": "Sectigo 'Mammoth' CT log",
           "log_id": "b1N2rDHwMRnYmQCkURX/dxUcEdkCwQApBo2yCJo32RM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE7+R9dC4VFbbpuyOL+yy14ceAmEf7QGlo/EmtYU6DRzwat43f/3swtLr/L8ugFOOt1YU/RFmMjGCL17ixv66MZw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE7+R9dC4VFbbpuyOL+yy14ceAmEf7QGlo/EmtYU6DRzwat43f/3swtLr/L8ugFOOt1YU/RFmMjGCL17ixv66MZw==",
           "url": "https://mammoth.ct.comodo.com/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2017-10-10T00:38:10Z"
             }
           }
@@ -1024,27 +1024,27 @@
       "email": [
         "ctlog-admin@venafi.com"
       ],
       "logs": [
         {
           "description": "Venafi log",
           "log_id": "rDua7X+pZ0dXFZ5tfVdWcvnZgQCUHpve/+yhMTt1eC0=",
-          "key": "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAolpIHxdSlTXLo1s6H1OCdpSj/4DyHDc8wLG9wVmLqy1lk9fz4ATVmm+/1iN2Nk8jmctUKK2MFUtlWXZBSpym97M7frGlSaQXUWyA3CqQUEuIJOmlEjKTBEiQAvpfDjCHjlV2Be4qTM6jamkJbiWtgnYPhJL6ONaGTiSPm7Byy57iaz/hbckldSOIoRhYBiMzeNoA0DiRZ9KmfSeXZ1rB8y8X5urSW+iBzf2SaOfzBvDpcoTuAaWx2DPazoOl28fP1hZ+kHUYvxbcMjttjauCFx+JII0dmuZNIwjfeG/GBb9frpSX219k1O4Wi6OEbHEr8at/XQ0y7gTikOxBn/s5wQIDAQAB",
+          "keys": "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAolpIHxdSlTXLo1s6H1OCdpSj/4DyHDc8wLG9wVmLqy1lk9fz4ATVmm+/1iN2Nk8jmctUKK2MFUtlWXZBSpym97M7frGlSaQXUWyA3CqQUEuIJOmlEjKTBEiQAvpfDjCHjlV2Be4qTM6jamkJbiWtgnYPhJL6ONaGTiSPm7Byy57iaz/hbckldSOIoRhYBiMzeNoA0DiRZ9KmfSeXZ1rB8y8X5urSW+iBzf2SaOfzBvDpcoTuAaWx2DPazoOl28fP1hZ+kHUYvxbcMjttjauCFx+JII0dmuZNIwjfeG/GBb9frpSX219k1O4Wi6OEbHEr8at/XQ0y7gTikOxBn/s5wQIDAQAB",
           "url": "https://ctlog.api.venafi.com/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2017-02-28T18:42:26Z"
             }
           }
         },
         {
           "description": "Venafi Gen2 CT log",
           "log_id": "AwGd8/2FppqOvR+sxtqbpz5Gl3T+d/V5/FoIuDKMHWs=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjicnerZVCXTrbEuUhGW85BXx6lrYfA43zro/bAna5ymW00VQb94etBzSg4j/KS/Oqf/fNN51D8DMGA2ULvw3AQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjicnerZVCXTrbEuUhGW85BXx6lrYfA43zro/bAna5ymW00VQb94etBzSg4j/KS/Oqf/fNN51D8DMGA2ULvw3AQ==",
           "url": "https://ctlog-gen2.api.venafi.com/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-05-08T00:00:00Z"
             }
           }
@@ -1056,15 +1056,15 @@
       "email": [
         "ctlog-admin@cnnic.cn"
       ],
       "logs": [
         {
           "description": "CNNIC CT log",
           "log_id": "pXesnO11SN2PAltnokEInfhuD0duwgPC7L7bGF8oJjg=",
-          "key": "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAv7UIYZopMgTTJWPp2IXhhuAf1l6a9zM7gBvntj5fLaFm9pVKhKYhVnno94XuXeN8EsDgiSIJIj66FpUGvai5samyetZhLocRuXhAiXXbDNyQ4KR51tVebtEq2zT0mT9liTtGwiksFQccyUsaVPhsHq9gJ2IKZdWauVA2Fm5x9h8B9xKn/L/2IaMpkIYtd967TNTP/dLPgixN1PLCLaypvurDGSVDsuWabA3FHKWL9z8wr7kBkbdpEhLlg2H+NAC+9nGKx+tQkuhZ/hWR65aX+CNUPy2OB9/u2rNPyDydb988LENXoUcMkQT0dU3aiYGkFAY0uZjD2vH97TM20xYtNQIDAQAB",
+          "keys": "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAv7UIYZopMgTTJWPp2IXhhuAf1l6a9zM7gBvntj5fLaFm9pVKhKYhVnno94XuXeN8EsDgiSIJIj66FpUGvai5samyetZhLocRuXhAiXXbDNyQ4KR51tVebtEq2zT0mT9liTtGwiksFQccyUsaVPhsHq9gJ2IKZdWauVA2Fm5x9h8B9xKn/L/2IaMpkIYtd967TNTP/dLPgixN1PLCLaypvurDGSVDsuWabA3FHKWL9z8wr7kBkbdpEhLlg2H+NAC+9nGKx+tQkuhZ/hWR65aX+CNUPy2OB9/u2rNPyDydb988LENXoUcMkQT0dU3aiYGkFAY0uZjD2vH97TM20xYtNQIDAQAB",
           "url": "https://ctserver.cnnic.cn/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2018-09-18T00:00:00Z"
             }
           }
@@ -1076,15 +1076,15 @@
       "email": [
         "ct@startssl.com"
       ],
       "logs": [
         {
           "description": "StartCom log",
           "log_id": "NLtq1sPfnAPuqKSZ/3iRSGydXlysktAfe/0bzhnbSO8=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESPNZ8/YFGNPbsu1Gfs/IEbVXsajWTOaft0oaFIZDqUiwy1o/PErK38SCFFWa+PeOQFXc9NKv6nV0+05/YIYuUQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESPNZ8/YFGNPbsu1Gfs/IEbVXsajWTOaft0oaFIZDqUiwy1o/PErK38SCFFWa+PeOQFXc9NKv6nV0+05/YIYuUQ==",
           "url": "https://ct.startssl.com/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2018-02-12T23:59:59Z"
             }
           }
@@ -1096,15 +1096,15 @@
       "email": [
         "certificatetransparency@outlook.com"
       ],
       "logs": [
         {
           "description": "PuChuangSiDa CT log",
           "log_id": "4BJ2KekEllZOPQFHmESYqkj4rbFmAOt5AqHvmQmQYnM=",
-          "key": "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEArM8vS3Cs8Q2Wv+gK/kSd1IwXncOaEBGEE+2M+Tdtg+QAb7FLwKaJx2GPmjS7VlLKA1ZQ7yR/S0npNYHd8OcX9XLSI8XjE3/Xjng1j0nemASKY6+tojlwlYRoS5Ez/kzhMhfC8mG4Oo05f9WVgj5WGVBFb8sIMw3VGUIIGkhCEPFow8NBE8sNHtsCtyR6UZZuvAjqaa9t75KYjlXzZeXonL4aR2AwfXqArVaDepPDrpMraiiKpl9jGQy+fHshY0E4t/fodnNrhcy8civBUtBbXTFOnSrzTZtkFJkmxnH4e/hE1eMjIPMK14tRPnKA0nh4NS1K50CZEZU01C9/+V81NwIDAQAB",
+          "keys": "MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEArM8vS3Cs8Q2Wv+gK/kSd1IwXncOaEBGEE+2M+Tdtg+QAb7FLwKaJx2GPmjS7VlLKA1ZQ7yR/S0npNYHd8OcX9XLSI8XjE3/Xjng1j0nemASKY6+tojlwlYRoS5Ez/kzhMhfC8mG4Oo05f9WVgj5WGVBFb8sIMw3VGUIIGkhCEPFow8NBE8sNHtsCtyR6UZZuvAjqaa9t75KYjlXzZeXonL4aR2AwfXqArVaDepPDrpMraiiKpl9jGQy+fHshY0E4t/fodnNrhcy8civBUtBbXTFOnSrzTZtkFJkmxnH4e/hE1eMjIPMK14tRPnKA0nh4NS1K50CZEZU01C9/+V81NwIDAQAB",
           "url": "https://www.certificatetransparency.cn/ct/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2017-06-08T00:00:00Z"
             }
           }
@@ -1116,49 +1116,49 @@
       "email": [
         "linus@nordu.net"
       ],
       "logs": [
         {
           "description": "Nordu 'flimsy' log",
           "log_id": "U3tpo1ZDNanASQTjlZOywpjrjXpugwI2NcYnJIzWtEA=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE4qWq6afhBUi0OdcWUYhyJLNXTkGqQ9PMS5lqoCgkV2h1ZvpNjBH2u8UbgcOQwqDo66z6BWQJGolozZYmNHE2kQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE4qWq6afhBUi0OdcWUYhyJLNXTkGqQ9PMS5lqoCgkV2h1ZvpNjBH2u8UbgcOQwqDo66z6BWQJGolozZYmNHE2kQ==",
           "url": "https://flimsy.ct.nordu.net:8080/",
           "mmd": 86400
         },
         {
           "description": "Nordu 'plausible' log",
           "log_id": "qucLfzy41WbIbC8Wl5yfRF9pqw60U1WJsvd6AwEE880=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE9UV9+jO2MCTzkabodO2F7LM03MUBc8MrdAtkcW6v6GA9taTTw9QJqofm0BbdAsbtJL/unyEf0zIkRgXjjzaYqQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE9UV9+jO2MCTzkabodO2F7LM03MUBc8MrdAtkcW6v6GA9taTTw9QJqofm0BbdAsbtJL/unyEf0zIkRgXjjzaYqQ==",
           "url": "https://plausible.ct.nordu.net/",
           "mmd": 86400
         }
       ]
     },
     {
       "name": "SHECA",
       "email": [
         "CTLS@sheca.com"
       ],
       "logs": [
         {
           "description": "SHECA CT log 1",
           "log_id": "z1XiiSNJfDQNUgbQU1Ouslg0tS8fjclSaAnyEu/dfKY=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEalgK7RxRWbgLt7VhzvV/vCSN/RoxpLdPxrivAwi1pljKW4yKBTAdiyAqCJRkdbrptjx7PAHfrD8dnB2cnyR6Q==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEEalgK7RxRWbgLt7VhzvV/vCSN/RoxpLdPxrivAwi1pljKW4yKBTAdiyAqCJRkdbrptjx7PAHfrD8dnB2cnyR6Q==",
           "url": "http://ctlog.sheca.com/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2017-03-22T14:16:19Z"
             }
           }
         },
         {
           "description": "SHECA CT log 2",
           "log_id": "MtxZwtTEGWjVbhS8YayPDkXbOfrzwVWqQlL1AB+gxiM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsY4diqo6rM6Gy1N26KidWb4XiAMH8ifggr6x/Gc7Ru7T8Y3Wd+ijtNsJXKAJQ/xf0Gg0IyQIwk/Y0rad7dWM2w==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsY4diqo6rM6Gy1N26KidWb4XiAMH8ifggr6x/Gc7Ru7T8Y3Wd+ijtNsJXKAJQ/xf0Gg0IyQIwk/Y0rad7dWM2w==",
           "url": "https://ct.sheca.com/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2017-12-01T20:22:23Z"
             }
           }
@@ -1170,15 +1170,15 @@
       "email": [
         "ct-help@akamai.com"
       ],
       "logs": [
         {
           "description": "Akamai CT Log",
           "log_id": "lgbALGkAM6odFF9ZxuJkjQVJ8N+WqrjbkVpw2OzzkKU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEQ3nrSVxQKkpqj1mTvMNCdsKZ+CeBPAZs0sgEj3R7tLUh8uOo3DO5/iXpPQT8P7SuQONFfoSSKthS6x8/cxPQyA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEQ3nrSVxQKkpqj1mTvMNCdsKZ+CeBPAZs0sgEj3R7tLUh8uOo3DO5/iXpPQT8P7SuQONFfoSSKthS6x8/cxPQyA==",
           "url": "https://ct.akamai.com/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2015-11-27T15:37:00Z"
             }
           }
@@ -1190,15 +1190,15 @@
       "email": [
         "ops@ctlogs.org"
       ],
       "logs": [
         {
           "description": "Alpha CT Log",
           "log_id": "OTdvVF97Rgf1l0LXaM1dJDe/NHO2U0pINLz3Lmgcg8k=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEovftE+HTXAIIxI6Lm4s7OWjHkmo4oU8jxaVvb9dlgfjBm/SfqYtF9LlOG8miaReleIfZzohvQQO7oyrjd5eNeA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEovftE+HTXAIIxI6Lm4s7OWjHkmo4oU8jxaVvb9dlgfjBm/SfqYtF9LlOG8miaReleIfZzohvQQO7oyrjd5eNeA==",
           "url": "https://alpha.ctlogs.org/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2015-01-27T14:38:00Z"
             }
           }
@@ -1210,15 +1210,15 @@
       "email": [
         "sre@letsencrypt.org"
       ],
       "logs": [
         {
           "description": "Let's Encrypt 'Oak2019' log",
           "log_id": "ZZszUPQ7EsxepatOx2XT/ebIgkN3d3jnIAP56yuMMSk=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEFkqNKRuZ+Z8IOsnNJrUZ8gwp+KKGOdQrJ/HKhSadK/SJuoCc9+dxQ7awpmWIMr9SKcQeG5uRzG1kVSyFN4Wfcw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEFkqNKRuZ+Z8IOsnNJrUZ8gwp+KKGOdQrJ/HKhSadK/SJuoCc9+dxQ7awpmWIMr9SKcQeG5uRzG1kVSyFN4Wfcw==",
           "url": "https://oak.ct.letsencrypt.org/2019/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-02-07T00:00:00Z"
             }
           },
@@ -1226,15 +1226,15 @@
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Oak2020' log",
           "log_id": "5xLysDd+GmL7jskMYYTx6ns3y1YdESZb8+DzS/JBVG4=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEfzb42Zdr/h7hgqgDCo1vrNJqGqbcUvJGJEER9DDqp19W/wFSB0l166hD+U5cAXchpH8ZkBNUuvOHS0OnJ4oJrQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEfzb42Zdr/h7hgqgDCo1vrNJqGqbcUvJGJEER9DDqp19W/wFSB0l166hD+U5cAXchpH8ZkBNUuvOHS0OnJ4oJrQ==",
           "url": "https://oak.ct.letsencrypt.org/2020/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2021-04-21T00:00:00Z"
             }
           },
@@ -1242,15 +1242,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Oak2021' log",
           "log_id": "lCC8Ho7VjWyIcx+CiyIsDdHaTV5sT5Q9YdtOL1hNosI=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELsYzGMNwo8rBIlaklBIdmD2Ofn6HkfrjK0Ukz1uOIUC6Lm0jTITCXhoIdjs7JkyXnwuwYiJYiH7sE1YeKu8k9w==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAELsYzGMNwo8rBIlaklBIdmD2Ofn6HkfrjK0Ukz1uOIUC6Lm0jTITCXhoIdjs7JkyXnwuwYiJYiH7sE1YeKu8k9w==",
           "url": "https://oak.ct.letsencrypt.org/2021/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2020-01-27T18:18:26Z"
             }
           },
@@ -1258,15 +1258,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Oak2022' log",
           "log_id": "36Veq2iCTx9sre64X04+WurNohKkal6OOxLAIERcKnM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEhjyxDVIjWt5u9sB/o2S8rcGJ2pdZTGA8+IpXhI/tvKBjElGE5r3de4yAfeOPhqTqqc+o7vPgXnDgu/a9/B+RLg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEhjyxDVIjWt5u9sB/o2S8rcGJ2pdZTGA8+IpXhI/tvKBjElGE5r3de4yAfeOPhqTqqc+o7vPgXnDgu/a9/B+RLg==",
           "url": "https://oak.ct.letsencrypt.org/2022/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2020-01-27T18:18:26Z"
             }
           },
@@ -1274,15 +1274,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Oak2023' log",
           "log_id": "tz77JN+cTbp18jnFulj0bF38Qs96nzXEnh0JgSXttJk=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsz0OeL7jrVxEXJu+o4QWQYLKyokXHiPOOKVUL3/TNFFquVzDSer7kZ3gijxzBp98ZTgRgMSaWgCmZ8OD74mFUQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEsz0OeL7jrVxEXJu+o4QWQYLKyokXHiPOOKVUL3/TNFFquVzDSer7kZ3gijxzBp98ZTgRgMSaWgCmZ8OD74mFUQ==",
           "url": "https://oak.ct.letsencrypt.org/2023/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2021-03-01T19:24:00Z"
             }
           },
@@ -1290,100 +1290,100 @@
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Testflume2019' log",
           "log_id": "hJ9ff1jSv3tU7L10YRzqRcScmPHWSBvG9p6MF08k888=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAg3+vFOesFW51rKECekioAt9Zo50atRoOJ0qLxF7DIEHsHneXLEpgO1WMreleRy1vEbUJD7TXoH9r8qSDGvyew==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAg3+vFOesFW51rKECekioAt9Zo50atRoOJ0qLxF7DIEHsHneXLEpgO1WMreleRy1vEbUJD7TXoH9r8qSDGvyew==",
           "url": "https://testflume.ct.letsencrypt.org/2019/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2019-01-01T00:00:00Z",
             "end_exclusive": "2020-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Testflume2020' log",
           "log_id": "xj8iGMN9VqaqBrWW2o5T1NcVbR6brI5E0iAt5k1p2dw=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjdjcoKpeBShHgHvRm3BxD5+l+eHZudv3KmD5SDcLcI01Vj5TDTmxanQKCgpvm9pfnfB6URMQV3hhU1I02jRoRw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjdjcoKpeBShHgHvRm3BxD5+l+eHZudv3KmD5SDcLcI01Vj5TDTmxanQKCgpvm9pfnfB6URMQV3hhU1I02jRoRw==",
           "url": "https://testflume.ct.letsencrypt.org/2020/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Testflume2021' log",
           "log_id": "A+3x2pd2tvOMNB457Z1wenVwNpz5hE8yf+nhQTg2G2A=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEdCLoJNt1QcNa7sNDp7g7oTJ+o/UIYEM6N/IZWT+dhdqtJZC+AODJ/4exdOwG04B4K6WrN1VB2ELKQIc/wU1lCw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEdCLoJNt1QcNa7sNDp7g7oTJ+o/UIYEM6N/IZWT+dhdqtJZC+AODJ/4exdOwG04B4K6WrN1VB2ELKQIc/wU1lCw==",
           "url": "https://testflume.ct.letsencrypt.org/2021/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Testflume2022' log",
           "log_id": "Iyfv2jUlENvAGe9JGuP/HMWkebzjeHg2DuMYz/tk+Mg=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjy/rXcABuf0yhrm1+XgjDnh4XPD7vfMoyJOyT+KA+c2zuXVR98yQmp/Bl5ZFdGFwJuFcVrCw7IDo0EGKs7UCww==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjy/rXcABuf0yhrm1+XgjDnh4XPD7vfMoyJOyT+KA+c2zuXVR98yQmp/Bl5ZFdGFwJuFcVrCw7IDo0EGKs7UCww==",
           "url": "https://testflume.ct.letsencrypt.org/2022/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Testflume2023' log",
           "log_id": "VTS3q1pqw6fL66ZUh7Ki1xtI9lD6F8UZfJegyyB288Y=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8aLpnumqeISmQEB3hKPgtPJQG3jP2IftfaUQ4WPUihNBwUOEk1R9BMg5RGQwebWSsRlGIRiCvtE97Q45Vh3mqA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE8aLpnumqeISmQEB3hKPgtPJQG3jP2IftfaUQ4WPUihNBwUOEk1R9BMg5RGQwebWSsRlGIRiCvtE97Q45Vh3mqA==",
           "url": "https://testflume.ct.letsencrypt.org/2023/",
           "mmd": 86400,
           "temporal_interval": {
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-07T00:00:00Z"
           }
         },
         {
           "description": "Let's Encrypt 'Clicky' log",
           "log_id": "KWr6LVaLyg0uqESVaulyH8Nfo1Xs2plpOq/UWKca790=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEHxoVg3cAdWK5n/YGBe2ViYNBgZfn4NQz/na6O8lJws3xz/4ScNe+qCJfsqRnAntxrh2sqOnRCNXO7zN6w18A3A==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEHxoVg3cAdWK5n/YGBe2ViYNBgZfn4NQz/na6O8lJws3xz/4ScNe+qCJfsqRnAntxrh2sqOnRCNXO7zN6w18A3A==",
           "url": "https://clicky.ct.letsencrypt.org/",
           "mmd": 86400
         }
       ]
     },
     {
       "name": "Up In The Air Consulting",
       "email": [
         "filippo@cloudflare.com"
       ],
       "logs": [
         {
           "description": "Up In The Air 'Behind the Sofa' log",
           "log_id": "sLeEvIHA3cR1ROiD8FmFu5B30TTYq4iysuUzmAuOUIs=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEWTmyppTGMrn+Y2keMDujW9WwQ8lQHpWlLadMSkmOi4+3+MziW5dy1eo/sSFI6ERrf+rvIv/f9F87bXcEsa+Qjw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEWTmyppTGMrn+Y2keMDujW9WwQ8lQHpWlLadMSkmOi4+3+MziW5dy1eo/sSFI6ERrf+rvIv/f9F87bXcEsa+Qjw==",
           "url": "https://ct.filippo.io/behindthesofa/",
           "mmd": 86400
         }
       ]
     },
     {
       "name": "Qihoo 360",
       "email": [
         "yanshousheng@360.cn"
       ],
       "logs": [
         {
           "description": "Qihoo 360 2020",
           "log_id": "R0RHfHXeQm1cRO/UqSyWd1l/ZXqP4MrbxtYW7aSXxCU=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE+nuP0XjE36o+HfjS9IKnDItCnp3Tn9NafZpJ/8GHvLm+Z/AXY7FuN3L4LPXRrlTydBLt60OqzktHoJ4t4fNodg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE+nuP0XjE36o+HfjS9IKnDItCnp3Tn9NafZpJ/8GHvLm+Z/AXY7FuN3L4LPXRrlTydBLt60OqzktHoJ4t4fNodg==",
           "url": "https://ct.browser.360.cn/2020/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-04-30T00:00:00Z"
             }
           },
@@ -1391,15 +1391,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "Qihoo 360 2021",
           "log_id": "xtftntuOdPCnG01KmEvL66u9KMwf12Mp6IcmzUwlRmM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEYFtlqKC59Iy+N1sWaUktnSNpbhrrL4VMiaqdDrQu/3hOnxOJiXFlkSYQjBoHDCVA5iC9pu8+1DsrmNsv5FIMDw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEYFtlqKC59Iy+N1sWaUktnSNpbhrrL4VMiaqdDrQu/3hOnxOJiXFlkSYQjBoHDCVA5iC9pu8+1DsrmNsv5FIMDw==",
           "url": "https://ct.browser.360.cn/2021/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-04-30T00:00:00Z"
             }
           },
@@ -1407,15 +1407,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Qihoo 360 2022",
           "log_id": "ZjywnB/Nm6pidjzLU07sgFgSKAUHrGmkX804z0zHTPE=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEQ4UlFLE3vxDHtsqO5s6kNURSyaLK9bHV5+zDXTQzwiUJdTFKxE85L5uvYvobuyZpySo7GXYyopL4Exsrm4HJMA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEQ4UlFLE3vxDHtsqO5s6kNURSyaLK9bHV5+zDXTQzwiUJdTFKxE85L5uvYvobuyZpySo7GXYyopL4Exsrm4HJMA==",
           "url": "https://ct.browser.360.cn/2022/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-04-30T00:00:00Z"
             }
           },
@@ -1423,15 +1423,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "Qihoo 360 2023",
           "log_id": "4mR/bto0BQPGTU4QqGloH96cWizzsy1fIAuWNgWQiCM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEBQLIGtQH3merk2OMO2LbKU2TFroKxaChr4+FxB5UMLYTspqEJKcD7mRXEObHJ6PI/39UvEI98N4Cyf9x2rKvqg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEBQLIGtQH3merk2OMO2LbKU2TFroKxaChr4+FxB5UMLYTspqEJKcD7mRXEObHJ6PI/39UvEI98N4Cyf9x2rKvqg==",
           "url": "https://ct.browser.360.cn/2023/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-04-30T00:00:00Z"
             }
           },
@@ -1439,15 +1439,15 @@
             "start_inclusive": "2023-01-01T00:00:00Z",
             "end_exclusive": "2024-01-01T00:00:00Z"
           }
         },
         {
           "description": "Qihoo 360 v1 2020",
           "log_id": "xc/lS2FRtJsULtJjvecykzY3mXmVUK5ENc0aaZfJw8M=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEDkQZn1WqxqWyWHG86i14qbZ+yk/17Q8SWeubOJw6jQdNJRh4zJwD3mGTZIQge3T6QOgzChWUJA3wYztSx+M/gg==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEDkQZn1WqxqWyWHG86i14qbZ+yk/17Q8SWeubOJw6jQdNJRh4zJwD3mGTZIQge3T6QOgzChWUJA3wYztSx+M/gg==",
           "url": "https://ct.browser.360.cn/v1/2020/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-10-06T00:00:00Z"
             }
           },
@@ -1455,15 +1455,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "Qihoo 360 v1 2021",
           "log_id": "SBRYfPKLCP5oP9K82UWZTC63TIroyH/OQpt80x1RvcQ=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEB1OT52y/+wSUqaHfJh6/BvTD27mlZr6qZddkis7McckOaXx5cU3I/fBx90p3j1tWqEArVuiKmHHAvu82l9Uz2w==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEB1OT52y/+wSUqaHfJh6/BvTD27mlZr6qZddkis7McckOaXx5cU3I/fBx90p3j1tWqEArVuiKmHHAvu82l9Uz2w==",
           "url": "https://ct.browser.360.cn/v1/2021/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-10-06T00:00:00Z"
             }
           },
@@ -1471,15 +1471,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Qihoo 360 v1 2022",
           "log_id": "SRG41hTP09mfFtN2VF7huMz8UR9QnwgLoKCH2R367qk=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAvqcteHHGEZCE5kSvEpQIPD4fXsEFHYt1/EViSMv7QDtBHTpK94IZSScZLOWk5fpvGIB/6ibVzR5MCW/NSMd/A==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEAvqcteHHGEZCE5kSvEpQIPD4fXsEFHYt1/EViSMv7QDtBHTpK94IZSScZLOWk5fpvGIB/6ibVzR5MCW/NSMd/A==",
           "url": "https://ct.browser.360.cn/v1/2022/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-10-06T00:00:00Z"
             }
           },
@@ -1487,15 +1487,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "Qihoo 360 v1 2023",
           "log_id": "tnQLEgAuAz/Q5+lB9Lo+4b/BSbUktM9ijVPv6h9AOo0=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEpvIPBkpPfcg4boasja6srvBflgDAznTytKjNWPX151jP/EaRZmNOFBavgBI2oxlQabgYG11YHdwH+FHnyVTaNw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEpvIPBkpPfcg4boasja6srvBflgDAznTytKjNWPX151jP/EaRZmNOFBavgBI2oxlQabgYG11YHdwH+FHnyVTaNw==",
           "url": "https://ct.browser.360.cn/v1/2023/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-10-06T00:00:00Z"
             }
           },
@@ -1511,15 +1511,15 @@
       "email": [
         "trustasia-ct-logs@trustasia.com"
       ],
       "logs": [
         {
           "description": "Trust Asia Log1",
           "log_id": "RTWUmNk6ieAoAwjTfWJtxCN1R1jc4DcANvurDt+Ka88=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEeUeY0SDoUdASOMgFwOFUSkaZ8NoKnknjtbakmBDUJJhPVn9L/adnExZBXl46ZBT701qUixfqd/s3BpuMyWWUdw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEeUeY0SDoUdASOMgFwOFUSkaZ8NoKnknjtbakmBDUJJhPVn9L/adnExZBXl46ZBT701qUixfqd/s3BpuMyWWUdw==",
           "url": "https://ct.trustasia.com/log1/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2020-04-28T00:00:00Z"
             }
           },
@@ -1527,15 +1527,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2025-01-01T00:00:00Z"
           }
         },
         {
           "description": "Trust Asia Log2020",
           "log_id": "pZWUO1NwvukG4AUNH7W7xqQOZfJlroUsdjY/rbIzNu0=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEbsWC7ukn2WYOMxTAcqL8gMRZEQTZF9+Ho1MB9WLhHIaCHpHsJSx0DjJdVILW9mtM5xZtWywMWMQ9/R3OBgQEXQ==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEbsWC7ukn2WYOMxTAcqL8gMRZEQTZF9+Ho1MB9WLhHIaCHpHsJSx0DjJdVILW9mtM5xZtWywMWMQ9/R3OBgQEXQ==",
           "url": "https://ct.trustasia.com/log2020/",
           "mmd": 86400,
           "state": {
             "rejected": {
               "timestamp": "2021-04-21T00:00:00Z"
             }
           },
@@ -1543,15 +1543,15 @@
             "start_inclusive": "2020-01-01T00:00:00Z",
             "end_exclusive": "2021-01-01T00:00:00Z"
           }
         },
         {
           "description": "Trust Asia CT2021",
           "log_id": "qNxS9j1rJCXlMeN89ORKcU8UKiCAOw0E0uLuBmR5SiM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESdAfC+h1SZNsSARs188n/dCiNYjGSgkT7avLYe1mmXJzzHhsmxmAorHtOzhDkFgaCSCrUPrXdunK946eyIeSmA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAESdAfC+h1SZNsSARs188n/dCiNYjGSgkT7avLYe1mmXJzzHhsmxmAorHtOzhDkFgaCSCrUPrXdunK946eyIeSmA==",
           "url": "https://ct2021.trustasia.com/log2021/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2021-10-16T01:46:32Z"
             }
           },
@@ -1559,15 +1559,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Trust Asia Log2021",
           "log_id": "Z422Wz50Q7bzo3DV4TqxtDvgoNNR98p0IlDHxvpRqIo=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjwlzYzssDEG4DpPoOS73Ewsdohc0MzaohzRmUz9dih7Z8SHyyviKmnQL1KKfY6VGFnt0ulbVupzGXSaYUAoupA==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEjwlzYzssDEG4DpPoOS73Ewsdohc0MzaohzRmUz9dih7Z8SHyyviKmnQL1KKfY6VGFnt0ulbVupzGXSaYUAoupA==",
           "url": "https://ct.trustasia.com/log2021/",
           "mmd": 86400,
           "state": {
             "retired": {
               "timestamp": "2020-12-01T00:00:00Z"
             }
           },
@@ -1575,15 +1575,15 @@
             "start_inclusive": "2021-01-01T00:00:00Z",
             "end_exclusive": "2022-01-01T00:00:00Z"
           }
         },
         {
           "description": "Trust Asia Log2022",
           "log_id": "w2X5s2VPMoPHnamOk9dBj1ure+MlLJjh0vBLuetCfSM=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEu1LyFs+SC8555lRtwjdTpPX5OqmzBewdvRbsMKwu+HliNRWOGtgWLuRIa/bGE/GWLlwQ/hkeqBi4Dy3DpIZRlw==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEu1LyFs+SC8555lRtwjdTpPX5OqmzBewdvRbsMKwu+HliNRWOGtgWLuRIa/bGE/GWLlwQ/hkeqBi4Dy3DpIZRlw==",
           "url": "https://ct.trustasia.com/log2022/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2021-03-01T19:24:00Z"
             }
           },
@@ -1591,15 +1591,15 @@
             "start_inclusive": "2022-01-01T00:00:00Z",
             "end_exclusive": "2023-01-01T00:00:00Z"
           }
         },
         {
           "description": "Trust Asia Log2023",
           "log_id": "6H6nZgvCbPYALvVyXT/g4zG5OTu5L79Y6zuQSdr1Q1o=",
-          "key": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEpBFS2xdBTpDUVlESMFL4mwPPTJ/4Lji18Vq6+ji50o8agdqVzDPsIShmxlY+YDYhINnUrF36XBmhBX3+ICP89Q==",
+          "keys": "MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAEpBFS2xdBTpDUVlESMFL4mwPPTJ/4Lji18Vq6+ji50o8agdqVzDPsIShmxlY+YDYhINnUrF36XBmhBX3+ICP89Q==",
           "url": "https://ct.trustasia.com/log2023/",
           "mmd": 86400,
           "state": {
             "usable": {
               "timestamp": "2021-03-01T19:24:00Z"
             }
           },
```

## Comparing `axonius_api_client/cert_human/constants.py` & `axonius_api_client/projects/cert_human/constants.py`

 * *Files identical despite different names*

## Comparing `axonius_api_client/cert_human/convert.py` & `axonius_api_client/projects/cert_human/convert.py`

 * *Files 2% similar despite different names*

```diff
@@ -187,29 +187,29 @@
 
 
 def der_to_pem_cert(value: bytes, headers: Optional[dict] = None) -> bytes:
     """Convert from DER to PEM format.
 
     Args:
         value (bytes): The DER cert to convert to PEM
-        cert_type (str, optional): the text to use in the BEGIN and END barriers
-        headers (Optional[dict], optional): header lines to write after the BEGIN barrier
+        cert_type (str, optional): the text to use in the `BEGIN` and `END` barriers
+        headers (Optional[dict], optional): header lines to write after the `BEGIN` barrier
     """
     return der_to_pem(value=value, cert_type=CertTypes.cert.value, headers=headers)
 
 
 def der_to_pem(
     value: bytes, cert_type: str, headers: Optional[dict] = None, as_str: bool = False
 ) -> Union[str, bytes]:
     """Convert from DER to PEM format.
 
     Args:
         value (bytes): The DER cert to convert to PEM
-        cert_type (str, optional): the text to use in the BEGIN and END barriers
-        headers (Optional[dict], optional): header lines to write after the BEGIN barrier
+        cert_type (str, optional): the text to use in the `BEGIN` and `END` barriers
+        headers (Optional[dict], optional): header lines to write after the `BEGIN` barrier
     """
     check_type(value=value, exp=bytes)
     ret = asn1crypto.pem.armor(type_name=cert_type, der_bytes=value, headers=headers)
     if as_str:
         ret = bytes_to_str(value=ret)
     return ret
```

## Comparing `axonius_api_client/cert_human/ct_logs.py` & `axonius_api_client/projects/cert_human/ct_logs.py`

 * *Files 1% similar despite different names*

```diff
@@ -14,15 +14,15 @@
 LOG: logging.Logger = logging.getLogger(__name__)
 CACHE = cachetools.LRUCache(maxsize=10)
 
 
 @cachetools.cached(cache=CACHE)
 def load_ct_logs(**kwargs) -> dict:
     """Pass."""
-    kwargs["path"] = pathify(path=CT_LOGS.get(key="path", kwargs=kwargs))
+    kwargs["value"] = pathify(path=CT_LOGS.get(key="path", kwargs=kwargs))
 
     refetch = calc_refetch(**kwargs)
     data = None
 
     if refetch:
         kwargs["data"] = data = refetch_data(**kwargs)
         write_data(**kwargs)
```

## Comparing `axonius_api_client/cert_human/enums.py` & `axonius_api_client/projects/cert_human/enums.py`

 * *Files identical despite different names*

## Comparing `axonius_api_client/cert_human/exceptions.py` & `axonius_api_client/projects/cert_human/exceptions.py`

 * *Files identical despite different names*

## Comparing `axonius_api_client/cert_human/paths.py` & `axonius_api_client/projects/cert_human/paths.py`

 * *Files 5% similar despite different names*

```diff
@@ -17,15 +17,15 @@
 ) -> Tuple[pathlib.Path, bytes]:
     """Read data from a file as bytes.
 
     Args:
         path (PathLike): str or pathlib.Path object to file to read data from
 
     Returns:
-        Tuple[pathlib.Path, bytes]: resolved path from value, data from path as bytes
+        Tuple[pathlib.Path, bytes]: resolved value from value, data from value as bytes
     """
     kwargs["as_file"] = True
     path = pathify(path=path, exts=exts, **kwargs)
     data = path.read_bytes()
     return path, data
 
 
@@ -34,15 +34,15 @@
 ) -> Tuple[pathlib.Path, List[pathlib.Path]]:
     """Pass."""
     kwargs["as_dir"] = error
     path = pathify(path=path, **kwargs)
     files = [x for x in path.glob("*") if x.suffix in exts] if path.is_dir() else []
 
     if not files and error:
-        raise PathNotFoundError(f"Supplied path {str(path)!r} has no files with extensions {exts}")
+        raise PathNotFoundError(f"Supplied value {str(path)!r} has no files with extensions {exts}")
 
     return path, files
 
 
 def write_bytes(
     path: PathLike,
     content: Union[str, bytes],
@@ -139,15 +139,15 @@
     if expanduser:
         resolved = resolved.expanduser()
 
     if resolve:
         resolved = resolved.resolve()
 
     vstr = f"(supplied {type_str(path)})"
-    rstr = f"Resolved path {str(resolved)!r}"
+    rstr = f"Resolved value {str(resolved)!r}"
 
     if as_file and not resolved.is_file():
         raise PathNotFoundError(f"{rstr} does not exist as a file {vstr}")
 
     if as_dir and not resolved.is_dir():
         raise PathNotFoundError(f"{rstr} does not exist as a directory {vstr}")
 
@@ -163,39 +163,39 @@
     make_parent: bool = True,
     make_file: bool = True,
     perm_file: Optional[Union[int, str, bytes]] = "0600",
     perm_parent: Optional[Union[int, str, bytes]] = "0700",
     error: bool = True,
     **kwargs,
 ) -> pathlib.Path:
-    """Create a file at a path.
+    """Create a file at a value.
 
     Args:
-        path (PathLike): str of path or pathlib.Path object to file to create
+        path (PathLike): str of value or pathlib.Path object to file to create
         overwrite (bool, optional): error if the file exists already
         make_parent (bool, optional): make the parent directory of value if it does not exist
         make_file (bool, optional): create an empty file if it does not exist
         perm_file (Optional[Union[int, str, bytes]], optional): permissions to set on file
             when creating
         perm_parent (Optional[Union[int, str, bytes]], optional): permissions to set on
             parent directory when creating
-        error (bool, optional): raise errors, otherwise just return the resolved path
+        error (bool, optional): raise errors, otherwise just return the resolved value
 
     Raises:
         PathError: if value exists as file and overwrite is False
         PathNotFoundError: if parent directory of value does not exist and make_parent is False
 
     Returns:
-        pathlib.Path: the resolved path of value
+        pathlib.Path: the resolved value of value
     """
     resolved = pathify(path=path, **kwargs)
     perm_file_int, perm_file_oct = octify(value=perm_file, allow_empty=True, error=error)
     perm_parent_int, perm_parent_oct = octify(value=perm_parent, allow_empty=True, error=error)
 
-    rstr = f"Resolved path {str(resolved)!r}"
+    rstr = f"Resolved value {str(resolved)!r}"
     pstr = f"Parent directory {str(resolved.parent)!r} of {rstr}"
 
     try:
         if resolved.is_file() and overwrite is False:
             raise PathError(f"{rstr} already exists as a file and overwrite is False")
 
         if not resolved.parent.is_dir():
@@ -257,15 +257,15 @@
     def exists(self) -> bool:
         """Pass."""
         return self.path.is_file()
 
     def __str__(self) -> str:
         """Pass."""
         info = [
-            f"path={str(self.path)!r}",
+            f"value={str(self.path)!r}",
             f"exists={self.exists}",
             f"modified_dt={str(self.modified_dt)!r}",
             f"modified_days={self.modified_days}",
         ]
         info = ", ".join(info)
         return f"{self.__class__.__name__}({info})"
```

## Comparing `axonius_api_client/cert_human/ssl_capture.py` & `axonius_api_client/projects/cert_human/ssl_capture.py`

 * *Files 0% similar despite different names*

```diff
@@ -1,19 +1,17 @@
 # -*- coding: utf-8 -*-
 """Base example for setting up the API client."""
+import sys
 import types
 import typing as t
-import sys
 from typing import List, Optional
 
 import OpenSSL
-
 from urllib3 import connectionpool, poolmanager
 
-
 INJECT_WITH_PYOPENSSL: bool = True
 
 if sys.version_info >= (3, 10, 1):
     # get_verified_chain is mo' betta
     INJECT_WITH_PYOPENSSL: bool = False
```

## Comparing `axonius_api_client/cert_human/ssl_context.py` & `axonius_api_client/projects/cert_human/ssl_context.py`

 * *Files identical despite different names*

## Comparing `axonius_api_client/cert_human/ssl_extensions.py` & `axonius_api_client/projects/cert_human/ssl_extensions.py`

 * *Files 0% similar despite different names*

```diff
@@ -53,15 +53,15 @@
         extn_id = ext.native["extn_id"]
         dotted = ext.children[0].dotted
         for subcls in get_subcls(SSLExtension):
             if subcls.EXTN_ID == extn_id or subcls.DOTTED == dotted:
                 return subcls(ext=ext, **kwargs)
 
         LOG.getChild(cls.__name__).debug(f"Unmapped extension ID {extn_id!r} dotted {dotted!r}")
-        return cls(ext=ext, **kwargs)
+        return cls(ext=ext)
 
     @classmethod
     def load_list(cls, exts: List[asn1crypto.x509.Extension], **kwargs) -> List["SSLExtension"]:
         """Pass."""
         check_type(value=exts, exp=list)
         return [cls.load(ext=x, **kwargs) for x in exts]
 
@@ -271,14 +271,15 @@
             return []
 
     @property
     def value_for_str(self) -> Any:
         """Pass."""
 
         def handle(item):
+            """Pass."""
             operator = item.pop("log_operator")
             ret = {f"operator_{k}": operator.get(k) for k in op_keys}
             ret.update(item)
             return ret
 
         value = self.value
         op_keys = ["name", "description", "url"]
@@ -361,19 +362,20 @@
         lookup = self.log_key_id_base64
         data = load_ct_logs()
         operators: List[dict] = data["operators"]
         for operator in operators:
             logs: List[dict] = operator["logs"]
             for log in logs:
                 if lookup == log["log_id"]:
-                    ret = {}
-                    ret["log_id"] = b64_to_hex(value=log["log_id"])
-                    ret["key"] = b64_to_hex(value=log["key"])
-                    ret["name"] = operator["name"]
-                    ret["email"] = operator["email"]
+                    ret = {
+                        "log_id": b64_to_hex(value=log["log_id"]),
+                        "key": b64_to_hex(value=log["key"]),
+                        "name": operator["name"],
+                        "email": operator["email"],
+                    }
                     ret.update(log)
                     ret = {k: v for k, v in sorted(ret.items())}
                     return ret
 
         return {
             "description": f"Unable to find operator for bas64 Log Key ID: {lookup!r}",
             "log_id": self.log_key_id,
```

## Comparing `axonius_api_client/cert_human/utils.py` & `axonius_api_client/projects/cert_human/utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,14 +5,25 @@
 from typing import Any, List, Optional, Type, Union
 
 LOG: logging.Logger = logging.getLogger(__name__)
 HUMAN_KEY_ALIGN: int = 32
 HUMAN_CAP_WORDS: List[str] = ["id", "url", "crl", "sha256", "sha1", "ssl"]
 
 
+def parse_url(url: str) -> str:
+    """Pass."""
+    try:
+        from url_parser import UrlParser
+
+        url_parsed = UrlParser(url=url, default_scheme="https")
+        return url_parsed.url
+    except ImportError:
+        return url
+
+
 def type_str(value: Any, max_len: int = 60, join: Optional[str] = ", ") -> Union[str, List[str]]:
     """Pass."""
     length = len(str(value))
     svalue = f"{str(value)[:max_len]}...snip..." if length >= max_len else f"{value}"
     items = [
         f"type={type(value)}",
         f"length={length}",
```

## Comparing `axonius_api_client/cert_human/stores/cert.py` & `axonius_api_client/projects/cert_human/stores/cert.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,31 +2,32 @@
 """Make SSL certs and their attributes generally more accessible."""
 import datetime
 import logging
 from typing import Any, Dict, List, Union
 
 import asn1crypto.keys
 import asn1crypto.x509
+import asn1crypto.core
+
 import OpenSSL
 import requests
 
-from ...parsers.url_parser import UrlParser
 from ..convert import (
     asn1_to_der,
     der_to_asn1_cert,
     get_der_cert_count,
     get_pem_cert_count,
     get_pkcs7_cert_count,
     pkcs7_to_asn1_cert,
     x509_to_der,
 )
 from ..enums import CertTypes
 from ..ssl_capture import inject_into_urllib3
 from ..ssl_context import get_cert, get_chain
-from ..utils import bytes_to_hex, str_strip_to_int
+from ..utils import bytes_to_hex, str_strip_to_int, parse_url
 from .store import Store
 
 LOG: logging.Logger = logging.getLogger(__name__)
 
 
 class Cert(Store):
     """Make SSL certs and their attributes generally more accessible."""
@@ -137,15 +138,15 @@
             *cls.FILE_EXTS_PEM_DER,
             *cls.FILE_EXTS_PKCS7,
         ]
         return list(set(file_exts))
 
     # ABC
     @classmethod
-    def from_content(cls, value: Union[str, bytes], source: Any = None) -> List["Cert"]:
+    def from_content(cls, value: Union[str, bytes], source: Any = None) -> List["Store"]:
         """Pass."""
         if source is not None and not isinstance(source, dict):
             source = {"source": source}
 
         source = source or {}
 
         if get_pem_cert_count(value=value) >= 1:
@@ -164,15 +165,15 @@
         raise ValueError("\n".join(msgs))
 
     # ABC
     def to_dict(self, with_extensions: bool = True) -> Dict[str, dict]:
         """Convert this object into a dictionary.
 
         Returns:
-            Dict[str, dict]: keys as sections, values as proprties for the section
+            Dict[str, dict]: keys as sections, values as properties for the section
         """
         ret = {
             "public_key": self.section_public_key,
             "fingerprints": self.section_fingerprints,
             "issuer": self.section_issuer,
             "subject": self.section_subject,
             "details": self.section_details,
@@ -217,15 +218,15 @@
         """Check if this cert is expired."""
         return datetime.datetime.now(datetime.timezone.utc) > self.ASN1.not_valid_after
 
     # CERT ONLY
     @property
     def is_self_signed_bool(self) -> bool:
         """Pass."""
-        return self.is_self_signed not in ["no"]
+        return self.is_self_signed_str not in ["no"]
 
     # CERT ONLY
     @property
     def is_self_signed_str(self) -> str:
         """Pass."""
         return self.ASN1.self_signed
 
@@ -259,30 +260,28 @@
         """Get the properties for the issuer section."""
         return dict(self.ASN1.issuer.native)
 
     # CERT ONLY
     @classmethod
     def from_requests_cert(cls, url: str, **kwargs) -> "Cert":
         """Pass."""
-        url_parsed = UrlParser(url=url, default_scheme="https")
-        url = url_parsed.url
+        url = parse_url(url)
         cls.inject_results = inject_into_urllib3()
         source = {"url": url, "method": f"{cls.__module__}.{cls.__name__}.from_requests_cert"}
         kwargs.setdefault("verify", False)
         response: requests.Response = requests.get(url, **kwargs)
         cert: OpenSSL.crypto.X509 = response.raw.captured_cert
         cls._get_log().debug(f"Loaded 1 certificate from {source}")
         return cls(cert=cert, source=source)
 
     # CERT ONLY
     @classmethod
     def from_requests_chain(cls, url: str, **kwargs) -> List["Cert"]:
         """Pass."""
-        url_parsed = UrlParser(url=url, default_scheme="https")
-        url = url_parsed.url
+        url = parse_url(url)
         cls.inject_results = inject_into_urllib3()
         source = {"url": url, "method": f"{cls.__module__}.{cls.__name__}.from_requests_chain"}
         kwargs.setdefault("verify", False)
         response: requests.Response = requests.get(url, **kwargs)
         certs: List[OpenSSL.crypto.X509] = response.raw.captured_chain
         cls._get_log().debug(f"Loaded {len(certs)} certificates from {source}")
         return [cls(cert=x, index=idx, source=source) for idx, x in enumerate(certs)]
```

## Comparing `axonius_api_client/cert_human/stores/cert_request.py` & `axonius_api_client/projects/cert_human/stores/cert_request.py`

 * *Files 1% similar despite different names*

```diff
@@ -89,15 +89,15 @@
     def get_file_exts(cls) -> List[str]:
         """Pass."""
         file_exts = [*cls.FILE_EXTS_PEM, *cls.FILE_EXTS_DER, *cls.FILE_EXTS_PEM_DER]
         return list(set(file_exts))
 
     # ABC
     @classmethod
-    def from_content(cls, value: Union[str, bytes], source: Any = None) -> List["CertRequest"]:
+    def from_content(cls, value: Union[str, bytes], source: Any = None) -> List["Store"]:
         """Pass."""
         if source is not None and not isinstance(source, dict):
             source = {"source": source}
 
         source = source or {}
 
         if get_pem_csr_count(value=value) >= 1:
@@ -112,15 +112,15 @@
         raise ValueError("\n".join(msgs))
 
     # ABC
     def to_dict(self, with_extensions: bool = True) -> Dict[str, dict]:
         """Convert this object into a dictionary.
 
         Returns:
-            Dict[str, dict]: keys as sections, values as proprties for the section
+            Dict[str, dict]: keys as sections, values as properties for the section
         """
         ret = {
             "public_key": self.section_public_key,
             "fingerprints": self.section_fingerprints,
             "subject": self.section_subject,
             "details": self.section_details,
             "source": self.section_source,
```

## Comparing `axonius_api_client/cert_human/stores/store.py` & `axonius_api_client/projects/cert_human/stores/store.py`

 * *Files 1% similar despite different names*

```diff
@@ -3,14 +3,16 @@
 import abc
 import datetime
 import logging
 from typing import Any, Dict, List, Optional, Tuple, Union
 
 import asn1crypto.algos
 import asn1crypto.keys
+import asn1crypto.x509
+import asn1crypto.core
 
 from ..convert import asn1_to_der, der_to_pem, pem_to_bytes_types
 from ..enums import ChainTypes
 from ..exceptions import InvalidCertError
 from ..paths import PathLike, find_file_exts, pathlib, read_bytes, write_bytes
 from ..ssl_extensions import SSLExtension
 from ..utils import bytes_to_hex, check_type, human_dict, int_to_hex, str_to_bytes
@@ -57,15 +59,15 @@
         raise NotImplementedError()
 
     @abc.abstractmethod
     def to_dict(self, with_extensions: bool = True) -> Dict[str, dict]:
         """Convert this object into a dictionary.
 
         Returns:
-            Dict[str, dict]: keys as sections, values as proprties for the section
+            Dict[str, dict]: keys as sections, values as properties for the section
         """
         raise NotImplementedError()
 
     @property
     @abc.abstractmethod
     def sans(self) -> List[str]:
         """Get the subject alternative names for this cert."""
@@ -236,15 +238,15 @@
     def is_valid_host(
         self, host: str, and_cn: bool = True, error: bool = True
     ) -> Tuple[bool, List[str]]:
         """Check if a given domain or IP is valid for this cert.
 
         Args:
             host (str): domain or IP to check
-            error (bool, optional): raise an exception if its not valid
+            error (bool, optional): raise an exception if it is not valid
 
         Returns:
             bool: if the supplied host is valid for this cert
 
         Raises:
             ValueError: If the supplied host is not valid and error is True
         """
@@ -282,15 +284,15 @@
         )
         return [cls(**x) for x in certs]
 
     @classmethod
     def from_file(cls, path: PathLike) -> List["Store"]:
         """Pass."""
         path, data = read_bytes(path=path)
-        source = {"path": path}
+        source = {"value": path}
         return cls.from_content(value=data, source=source)
 
     @classmethod
     def from_directory(cls, path: PathLike) -> Dict[str, List["Store"]]:
         """Pass."""
         path, files = find_file_exts(path=path, exts=cls.get_file_exts(), error=True)
         return {x.name: cls.from_file(path=x) for x in files}
@@ -398,15 +400,15 @@
     @classmethod
     def _short_trans(cls, obj: dict) -> dict:
         """Pass."""
         return {v: obj[k] for k, v in cls.SHORT_TRANS.items() if k in obj}
 
     @classmethod
     def _to_str_short(cls, obj: dict) -> Union[str, List[str]]:
-        """Get a short one liner string of subject or issuer."""
+        """Get a short one-liner string of subject or issuer."""
         items = [f"{k}={v}" for k, v in cls._short_trans(obj=obj).items()]
         return cls.SHORT_JOIN.join(items) if isinstance(cls.SHORT_JOIN, str) else items
 
     SHORT_TRANS: dict = {
         "country_name": "C",
         "state_or_province_name": "ST",
         "locality_name": "L",
```

## Comparing `axonius_api_client/parsers/url_parser.py` & `axonius_api_client/projects/url_parser.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,20 +1,20 @@
 # -*- coding: utf-8 -*-
 """HTTP client."""
-from typing import Union
+import typing as t
 from urllib.parse import urlparse, urlunparse
 
-from ..exceptions import HttpError
-
 
 class UrlParser:
-    """Parse a URL and ensure it has the neccessary bits."""
+    """Parse a URL and ensure it has the necessary bits."""
 
-    def __init__(self, url: str, default_scheme: str = "https"):
-        """Parse a URL and ensure it has the neccessary bits.
+    def __init__(
+        self, url: str, default_scheme: str = "https", exc: t.Type[Exception] = ValueError
+    ):
+        """Parse a URL and ensure it has the necessary bits.
 
         Args:
             url: URL to parse
             default_scheme: scheme to use if url does not contain a scheme
 
         Raises:
             :exc:`HttpError`: if parsed URL winds up without a hostname, port, or scheme.
@@ -26,34 +26,35 @@
         """default scheme provided"""
 
         self.INIT_PARSED = urlparse(url)
         """:obj:`urllib.parse.ParseResult`: first pass of parsing URL"""
 
         self.parsed = self.reparse(parsed=self.INIT_PARSED, default_scheme=default_scheme)
         """:obj:`urllib.parse.ParseResult`:second pass of parsing URL"""
+        self.exc = exc
 
         for part in ["hostname", "port", "scheme"]:
             if not getattr(self.parsed, part, None):
                 error = (
                     f"Parsed URL into {self.parsed_str!r} and no {part!r} provided"
                     f" in URL {url!r}"
                 )
-                raise HttpError(error)
+                raise exc(error)
 
     def __str__(self) -> str:
         """Show object info."""
         cls = self.__class__
         return f"{cls.__name__}({self.parsed_str})"
 
     def __repr__(self) -> str:
         """Show object info."""
         return self.__str__()
 
     @property
-    def hostname(self) -> str:
+    def hostname(self) -> t.Optional[t.Union[str, bytes]]:
         """Hostname part from :attr:`UrlParser.parsed`."""
         return self.parsed.hostname
 
     @property
     def port(self) -> int:
         """Port part from :attr:`UrlParser.parsed`."""
         return int(self.parsed.port)
@@ -71,36 +72,38 @@
     @property
     def url_full(self) -> str:
         """Get full URL from :attr:`UrlParser.parsed`."""
         return self.unparse_all(parsed_result=self.parsed)
 
     @property
     def parsed_str(self) -> str:
-        """Get a str value of :attr:`UrlParser.parsed`."""
+        """Get a str token of :attr:`UrlParser.parsed`."""
         parsed = getattr(self, "parsed", None)
         attrs = [
             "scheme",
             "netloc",
             "hostname",
-            "port",
             "path",
+            "port",
+            "token",
             "params",
             "query",
             "fragment",
         ]
-        atmpl = "{a}={v!r}".format
-        attrs = [atmpl(a=a, v="{}".format(getattr(parsed, a, "")) or "") for a in attrs]
+        template = "{a}={v!r}".format
+        attrs = [template(a=a, v="{}".format(getattr(parsed, a, "")) or "") for a in attrs]
         return ", ".join(attrs)
 
-    def make_netloc(self, host: str, port: Union[str, int]) -> str:
+    @staticmethod
+    def make_netloc(host: str, port: t.Union[str, int]) -> str:
         """Create netloc from host and port.
 
         Args:
-            hosthost: hostname to use in netloc
-            portport: port to use in netloc
+            host: hostname to use in netloc
+            port: port to use in netloc
         """
         return ":".join([str(x) for x in [host, port] if x])
 
     def reparse(self, parsed, default_scheme: str = ""):
         """Reparse a parsed URL into a parsed URL with values fixed.
 
         Args:
@@ -114,32 +117,32 @@
         host = parsed.hostname
         port = format(parsed.port or "")
 
         if not netloc and scheme and path and path.split("/")[0].isdigit():
             """For case:
             >>> urllib.parse.urlparse('host:443/')
             ParseResult(
-                scheme='host', netloc='', path='443/', params='', query='', fragment=''
+                scheme='host', netloc='', token='443/', params='', query='', fragment=''
             )
             """
             host = scheme  # switch host from scheme to host
-            port = path.split("/")[0]  # remove / from path and assign to port
-            path = ""  # empty out path
+            port = path.split("/")[0]  # remove / from token and assign to port
+            path = ""  # empty out token
             scheme = default_scheme
             netloc = ":".join([host, port])
 
         if not netloc and path:
             """For cases:
             >>> urllib.parse.urlparse('host:443')
             ParseResult(
-                scheme='', netloc='', path='host:443', params='', query='', fragment=''
+                scheme='', netloc='', token='host:443', params='', query='', fragment=''
             )
             >>> urllib.parse.urlparse('host')
             ParseResult(
-                scheme='', netloc='', path='host', params='', query='', fragment=''
+                scheme='', netloc='', token='host', params='', query='', fragment=''
             )
             """
             netloc, path = path, netloc
             if ":" in netloc:  # pragma: no cover
                 # can't get this to trigger anymore, ignore test coverage
                 host, port = netloc.split(":", 1)
                 netloc = ":".join([host, port]) if port else host
@@ -158,24 +161,26 @@
         return urlparse(pass2)
 
     @property
     def port_scheme_map(self) -> dict:
         """Get the schemes to use based on port."""
         return {"443": "https", "80": "http"}
 
-    def unparse_base(self, parsed_result) -> str:
+    @staticmethod
+    def unparse_base(parsed_result) -> str:
         """Unparse a parsed URL into just the scheme, hostname, and port parts.
 
         Args:
-            parsed (:obj:`urllib.parse.ParseResult`): parsed URL to unparse
+            parsed_result (:obj:`urllib.parse.ParseResult`): parsed URL to unparse
         """
         # only unparse self.parsed into url with scheme and netloc
         bits = (parsed_result.scheme, parsed_result.netloc, "", "", "", "")
         return urlunparse(bits)
 
-    def unparse_all(self, parsed_result) -> str:
+    @staticmethod
+    def unparse_all(parsed_result) -> str:
         """Unparse a parsed URL with all the parts.
 
         Args:
-            parsed (:obj:`urllib.parse.ParseResult`): parsed URL to unparse
+            parsed_result (:obj:`urllib.parse.ParseResult`): parsed URL to unparse
         """
         return urlunparse(parsed_result)
```

## Comparing `axonius_api_client-4.60.4.dist-info/LICENSE` & `axonius_api_client-5.0.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `axonius_api_client-4.60.4.dist-info/METADATA` & `axonius_api_client-5.0.0.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: axonius-api-client
-Version: 4.60.4
+Version: 5.0.0
 Summary: Axonius API client for Python
 Home-page: https://github.com/Axonius/axonius_api_client
 Author: Axonius
 Author-email: support@axonius.com
 License: MIT
 Keywords: Axonius,API Library
 Classifier: Development Status :: 5 - Production/Stable
```

## Comparing `axonius_api_client-4.60.4.dist-info/RECORD` & `axonius_api_client-5.0.0.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,136 +1,160 @@
-axonius_api_client/__init__.py,sha256=JznUZPPV524eovNpHPZQEBH7hP-dhdDhSrtcln0zT9c,2409
-axonius_api_client/connect.py,sha256=pC4kyqvIoxo4q6AjFkXN9g1RZOH2Zi0fQcZK4Ts-580,23367
+axonius_api_client/__init__.py,sha256=dEYcOBOzi19UogZ5J8lH8LZke1Tbslk71_hcGKvBfjc,3139
+axonius_api_client/connect.py,sha256=58Fm3bSmBBuzW8Ca07hqRtlWnAqcnXvexGNM9Nxk9sU,34483
 axonius_api_client/data.py,sha256=H1RleG9w98ks7z-GR69V4Q-lIhQa6h-A_IteDmXT5yg,4025
-axonius_api_client/exceptions.py,sha256=zCkU02Komdj4uusp13l7jxfhQkiHwksTqkN9mF_4X2w,15376
+axonius_api_client/exceptions.py,sha256=5H3st4FfZupKTHj-xkrUzWUiUoVzUq6q2iTuUBOMrQ0,15429
 axonius_api_client/features.py,sha256=QCpHgAkbfDxELQvE4CWiksR8RvGEPI51_Dm6xNl7DPQ,5153
-axonius_api_client/http.py,sha256=3HoLSmsU6F4TmUefxlkGQ9KCl-17PScwKsYqEHr0KmI,19884
+axonius_api_client/http.py,sha256=bF3RT3no7mqb7AAWnyG_ihAWfNL_9YdC_fvrmR0u7sw,31439
 axonius_api_client/logs.py,sha256=fy-5p_MI6OH47InlzIxh2e8DjEqV3P9V9qT3h50kt4A,12568
-axonius_api_client/setup_env.py,sha256=X3xvWBqbppq30ztBhdUFQQnPkHGPW-HElHtg6ddc4x8,9825
-axonius_api_client/tools.py,sha256=ZADxtUO_GmdTzFrbt_sZCyt_SY7PDEjfxQzkBItc6D0,69564
-axonius_api_client/utils.py,sha256=13h2gVajsN7MsOpdcMdqoVkhQ8lhAmFGbe3p4e6-lPk,20485
-axonius_api_client/version.py,sha256=JRapWQckmKSELSpXiIK3BFR1bEtqmhiv0CX0E9fem9o,1022
-axonius_api_client/api/__init__.py,sha256=dY9u2RBOcSC_TKT2jxCV8KsGwfNQib3JOAAJCGluxYM,1255
-axonius_api_client/api/api_endpoint.py,sha256=NwIqV4SShhPu9F3appIfUIl_XDve1dyr9dn8XmRGXG8,18430
-axonius_api_client/api/api_endpoints.py,sha256=SHRIpVdVapxKNomSAgdty8mg4F3ueZE2cJpJQ0iY-LQ,50479
-axonius_api_client/api/mixins.py,sha256=2WCH3ytc46S8kcyT8tXjIqmq9enHAG4wy3QGk-O-ne8,2304
+axonius_api_client/setup_env.py,sha256=eX9pkRR7wrlptopBgBKVCvM-OqPFaVJUTuXhlwcMLQ8,23869
+axonius_api_client/tools.py,sha256=vJHuTg4C5i2Kw3ZtiKlWOkLI3cnfvDfcHKWhgrf59cs,76664
+axonius_api_client/version.py,sha256=LNHtmKI8REyIQScbu4eo2uQ9IZcEvPscs3DC3PTR2VM,1021
+axonius_api_client/api/__init__.py,sha256=yRStW-3A0p-HZpvZf7HevljPmT5vmSCJv95wycx6i94,1368
+axonius_api_client/api/api_endpoint.py,sha256=T2ybS4lWAIHm_TpfbDuDF9MWMpAd44291sEbiHLdPYg,19821
+axonius_api_client/api/api_endpoints.py,sha256=heDb5XvCOWFoqx4lo-hBrVQhVMMTroTh0xNGB95acm0,54346
+axonius_api_client/api/mixins.py,sha256=Syvx-KCgdLBouth1XU_EetP2k3BDaGlkSrS9AHnyntQ,3284
 axonius_api_client/api/adapters/__init__.py,sha256=_cKJH5Ymo596vGRaQ5M-0CPZkvMbfUkixp1vRq1_lk8,161
-axonius_api_client/api/adapters/adapters.py,sha256=qIhg2pp_0xxYy-79VB-3_dZY6vbzeCW7p63TU2vUp68,25243
-axonius_api_client/api/adapters/cnx.py,sha256=MRsogbIih-Y9auszXQaHjbtgbXrYkUjpWpybRaT0j14,46603
-axonius_api_client/api/asset_callbacks/__init__.py,sha256=bSRsE9eNg3Je4gff5V0XOjbt4_3UXqiL94jk9-MLpOc,488
-axonius_api_client/api/asset_callbacks/base.py,sha256=wEGKzvYKR205Fzn4B8airXoyJ6CBj436tcpLqnARxH0,49862
-axonius_api_client/api/asset_callbacks/base_csv.py,sha256=GJMZDmmmGMNnKDBu0fQSpgPep5IRjF5ZDumJ4CTXwww,7510
+axonius_api_client/api/adapters/adapters.py,sha256=v0ypvrFXme-VL14FjzM_w2KXu0zSD3Oj6kNKRlFuJnM,25244
+axonius_api_client/api/adapters/cnx.py,sha256=DQMNRi2NnfDlltx6RnR2kxJ0RajyyZDm446Kv0QgjCQ,46857
+axonius_api_client/api/asset_callbacks/__init__.py,sha256=6UmXodgRdAK-sLICxiK63_Uykcvqz9BRwEFvqOIliys,522
+axonius_api_client/api/asset_callbacks/base.py,sha256=w5mQTE_EbJQfr5NyUS_oN8y0MR-aKFU7Z942d-p03Bw,50174
+axonius_api_client/api/asset_callbacks/base_csv.py,sha256=uW0lUTU1TUFsA9FQTdKn_i2oSUqThEoVqMnPxjYeAg0,7609
 axonius_api_client/api/asset_callbacks/base_json.py,sha256=FgafJchXzH927ctOVkfdRjgL-F4ybhfqd5pXXloiepc,4497
 axonius_api_client/api/asset_callbacks/base_json_to_csv.py,sha256=ZF5auSPnzucaKdWc0dapeagGO_wQu7ySu9nAkWMIBAM,5817
 axonius_api_client/api/asset_callbacks/base_table.py,sha256=cUCN8BwsPgVebkewfmR5GPJpiQ5GyNiFqssN-S3e06w,6435
-axonius_api_client/api/asset_callbacks/base_xlsx.py,sha256=9Jf0CZoNVSt_VFkNIRLQw_koQd1-7BbpXN1uXY_WUrs,6043
+axonius_api_client/api/asset_callbacks/base_xlsx.py,sha256=K3EsHCVWzQWvq-Fn7k5wHIkXLqIPGu8-IriqGwN3n3Y,6142
 axonius_api_client/api/asset_callbacks/base_xml.py,sha256=mae2O54adooXaH6kQkw5-hX0fzAlkUBFTGL-x_ohXdg,3823
-axonius_api_client/api/asset_callbacks/tools.py,sha256=Sd6TTVKkcnmgH8XB9AW8Ya7ogoMbFHLHYR-H842hG4c,744
+axonius_api_client/api/asset_callbacks/tools.py,sha256=W7ASR6sRfz7WMJHtjbM0J_mRJhfYAMCexOjYqt08fq4,757
 axonius_api_client/api/assets/__init__.py,sha256=VSPIAmGYp_WKk0s1HV9-QsuuySG5jYCJni3XXLYz31c,489
-axonius_api_client/api/assets/asset_mixin.py,sha256=Otc8E_ZvhDfyOB8Pnwkz42JgJt0Zrh2cJ_6itPBBIDA,69493
-axonius_api_client/api/assets/devices.py,sha256=SlqAOJQ8bVej92XGWPKutEvfY3z-YMmn3Kee5MrfIBI,7767
-axonius_api_client/api/assets/fields.py,sha256=4vUkDw5CoobpT2TUpbScqQy6dQhVy-SBWAIIMyVg-78,22191
+axonius_api_client/api/assets/asset_mixin.py,sha256=6ZBUaB_4e-1xuOHmD5Pruzr7FwZgz_jHYsGgwICe74s,86029
+axonius_api_client/api/assets/devices.py,sha256=IG_vNkRWmwAf2G33sv4vxBj9njAdgGxIDQsQz_BqUcU,7758
+axonius_api_client/api/assets/fields.py,sha256=3DPSYCl9zCX2wioHb97xAacgWyZ2vNrI60K4iq1GibE,23388
 axonius_api_client/api/assets/labels.py,sha256=ewrS6xpuhNqxk2qvlcRQRJjCY_wQMOZY-xgnEw2URrI,5207
-axonius_api_client/api/assets/runner.py,sha256=vgsam3zBu8gPr-XaBsT7FrZNE0sV5HQ_t3fKlsU2EG4,20162
-axonius_api_client/api/assets/saved_query.py,sha256=ekJpl80-wKp16sCjDjP_AbLcrxz0v4LYHTe41HYHOYs,48521
+axonius_api_client/api/assets/runner.py,sha256=m6zVoPN2cwmFc5xuZVBRI_VK_coT_iVmx18kMe9f7W8,20161
+axonius_api_client/api/assets/saved_query.py,sha256=Nb4Wg6hOrYkV16eFUggNZIE_LqDsu6uZrNPL60Gq_Fg,49441
 axonius_api_client/api/assets/users.py,sha256=djxkaBVYFtO8mVfwJkufnIwvR-or3s8udFuOACwzkVE,5226
 axonius_api_client/api/assets/vulnerabilities.py,sha256=RKW-moHTc46VxUheuh7KJ8lQl63R7TjfLldyx8n2ZJE,1726
 axonius_api_client/api/enforcements/__init__.py,sha256=FnwkQAwpCbh8NUvz2Yr7LLi3U924TVafDa1rYm7IcXE,179
-axonius_api_client/api/enforcements/enforcements.py,sha256=wagLIO3RkEtQojfjP-Qkkxo4lalqTeToocG1asVdHnI,41652
-axonius_api_client/api/enforcements/tasks.py,sha256=SU3oYbEvGhLHIg10bOZiPgu0mIngO0bINSRczZ8DxXc,4288
+axonius_api_client/api/enforcements/enforcements.py,sha256=UNAKfgvFg1bH3PPn09nIJpgZtynC01Ys21RMpKmatas,40371
+axonius_api_client/api/enforcements/tasks.py,sha256=ixnuhpymsSfxgg8KklDE6bDdjtpnS2pSJLQ3DfYf0xw,16252
 axonius_api_client/api/folders/__init__.py,sha256=CcdC-uaoI3OPGJQLm_P1coplReGD-oAOO-0tZ87UXQA,208
-axonius_api_client/api/folders/folders.py,sha256=0f-Otc4h3cjoAgDD2SnMmEf-OH4NFzmbv1_Eh7NOcSg,20842
-axonius_api_client/api/json_api/__init__.py,sha256=ly-g2AEtX-5xHaOikYtohtkapv75ABq1wSOmMZGxkuk,1137
-axonius_api_client/api/json_api/account.py,sha256=eJYtm-cEGbJYJdvCD393nAckRwiIUGn2gMl_K0De1LY,3644
-axonius_api_client/api/json_api/adapters.py,sha256=OmonWzctw5lEDbD0yrOg96G4YCD9mooJeFztMsP6drw,58615
-axonius_api_client/api/json_api/assets.py,sha256=HdhNA97w0RphjfoBq6pEGC1U-HaYdR49vdJttREQ2m8,22751
-axonius_api_client/api/json_api/audit_logs.py,sha256=uF2FeJG_oRpWr1LldhgaGl479RNfW21t9pFkXzDi_20,4791
-axonius_api_client/api/json_api/base.py,sha256=bJcVkNb6iY1iMTLL9MVsf6KlTuWhMV0aX4NyIuMUwCE,22598
-axonius_api_client/api/json_api/base2.py,sha256=mIzjnzYzoy7UsBGFVn-CNxiFJH5_Ur6MQ97z7GShzYY,6705
+axonius_api_client/api/folders/folders.py,sha256=NSCb7YKxszMrpoSo2H_eJ4WXrXD8UNlND0eZEmTuAFo,20856
+axonius_api_client/api/json_api/__init__.py,sha256=6mvmEjmjFQuQiqNJ2vJlWM50XFaEomYoYXQgs9sH_oE,1219
+axonius_api_client/api/json_api/base.py,sha256=jQ3J1lk-Wwmq02ThSthqmgYSmHfo5K5BaeF7kkKnv_U,24980
+axonius_api_client/api/json_api/base2.py,sha256=gbVDwtTexTpjSBmCWERi25cdH_ZWAyBeeXKB-pTn8YM,6649
 axonius_api_client/api/json_api/central_core.py,sha256=gyXJTZH5HOhlmnXgUjesaaRarAcBaI3bRlHgnIaL7fU,3498
-axonius_api_client/api/json_api/count_operator.py,sha256=fNAIjP63rdYCGVOkFAMFr_wb7h1GyJL5sPcho3Pt2U4,1375
-axonius_api_client/api/json_api/custom_fields.py,sha256=eb3TQjpNeWb2OyFzcwHFjFm7ewX-oD_nJ7EmV8QrCaI,6277
+axonius_api_client/api/json_api/count_operator.py,sha256=EsBkAxKNkLXQ7oNRXKSoy7ty029pICF09HMek-tntj0,2169
+axonius_api_client/api/json_api/custom_fields.py,sha256=X2liX1e3fknh5Ei4L8jjre0ZPYs_LcDN-U04_ZZs4Y4,6495
 axonius_api_client/api/json_api/dashboard_spaces.py,sha256=3E4_FQXq5nsuOLAnkaZPONBRVntMGX1M_-O0nDrwtSU,25113
-axonius_api_client/api/json_api/data_scopes.py,sha256=jVpohIrmxdgJ6GBJGnreMIFg92TWuhWK8BZSYy22XZk,9715
-axonius_api_client/api/json_api/duration_operator.py,sha256=SrUZOKR-OJauoukDHhUwU8SmwiRnxRSSklVAAHcQgX0,1717
-axonius_api_client/api/json_api/enforcements.py,sha256=Hug5kTj87puMUmkZNTzcVHUp2Gv4iOOdCv_xhcc_3S0,57753
-axonius_api_client/api/json_api/generic.py,sha256=Nb17jpULbXBC6JSAWGmOl7HN27sXuRnWsKy7FiqO1-c,6931
+axonius_api_client/api/json_api/data_scopes.py,sha256=_ya8dx5FFkVkoNOcUprdXC3rih0qk5xJFoPITKHvfsI,9714
+axonius_api_client/api/json_api/duration_operator.py,sha256=kOCco1Jvbu40DS6N6RxbyRXyBMs4aUzEo3Bk-WkF_po,1943
+axonius_api_client/api/json_api/enforcements.py,sha256=Cy3ggoOGnkv9d23Q8LsbeTiX0EpPMSaCrRzwQw4n7Y0,57755
+axonius_api_client/api/json_api/generic.py,sha256=hI24fFMdYUPWEWLFv1f4--PRmmjzEWkdIGD3hbPXDO4,6802
 axonius_api_client/api/json_api/instances.py,sha256=IG-GvT8COc495N5dUOvD_BsW61VpIwG0WjuN7G6chbU,11745
 axonius_api_client/api/json_api/lifecycle.py,sha256=cZ0AIyD0B3N6diQOOVfkKux0RFe1i1wMrCUKu-knCRM,1338
 axonius_api_client/api/json_api/nested_access.py,sha256=UfgIfyW0-AcMJQUlI7c6h30zqcyNBInB63V-Zd_n_Ps,2668
-axonius_api_client/api/json_api/paging_state.py,sha256=Qgz0AqqKBpQMOaeRoO3o0meCjbmuQ250J6TKc7HdlGk,7743
+axonius_api_client/api/json_api/paging_state.py,sha256=8nOaXTDD6fkpXJ7PySZavMXJ7jr_ZYxRHPHuRQYK93k,8156
 axonius_api_client/api/json_api/password_reset.py,sha256=5ZgXgTJMKM9JdxzzKSNZXrY3nCVHLeCS9hq7KBIo7_I,2056
 axonius_api_client/api/json_api/remote_support.py,sha256=sUiUuLquqMS7SPzsCNwkeXBySXtaetpMXYQVCXWEQZU,4807
-axonius_api_client/api/json_api/resources.py,sha256=BTpfWFPj84BMz3z_f4lTSIiVPnVRNojCdbS9p-fnAAw,4259
-axonius_api_client/api/json_api/saved_queries.py,sha256=-cM4N6QodB3LmSenL202icGIDUzDVH7j5fGWCsfRLP0,40498
+axonius_api_client/api/json_api/resources.py,sha256=4ylB32L_DwmrMolSnWVCwL8Ay2L5Ciqczkt1szf_ZAE,4929
+axonius_api_client/api/json_api/saved_queries.py,sha256=x3HJdV115zt8BZjZmXf4auTjRtuNEr6-FZgdUkJFahI,40502
 axonius_api_client/api/json_api/selection.py,sha256=A_eQM94kPzzXcMSIztnz1Kga3WcSdns_q6FH3_LWkK0,879
-axonius_api_client/api/json_api/signup.py,sha256=erwasIzAPhcZLt-MDPFis-A_RPINTXHazPJ4BJMlKWo,2729
 axonius_api_client/api/json_api/spaces_export.py,sha256=n0WBBg3RjsnbChdgVCRIDBOblkDNFnm5jpeM7nsSZ3Y,19576
 axonius_api_client/api/json_api/system_meta.py,sha256=ZWQD1B0gMz8AkjR9W6j9DsX1FqlN_UkQ2CchntMCPdQ,1281
-axonius_api_client/api/json_api/system_roles.py,sha256=W_QOlWveWw0UQ2AX40X2bJKO2RepYkUuJgn9W6R7JNk,9819
+axonius_api_client/api/json_api/system_roles.py,sha256=NM2z8b3m-aPLeTWmwtyv9JqiiJIS5mGMVeclQGK9UJY,9813
 axonius_api_client/api/json_api/system_settings.py,sha256=1GHW-INauD_6EvKdma0oBswaxJtdV1O4sPTpd5ZfYXg,7413
-axonius_api_client/api/json_api/system_users.py,sha256=c20Ehi6ub9KPbyNYDxssDlaHuzZGBIa-onB1tG_bDqM,7403
+axonius_api_client/api/json_api/system_users.py,sha256=MlavsFccPz_EiEQkmFnAAqv_uAnT92UEsRLzu0e90fo,7644
 axonius_api_client/api/json_api/time_range.py,sha256=v1iWiuM-DM1PKTDLp4CR2D9ttlZJRwCjXaJJ6sd6vWk,4290
+axonius_api_client/api/json_api/account/__init__.py,sha256=nEazEQ8QNRfNLVnIh8u1_yY3ExrV9sx4oA3TPTW7xBg,400
+axonius_api_client/api/json_api/account/current_user.py,sha256=5l-RYf1XPczHC7eVnEOGeV_Q2dLWAoiyqcS2hA1cSrE,4032
+axonius_api_client/api/json_api/account/login_request.py,sha256=fdaq7N-SJvcOhQyE1UiOh_9-qSTt31WebcPrzr9Q22U,2892
+axonius_api_client/api/json_api/account/login_response.py,sha256=fcqhzbEUnmSSYAcKQoZilKXSeAp4ozODDxVWtOpx-hs,3252
+axonius_api_client/api/json_api/adapters/__init__.py,sha256=yU6b_hueC3XyCC47GIvca7UQ8L_Lam50CB028a7ulyk,2356
+axonius_api_client/api/json_api/adapters/adapter_node.py,sha256=AwK-wSOpEKobA7RTggyNzaWGLGJqlGvObCS1YRXReSk,10010
+axonius_api_client/api/json_api/adapters/adapter_settings_response.py,sha256=qhLSWpK35-JV2H6MlO2FzUdMaJKeapHMSEgsS7EZi-A,4702
+axonius_api_client/api/json_api/adapters/adapter_settings_update_request.py,sha256=VudtMYYWPErChenyJEJAkkj2zdytovHPiteEOZpzAQQ,1335
+axonius_api_client/api/json_api/adapters/adapters_list_response.py,sha256=LMgx6HCAarrjN3tLGz_K9_tnLy8pT2yW2Z6kHihnp4s,2687
+axonius_api_client/api/json_api/adapters/adapters_request.py,sha256=WnrQ3Mayxm6eSTfcWTMQCcbnL4cJSjLdcJBk8BlDCkI,1192
+axonius_api_client/api/json_api/adapters/adapters_response.py,sha256=9VX6d4Nmk_wL7qhCbMJDaivp9LdnPJNzqw1Ru87OPg4,3229
+axonius_api_client/api/json_api/adapters/clients_count.py,sha256=fQRtAkw1QuflGswmgSe8kLtoUTD6T66dcx5QoIHqyN8,792
+axonius_api_client/api/json_api/adapters/cnx_create_request.py,sha256=VTUj5yqGFyi_QBoi6NxGyi9aCz2YxV38ZFFj9PHRoME,3633
+axonius_api_client/api/json_api/adapters/cnx_create_response.py,sha256=E7bwgbbKSnONFhsLU_lO7Gv51g5huFHpS5kSRku0jww,1994
+axonius_api_client/api/json_api/adapters/cnx_delete_request.py,sha256=SUENLgq2D04SjyIeAU2fyRrr5QhvvWXTL3BaiiERNXM,1338
+axonius_api_client/api/json_api/adapters/cnx_delete_response.py,sha256=YcxdphLpMkOOoEh81-Ob6_RoXDo0j96I2YlfAPfJkys,1264
+axonius_api_client/api/json_api/adapters/cnx_labels_response.py,sha256=3YQ-OPycnTp-uIEhkKahDOWKaeZqWGJSUiDMbFK8314,1598
+axonius_api_client/api/json_api/adapters/cnx_test_request.py,sha256=JSdyEtiNd8d7VovdXgTIAGQhoZkTbu37XjIYWZfDm0k,1464
+axonius_api_client/api/json_api/adapters/cnx_update_request.py,sha256=-rjDC0vnf4OMd-rgq6plxCk6OdbIUjRL62lbteRh-l0,3017
+axonius_api_client/api/json_api/adapters/cnx_update_response.py,sha256=y4r7ZLQKcnwBj2iTQUAgiD8QP4QS-g3zbyo6h7HF4oU,834
+axonius_api_client/api/json_api/adapters/cnxs_response.py,sha256=xZLWgtQL21YIQiPywl0hnQEC0oj5Dm01josohFyUGKE,5244
+axonius_api_client/api/json_api/adapters/fetch_history_filters_response.py,sha256=0C9nhfQafaRT63MKB6FWTRNeyU-4mSHfu7OygRxyiTs,6282
+axonius_api_client/api/json_api/adapters/fetch_history_request.py,sha256=MPpjqNki-qsCuSAg9edE0qgAchQJTBzVAmm15rb_fxg,6246
+axonius_api_client/api/json_api/adapters/fetch_history_response.py,sha256=TxNSke7dbeiVeSvn8mPWI___feHJ2PGUvcQ5tq1PkIM,8950
+axonius_api_client/api/json_api/assets/__init__.py,sha256=ujsXNp5M84304MRqFVq4jwm3UE932wHhSB7OeqjyITY,1581
+axonius_api_client/api/json_api/assets/asset_id_request.py,sha256=s79w5KLLbFI2icZhOB6hv0wKXJH9kDVRBx0H7LvpKag,1434
+axonius_api_client/api/json_api/assets/asset_id_response.py,sha256=DKD_znA9U2saQcJJgNUR9phJrNF4iHnDmD4bVKcI0cA,4138
+axonius_api_client/api/json_api/assets/asset_request.py,sha256=bCm3FDH6wKY4BEHoTKJFA4P35Wc_J0Koz5mb48wYPV0,14623
+axonius_api_client/api/json_api/assets/asset_response.py,sha256=YpgDE7ZQxSw-X5zRyVOZrbsfweN7oQzQOOTV_M2fMYo,11019
+axonius_api_client/api/json_api/assets/count_request.py,sha256=jHDvwjIZ_0XAHgtUZK4ZOUeqHtf93dyMCozPx1rp3oQ,3296
+axonius_api_client/api/json_api/assets/count_response.py,sha256=NAFN44F7ku_4zSqsB7I7ScXUX3Kw26RcrhCSy5ybkmU,1059
+axonius_api_client/api/json_api/assets/destroy_request.py,sha256=0U6AxghLJzhtRYmF5aOyqwPcka1wamWiZDTOpWrUv-I,1206
+axonius_api_client/api/json_api/assets/destroy_response.py,sha256=x48N7RMhBIVNG1ykJopRREN_gfPGcYtEB5R6hQFdQq8,745
+axonius_api_client/api/json_api/assets/fields_response.py,sha256=z-5eGo02uyER47lAkD_tW55PUBF9FM_HVYAAbDFw3QE,784
+axonius_api_client/api/json_api/assets/history_dates_human.py,sha256=ej9KczwLBmfBJtBNfT2yUHMTV2RUawaMm3FE4Z6H9Qs,7266
+axonius_api_client/api/json_api/assets/history_dates_response.py,sha256=ePYol8IL4CITyihJ5SVFuFJ6yI_x5aLY3jxGwJa1P_Q,1198
+axonius_api_client/api/json_api/assets/modify_tags_request.py,sha256=YObU50HXnSwCtkRnRNs_U-An9zGeZLh96tCDCIcjr_A,1506
+axonius_api_client/api/json_api/assets/modify_tags_response.py,sha256=yrDK5s7LwqE1mN-rD2VEk0tZT_9-NmmEWfOANS7ZElw,687
+axonius_api_client/api/json_api/assets/run_enforcement_request.py,sha256=SKxWDk5flRaiMc6DhkwkgGYOMt5xQDftW4g3nIHkjPw,1713
+axonius_api_client/api/json_api/audit_logs/__init__.py,sha256=mzMWelHhSsYcA91N3_HyGwtKPjQOdM9c_XWFpmVvJPQ,299
+axonius_api_client/api/json_api/audit_logs/audit_log_request.py,sha256=AC-8yL5PWr7SlJ1pSUwGl8C8FMte_DUB8Qq4yhgKVqk,1158
+axonius_api_client/api/json_api/audit_logs/audit_log_response.py,sha256=wTbRSLmHDZgnGiVrH1TM60v8GrkfrQ0EmNkFn_H_0U8,4440
 axonius_api_client/api/json_api/folders/__init__.py,sha256=KxfUBl-dWs__5gDtok5PJi0zPHTbzEgpGvbVn6Knc04,192
-axonius_api_client/api/json_api/folders/base.py,sha256=XirREcFK6gZmRVUrPy7pmBSEZnGXYQ0qHN8Poq3ob3w,67702
+axonius_api_client/api/json_api/folders/base.py,sha256=pGc0J77QiCaXxnoIoQX3Gl-uSJk950LAmP7yyuI9snI,67993
 axonius_api_client/api/json_api/folders/enforcements.py,sha256=ZV3py1phVRqZAAixRj6vbYqXZlQBpCr86cdfvrfV-Jk,7283
-axonius_api_client/api/json_api/folders/queries.py,sha256=HKPWn6eXozOeZkL2a6Bxf7btSl6-sDI2kFmCb7qeX70,9912
-axonius_api_client/api/json_api/tasks/__init__.py,sha256=20bLTRPJysmIoilJUMO5QdVe09r2hgeAfU1mbCdD1nU,700
-axonius_api_client/api/json_api/tasks/get_tasks.py,sha256=LHfhF8cHByzu5LFy7ZfMEUhuAg07P5rqkYQ7kTdE2YE,7104
-axonius_api_client/api/json_api/tasks/result.py,sha256=Ks2Ypg5k8sIJs7Z1T3VAMSCtqf7KUZHtGJI_ooAxQo4,6266
-axonius_api_client/api/json_api/tasks/task.py,sha256=p6olVLSFlUTnz34GpHgyBLSxOu7Tj3xUdnD0MX5wXNI,20487
-axonius_api_client/api/json_api/tasks/task_basic.py,sha256=hkEe_oizV1jcNthBFkQZQw5eXczBpkY1AXQ5qBTRAbk,8098
-axonius_api_client/api/json_api/tasks/task_filters.py,sha256=UleOa0Ot0ovU3RnEbqbUrd4Uqabc-ewmPLgi95MXHKY,3590
-axonius_api_client/api/json_api/tasks/task_full.py,sha256=UTsfK2okgB-VGzHneDcbqx9pI4u_IernhyLd5rpkmEc,3904
+axonius_api_client/api/json_api/folders/queries.py,sha256=jT5079eqCnZ_2QCZNN1FDkCZT2tBtHXClgwxXK6c_Zo,10747
+axonius_api_client/api/json_api/signup/__init__.py,sha256=MNn76zwxyPtiqgUTTIPxnB2d_xFz5pV3PsXkyE8yhg8,415
+axonius_api_client/api/json_api/signup/signup_request.py,sha256=76gzWJMPo6eRMAKbzHkPCv9DlS-8W8c_R7FMYAY8iBQ,1915
+axonius_api_client/api/json_api/signup/signup_response.py,sha256=M8B2mNznl4uJaEkT7njW8MkecKSxpHL5Tqrv2j7gH90,1301
+axonius_api_client/api/json_api/signup/system_status.py,sha256=44jJrnTST36a54HVX_exd8Sn0SU9sArv5JBQQUZCC_E,1405
+axonius_api_client/api/json_api/tasks/__init__.py,sha256=DounRVjL8xKYGA1h4DJtUKB_k24tN0ELnhLL8zqJPco,701
+axonius_api_client/api/json_api/tasks/get_tasks.py,sha256=aa1brneR4OAh1yvjtzmTUvkDGZl4KTu8NlbHgH3y1DQ,13475
+axonius_api_client/api/json_api/tasks/result.py,sha256=SunM3KvGxH5PCo5D3ay3k3nsStEQSaJWG01SgvSf1vo,6170
+axonius_api_client/api/json_api/tasks/task.py,sha256=d-ACh6TPUmqkOMnBoBU4CMg6cxzp_utaGKa4GMbr5qk,21942
+axonius_api_client/api/json_api/tasks/task_basic.py,sha256=gqJfuxE96wrZs3raMrlwRBHCJNoTgVo71U77bHx1WN4,7988
+axonius_api_client/api/json_api/tasks/task_filters.py,sha256=7U2UbZ5eztKRA5wBgEmp9BDfMBz8WSIYHJr-6fahiAE,12876
+axonius_api_client/api/json_api/tasks/task_full.py,sha256=-R-W2MvZAxXLQ00r-ZcdmcCJgYpC7d2mt5Jy09WZ7C8,3683
 axonius_api_client/api/openapi/__init__.py,sha256=qzxmdKOvONZCLmxfkUg3kbENQzGNBCL-5DrzIjdtUqs,145
 axonius_api_client/api/openapi/openapi_spec.py,sha256=9FGYW8StCFtb7wwEWjKNmP27JK6IeECsZHvknAAfpWU,639
 axonius_api_client/api/system/__init__.py,sha256=DaaScZs3pIUOQXHD-j2RLaJFH-2EqLjcVwJyuaByC2s,817
 axonius_api_client/api/system/activity_logs.py,sha256=Wo1zGiPmK3YDS-CLxVaJT-Ap3U3xTcnkIYXe65iEf0I,3987
-axonius_api_client/api/system/dashboard.py,sha256=ONAPElEVjXKck9MC1Uf_LfWv_PAQ4VIlDoJgzxPxdEg,11592
+axonius_api_client/api/system/dashboard.py,sha256=MxlL33bSNf7brQz5aMRUdAHh58vNCsaGbgTth4k73-k,11606
 axonius_api_client/api/system/dashboard_spaces.py,sha256=EnJThN8DFCprMg70d-Qq5ewAujbxe0B9BRm_Ocx975I,14794
 axonius_api_client/api/system/data_scopes.py,sha256=cR89OI8h2hwd8A6toRaCerIHLEVPHYLCR-QOBQepbZs,12622
-axonius_api_client/api/system/instances.py,sha256=384TXM3Ike4cq98ZmarlotpRj6Ft4g8Nbivdbh1TMAU,28156
-axonius_api_client/api/system/meta.py,sha256=j7RmNg6VgvP_O9otQDC2fkVc3Yet5Cdw-3qI1NlBOck,3718
+axonius_api_client/api/system/instances.py,sha256=sHv7aAcUfCx4wLmj0rUGGISapczrypfv7NejLuC9BNk,28173
+axonius_api_client/api/system/meta.py,sha256=WzA5b9XnurQ-X6MSqvWXOvFfCsvw5BHAzbrS9WuwH-Q,3778
 axonius_api_client/api/system/remote_support.py,sha256=grXFFzo61wQzoV3v7Cw6Z4sihKlVzcNy1OVA1ugvxMY,3966
-axonius_api_client/api/system/settings.py,sha256=u05gwHz2EMz_do5UfvB0oi692qYRY47YEQBMCgEd_yE,21545
-axonius_api_client/api/system/signup.py,sha256=RYAYC_nfX3IgvZm72zW_-avlTyNjuBWgReTOFDTBW0o,5292
+axonius_api_client/api/system/settings.py,sha256=Fw38DPdAyaUKsMkanT2Gwetk1s29fikMmHohPwvMXjA,21566
+axonius_api_client/api/system/signup.py,sha256=5wiPledEMNrnVecF4OaKUfg5I3bCwCDqaQwDl1Juu7c,6590
 axonius_api_client/api/system/system_roles.py,sha256=DOmM1m6Sbtzu68px2qbWDett8615l_lYwq14fDTiCio,19278
-axonius_api_client/api/system/system_users.py,sha256=xkn5QwCMfoF6wHmekzZsOc-VRngRLRwAurN1Lgt6aCA,19330
+axonius_api_client/api/system/system_users.py,sha256=U-eQYjB_CJsVnKmxVHZquBNByjS3114y7hAQ6paRCZ0,19313
 axonius_api_client/api/wizards/__init__.py,sha256=pj-lzzzKi6Xvq7txit3WsKvFGbSSty_98hzSHqhlO58,236
-axonius_api_client/api/wizards/wizard.py,sha256=jrRVy4IFaZXC3oZIKCapwHcQJtmxQ81cBFGOhwyq0Y0,22152
-axonius_api_client/api/wizards/wizard_csv.py,sha256=Yf_kBcSOcLU6LfcGi6E1irUMwPZ623ILjIjX5lnhOUk,12642
-axonius_api_client/api/wizards/wizard_text.py,sha256=Xb0_MFgyu3HZMYySEEUdrquuP5-DrQ5ZDbsuBhIFRpg,5023
-axonius_api_client/auth/__init__.py,sha256=HKuC68ymKN_Y4Dqcsm_4VZOO7SgWvOLzoorCmFp2k_M,324
-axonius_api_client/auth/api_key.py,sha256=yPmAxaI8Jqvca_F9s7YF3wvSxE5zjTuOEnvKxsJvzd0,1504
-axonius_api_client/auth/credentials.py,sha256=VG9TL468kunvFWcogDkrE5OzmoRh0UiEjIduZ79PGB0,2559
-axonius_api_client/auth/models.py,sha256=uUR64pcjCXGGBvgEZTJ55K0uMjnCgnzNLZ0gGuJRcY4,4271
-axonius_api_client/cert_human/__init__.py,sha256=-DVmyZREGcvexhfTTdMzhdIJI0c7ZK2sD7FrTGhtqV4,505
-axonius_api_client/cert_human/all_logs_list.json,sha256=AYn29uCyHSrKVkydJgOhXd0YL1EoP3ng-L6G0M6tMvg,88574
-axonius_api_client/cert_human/all_logs_list.py,sha256=ARHKoD_do2byr_W9DSpNrQqiKztxqA3rGTQrfmV4Rqg,62859
-axonius_api_client/cert_human/constants.py,sha256=dOLqQzN6LjqH41TedtuYF3k4O5GujBAjjnpHQPTZW28,604
-axonius_api_client/cert_human/convert.py,sha256=-96BvZZwvH9euS6PXVY0mDjPZaz8-GxeP7xzJ8NbMTE,7401
-axonius_api_client/cert_human/ct_logs.py,sha256=dydauOeOY8RNHWR7i_JiLi3POzhJnInGEEOFXDL1X40,3702
-axonius_api_client/cert_human/enums.py,sha256=ZESNErm--13fy7UA_46zM97aN1AjfD_uJQjbBfqj9s0,1374
-axonius_api_client/cert_human/exceptions.py,sha256=x6Fqyu-lNV7-EESZte34qFWL9e2OrBdT7tAOuQ-vnRs,618
-axonius_api_client/cert_human/paths.py,sha256=6TFcHHQHe5kuojZxhJBZhE93H7hxWkS6VWtqS54Sd14,9071
-axonius_api_client/cert_human/ssl_capture.py,sha256=0g_iNtNnDSp7xhO_U3fqC1lkrT-5emrRctCF6irTwRk,9048
-axonius_api_client/cert_human/ssl_context.py,sha256=_7RhaiEJLhpW22ltffUEcexA2iamA5qUEER3bTVlDcw,1656
-axonius_api_client/cert_human/ssl_extensions.py,sha256=ZkvaWRtbngea-l5p7fyA4pdTApFckANF9fNHq5xbUhE,13297
-axonius_api_client/cert_human/utils.py,sha256=bZB4nznb7mU2CKNSHiNRD_TOtJ_pvh2d2B01JmTEJlg,4879
-axonius_api_client/cert_human/stores/__init__.py,sha256=rSgzbvyrSpEEMwx0gAaLh8KfbAIo8UU-CS6Y4qqwOw4,307
-axonius_api_client/cert_human/stores/cert.py,sha256=hgeHYj5Ke5SVTk9o-74vilTGMQJSlzblNxrn6MQjBMk,10738
-axonius_api_client/cert_human/stores/cert_request.py,sha256=_-C1K-FWfpRzKXDoQOLVD8zMli_ED-llHXzDlI1triU,5793
-axonius_api_client/cert_human/stores/store.py,sha256=cnn3_wGvl8pDwibvNMyFNXwi0M3zIc6RwW0t90k1kPw,13661
-axonius_api_client/cli/__init__.py,sha256=AyMdoTfV55FTTYyAQfQ7lU0L7WDR4cjtOk2i6x6e0fk,10726
-axonius_api_client/cli/context.py,sha256=yvgGg3QSyDRBoIrdvqt8jJ6bURkuDJjsE23GJn8L4HY,11259
-axonius_api_client/cli/helps.py,sha256=jtH1zd1ouMi3iOYzNo88vVohS0SPTFgomDUaTjBKt2M,6649
-axonius_api_client/cli/options.py,sha256=uVtoZKGx4asWjXCQSfoDwEVlaaWzsSvRptTK52Oynsk,9240
-axonius_api_client/cli/grp_account/__init__.py,sha256=A2sXOPolZg9IH89GPItknUghoPH6cS_XmsrKFIXuhys,492
+axonius_api_client/api/wizards/wizard.py,sha256=acXbd7QrzVF-uVOms5pLh1sdZVaQCOg8fBI1x5gqoDA,22147
+axonius_api_client/api/wizards/wizard_csv.py,sha256=JJEt0kKnKtITqjhKDDIcTxZxXeYVdOZIYehAwn_L4TY,12635
+axonius_api_client/api/wizards/wizard_text.py,sha256=g-Y9Lbkh7eaXFrRmMZAFf0Z5v3jhQnaXAW9jMRnNax4,4997
+axonius_api_client/auth/__init__.py,sha256=BO6GxSdFTgDY6hqxV2FhDzMXNDCdJYuukpqSHN9PuiE,388
+axonius_api_client/auth/api_key.py,sha256=FcTNMw9U0Mv2_NXqwiZza1VmZ3cGcY_fuf0eeMNH4fM,1505
+axonius_api_client/auth/credentials.py,sha256=He-feRA5PwHef7zIKCflcrdZzNbJ8JpfHJIOibo9ols,2476
+axonius_api_client/auth/model.py,sha256=HrMTN7u4la9iqDsT8jvuvA1g7A-fuanErNG0mxAx_MU,3804
+axonius_api_client/auth/null.py,sha256=9Zhl8hozWXoTdZ3IyqTTi2MTDhMkM-Jt4o8E4lthAWI,778
+axonius_api_client/cli/__init__.py,sha256=5xnWDsjInjX2NcXDs8NhHZIob_EetvXS__tWGJB57C0,14940
+axonius_api_client/cli/context.py,sha256=WCyuL4cINB5rX4M47Ueq9t_2duY0KiqMxu7dnkAAc8g,13167
+axonius_api_client/cli/helps.py,sha256=z1lIG9AD1ES_S2N-y4LClncCIeDRRv0LdOFv2zSUk7w,6753
+axonius_api_client/cli/options.py,sha256=LldaHrGT6CD-EecyZTaIZaQkMauyLFMyBl0jtiZcy7U,9376
+axonius_api_client/cli/grp_account/__init__.py,sha256=PMICLJXZE2gprz-AtVG04SxZ17LK16NFH_SDed3_Vxc,546
 axonius_api_client/cli/grp_account/cmd_get_api_keys.py,sha256=V0kDbigxGzO1YARaDeTisNVlskDjmYYbg3CWhmDVl3I,791
-axonius_api_client/cli/grp_adapters/__init__.py,sha256=RMBAHmtJWTd0TsoT2esK3iCCPt7H8LffOIrD4RSpdxc,367
+axonius_api_client/cli/grp_adapters/__init__.py,sha256=pYECAxuawTW1LmnXfZdV4O6y15Xhkrjslh7rBSxOxQg,506
 axonius_api_client/cli/grp_adapters/cmd_config_get.py,sha256=ntVY8QiBR8fkCqt-JV6wMbpqpOMB34KrzGwtf4dOpPs,801
 axonius_api_client/cli/grp_adapters/cmd_config_update.py,sha256=kg4u8Uz7H8-3scn-PSUXjRII0f2_jmQCo93CAG1cT3I,983
 axonius_api_client/cli/grp_adapters/cmd_config_update_from_json.py,sha256=GTqNb0t2b59UwBVOkqjHcfLzFb2XojRleKkshoeLVJc,1052
 axonius_api_client/cli/grp_adapters/cmd_file_upload.py,sha256=ZYx1XAjFe4GT1S5bu6qJOLHRauSD7PvZTxgTiJFm6mc,1204
 axonius_api_client/cli/grp_adapters/cmd_get.py,sha256=DRdgf4_7dCToh5kqWwsl0N0kcl9B9UPeJ8HvR99f7FQ,2098
 axonius_api_client/cli/grp_adapters/cmd_get_fetch_history.py,sha256=UmzTVmuQR4Ra-MM4AhepbqqSAfQtFbSDNj89MeHLjww,5240
 axonius_api_client/cli/grp_adapters/cmd_get_fetch_history_filters.py,sha256=95VMMkg5c1z4LohPGf-usGCPA8ZdREXx_bMre4Fz6bU,1040
@@ -145,27 +169,27 @@
 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_set_active.py,sha256=zAwOTl5_nBqX_c3Zy0HDgvTf6SU5THK6xEXNxLiTTJQ,1903
 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_set_label.py,sha256=m67TiUNeLccB6xzPBzSO8zP6jahBo7gb-X8mbwn5y0w,1849
 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_test.py,sha256=2fUHCxPz1PI9bdNZOkDYjsZvH-egSB-FV93Q875NdJg,2435
 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_test_by_id.py,sha256=YgoxtHlA-bXcVe8hx4JsG4GrG_L-oqnRRMPFSZ9hzDo,1453
 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_update_by_id.py,sha256=vjYcjrn3gaGp_uMLtdNiUHeawN9Q-b4uVSkOTlJCyFs,3224
 axonius_api_client/cli/grp_adapters/grp_cnx/cmd_update_by_id_from_json.py,sha256=-I0p3y39TWM2001vDtGdpXtjzXdkoMmHze7Dx-Pq_DQ,714
 axonius_api_client/cli/grp_adapters/grp_cnx/grp_common.py,sha256=O43gdYO7lqAdaZIU3Nib9hgqR9QjCCQiYAXnhVSJQdQ,6523
-axonius_api_client/cli/grp_adapters/grp_cnx/parsing.py,sha256=bNA75vxfq_aXuU_iJPFrmMhXXYF95iVG8B-LFFYOv3Q,33887
+axonius_api_client/cli/grp_adapters/grp_cnx/parsing.py,sha256=m12TRUUBViU_SiBpjhi1GcGaS7Bw_RKbLIHwuuFWgEU,33886
 axonius_api_client/cli/grp_assets/__init__.py,sha256=2SNweie04d8CqbnHTwYKo06jzrEmNnVTd3R80TaVEO4,3643
-axonius_api_client/cli/grp_assets/cmd_count.py,sha256=g-klAc24w-DbnflN2ftMgy7eHgAKcoHvbiRku877BJQ,1042
-axonius_api_client/cli/grp_assets/cmd_count_by_saved_query.py,sha256=9aeA6Z4vrOJqNnEfltRI_MoVumMVb9bp813v_yMff48,844
+axonius_api_client/cli/grp_assets/cmd_count.py,sha256=X4MW2xNH7edy_Wlh9crXYphCpkUyOYLv9ifyf0F3t20,1158
+axonius_api_client/cli/grp_assets/cmd_count_by_saved_query.py,sha256=Tboy7EOnMb4P6QnXfbz5J8YeQ2K5aMbOOGAaA1a658A,960
 axonius_api_client/cli/grp_assets/cmd_destroy.py,sha256=A7fFexpTLLdAcKynHSCv-4ZRehJiMaEizS-mUDLQe5M,1263
-axonius_api_client/cli/grp_assets/cmd_get.py,sha256=3Muuo9vtZ7sJUPO07y7gKO4o4ZQPwM7TWmRPIzXYWPY,1328
+axonius_api_client/cli/grp_assets/cmd_get.py,sha256=LjsOUzeMkIMvVrNP7rmu4I2tQr2TFAgQuYidOx35enI,1358
 axonius_api_client/cli/grp_assets/cmd_get_by_id.py,sha256=gFqN4IaKN9g02gzE_bP_icUliHVs0g0D0AzUHp-EduQ,891
-axonius_api_client/cli/grp_assets/cmd_get_by_saved_query.py,sha256=tfx4mSxxy9dT6Hu_B9K6uQsfapLZ_IKKv1g9yDZaHPM,1034
+axonius_api_client/cli/grp_assets/cmd_get_by_saved_query.py,sha256=27J_RF9KBpjYAiRQ6UsXHtGVGidaBRm8wZV15_m2jEA,1087
 axonius_api_client/cli/grp_assets/cmd_get_fields.py,sha256=KzySkGysx9skdOTOdPTzlWK8WqizYBvTg7YsstUbHB8,5959
 axonius_api_client/cli/grp_assets/cmd_get_fields_default.py,sha256=GXicoYBsfynE_XbDeuxHkkETDM_a0K7YKtx8qBtIFj8,627
 axonius_api_client/cli/grp_assets/cmd_get_tags.py,sha256=hcX0aBNgysslrCuRLmR-V5rvNz3K2GZxH8-6Q9i_HM0,741
 axonius_api_client/cli/grp_assets/cmds_run_enforcement.py,sha256=pVOyMsi-unFn7diBosmD4-Npbnt0rHw0_RvVvwDxKl8,8652
-axonius_api_client/cli/grp_assets/grp_common.py,sha256=bA0tlY1DNNKlD_txUxbETBO9nEYB4twts2OSAx2L12U,14981
+axonius_api_client/cli/grp_assets/grp_common.py,sha256=rzURd5eL97mKaqkAcuJqYejF4_WUS5NVPTcx-gMqQTw,20821
 axonius_api_client/cli/grp_assets/grp_saved_query/__init__.py,sha256=PKvRfzACtvVsFxyLlvfDm1UNR7_C1neiJssLsUiHX5g,300
 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_add.py,sha256=7e4SFxii3FguhyybJ8eJX71Iq_h2BMiiQ4pXJqk7GF8,3943
 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_add_from_json.py,sha256=Uht9fGE8OSPchJBeMnBspPeUp3Thg4DliTTd_nhAUYc,6458
 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_add_from_wiz_csv.py,sha256=zof4g7eh5Q8p7oY9h3u95PK4fNzN-ek9QLDJLLUC1Wg,1544
 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_copy.py,sha256=zNEHofxqBVLf-6FYP9qzDl1HUn8vr4UgYpx_9xM7Dsg,2246
 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_delete_by_name.py,sha256=_JDsDnwMtmUhz7j1kuqFoFjALf_NyzEcj8XTJgMFkeE,1010
 axonius_api_client/cli/grp_assets/grp_saved_query/cmd_delete_by_tags.py,sha256=PREHtrtqREe9dev6jCZ9esS7TtOhQhMhw2UF5JxSaCk,1288
@@ -194,18 +218,18 @@
 axonius_api_client/cli/grp_certs/cmd_csr_get.py,sha256=k6BbEEM6fKJS9I5sWpeQHsIxYeTVZWWvmSETCv7gx2g,764
 axonius_api_client/cli/grp_certs/cmd_from_path.py,sha256=sBRP2eRP4dEx3p_9WfoL2GLCB_0JXP3WX4nhTkA_HUg,900
 axonius_api_client/cli/grp_certs/cmd_from_url.py,sha256=sXnbLplvqSYLzl7MhFN0g58cI41bOT9fDCdLE4t9iMc,808
 axonius_api_client/cli/grp_certs/cmd_gui_get.py,sha256=rIGjMQ6_yiLOaZAVmAWzkxZdgb2lTT8qyfqj3fDO7DM,2473
 axonius_api_client/cli/grp_certs/cmd_gui_info.py,sha256=1iYSaPfL0QqgohiieYsk1fnZ1MLGF_5DOowNUlzmSTs,627
 axonius_api_client/cli/grp_certs/cmd_gui_reset.py,sha256=bQxufN-9MXzuKCVEakSfWdYUMkMAmk647Ws8qx3nyF8,1431
 axonius_api_client/cli/grp_certs/cmd_gui_update.py,sha256=Vk3DkwSgDCXd6ZwP9h5U0-d-PMb1JW6S5Au055SL2xs,2840
-axonius_api_client/cli/grp_certs/grp_common.py,sha256=2fJxqx3uEsBJj7HSCtLXOveIxviuU9ox-CS8jJw_AYM,9392
-axonius_api_client/cli/grp_enforcements/__init__.py,sha256=l-qj7A9Hssu4ChgOeOpipY2vQ7hfFblhb8UW1WEKPBU,309
+axonius_api_client/cli/grp_certs/grp_common.py,sha256=lAF3pbzUFEMw50Un8CmXcIIQaDJlzr9bHmhj-mAPjVs,9394
+axonius_api_client/cli/grp_enforcements/__init__.py,sha256=QBS9BmeQOd8oFLKoJs8G3xb1H6x2XFypiZMgi-_6g_M,447
 axonius_api_client/cli/grp_enforcements/cmd_copy.py,sha256=Ye_zZZvxqaLLdYJYUHTo97fmys0lWhGrWIF4SBtL_FQ,942
-axonius_api_client/cli/grp_enforcements/cmd_create.py,sha256=B32bdUBReapmutTRCCFiZV4di231E90_VS3ogDnmcIg,950
+axonius_api_client/cli/grp_enforcements/cmd_create.py,sha256=5QXBc3SmBQRykAY_pVAM2S6uXcPnSHaCFqS1cTy3JoQ,938
 axonius_api_client/cli/grp_enforcements/cmd_delete.py,sha256=U7HTwFDtiTPmqH6PQoHrfthm2XH5ZZdq7P5H3QzZK14,752
 axonius_api_client/cli/grp_enforcements/cmd_get.py,sha256=vTO0dLL8Kh5p0Tl2QxijffitHbuZoLQbmAPHb_Lbbsk,831
 axonius_api_client/cli/grp_enforcements/cmd_get_action_types.py,sha256=5hfjIPpsA2o-p_NrleSOsGawWbX7T7DxIGmiYaByI2s,862
 axonius_api_client/cli/grp_enforcements/cmd_run.py,sha256=ZSIaf5HGoSDiB1hV4hROWeeaio7bf5DOTBbNVEuHA-o,853
 axonius_api_client/cli/grp_enforcements/cmd_update_action_add.py,sha256=11cieHjbCazzvcxZcI6NKRMTq9l0KEFwcxIoub4Qilw,957
 axonius_api_client/cli/grp_enforcements/cmd_update_action_main.py,sha256=IJ-lDhHXKeuakL3wrOPVUV6QZI_eo3FR5OB-s2jGXd8,901
 axonius_api_client/cli/grp_enforcements/cmd_update_action_remove.py,sha256=M-w8u9EdHHFKsipzNb88Ws2FtcCTT8cKMyPyzxy_eoc,883
@@ -221,15 +245,23 @@
 axonius_api_client/cli/grp_enforcements/cmd_update_query_remove.py,sha256=Qzc68LfzUj_8rTqq7nuxgWOyrj1E8krwgF38HlVQpOA,780
 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_daily.py,sha256=3VyejLLvNO-3SBCPXWotINS7JfTvjqIZJQo_n6qTxxo,892
 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_discovery.py,sha256=KGhnUBMjTxTGIxoha7FD9A-whm7IJIGZaQHu7gsJ41o,797
 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_hourly.py,sha256=cIolJQ0mKvLO8m1RshJPkqs0Ei1PKIBv6IUSlv_-Xjw,831
 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_monthly.py,sha256=xQ7ykZApzXNpKDXIt1jYf8q_sqjgQ7qaUgH0s_x9Sb8,902
 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_never.py,sha256=y7A271S9yAEosnfM7HTLPDgnHfwO0O0x7AgfO6RZ8zY,782
 axonius_api_client/cli/grp_enforcements/cmd_update_schedule_weekly.py,sha256=INFdRWsHWq3SwtQSp2FUu5vu3pNqp3Onir-lYXJo-28,896
-axonius_api_client/cli/grp_enforcements/grp_common.py,sha256=QCl-YaInlrZNopv3eEuriPFsEHmlhhC07Nei6tBzutc,10586
+axonius_api_client/cli/grp_enforcements/grp_common.py,sha256=BXbMlKqClJVnrvjSLvx6nViofB21aeAr9U-lPh4vTqw,10537
+axonius_api_client/cli/grp_enforcements/grp_tasks/__init__.py,sha256=g09n7zlY_QdxzBXdhbV0WaLJ2oPoPW08M6JEmEiZTJo,309
+axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_count.py,sha256=0GbiIDj7JGhIKoCOzl5mNbOBvtq5S3l3mFWiJ5C9wxg,741
+axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_get.py,sha256=O35kPOYjgIs95oN3fIJ4mEvQYg-EoIcCq0n9ZCBMZZs,942
+axonius_api_client/cli/grp_enforcements/grp_tasks/cmd_get_filters.py,sha256=APryvOkCitMvU7vzfz-39ngh0BBEa9MzeE-UiNGzSq4,919
+axonius_api_client/cli/grp_enforcements/grp_tasks/export_get.py,sha256=RU5NsgAYr8VCFg1YHuIuYXTMHyehgtyDfwJ9Gc4AhCk,3084
+axonius_api_client/cli/grp_enforcements/grp_tasks/export_get_filters.py,sha256=Sv-ghNzqxelNfkzcyce_xOnd7Ofr5rlfJP7N-vyKuIY,1749
+axonius_api_client/cli/grp_enforcements/grp_tasks/options_get.py,sha256=MfHaQ3aQPU6YynPx9F2DWwQzuLGT9t2cYMzZ6tuVahk,10469
+axonius_api_client/cli/grp_enforcements/grp_tasks/options_get_filters.py,sha256=LTznCtqsr1p1NUMW8dmjg5TC2b5Js4sfJX9xUW0KMiw,779
 axonius_api_client/cli/grp_folders/__init__.py,sha256=7qWg_mwq6CiC_jSLLMKCP7P5gp09AMr-FbLmk9qp6eI,574
 axonius_api_client/cli/grp_folders/cmd_create.py,sha256=RT0VkJZwuQc9LI5IST9mmUIx512RQYEvN-eGdtN4GFA,879
 axonius_api_client/cli/grp_folders/cmd_delete.py,sha256=1IZ1OsCDdMZSp0tWR1pqUu8mBqmJDNghQgTVdScOLq4,889
 axonius_api_client/cli/grp_folders/cmd_find.py,sha256=R2WWc0pJs4QeE8mATc9BPqhTp_S1Q-u-qtMLvB-8DNU,1107
 axonius_api_client/cli/grp_folders/cmd_get_tree.py,sha256=buRlmeyNNNIw1hSlRP177ZrHJSOwX4iMV7BXFzusEhM,894
 axonius_api_client/cli/grp_folders/cmd_move.py,sha256=VzQT-hhMYvLEd1TdxbUKbk_usyg1P1YriW_Mc2hCe60,867
 axonius_api_client/cli/grp_folders/cmd_rename.py,sha256=VlcwkemE60hpo6BsXgl_HU2BVESMxobF9G6gqjqtHhE,879
@@ -261,15 +293,15 @@
 axonius_api_client/cli/grp_system/grp_data_scopes/cmd_update_device_scopes.py,sha256=vGFd88gXyIqWA53VwBfOBH9zyEETKoF2Z1gCQ64e0eA,1014
 axonius_api_client/cli/grp_system/grp_data_scopes/cmd_update_name.py,sha256=Bu9bLZ6VGnuet6FWX8IzQgWCwvXxIt3NGA5nIrxlphM,1042
 axonius_api_client/cli/grp_system/grp_data_scopes/cmd_update_user_scopes.py,sha256=ZMScbR08itsoQfSLLbmu0R-FQzZuBNKNCgqWmUOQ_j4,1008
 axonius_api_client/cli/grp_system/grp_data_scopes/grp_common.py,sha256=xTBQCgfxVu0BIu0j35T6KpKLcZv7XwjzegtB7IW7IW0,1963
 axonius_api_client/cli/grp_system/grp_discover/__init__.py,sha256=inbfmpvT7jPPBvR-9dEX9Y5TnPWpIvIR_kufr_E8R1g,303
 axonius_api_client/cli/grp_system/grp_discover/cmd_get.py,sha256=I628lhPkVLv-Cqz8LyFjtokPsuzox9gq-xfh6PxVNF0,749
 axonius_api_client/cli/grp_system/grp_discover/cmd_is_data_stable.py,sha256=JkmsXcSJdT2wNEfAwi1NWIVK5CVSc4TaiBATLb1_rsE,1250
-axonius_api_client/cli/grp_system/grp_discover/cmd_is_running.py,sha256=cpl4U0xtodeRAfKJY96PqbzbThBfSLaalQid7uPlK8w,845
+axonius_api_client/cli/grp_system/grp_discover/cmd_is_running.py,sha256=QkX5zGuh49YFZ-rYRoZb1A1iCZ3gcXY575_fanZK7PA,846
 axonius_api_client/cli/grp_system/grp_discover/cmd_start.py,sha256=DvDrbzbXGALHcM2DBOxMnvfURRpVNgzDjGnqRaIxAN4,1241
 axonius_api_client/cli/grp_system/grp_discover/cmd_stop.py,sha256=YqPi5NR1EJh0r2sQcRk6awQOUGcsZogPGZouzYg0Lfo,787
 axonius_api_client/cli/grp_system/grp_discover/cmd_wait_data_stable.py,sha256=EnrK6mWsQtoJwNrGl5cHaBgRfbNSkqqBmgTLGA4uw50,1128
 axonius_api_client/cli/grp_system/grp_discover/grp_common.py,sha256=BQlMuEjwwY293kkfABStPgqZEsaUkrdKmAT6rKmpURM,3332
 axonius_api_client/cli/grp_system/grp_meta/__init__.py,sha256=r7Tx8NkZHDdNTihKJBrgfkjlEl7c3mOGmbMGrkaszYs,277
 axonius_api_client/cli/grp_system/grp_meta/cmd_about.py,sha256=u5SAKbv_q09plMugimj6JDtA9ypblujfYJd9ZggQENo,1258
 axonius_api_client/cli/grp_system/grp_meta/cmd_sizes.py,sha256=NiTjiAfwS2kZiP6Nw8mO2tVmDCpU1t2bzuThQr104uY,1114
@@ -293,81 +325,104 @@
 axonius_api_client/cli/grp_system/grp_roles/cmd_update_name.py,sha256=O0jPXbV-Mg3FlZAcyCuN0NXcZqn1iEfjLuojK13_seE,1068
 axonius_api_client/cli/grp_system/grp_roles/cmd_update_perms.py,sha256=kjBMsO1JiRZ2HSm_INwpp_yxcq1TeufoJtPaedXRenM,1172
 axonius_api_client/cli/grp_system/grp_roles/grp_common.py,sha256=j-L4mB3GexEeKPyZQGrPL6jezoda19gpSDts9OhXbK4,3529
 axonius_api_client/cli/grp_system/grp_settings/__init__.py,sha256=m6RJy_QZ2c97hWPw1On9ub1aeq_AavksRzQoExlKIYs,1012
 axonius_api_client/cli/grp_system/grp_settings/cmd_configure_destroy.py,sha256=IpSAczTu9jAadrtyA8zIjbMBdD3DYqgBkI4ukNAINnQ,1870
 axonius_api_client/cli/grp_system/grp_settings/cmd_get.py,sha256=o1x9_9klRdHjVZICjpTUDKMHubUqCMMRBj_-uvAZTQ4,1062
 axonius_api_client/cli/grp_system/grp_settings/cmd_get_section.py,sha256=Y4G_ay4yxBRRZw8WtAWm7wEHDuQhZ6hsOXGOSHPbF2A,1136
-axonius_api_client/cli/grp_system/grp_settings/cmd_get_subsection.py,sha256=XQvLIwuLRvwDYMxP2YjiFX8nlIR4DWDPzW60fu1P8m8,1236
+axonius_api_client/cli/grp_system/grp_settings/cmd_get_subsection.py,sha256=pHDjVjY66xFBYeNSGRdN-oSJgzN_AhHDU6Mo0ai1E5g,1235
 axonius_api_client/cli/grp_system/grp_settings/cmd_update_section.py,sha256=0vQlnOUyrS-1QzpN_bNh4tSKw8X6Rj1KeKwDqtOfelU,1359
 axonius_api_client/cli/grp_system/grp_settings/cmd_update_section_from_json.py,sha256=7wolfxh1_rSm03g292LebrHvIHCmwfp9sN5JcXUPbMk,1382
-axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection.py,sha256=JIJ2PzminVqx-kEenWmhBwnpp3ftUnoE4w_14wxqKSQ,1426
-axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection_from_json.py,sha256=TNvlybkQruA0WbOD9vMsmGt6fw3xT2u3Qiz8iVJqKB4,1500
+axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection.py,sha256=agor6SC9WFS8rg87-gCx3GV37D_VNx3Lw8To9X4gR20,1425
+axonius_api_client/cli/grp_system/grp_settings/cmd_update_subsection_from_json.py,sha256=OuPIHl6RKSDcuGLXGhbe2bmgMJBnOw9GNuM41-F9q2g,1499
 axonius_api_client/cli/grp_system/grp_settings/grp_common.py,sha256=0gKYkIiZi0tJf7Q6My1hmc3C6h-yRwNsCF6xYsS6Fmk,2434
 axonius_api_client/cli/grp_system/grp_users/__init__.py,sha256=XS0zKBn5lsbt46WlMyRIIL_0kaqvOHuE5IFvGtvG7Co,276
 axonius_api_client/cli/grp_system/grp_users/cmd_add.py,sha256=YB9IU8TPk8-SD9e2oSmgGsc2eD0yUsNSyNo5MXrHYh8,2530
 axonius_api_client/cli/grp_system/grp_users/cmd_add_from_csv.py,sha256=QI0wbWhY77O7Oa3oWdVlV0LhzOB24aQp_0I8VmOEQFM,3159
 axonius_api_client/cli/grp_system/grp_users/cmd_delete.py,sha256=RERZczjO4XpD54cF1MZ1co7t7LlLUSJcekvypt0jkDY,752
 axonius_api_client/cli/grp_system/grp_users/cmd_email_password_reset_link.py,sha256=zsBd6h9N41BTRU4pW1yz2tv17zArEH0f4KLJdNee-yY,1042
 axonius_api_client/cli/grp_system/grp_users/cmd_get.py,sha256=WTMGacu6jjkjuWnsPXTbNTbq8S74iaqob_GY9w2-kI0,636
-axonius_api_client/cli/grp_system/grp_users/cmd_get_by_name.py,sha256=kUqx24QUZKO6LfnSKjtC3cuVlKlL4nbcWFKP92cysDg,882
+axonius_api_client/cli/grp_system/grp_users/cmd_get_by_name.py,sha256=j0FUq8X3KfWxkTDVcXpG3rierSoxBg6Yfwd73bEKATY,881
 axonius_api_client/cli/grp_system/grp_users/cmd_get_password_reset_link.py,sha256=nvyzEreA1K8eKCykp9Birf8DiG1broDmAR1Yy-5KVo0,857
 axonius_api_client/cli/grp_system/grp_users/cmd_update.py,sha256=sCvTisO67b3TTanJ6em8gU-1gcbTD5Z7LtwSlsIqx1k,2702
 axonius_api_client/cli/grp_system/grp_users/grp_common.py,sha256=-d1jrScnT49Pxb_rOCCwYVWr43a-G1GBK_iRyJX1e6g,1736
 axonius_api_client/cli/grp_tools/__init__.py,sha256=ZdkLUifn_rqOlF9tCDgIQlrVRwTtvyD0XOrFTGpqERk,272
 axonius_api_client/cli/grp_tools/cmd_help_features.py,sha256=ccix9_FZHA2RLmLrq81U2xzgyzHWv8JB_YFNmUIr5GE,1135
-axonius_api_client/cli/grp_tools/cmd_shell.py,sha256=GM9WYu0zpIpLuRpZvCgMWlhmXV0tWkfD2VVuzgP7qjE,6899
+axonius_api_client/cli/grp_tools/cmd_shell.py,sha256=R-uHO8krm5ffU5gVNYALSSvI9u4eafIomtFkj7fSQco,5956
 axonius_api_client/cli/grp_tools/cmd_signup.py,sha256=Xs5NGcGXxDnM5eiFptLWOb6uvP44CTJqRSu_UTTf_Nw,1727
 axonius_api_client/cli/grp_tools/cmd_sysinfo.py,sha256=Cb6g9JrFrGrYWRoQgooefU8DuMbIM8VF3MvKhYQZuG0,1005
 axonius_api_client/cli/grp_tools/cmd_system_status.py,sha256=3v5SAlqGvcRreFPaKzFmxgyMYIgFhxQiB5jVOmEKC7E,2338
 axonius_api_client/cli/grp_tools/cmd_use_token_reset_token.py,sha256=i-9hvoB4wAjKYzExFa2gGrlRvDNydxudjF2U24nmn68,1099
 axonius_api_client/cli/grp_tools/cmd_write_config.py,sha256=NbljLN-TpaoMAHb5mH-rlGv4PC0By3RfZf28jAN8tsM,677
 axonius_api_client/cli/grp_tools/grp_common.py,sha256=dF4vIW4jtTjmLcW0-hW-1wY61HR6euF2MTkJTS78yfU,2101
 axonius_api_client/cli/grp_tools/grp_options.py,sha256=N6EAywbXkaslCE5Gqhu3Op0tECDv1srZNgaG2Xsi6Ho,554
-axonius_api_client/constants/__init__.py,sha256=5vlA4CShRmC3gvzCcRRrbr638aR2l8yjDAEVgu6nJxU,298
-axonius_api_client/constants/adapters.py,sha256=fK5G3SReQ-BR7K84sv84I692xVtoif8j2WgAYq26F5w,2973
-axonius_api_client/constants/api.py,sha256=_9LzwDWJYaZAeOSHssz22-7bbUvJPsRoslsCrBFUUj0,2574
-axonius_api_client/constants/ctypes.py,sha256=F1bb_pyZnvC8qzLRwb2RkLgxXDnVSUeBycdiYCHYoXo,673
+axonius_api_client/constants/__init__.py,sha256=hIMu5lK5c_MM2bzsTRpr8KhWbq26TeSxQvvSgZ2gTiI,334
+axonius_api_client/constants/adapters.py,sha256=gIe9bVsVOCWxU1dGqRE8FdSkBcXSLHLyvf7lyVll8sU,2973
+axonius_api_client/constants/api.py,sha256=97lpzVqYs8RtPUVhCzFNd19lwyQV2ImhEI0VwL6VYT0,2868
+axonius_api_client/constants/asset_helpers.py,sha256=sFGr9fShsdq7luUPBQhMAYWjfww13N7OY69qW-DJZfg,18413
+axonius_api_client/constants/ctypes.py,sha256=FiVVkUk7SgQjGoju6cUdBoNHMDozntaICso72Qq5PPA,1258
 axonius_api_client/constants/enforcements.py,sha256=9o5TQ1mVjYrKkZTWbKuwEwgCipd5vT2TZwQBqe6PF1w,5056
-axonius_api_client/constants/fields.py,sha256=LuZREjXs2o-pv753Uw1AriXFKXDae0mhgaqrmMRFS-Y,38017
-axonius_api_client/constants/general.py,sha256=EL7M9WDSW5SZF8DZtLwkcWgoZLQG3hYt-TeZcCWuORk,2591
-axonius_api_client/constants/logs.py,sha256=jCfHqawbX0L99TaJ5s3CVMe58w1KmJmIYsEIBJdIOSE,3730
+axonius_api_client/constants/fields.py,sha256=_nP7PQsC1qD2_d0mzTLsMLu-n_3JOPkfT9Uw4A6OBjA,38018
+axonius_api_client/constants/general.py,sha256=yYDgkT9bcjdR2zAiTdgFLNWfmr6_PAINmiK6SCvy-pI,2669
+axonius_api_client/constants/logs.py,sha256=203BzqlLYA-ZPEKwOtpzKTH0nDFAJxnreMg9idv-Rz0,4180
 axonius_api_client/constants/tables.py,sha256=iFDcIz7gt6mHgF88L2X2ZEvnEYwc8QfBtv6NhGWSmXg,1151
-axonius_api_client/constants/wizards.py,sha256=sLDYiVMfWlTnfQ625MEqLF2G4MDd1Dr891OF8MYjp_A,15122
+axonius_api_client/constants/wizards.py,sha256=wEqxuDoWZSSp5v3b5TIJc0tCHUwBYVy7bLPo3VJPtdc,15197
 axonius_api_client/examples/__init__.py,sha256=J2HXxzf9cqBgfqeF3DarsDA98Jzoq3e17ks5NWjknXk,119
-axonius_api_client/examples/adapter_fetch_history_addin.py,sha256=N__dXRM-4FJ0ZxdoeUp-6qJJOLMACx3KulKujJPzckU,18356
+axonius_api_client/examples/adapter_fetch_history_addin.py,sha256=ymrrrnr8pSYdx-67Z6xoOgRBVP7w7U4OqVeO3Ejzc3I,18359
 axonius_api_client/examples/add_adapter_cnxs_from_csv.py,sha256=Tx3U3dhW_odQkrZeywzJeblLGIRoJKx8tm9dQO4Clyw,6948
 axonius_api_client/examples/example_api_query_wizard.py,sha256=POQhYqZElNNTkd18G9rnzDPZ41y5Q_ywKrzAmBKHypE,3058
 axonius_api_client/examples/example_cvss_filtering.py,sha256=dQA62ysBBmP49nkyhbucpr9OYtgLL5itL2RuCKdR88s,2946
 axonius_api_client/examples/example_details.py,sha256=NjyzzTDtP09BS52r_juLqwDFpCo1GlJspkHrt_OnGWM,2776
 axonius_api_client/examples/example_get_by_sq.py,sha256=u2NqQTfVyFwZJpS80cV7iEh7rG7REQPOEDiy3Vzyh90,1445
-axonius_api_client/examples/example_os_count_magic.py,sha256=JZ3MnuV4ovkhXk_SE3hbNTakF5irdTJqik6_Nu23m14,11207
+axonius_api_client/examples/example_os_count_magic.py,sha256=OPUHuyjqafKb9hFTSk8F1iYmh9qkd9QvQfdv9aCNSsk,11207
 axonius_api_client/examples/example_sw_missing.py,sha256=yFQFQkwGpn0H4giemLQiInGDkfJ-Bvye1csGb7_E6tk,2211
 axonius_api_client/examples/example_user_device_associated_agent_versions.py,sha256=HG3zVgFlXE5hpOyvUywf2CSzEFcdkQopBUXndtxkjGU,13284
 axonius_api_client/examples/example_wmi_last_used_users.py,sha256=SC1C2fih9AC4Y3BZOW4Qj8x98uDc6RWLTjQ7dKzYewM,3180
 axonius_api_client/examples/sa_user_pairing.py,sha256=aqDgArOHQUzePNBp5xg7wq_apFjkroZUL1B-YBaINKg,10240
-axonius_api_client/examples/script_base.py,sha256=gh0nBilF5p0y_Q1LI6IN1HxEnue47c3RqNWIwVmWjPs,2730
+axonius_api_client/examples/script_base.py,sha256=GrKmjbxOLeMeJUbZJuaF8aXiKgQE3jTmCWT4XEjtw8o,2729
 axonius_api_client/examples/update_action_config.py,sha256=EXYoK7Z93P3Gp5ZJhFi3G7-uib2ibAUsjq0cCY4v3UA,3993
 axonius_api_client/examples/user_to_device_correlation_example.py,sha256=ixLDSnA-SAOt3BlPShtV_t_h5ZdQlJn5yFfYiychGn0,21603
-axonius_api_client/examples/wip_roles.py,sha256=QKP_Tl6Qr6YgWiIqzKTjItBesenBrRubu1KT7Eul2ms,1355
-axonius_api_client/parsers/__init__.py,sha256=YOYPFxkf3E6KWHtR6xYUe_nqdhW7Kh6BlaHGINiRLkU,304
-axonius_api_client/parsers/config.py,sha256=Edn_pcuzxbSVL0V1AO6uzNgIG79qazAWlH3F_LV9xnQ,19040
-axonius_api_client/parsers/dt_delta.py,sha256=1Lya9ZcEOhytU9yJR15bTMUdJcZkMjd4OCq74CL52uw,9206
-axonius_api_client/parsers/fields.py,sha256=uIUmFznKr8fpMGj4dn1mE75mVdU2Ag2r60sS47wq330,13385
-axonius_api_client/parsers/grabber.py,sha256=bF6VIkdrtLGzaPFUqS12OqJ1QeoVFoVWBX2M4JTc_y4,14278
-axonius_api_client/parsers/matcher.py,sha256=nnnZDaksi12csBoBR3BUwI69WfsKL_qA6z87LFVAOks,9123
-axonius_api_client/parsers/search_wip.py,sha256=inh7I856rjjw4lMPAaN5Qq_h-tq17ocnJSlsrPBGelY,9783
+axonius_api_client/examples/wip_roles.py,sha256=W2afWyegM3wF7AT-3irpA83_3J5RrcS2VAFOz8Qp9W0,1354
+axonius_api_client/parsers/__init__.py,sha256=o01BULxun6OLB9nIemmR3uQcOi__h8dGsPWsd7pgQuI,300
+axonius_api_client/parsers/config.py,sha256=0PoxvctaWb7G8QmxlRGRjuVso6jQhVA8kvf8guEpgL0,19041
+axonius_api_client/parsers/fields.py,sha256=r7eRWQb08wb9Cycafecrkqqyi5KO0hYJmCmaOTOq-P8,13385
+axonius_api_client/parsers/grabber.py,sha256=N0h5G08Be2CGR07RZlTkJV50f328HHhVWFDDsGYgcNY,14319
+axonius_api_client/parsers/matcher.py,sha256=lm056c84d1Cl3Pg62XeDaWg7kdPuYMsF-BqxhoGMdbY,9629
 axonius_api_client/parsers/searchers.py,sha256=zv5w6n3Z6PS2diRkhYC0UMCA6VqeyDvuqs06qbsxVDI,5908
 axonius_api_client/parsers/tables.py,sha256=_TvsFW5ql4GNkqikfxFBcQyIRX9DL99vqfK-POBweT4,10770
-axonius_api_client/parsers/url_parser.py,sha256=rp196xUBCecUDGAk63OpKsdMsCZdx5iJ8nLGPlpS6Oo,6235
 axonius_api_client/parsers/wizards.py,sha256=46Oi38f21acwyuQ8vL410UwYOxX9abM0gbdk9NHjy2o,19266
+axonius_api_client/projects/__init__.py,sha256=ebepAju_8k1ID0xsTpfzcA7l2LQLq8s86E92UM_5W5M,150
+axonius_api_client/projects/cli_cf_token.py,sha256=jS4mubKwefJiEuhs-NydQLGYsZUPD8Z7IosvN-wUZXc,293
+axonius_api_client/projects/url_parser.py,sha256=uz8DZjyR5DihdNo5SKqysnMVEGGpO9nNhvdOl9TtTRo,6363
+axonius_api_client/projects/cert_human/__init__.py,sha256=eTaSKGeRuYL39sTmesg7v533i2t-CSoE2r1UdxXIEaw,533
+axonius_api_client/projects/cert_human/all_logs_list.json,sha256=traKIEAJoIn7u3fZc_i9wcgbSWU6UYJOGo4YIddCvW8,88575
+axonius_api_client/projects/cert_human/all_logs_list.py,sha256=6j0uM8-_oSxBWmXe2MUeneX91yQDsTqrfElVvsI_fuI,62965
+axonius_api_client/projects/cert_human/constants.py,sha256=dOLqQzN6LjqH41TedtuYF3k4O5GujBAjjnpHQPTZW28,604
+axonius_api_client/projects/cert_human/convert.py,sha256=I66jg6cAUPqk-qMGo8JfwDES-p6vLmSw6Pv6pfS6Vos,7413
+axonius_api_client/projects/cert_human/ct_logs.py,sha256=5kKijk_424CLC9hBaGGUzsfI3enOQzgLAao3XwNqr2Y,3703
+axonius_api_client/projects/cert_human/enums.py,sha256=ZESNErm--13fy7UA_46zM97aN1AjfD_uJQjbBfqj9s0,1374
+axonius_api_client/projects/cert_human/exceptions.py,sha256=x6Fqyu-lNV7-EESZte34qFWL9e2OrBdT7tAOuQ-vnRs,618
+axonius_api_client/projects/cert_human/paths.py,sha256=P1ut5_L0GFAIkqFFfiyckZl98z5wP2xfM9IGQ5UTSyU,9081
+axonius_api_client/projects/cert_human/ssl_capture.py,sha256=ezoc2l8y2VhREaNUfK4kYKnYuoQD2QV2VU187QF9E6Y,9046
+axonius_api_client/projects/cert_human/ssl_context.py,sha256=_7RhaiEJLhpW22ltffUEcexA2iamA5qUEER3bTVlDcw,1656
+axonius_api_client/projects/cert_human/ssl_extensions.py,sha256=z0_wwMI6hyDf4Xf7wmq4reITDDyvJ18ZhBlfY9r_v8E,13328
+axonius_api_client/projects/cert_human/utils.py,sha256=Me-8_n8zwZYfL8pJFiLb_9BG6dTTkACM9Y9atsT8iNU,5117
+axonius_api_client/projects/cert_human/stores/__init__.py,sha256=rSgzbvyrSpEEMwx0gAaLh8KfbAIo8UU-CS6Y4qqwOw4,307
+axonius_api_client/projects/cert_human/stores/cert.py,sha256=98MjkDUcpbt6XfXzSN3KOccGUCd2rIuCSRipk0Yjm8g,10607
+axonius_api_client/projects/cert_human/stores/cert_request.py,sha256=36krN6U3rZ6fukez2JgCV8HRgKa7aDGA6d7jZrGlNuY,5788
+axonius_api_client/projects/cert_human/stores/store.py,sha256=ZQiJy-4_kAK9L2oU-rZ-0gbUXkN2fdGOcrqcAKpISDI,13711
+axonius_api_client/projects/cf_token/__init__.py,sha256=FwbAMSUaFWExQUE0ZF-WjZy2gOlSkbPkeF20rgNRUQs,301
+axonius_api_client/projects/cf_token/cli.py,sha256=8v58rU_2S5CuDN1IoBBFjliBggEopa7fuKxwfWkI_vQ,5090
+axonius_api_client/projects/cf_token/constants.py,sha256=I4xG2XjsUs04jBHapbDMesCdH2dLFcNotV27RJkV1yU,10039
+axonius_api_client/projects/cf_token/flows.py,sha256=J89il-JUUZhYffWPXGj6HwJ-PDILFP3x2UZu92iJku0,11805
+axonius_api_client/projects/cf_token/tools.py,sha256=ZrvpxytCMcjNWONVHL-5q0Zd0iDHDeEcvxfaCPfN8WA,24145
 axonius_api_client/tests/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/conftest.py,sha256=Fcxq5_wCDKODy4vgA1SYgDEhVdq2Js0SuMzBYfEEfHs,8904
+axonius_api_client/tests/conftest.py,sha256=-0FbsXOZIENYZXPc6mnV2c98GlzOnP6kPUezgb1ghqM,12671
 axonius_api_client/tests/meta.py,sha256=nBofzmjH6bKM-9iizwT8ej5woIykbZA6ndrZBMk3KmQ,8321
-axonius_api_client/tests/utils.py,sha256=fTul3pCER5AL9yukHqJfAXv4pEsFGSy07ReH91HQ0Mw,10078
+axonius_api_client/tests/utils.py,sha256=3qkT3esQ289ynchPhx_Yg_ITjJOUYbgrHJX_kDEHv9o,12347
 axonius_api_client/tests/datafiles/common.sh,sha256=E4A0cDleLLwr0ni3MKmut1gMnO2hIxAJOc20XS2EfnM,1455
 axonius_api_client/tests/datafiles/create_certs.sh,sha256=GxL12tYk0lgJ9YhxdpMHVseJPTWXvUHeRT846UZlUvM,5219
 axonius_api_client/tests/datafiles/certs/ca_ec.crt.pem,sha256=b_NWHNx4ftEFeh2vTbMKpH37mQBHoIAVoH-i8okH89A,1058
 axonius_api_client/tests/datafiles/certs/ca_ec.key,sha256=_p1nQlhFa5yeqVUaKTAtLdJ1rzqwNiVmzeZ4Obn45u8,302
 axonius_api_client/tests/datafiles/certs/ca_rsa.crt.pem,sha256=Umg-1whPm1ygUyjdiers3yEcx5Kl13imWkDdB4cRZrA,1594
 axonius_api_client/tests/datafiles/certs/ca_rsa.key,sha256=5oUS4J1HLY8rmpzJqvWBo2D6coMO7y_avBrmGBSxvgA,1704
 axonius_api_client/tests/datafiles/certs/server_ec.crt.p7b,sha256=OdRDhT8YwGF4GDRDLdecZIhxX_bwho-2wf1YGA-zsBM,2078
@@ -375,67 +430,72 @@
 axonius_api_client/tests/datafiles/certs/server_ec.csr.pem,sha256=9chWZSjh-qgb8D8P8CGaXgeQ-GtkjenJ-5cSa6Uu_mw,655
 axonius_api_client/tests/datafiles/certs/server_ec.key,sha256=_Qt54yWJT8fSgM8EFzTtYG3vw-xLOhv1dqvG9b7vmMw,302
 axonius_api_client/tests/datafiles/certs/server_rsa.crt.p7b,sha256=gM1qhjWfmywaZmS7EgwWXxKkU9AXKOFSQ-x5EXRaoCc,3146
 axonius_api_client/tests/datafiles/certs/server_rsa.crt.pem,sha256=Ve4ksgyp5CqiECG9t3hhG_1e1-ebH-zO5HvWkrhd2fE,1558
 axonius_api_client/tests/datafiles/certs/server_rsa.csr.pem,sha256=xioPTxWAK-ufiyl7u61uX4G-JXBg7_lKrFy3mvaIPGM,1188
 axonius_api_client/tests/datafiles/certs/server_rsa.key,sha256=LEzWqDebAz5MsmseuDL45H6IxqumGanCzM3wQ2XnYLk,1708
 axonius_api_client/tests/tests_api/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_api/test_api_endpoints.py,sha256=cRKOwXeqfuddP5tT6WT2qsq5UvyCTLN9HnwhKrUhvVc,21672
-axonius_api_client/tests/tests_api/test_signup.py,sha256=HK63gEsFf77Dt1A0V4xHt7fwWBeyovsWTdRTUed4kAk,2401
+axonius_api_client/tests/tests_api/test_api_endpoints.py,sha256=l3WFb7HJeN65BBqXDJlqBGQ0Jce_3J-BRUZCobnrO0k,22291
+axonius_api_client/tests/tests_api/test_signup.py,sha256=o5EgUzyORXpCUVuWULuUlswrfddlmgD9gWOdFUjEaIM,3044
 axonius_api_client/tests/tests_api/tests_adapters/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_api/tests_adapters/test_adapters.py,sha256=nwLVb8JHS1Cwpj9e6P0jozTsK7HqIs-UI5audjQ4648,18039
-axonius_api_client/tests/tests_api/tests_adapters/test_cnx.py,sha256=WJRAl4g1aewuxcNQt1Q4fxOdbNsdoaoRn56-2s8kySs,22002
+axonius_api_client/tests/tests_api/tests_adapters/test_cnx.py,sha256=ZhlvkTkIkkg8lHE9tFSYp061u_eUXBqsCH6UHMjZgD8,20408
 axonius_api_client/tests/tests_api/tests_asset_callbacks/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks.py,sha256=QgqaqU0YM8hP5CqX-K7N8q4AZs2ZL3qbGuwzrWLQbv8,41532
+axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks.py,sha256=lKsUWKyoEUcWFHR19iZuS8WbW-7N73G911fJ4uEkBoY,42093
 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_base.py,sha256=YcfbwSyW1yxCecWyiIU_ZcXSGOTTKpemmlwb7JzzN_c,1543
-axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_csv.py,sha256=rxZx5pKFKS6HduzcIHXKkQUTJe2gkvnGMAaX5t6Mth4,2451
-axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json.py,sha256=gztSoUaDZc-dmSgcs-iFdbyNQJlxfakAax2cqQN2_JQ,2721
-axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json_to_csv.py,sha256=w4jPJxvGEglBUTH53GXngaiZSBtdZKnTupu6sn1wb6E,1140
-axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_table.py,sha256=EuvRO2dTRSKYNv7x9nki-eBgnIK4BErhFOlD9flKrw8,2389
+axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_csv.py,sha256=stNYk80PRQe6b7cUh7zl8d8BTls3sz5VHMrNxeQZ9wE,2429
+axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json.py,sha256=12C9P1o2n_vcjSx46XP8fRT5yucdNIWalvz1zjor6xg,2699
+axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_json_to_csv.py,sha256=UEJ0NRIvgUETIl7v10PpUlvzDmMJwBB0afPe0NV15uI,1118
+axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_table.py,sha256=mxzFIT0e-pmds_x34fYgteVdfjDwnbegb83ut4zksS0,2367
 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_xlsx.py,sha256=VrQjwAYaILVc7ZJpbjLMKE41kR9fNbO8So7nIoym4Co,1955
 axonius_api_client/tests/tests_api/tests_asset_callbacks/test_callbacks_xml.py,sha256=skuK6W8iQBO_e4EYkyKmHNxmuQw8vd3YB3BZEKdh1-8,920
 axonius_api_client/tests/tests_api/tests_assets/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_api/tests_assets/test_assets.py,sha256=Ry6vdPNGyaTLWe6cJRinThG9y_NOOgORBIeLBB-aFkY,15451
+axonius_api_client/tests/tests_api/tests_assets/test_assets.py,sha256=haOAW7gUhh3BLiBCmNq9u1ZsaxME2qNCnYpdBjvr1Xc,15411
 axonius_api_client/tests/tests_api/tests_assets/test_fields.py,sha256=w66r-3azcS3Ktr4Jnt6XcHyKtQER2hHUP7G2xw4xcCw,24596
-axonius_api_client/tests/tests_api/tests_assets/test_labels.py,sha256=0ygHUBffycW4sH08U9TVebB78YlIr4Ay-SKSOTkzoCQ,5056
+axonius_api_client/tests/tests_api/tests_assets/test_labels.py,sha256=0mc_L3DLodUFM0qVjBC7T7lmbOcJUkjyiqfVh9A8Rrg,5040
 axonius_api_client/tests/tests_api/tests_assets/test_runner.py,sha256=3KzSAr8lgalpn-3x73rPYpNYuV4p00xyVwqkoKdal3g,6385
-axonius_api_client/tests/tests_api/tests_assets/test_saved_query.py,sha256=J6l-OB7oETAhXQStJVF0sqXbTt4tePqsl_3e18DN5-0,38776
+axonius_api_client/tests/tests_api/tests_assets/test_saved_query.py,sha256=UwLNTuUBvRWsI8cRV27JRcEDiKDomWahhQoj4INSdUE,39238
 axonius_api_client/tests/tests_api/tests_enforcements/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_api/tests_enforcements/test_enforcements.py,sha256=QZlNauvzPplC867FS2HFnvfVjflXvxgJImoB8t3Ge58,21185
+axonius_api_client/tests/tests_api/tests_enforcements/test_enforcements.py,sha256=f0aKrmdVs5xUdzac3_XnlMqbNMWqyOd07iVlGlQ0Ki4,21225
+axonius_api_client/tests/tests_api/tests_enforcements/test_tasks.py,sha256=cjDrrcNGseS-J4UG8t7hI6QvR7B1PHdZ1jXT9375sfA,10666
 axonius_api_client/tests/tests_api/tests_folders/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_api/tests_folders/test_folders.py,sha256=Z_t2EngWrLJ2CbX64TDRLLYdBsD0tgrVmeXAEVryHeo,40838
+axonius_api_client/tests/tests_api/tests_folders/test_folders.py,sha256=mAw0LIk30-MXicbWoA5lowabASLckRKr1fJ_accwc3Q,41545
 axonius_api_client/tests/tests_api/tests_json_api/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_api/tests_json_api/test_custom_fields.py,sha256=8Iye94TgGUQgMjac4qhdKwi46AfJFeBwEsKFvOr7emc,3564
-axonius_api_client/tests/tests_api/tests_json_api/test_json_api.py,sha256=vnoxvcqcTVF_IN-bGWpsxV3aysl1N6uOrrwSckO5XlA,7433
+axonius_api_client/tests/tests_api/tests_json_api/test_json_api.py,sha256=2KHLZft9KakuO1tYCQ2w0m5AHOqbtbGoQq6tVVXJq3U,7419
 axonius_api_client/tests/tests_api/tests_openapi/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_api/tests_openapi/test_openapi.py,sha256=-I_eYE7B0bEaZEeNdwTtKBSatoOjsZF1bjMiBoSYTXw,810
 axonius_api_client/tests/tests_api/tests_parsers/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_api/tests_parsers/test_config.py,sha256=xCBh4lFr7HDd78zQ56FdXaPPfWjmoesdF2sANq3HJ_I,11092
 axonius_api_client/tests/tests_api/tests_parsers/test_constants.py,sha256=wb-g7qQjpBVRqUGvL4t9qC6WlxLjI3l10wHm9nvW5yM,1699
 axonius_api_client/tests/tests_api/tests_parsers/test_fields.py,sha256=9OwEddSf-CZy8_yOWpLNVusvypvtlpAIeBnsR0ii5tI,884
 axonius_api_client/tests/tests_api/tests_parsers/test_grabber.py,sha256=WyWwaz9l7Ij13ZGLeEn8I5-rWMsiMXfQpQrs6NZHLYk,4394
 axonius_api_client/tests/tests_api/tests_parsers/test_tables.py,sha256=vwrZZt4Asdhpy__JeSOlt20MK2ONtVvnYeGsonTzFdE,3261
 axonius_api_client/tests/tests_api/tests_system/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_api/tests_system/test_activity_logs.py,sha256=on2Tlu4ZFoTxyB25Okq_nDS03HyylzxttPHzcrkWkww,1900
-axonius_api_client/tests/tests_api/tests_system/test_dashboard.py,sha256=jD8CYnsDJG03MJ9qFC_2tqGnxsaRX8z5ZSW-aZMFHvI,5158
+axonius_api_client/tests/tests_api/tests_system/test_dashboard.py,sha256=bYbeshIuBBll-Sj2RZtOj6JlaDOfOUOlaIn2rGz1wNQ,5175
 axonius_api_client/tests/tests_api/tests_system/test_data_scopes.py,sha256=UmWnAQGSWqdCTUXGhSM_6IoWeQjXG6iypVg0czObxMY,11063
 axonius_api_client/tests/tests_api/tests_system/test_instances.py,sha256=aCjXNWwDUKOE--zOZGDrKFePNCSd3HsodaScj4g1gfc,8914
-axonius_api_client/tests/tests_api/tests_system/test_meta.py,sha256=hBa_NFOz683xp6_fVGqCd0tsyAWudsiv9pnP0lIgWqw,3934
+axonius_api_client/tests/tests_api/tests_system/test_meta.py,sha256=iY47I5pQtmZAEhsuVVnbxk8Q1FH4DLXAa3gvigZArWc,4025
 axonius_api_client/tests/tests_api/tests_system/test_remote_support.py,sha256=rzfWpa8_-W2z5XsJ-3RVlLXaqEFIv9IsIz4Uu5M5z20,4525
-axonius_api_client/tests/tests_api/tests_system/test_settings.py,sha256=_qxFw7xQse5icGJdMLGBGbuMC4Df98iUtvE1lo-3WMs,13681
+axonius_api_client/tests/tests_api/tests_system/test_settings.py,sha256=8D3tbT5-ocQlusotQlxTgjNbv-MVwIA1wpEZC-_-3dQ,13708
 axonius_api_client/tests/tests_api/tests_system/test_system_roles.py,sha256=HtnqV--8wJcWylz5cAwZLON6TxVCwRCsHLqsp8y25YE,8747
 axonius_api_client/tests/tests_api/tests_system/test_system_users.py,sha256=snTMzTZ7CqydaWhP6E36UvJLB6--Tqbc7TkYypgCmyc,7858
 axonius_api_client/tests/tests_api/tests_wizard/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_api/tests_wizard/test_constants.py,sha256=QSfZsyYoPlMj7HZxSxndCy5x4d8QYwrYFXoBN5Xb1ng,1905
 axonius_api_client/tests/tests_api/tests_wizard/test_wizard.py,sha256=yKpaWnPnHYRDgFobllnH3kvXMXToXTJ7AmNnoF0J-AI,28159
 axonius_api_client/tests/tests_api/tests_wizard/test_wizard_csv.py,sha256=drDfnvcQeKdSnbxSFNb9V7lqoQwC-LDlOYzjAjR--ME,21275
 axonius_api_client/tests/tests_api/tests_wizard/test_wizard_parser.py,sha256=0qM8HyAVcfneNnDN2GwllLuzXHND1lNxcC2JuzKxeiI,23020
 axonius_api_client/tests/tests_api/tests_wizard/test_wizard_text.py,sha256=5vZG8lJZGFfZRcjFY5MpbfZ8Q3ZNL0K3Wa7uLYbOo4E,4829
 axonius_api_client/tests/tests_auth/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_auth/test_auth.py,sha256=UoaseNUF3Tx1FGbBENGKcmjQoYEkG9pyWNhBJKwJ7Ck,2810
+axonius_api_client/tests/tests_auth/test_auth.py,sha256=zh-QDmFnfQk6TPz0f9GJbbwkCW_81qWJYjK2FM85peI,4424
+axonius_api_client/tests/tests_cf_token/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
+axonius_api_client/tests/tests_cf_token/meta.py,sha256=T5sIP3Il8GnTzfqXXlTistip_IIPN-LiwpUIx1sBmxQ,2638
+axonius_api_client/tests/tests_cf_token/test_flows.py,sha256=NPCO6JCgrNyPC8tyBPvFG8UQkOFmxjbCdTaWusgvNlY,8491
+axonius_api_client/tests/tests_cf_token/test_tools.py,sha256=G6yQPQS_ptEupaqFR3Oq1-Vkf_mSVmDUfBalnk02A9U,17116
 axonius_api_client/tests/tests_cli/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_folders_grp/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_folders_grp/test_folders_grp.py,sha256=uGekcoQQk6FhvUjAgxeOJiltfflKzNVYUebr4usG5S8,4990
 axonius_api_client/tests/tests_cli/tests_grp_activity_logs/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_activity_logs/test_cmd_get.py,sha256=dn0dgWUDT8KPJ1ETDvjoduqRTPAKcroff_0mpp-MQ1o,1672
 axonius_api_client/tests/tests_cli/tests_grp_adapters/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_adapters/test_cmd_config_get.py,sha256=3QRoScarmCi1KnD5HJV1HthggzCTSfr6_hhT6WzlD30,1027
@@ -449,15 +509,15 @@
 axonius_api_client/tests/tests_cli/tests_grp_central_core/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_central_core/test_cmd_get.py,sha256=cMGbaW8Pqg_X2ckN-fQaAHiYNiLUnZS3KZ7Lu56ZPMc,746
 axonius_api_client/tests/tests_cli/tests_grp_central_core/test_cmd_restore_from_aws_s3.py,sha256=E_S-JoxXr0EyfDAXE8puHk6Mbt7QsVqS8-vGJfpLHAE,776
 axonius_api_client/tests/tests_cli/tests_grp_central_core/test_cmd_update.py,sha256=1d7AYGpvCOjxshRE04s1Abjn212t3NcoQpNwG7v33XY,2694
 axonius_api_client/tests/tests_cli/tests_grp_cnx/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add.py,sha256=l6QJRK83akx6xDgth4506JkanAqB_OJ7-Bt32s6hHHc,4088
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add_from_json.py,sha256=XPCOgUSVzDJlA83vqD1oD3Fm4VNMaYedSbtwNMvReqk,3826
-axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add_multiple_from_json.py,sha256=K9x1L7HRC2HZbAsfDP0BkFoyBVOBezi_c7DQjRZTREI,8854
+axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_add_multiple_from_json.py,sha256=F86Q2ZflcyzGK1QUBqLudORQk9VQtJQE6VktSZJQhJg,8469
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_get.py,sha256=RRnR7eLypUpvSpRidPvqab7N9BvvE46QO95toI4mC60,3171
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_get_by_id.py,sha256=Bd3Dkz6EbqVampN0MW9iboStj4dRemVN4jLnE9PzXk0,2227
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_set_active.py,sha256=r-068lbI5qoTA9nMr2em96UuUJAoGaACoZ_827ArQCE,2258
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_set_label.py,sha256=bU8ojC5hgEa2JVHpZhZqTRFhY2J8ap84LkZcO0ffQz8,2245
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_test.py,sha256=MaJcfO69VIHiKh-NVkBEjI8ug35bjs9vsRH-GbU5vwQ,2287
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_test_by_id.py,sha256=0EfGsTdeCB9tNwCDBfQNth3ao434A47l-GVunBqdCpU,2645
 axonius_api_client/tests/tests_cli/tests_grp_cnx/test_cmd_update_by_id.py,sha256=nkchfXjjYv7bCVhHaDqgFVTYuck6C2wfdYmsQmQ-gMw,1488
@@ -469,19 +529,33 @@
 axonius_api_client/tests/tests_cli/tests_grp_data_scopes/test_cmd_create.py,sha256=t62_x17E8LO3oJGFrDvZ7uvEdCMW06Cf7h6gR0Ab2Jo,3214
 axonius_api_client/tests/tests_cli/tests_grp_data_scopes/test_cmd_get.py,sha256=9bPogk4JsJ0b8ZYtruIDpo8SOrXVyqEl6oxtuf480M0,2447
 axonius_api_client/tests/tests_cli/tests_grp_discover/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_discover/test_cmd_get.py,sha256=fdNK2K9rudv_Voh1tJxtVI1CXWS6oDyUSHB1MUloOF0,1491
 axonius_api_client/tests/tests_cli/tests_grp_discover/test_cmd_multi.py,sha256=JzpyZNokjjqDjp8H2i7_wmTZ7o3sBW-fYQ9gSvBC1yw,2250
 axonius_api_client/tests/tests_cli/tests_grp_discover/test_cmd_wait_data_stable.py,sha256=PFGPEKj62ATE6QjmyxTUO06iwGlBy9YltCZwNvEATzg,981
 axonius_api_client/tests/tests_cli/tests_grp_enforcements/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_copy.py,sha256=Fup_VNbDnojAULm9qBoO192Blg27JEvhpHPLbZLKd74,4358
-axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_create.py,sha256=c0_KM_fiBzeUjMJ-gxeIqLSm9cgcGl1EJE9tMfhtdT0,3752
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_copy.py,sha256=IhREGg-RJG639kT-ywjiSKFuPUDC0Wk257DfK1ojdc4,4507
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_create.py,sha256=ENaE2Ajj77xs3KSKB77WknAH3MkoZXBLu2aLlDc-dG0,3946
 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_get.py,sha256=9jWZtsCT-8JumDOYYlQAZhZZXxT7H7vLhbuUDxPUWtw,3036
 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_get_action_types.py,sha256=QfjhHnc2IjuWqGZIYqft6yWUBhKGaBxZ_74rvkqoMd8,3053
 axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_run.py,sha256=nz99Rwt3Rk9SJcL6qPo6deP2PSHwtHAEVp-z6wpYhNc,1112
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_description.py,sha256=njpdVTGLlv89M-E_27ru95BOdTvt_pkLM2g3KDIAkxo,1517
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_above.py,sha256=jXDluAAATZ_kvTaegjroDAIPgpmgT5gKPFWp3meW0cw,2203
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_below.py,sha256=89GW0_x963FTsK7c5OoPbbwUCcW_OwkI6NIacWkxRU0,2203
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_decreased.py,sha256=PlXIxMnORGn_FJ8_CDtFx_XPRSRhlFdoWXSypEpZeUk,1522
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_on_count_increased.py,sha256=h8osYxjIyCnnPqzXmplZ-xBXLXFdshyjZo8SGvw_9Ag,1522
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_only_new_assets.py,sha256=tIW1p4oF6CFBPL1lty3j2paFEQ2Rhf9vgz0nJ3BlqgM,1507
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_query.py,sha256=bSS3O5uOduxCdxoAhJElNKevFV4vtUfNnJInb8Iz5qU,1123
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_query_remove.py,sha256=nZtrLzDdP5Q_hTMtQApWsBSVgzULVL31B40SZGRYpoE,904
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_daily.py,sha256=8yzQWkM1FkF4fgPn06TQqLhYtFiWVkZUKWkz-j4W_rs,3368
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_discovery.py,sha256=cYAuJB_GjKMvs7ktQhj-jlN1SpRaOT8q1nCpfAq9Ch0,1043
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_hourly.py,sha256=GiSyy_6P5dM2SYtJlWe5on2lE7M0AiNeKeA9_eTRcw8,2499
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_monthly.py,sha256=nMQ5IT8zFuFUj0VXVBYluGDu1AkAhC-wKbwBmq0-a_8,3414
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_never.py,sha256=IE_TpgqyEPCLVT1H5sbmxYJmIRDErNIv059DajzRiEc,1007
+axonius_api_client/tests/tests_cli/tests_grp_enforcements/test_cmd_update_schedule_weekly.py,sha256=w1au08SJqS-TofDlCRTNhUVZEJomQz9Sx8KhBPI_CqQ,4098
 axonius_api_client/tests/tests_cli/tests_grp_meta/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_meta/test_cmd_about.py,sha256=BUj3KLD5egXHBckskeum2hU027ZE-YsoovNEXRRIsMI,4533
 axonius_api_client/tests/tests_cli/tests_grp_meta/test_cmd_history_sizes.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_openapi/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_openapi/test_cmd_get-spec.py,sha256=va8rbd88v_yBKaS37Dbqanb4lYAmNvpxUY6HJPnpwq0,1364
 axonius_api_client/tests/tests_cli/tests_grp_saved_query/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
 axonius_api_client/tests/tests_cli/tests_grp_saved_query/base.py,sha256=9Q4QN1qFJTQUSPrFzooW0hzaIXKocgjwBNCoOS1DUY0,2115
@@ -503,25 +577,29 @@
 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_delete.py,sha256=CYHXMsUVoGPscOTFOvq3lcj2Y6GqkQ8YNp42JyP0X7Y,1227
 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_get.py,sha256=64XWtdFkH9IPpGLchvbB2oNOK2pEpTmqMu-xnociUMo,1244
 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_get_by_name.py,sha256=gksJhBpxcZdeJcpej4wt40_1NczDRWCegAAFE1pff2c,1380
 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_get_perms.py,sha256=1Ag3A9S2dZEQ6z8KGQ1SVjzE5HNnXHCXJQuRunXUFEw,656
 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_data_scope.py,sha256=HeSNuAfSJLEB4dqh45arg60kEHpXcltWkRwq7pnhKNw,1686
 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_name.py,sha256=9Y27v6g2c58dg8hJj6IJMEucFPocBPkjQVQGPY-wyf0,1290
 axonius_api_client/tests/tests_cli/tests_grp_system_roles/test_cmd_update_perms.py,sha256=BzALTO071Ugd4Yrl52HkTaNkV-G9dOHj5FlqcUsY4kw,1343
+axonius_api_client/tests/tests_cli/tests_grp_tasks/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
+axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_count.py,sha256=ebBPTrKBajWIKDHaR3HGfN6GJSlu_VsUXqeSVCPTkKo,608
+axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_get.py,sha256=7rrg9ucspdHq1X0NMR9JIeju_tzXmfNyq6W2PY1gzqY,2976
+axonius_api_client/tests/tests_cli/tests_grp_tasks/test_cmd_get_filters.py,sha256=v9zcv4RHoGEeEpCkyQyAOxU4fqHcjCEwiE-zE_ts1qM,2055
 axonius_api_client/tests/tests_cli/tests_grp_tools/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_shell.py,sha256=nOPowIvLri9xZ8Os88-JvnlgwfgwAVAWn7zIPJQ4Ghw,5084
+axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_shell.py,sha256=XgPAKfZjt3WxnvBUgKGFWUkk3xUR27UzKNfX02yFM0E,5082
 axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_sysinfo.py,sha256=aHh_pVp70BKk5nTlUETP0y51weW540DQLBvEyq0PD68,995
 axonius_api_client/tests/tests_cli/tests_grp_tools/test_cmd_write_config.py,sha256=jgGDiUD-tbXOwl3yUicYUX5QE6tZXN1vUiWN7tT76a4,1271
 axonius_api_client/tests/tests_pkg/__init__.py,sha256=EeC6RMJKsW54bbcwIfBnxDnLkBUDbq0DJ1_Ey_fiFgI,42
-axonius_api_client/tests/tests_pkg/test_connect.py,sha256=U_W2Xu-TP1cyv7KZKDMW4rCJLwd2SETDp8VmWglXAnc,3616
+axonius_api_client/tests/tests_pkg/test_connect.py,sha256=pfHWyoP7-XvEjGKQSzCFKLQME5d5Q79YSFpxLPfY0ko,6783
 axonius_api_client/tests/tests_pkg/test_data.py,sha256=k3iL7D1Q-Dd3R0_3XXXXbg3KKWCJtglu4TZhGb4cJCg,2481
-axonius_api_client/tests/tests_pkg/test_http.py,sha256=k-hCzlUihmVxXK8l3GSEb0IGxhJBxsnYgtJuRB5CURM,9962
+axonius_api_client/tests/tests_pkg/test_http.py,sha256=RvVf3vqgv7WZ7Rd1pUE_kCqC0SxERF7YJgL_2a9nG4o,9914
 axonius_api_client/tests/tests_pkg/test_logs.py,sha256=bqIx4r3om-nEe_8Lg_T40jhbaYyOabH8DSyQYfXgZm0,4045
-axonius_api_client/tests/tests_pkg/test_setup_env.py,sha256=F8HYX0NbHMRgpkqmvvtTyyVZj-MR3uONn1tqDjuhItE,6332
-axonius_api_client/tests/tests_pkg/test_tools.py,sha256=84I93tG1mZXUWFDCme5FtMqpGnm4K9n03QjkHamV0xg,42069
-axonius_api_client/tests/tests_pkg/test_url_parser.py,sha256=RIsUCrQyLvZmUuBi4bqVVQ4fyLOhp-ibwTrsZuqP0PE,2985
-axonius_api_client-4.60.4.dist-info/LICENSE,sha256=D2xtlW7XSDr0U5FMGUUkBNZGOP7jeWjnmmHdBuE5SpE,1064
-axonius_api_client-4.60.4.dist-info/METADATA,sha256=WXUI2PaFgEXI-nQKRFbIs2xOjE_lbS0tQXuSgvuyHgI,2740
-axonius_api_client-4.60.4.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
-axonius_api_client-4.60.4.dist-info/entry_points.txt,sha256=m7s3YZRnG6lOa94ExOVHS35-ys79ncmsiyuGaU0q5NQ,57
-axonius_api_client-4.60.4.dist-info/top_level.txt,sha256=PujMecHFuFDWI-PaRPMS52vnGmVv-4pSOxud44wiO30,19
-axonius_api_client-4.60.4.dist-info/RECORD,,
+axonius_api_client/tests/tests_pkg/test_setup_env.py,sha256=5GgAYAqehKAW-F_4rGebtDe4Ji9nUXVekaxorJM5UCs,8318
+axonius_api_client/tests/tests_pkg/test_tools.py,sha256=AydEs5mijbeFn0BCC7L2xBJcGy6dyYznWZvndQ20g4o,42244
+axonius_api_client/tests/tests_pkg/test_url_parser.py,sha256=YdQ5sE7x8UyXcl1UzjGx6FYbOYuRGTktF7boxNt-Iqo,3189
+axonius_api_client-5.0.0.dist-info/LICENSE,sha256=D2xtlW7XSDr0U5FMGUUkBNZGOP7jeWjnmmHdBuE5SpE,1064
+axonius_api_client-5.0.0.dist-info/METADATA,sha256=Y6CUARQdZ2BRzyrpo41BUix2xu7Y6X6Nb1C3_zBaNe0,2739
+axonius_api_client-5.0.0.dist-info/WHEEL,sha256=a-zpFRIJzOq5QfuhBzbhiA1eHTzNCJn8OdRvhdNX0Rk,110
+axonius_api_client-5.0.0.dist-info/entry_points.txt,sha256=m7s3YZRnG6lOa94ExOVHS35-ys79ncmsiyuGaU0q5NQ,57
+axonius_api_client-5.0.0.dist-info/top_level.txt,sha256=PujMecHFuFDWI-PaRPMS52vnGmVv-4pSOxud44wiO30,19
+axonius_api_client-5.0.0.dist-info/RECORD,,
```

