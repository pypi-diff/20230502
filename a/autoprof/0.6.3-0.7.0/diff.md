# Comparing `tmp/autoprof-0.6.3-py2.py3-none-any.whl.zip` & `tmp/autoprof-0.7.0-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,41 +1,42 @@
-Zip file size: 138838 bytes, number of entries: 79
+Zip file size: 147818 bytes, number of entries: 80
 -rw-rw-r--  2.0 unx     3171 b- defN 23-Mar-17 23:11 autoprof/AP_config.py
--rw-rw-r--  2.0 unx     5650 b- defN 23-Apr-27 17:03 autoprof/__init__.py
+-rw-rw-r--  2.0 unx     5755 b- defN 23-May-02 20:09 autoprof/__init__.py
 -rw-rw-r--  2.0 unx      281 b- defN 23-Mar-17 23:11 autoprof/__main__.py
--rw-rw-r--  2.0 unx     1072 b- defN 23-Mar-17 23:11 autoprof/fit/__init__.py
--rw-rw-r--  2.0 unx     6689 b- defN 23-Apr-08 19:05 autoprof/fit/base.py
+-rw-rw-r--  2.0 unx     1092 b- defN 23-May-02 20:08 autoprof/fit/__init__.py
+-rw-rw-r--  2.0 unx     6698 b- defN 23-May-02 20:08 autoprof/fit/base.py
 -rw-rw-r--  2.0 unx       30 b- defN 23-Apr-08 19:05 autoprof/fit/gp.py
 -rw-rw-r--  2.0 unx     6704 b- defN 23-Apr-08 19:05 autoprof/fit/gradient.py
--rw-rw-r--  2.0 unx     9877 b- defN 23-Apr-27 15:35 autoprof/fit/hmc.py
+-rw-rw-r--  2.0 unx     4884 b- defN 23-May-02 20:08 autoprof/fit/hmc.py
 -rw-rw-r--  2.0 unx    13206 b- defN 23-Apr-08 19:05 autoprof/fit/iterative.py
--rw-rw-r--  2.0 unx    29711 b- defN 23-Apr-21 02:55 autoprof/fit/lm.py
--rw-rw-r--  2.0 unx     4546 b- defN 23-Mar-30 13:28 autoprof/fit/mhmcmc.py
+-rw-rw-r--  2.0 unx    31364 b- defN 23-May-02 20:08 autoprof/fit/lm.py
+-rw-rw-r--  2.0 unx     4317 b- defN 23-May-02 20:08 autoprof/fit/mhmcmc.py
+-rw-rw-r--  2.0 unx     5296 b- defN 23-May-02 20:08 autoprof/fit/nuts.py
 -rw-rw-r--  2.0 unx     1125 b- defN 23-Apr-21 02:55 autoprof/image/__init__.py
 -rw-rw-r--  2.0 unx    11195 b- defN 23-Apr-21 02:55 autoprof/image/image_header.py
 -rw-rw-r--  2.0 unx    21644 b- defN 23-Apr-21 02:55 autoprof/image/image_object.py
 -rw-rw-r--  2.0 unx     5269 b- defN 23-Apr-21 02:55 autoprof/image/jacobian_image.py
 -rw-rw-r--  2.0 unx     6615 b- defN 23-Apr-21 02:55 autoprof/image/model_image.py
 -rw-rw-r--  2.0 unx    15842 b- defN 23-Apr-21 02:55 autoprof/image/target_image.py
 -rw-rw-r--  2.0 unx    22836 b- defN 23-Apr-21 02:55 autoprof/image/window_object.py
 -rw-rw-r--  2.0 unx      654 b- defN 23-Apr-01 17:55 autoprof/models/__init__.py
 -rw-rw-r--  2.0 unx     4277 b- defN 23-Apr-21 02:55 autoprof/models/_model_methods.py
 -rw-rw-r--  2.0 unx    20263 b- defN 23-Apr-01 17:55 autoprof/models/_shared_methods.py
--rw-rw-r--  2.0 unx    18936 b- defN 23-Apr-21 02:55 autoprof/models/core_model.py
+-rw-rw-r--  2.0 unx    21176 b- defN 23-May-02 20:08 autoprof/models/core_model.py
 -rw-rw-r--  2.0 unx     5837 b- defN 23-Apr-21 02:55 autoprof/models/edgeon_model.py
 -rw-rw-r--  2.0 unx    12642 b- defN 23-Mar-30 13:28 autoprof/models/exponential_model.py
 -rw-rw-r--  2.0 unx     1945 b- defN 23-Apr-21 02:55 autoprof/models/flatsky_model.py
 -rw-rw-r--  2.0 unx     8266 b- defN 23-Mar-17 23:11 autoprof/models/foureirellipse_model.py
 -rw-rw-r--  2.0 unx     4825 b- defN 23-Apr-21 02:55 autoprof/models/galaxy_model_object.py
 -rw-rw-r--  2.0 unx    11660 b- defN 23-Mar-30 13:28 autoprof/models/gaussian_model.py
 -rw-rw-r--  2.0 unx    14753 b- defN 23-Apr-21 02:55 autoprof/models/group_model_object.py
--rw-rw-r--  2.0 unx    25820 b- defN 23-Apr-24 15:28 autoprof/models/model_object.py
+-rw-rw-r--  2.0 unx    25839 b- defN 23-May-02 20:08 autoprof/models/model_object.py
 -rw-rw-r--  2.0 unx     3380 b- defN 23-Mar-17 23:11 autoprof/models/moffat_model.py
 -rw-rw-r--  2.0 unx    17123 b- defN 23-Mar-17 23:11 autoprof/models/nuker_model.py
--rw-rw-r--  2.0 unx    17262 b- defN 23-Apr-01 17:55 autoprof/models/parameter_object.py
+-rw-rw-r--  2.0 unx    17694 b- defN 23-May-02 20:08 autoprof/models/parameter_object.py
 -rw-rw-r--  2.0 unx     2212 b- defN 23-Apr-21 02:55 autoprof/models/planesky_model.py
 -rw-rw-r--  2.0 unx     3439 b- defN 23-Apr-21 02:55 autoprof/models/psf_model.py
 -rw-rw-r--  2.0 unx     4600 b- defN 23-Apr-21 02:55 autoprof/models/ray_model.py
 -rw-rw-r--  2.0 unx    14516 b- defN 23-Mar-17 23:11 autoprof/models/sersic_model.py
 -rw-rw-r--  2.0 unx      875 b- defN 23-Mar-17 23:11 autoprof/models/sky_model_object.py
 -rw-rw-r--  2.0 unx    10193 b- defN 23-Apr-01 17:55 autoprof/models/spline_model.py
 -rw-rw-r--  2.0 unx     1125 b- defN 23-Mar-17 23:11 autoprof/models/star_model_object.py
@@ -43,22 +44,22 @@
 -rw-rw-r--  2.0 unx     4332 b- defN 23-Mar-17 23:11 autoprof/models/warp_model.py
 -rw-rw-r--  2.0 unx     3546 b- defN 23-Apr-21 02:55 autoprof/models/wedge_model.py
 -rw-rw-r--  2.0 unx       57 b- defN 23-Feb-16 02:48 autoprof/parse_config/__init__.py
 -rw-rw-r--  2.0 unx     4147 b- defN 23-Mar-17 23:11 autoprof/parse_config/basic_config.py
 -rw-rw-r--  2.0 unx     4590 b- defN 23-Mar-17 23:11 autoprof/parse_config/galfit_config.py
 -rw-rw-r--  2.0 unx        0 b- defN 23-Feb-16 02:48 autoprof/parse_config/shared_methods.py
 -rw-rw-r--  2.0 unx       67 b- defN 22-Oct-27 02:37 autoprof/plots/__init__.py
--rw-rw-r--  2.0 unx     6535 b- defN 23-Apr-27 15:19 autoprof/plots/image.py
--rw-rw-r--  2.0 unx     7559 b- defN 23-Apr-01 17:55 autoprof/plots/profile.py
+-rw-rw-r--  2.0 unx     6857 b- defN 23-May-02 20:08 autoprof/plots/image.py
+-rw-rw-r--  2.0 unx     7559 b- defN 23-May-01 22:29 autoprof/plots/profile.py
 -rw-rw-r--  2.0 unx     3048 b- defN 23-Mar-17 23:11 autoprof/plots/shared_elements.py
--rw-rw-r--  2.0 unx     3527 b- defN 23-Mar-17 23:11 autoprof/plots/visuals.py
+-rw-rw-r--  2.0 unx    19994 b- defN 23-May-02 20:08 autoprof/plots/visuals.py
 -rw-rw-r--  2.0 unx        0 b- defN 22-Sep-15 14:58 autoprof/utils/__init__.py
 -rw-rw-r--  2.0 unx      800 b- defN 23-Mar-30 13:28 autoprof/utils/angle_operations.py
 -rw-rw-r--  2.0 unx     8609 b- defN 23-Mar-17 23:11 autoprof/utils/interpolate.py
--rw-rw-r--  2.0 unx     6369 b- defN 23-Apr-21 02:55 autoprof/utils/operations.py
+-rw-rw-r--  2.0 unx     6369 b- defN 23-Apr-28 21:02 autoprof/utils/operations.py
 -rw-rw-r--  2.0 unx      963 b- defN 23-Mar-17 23:11 autoprof/utils/optimization.py
 -rw-rw-r--  2.0 unx     5990 b- defN 23-Apr-01 17:55 autoprof/utils/parametric_profiles.py
 -rw-rw-r--  2.0 unx        0 b- defN 22-Dec-16 16:54 autoprof/utils/conversions/__init__.py
 -rw-rw-r--  2.0 unx     2317 b- defN 23-Mar-17 23:11 autoprof/utils/conversions/coordinates.py
 -rw-rw-r--  2.0 unx     1095 b- defN 23-Mar-21 14:13 autoprof/utils/conversions/dict_to_hdf5.py
 -rw-rw-r--  2.0 unx     1829 b- defN 23-Mar-30 13:28 autoprof/utils/conversions/functions.py
 -rw-rw-r--  2.0 unx     2728 b- defN 23-Mar-30 13:28 autoprof/utils/conversions/optimization.py
@@ -68,14 +69,14 @@
 -rw-rw-r--  2.0 unx     3296 b- defN 23-Mar-30 13:28 autoprof/utils/initialize/construct_psf.py
 -rw-rw-r--  2.0 unx     3921 b- defN 23-Mar-17 23:11 autoprof/utils/initialize/initialize.py
 -rw-rw-r--  2.0 unx     7036 b- defN 23-Mar-30 13:28 autoprof/utils/initialize/segmentation_map.py
 -rw-rw-r--  2.0 unx        0 b- defN 22-Dec-16 16:54 autoprof/utils/isophote/__init__.py
 -rw-rw-r--  2.0 unx     1085 b- defN 23-Mar-17 23:11 autoprof/utils/isophote/ellipse.py
 -rw-rw-r--  2.0 unx     8531 b- defN 23-Mar-30 13:28 autoprof/utils/isophote/extract.py
 -rw-rw-r--  2.0 unx     7012 b- defN 23-Mar-17 23:11 autoprof/utils/isophote/integrate.py
--rw-rw-r--  2.0 unx    35149 b- defN 23-Apr-27 17:04 autoprof-0.6.3.dist-info/LICENSE
--rw-rw-r--  2.0 unx     3788 b- defN 23-Apr-27 17:04 autoprof-0.6.3.dist-info/METADATA
--rw-rw-r--  2.0 unx      110 b- defN 23-Apr-27 17:04 autoprof-0.6.3.dist-info/WHEEL
--rw-rw-r--  2.0 unx       57 b- defN 23-Apr-27 17:04 autoprof-0.6.3.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-Apr-27 17:04 autoprof-0.6.3.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     6931 b- defN 23-Apr-27 17:04 autoprof-0.6.3.dist-info/RECORD
-79 files, 524080 bytes uncompressed, 127822 bytes compressed:  75.6%
+-rw-rw-r--  2.0 unx    35149 b- defN 23-May-02 20:09 autoprof-0.7.0.dist-info/LICENSE
+-rw-rw-r--  2.0 unx     3812 b- defN 23-May-02 20:09 autoprof-0.7.0.dist-info/METADATA
+-rw-rw-r--  2.0 unx      110 b- defN 23-May-02 20:09 autoprof-0.7.0.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       57 b- defN 23-May-02 20:09 autoprof-0.7.0.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-May-02 20:09 autoprof-0.7.0.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     7009 b- defN 23-May-02 20:09 autoprof-0.7.0.dist-info/RECORD
+80 files, 545523 bytes uncompressed, 136686 bytes compressed:  74.9%
```

## zipnote {}

```diff
@@ -27,14 +27,17 @@
 
 Filename: autoprof/fit/lm.py
 Comment: 
 
 Filename: autoprof/fit/mhmcmc.py
 Comment: 
 
+Filename: autoprof/fit/nuts.py
+Comment: 
+
 Filename: autoprof/image/__init__.py
 Comment: 
 
 Filename: autoprof/image/image_header.py
 Comment: 
 
 Filename: autoprof/image/image_object.py
@@ -213,26 +216,26 @@
 
 Filename: autoprof/utils/isophote/extract.py
 Comment: 
 
 Filename: autoprof/utils/isophote/integrate.py
 Comment: 
 
-Filename: autoprof-0.6.3.dist-info/LICENSE
+Filename: autoprof-0.7.0.dist-info/LICENSE
 Comment: 
 
-Filename: autoprof-0.6.3.dist-info/METADATA
+Filename: autoprof-0.7.0.dist-info/METADATA
 Comment: 
 
-Filename: autoprof-0.6.3.dist-info/WHEEL
+Filename: autoprof-0.7.0.dist-info/WHEEL
 Comment: 
 
-Filename: autoprof-0.6.3.dist-info/entry_points.txt
+Filename: autoprof-0.7.0.dist-info/entry_points.txt
 Comment: 
 
-Filename: autoprof-0.6.3.dist-info/top_level.txt
+Filename: autoprof-0.7.0.dist-info/top_level.txt
 Comment: 
 
-Filename: autoprof-0.6.3.dist-info/RECORD
+Filename: autoprof-0.7.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## autoprof/__init__.py

```diff
@@ -1,15 +1,15 @@
 import sys
 import argparse
 import requests
 from .parse_config import galfit_config, basic_config
 from . import models, image, plots, utils, fit, AP_config
 
 # meta data
-__version__ = "0.6.3"
+__version__ = "0.7.0"
 __author__ = "Connor Stone"
 __email__ = "connorstone628@gmail.com"
 
 
 def run_from_terminal() -> None:
     """
     Execute AutoProf from the command line with various options.
@@ -112,14 +112,15 @@
         )
     if args.filename in ["tutorial", "tutorials"]:
         tutorials = [
             "https://raw.github.com/ConnorStoneAstro/AutoProf/main/docs/tutorials/GettingStarted.ipynb",
             "https://raw.github.com/ConnorStoneAstro/AutoProf/main/docs/tutorials/GroupModels.ipynb",
             "https://raw.github.com/ConnorStoneAstro/AutoProf/main/docs/tutorials/ModelZoo.ipynb",
             "https://raw.github.com/ConnorStoneAstro/AutoProf/main/docs/tutorials/JointModels.ipynb",
+            "https://raw.github.com/ConnorStoneAstro/AutoProf/main/docs/tutorials/FittingMethods.ipynb",
             "https://raw.github.com/ConnorStoneAstro/AutoProf/main/docs/tutorials/CustomModels.ipynb",
             "https://raw.github.com/ConnorStoneAstro/AutoProf/main/docs/tutorials/simple_config.py",
         ]
         for url in tutorials:
             try:
                 R = requests.get(url)
                 with open(url[url.rfind("/") + 1 :], "w") as f:
```

## autoprof/fit/__init__.py

```diff
@@ -1,13 +1,14 @@
 from .base import *
 from .lm import *
 from .gradient import *
 from .iterative import *
 from .hmc import *
 from .mhmcmc import *
+from .nuts import *
 
 """
 base: This module defines the base class BaseOptimizer, 
       which is used as the parent class for all optimization algorithms in AutoProf. 
       This module contains helper functions used across multiple optimization algorithms, 
       such as computing gradients and making copies of models.
```

## autoprof/fit/base.py

```diff
@@ -56,14 +56,15 @@
             loss_history (List[float]): A list of the optimization losses.
             message (str): An informational message.
         """
 
         self.model = model
         self.verbose = kwargs.get("verbose", 0)
         self.fit_parameters_identity = fit_parameters_identity
+        
         if fit_window is None:
             self.fit_window = self.model.window
         else:
             self.fit_window = fit_window & self.model.window
 
         if initial_state is None:
             try:
```

## autoprof/fit/hmc.py

```diff
@@ -1,261 +1,140 @@
 # Hamiltonian Monte-Carlo
 import os
 from time import time
 from typing import Optional, Sequence
 import warnings
+
 import torch
 from tqdm import tqdm
 import numpy as np
+import pyro
+import pyro.distributions as dist
+from pyro.infer import MCMC as pyro_MCMC
+from pyro.infer import HMC as pyro_HMC
+
 from .base import BaseOptimizer
 from .. import AP_config
 
 __all__ = ["HMC"]
 
-
 class HMC(BaseOptimizer):
-    """Hamiltonian Monte-Carlo sampler, based on:
-    https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo and
-    https://arxiv.org/abs/1701.02434 and
-    http://www.mcmchandbook.net/HandbookChapter5.pdf. This MCMC
-    algorithm uses gradients of the Chi^2 to more efficiently explore
-    the probability distribution.
+    """Hamiltonian Monte-Carlo sampler wrapper for the Pyro package.
+
+    This MCMC algorithm uses gradients of the Chi^2 to more
+    efficiently explore the probability distribution. Consider using
+    the NUTS sampler instead of HMC, as it is generally better in most
+    aspects.
+
+    More information on HMC can be found at:
+    https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo,
+    https://arxiv.org/abs/1701.02434, and
+    http://www.mcmchandbook.net/HandbookChapter5.pdf
 
     Args:
-      model (AutoProf_Model): The model which will be sampled.
-      initial_state (Optional[Sequence]): A 1D array with the values for each parameter in the model. Note that these values should be in the form of "as_representation" in the model.
-      max_iter (int): The number of sampling steps to perform. Default 1000
-      epsilon (float): The length of the integration step to perform for each leapfrog iteration. The momentum update will be of order elipson * score. Default 1e-2
-      leapfrog_steps (int): Number of steps to perform with leapfrog integrator per sample of the HMC. Default 20
-      mass (float or array): Mass vector which can tune the behavior in each dimension to ensure better mixing when sampling. Default 1.
+        model (AutoProf_Model): The model which will be sampled.
+        initial_state (Optional[Sequence]): A 1D array with the values for each
+            parameter in the model. Note that these values should be in the form
+            of "as_representation" in the model.
+        max_iter (int, optional): The number of sampling steps to perform.
+            Default is 1000.
+        epsilon (float, optional): The length of the integration step to perform
+            for each leapfrog iteration. The momentum update will be of order
+            elipson * score. Default is 1e-5.
+        leapfrog_steps (int, optional): Number of steps to perform with leapfrog
+            integrator per sample of the HMC. Default is 20.
+        mass_matrix (float or array, optional): Mass matrix which can tune the
+            behavior in each dimension to ensure better mixing when sampling.
+            Default is the identity.
 
     """
-
+    
     def __init__(
         self,
         model: "AutoProf_Model",
         initial_state: Optional[Sequence] = None,
         max_iter: int = 1000,
         **kwargs
     ):
         super().__init__(model, initial_state, max_iter=max_iter, **kwargs)
 
-        self.epsilon = kwargs.get("epsilon", 1e-5)
+        self.epsilon = kwargs.get("epsilon", 1e-3)
         self.leapfrog_steps = kwargs.get("leapfrog_steps", 20)
-        self.mass = kwargs.get("mass", None)
-        self.temperature = torch.tensor(kwargs.get("temperature", 1.0), dtype = AP_config.ap_dtype, device = AP_config.ap_device)
-        self.temper = torch.tensor(kwargs.get("temper", 1.0), dtype = AP_config.ap_dtype, device = AP_config.ap_device)
         self.progress_bar = kwargs.get("progress_bar", True)
-        self.min_accept = kwargs.get("min_accept", 0.1)
-        self.max_accept = kwargs.get("max_accept", 0.9)
-
-        self.Y = self.model.target[self.model.window].flatten("data")
-        #        1 / sigma^2
-        self.W = (
-            1.0 / self.model.target[self.model.window].flatten("variance")
-            if model.target.has_variance
-            else 1.0
-        )
-
-        self.reset_chain()
-
-    def reset_chain(self):
-        self.chain = []
-        self._accepted = 0
-        self._sampled = 0
-        self.last_accept_check = 0
+        self.prior = kwargs.get("prior", None)
+        self.warmup = kwargs.get("warmup", 100)
+        self.hmc_kwargs = kwargs.get("hmc_kwargs", {})
+        self.mcmc_kwargs = kwargs.get("mcmc_kwargs", {})
+        self.acceptance = None
         
+        if "mass_matrix" not in self.hmc_kwargs and "mass_matrix" in kwargs:
+            self.hmc_kwargs["mass_matrix"] = kwargs.get("mass_matrix")
+
     def fit(
         self,
         state: Optional[torch.Tensor] = None,
-        nsamples: Optional[int] = None,
-        restart_chain: bool = True,
     ):
-        """
-        Performs the MCMC sampling using a Hamiltonian Monte-Carlo step and records the chain for later examination.
-        """
+        """Performs MCMC sampling using Hamiltonian Monte-Carlo step.
 
-        if nsamples is None:
-            nsamples = self.max_iter
+        Records the chain for later examination.
 
-        if state is None:
-            state = self.current_state
-        score, chi2 = self.score_fn(state)
-
-        if restart_chain:
-            self.reset_chain()
-        else:
-            self.chain = list(self.chain)
-        for _ in self.iter_generator(nsamples):
-            while (
-                True
-            ):  # rerun step function if it encounters a numerical error. Note that many such re-runs will bias the final posterior
-                try:
-                    state, score, chi2 = self.step(state, score, chi2)
-                    break
-                except RuntimeError as e:
-                    print("Error encountered. Reducing step size epsilon by factor 3")
-                    self.epsilon /= 3.
-                    warnings.warn(
-                        "HMC numerical integration error. Perhaps rerun with smaller step size.",
-                        RuntimeWarning,
-                    )
-            if len(self.chain) > (100+self.last_accept_check) and self.acceptance < self.min_accept:
-                self.last_accept_check = len(self.chain)
-                print(f"Acceptance too low (accept < {self.min_accept}). Reducing step size epsilon by factor 3")
-                self.epsilon /= 3.
-            if len(self.chain) > (100+self.last_accept_check) and self.acceptance > self.max_accept:
-                self.last_accept_check = len(self.chain)
-                print("Acceptance too high (accept > {self.max_accept}). Increasing step size epsilon by factor 2")
-                self.epsilon *= 2.
-
-            self.append_chain(state)
-        self.current_state = state
-        self.chain = np.stack(self.chain)
-        return self
+        Args:
+            state (torch.Tensor, optional): Model parameters as a 1D tensor.
 
-    def append_chain(self, state: torch.Tensor) -> None:
-        """
-        Add a state vector to the MCMC chain
-        """
-        self.model.set_parameters(state, as_representation=True)
-        chain_state = self.model.get_parameter_vector(as_representation=False)
-        self.chain.append(chain_state.detach().cpu().clone().numpy())
+        Returns:
+            HMC: An instance of the HMC class with updated chain.
 
-    def score_fn(self, state: torch.Tensor) -> (torch.Tensor, torch.Tensor):
-        """
-        Compute the score for the current state. This is the gradient of the Chi^2 wrt the model parameters.
         """
-        # Set up state with grad
-        gradstate = torch.as_tensor(state).clone().detach()
-        gradstate.requires_grad = True
-
-        # Sample model
-        Y = self.model(parameters=gradstate, as_representation=True).flatten("data")
-
-        # Compute Chi^2
-        if self.model.target.has_mask:
-            loss = torch.sum(
-                ((self.Y - Y) ** 2 * self.W)[torch.logical_not(self.mask)]
-            ) / 2.
-        else:
-            loss = torch.sum(
-                (self.Y - Y) ** 2 * self.W
-            ) / 2.
-
-        # Compute score
-        loss.backward()
 
-        return -gradstate.grad, loss.detach()
-
-    @staticmethod
-    def accept(log_alpha) -> torch.Tensor:
-        """
-        Evaluates randomly if a given proposal is accepted. This is done in log space which is more natural for the evaluation in the step.
-        """
-        return torch.log(torch.rand(log_alpha.shape)) < log_alpha
+        def step(model, prior):
+            x = pyro.sample("x", prior)
+            # Log-likelihood function
+            log_likelihood_value = -model.negative_log_likelihood(
+                parameters=x, as_representation=True
+            )
+            # Observe the log-likelihood
+            pyro.factor("obs", log_likelihood_value)
 
-    def step(
-        self, state: torch.Tensor, score: torch.Tensor, chi2: torch.Tensor
-    ) -> torch.Tensor:
-        """
-        Takes one step of the HMC sampler by integrating along a path initiated with a random momentum.
-        """
-        momentum_0 = torch.distributions.MultivariateNormal(
-            loc = torch.zeros_like(state),
-            covariance_matrix = self.mass
-        ).sample()
-        momentum_t = torch.clone(momentum_0)
-        x_t = torch.clone(state)
-        score_t = torch.clone(score)
-        temper = torch.sqrt(self.temper)
-        for leap in range(self.leapfrog_steps):
-            # Update step
-            momentum_tp = (
-                temper if leap < self.leapfrog_steps / 2 else (1 / temper)
-            ) * (momentum_t + self.epsilon * score_t / 2)
-            x_tp1 = x_t + self.epsilon * (self._inv_mass @ momentum_tp)
-            score_tp1, chi2_tp1 = self.score_fn(x_tp1)
-            momentum_tp1 = (
-                temper if leap < self.leapfrog_steps // 2 else (1 / temper)
-            ) * (momentum_tp + self.epsilon * score_tp1 / 2)
-
-            # set for next step
-            x_t = torch.clone(x_tp1)
-            momentum_t = momentum_tp1
-            score_t = torch.clone(score_tp1)
-
-            # Test for failure case
-            if torch.any(torch.logical_not(torch.isfinite(momentum_t))):
-                raise RuntimeError(
-                    "HMC numerical integration error, infinite momentum, consider smaller step size epsilon"
-                )
-            
-
-        # Set the proposed values as the end of the leapfrog integration
-        proposal_state = x_t
-        proposal_chi2 = chi2_tp1
-        proposal_score = score_tp1
-
-        # Evaluate the Hamiltonian likelihood
-        DU = chi2 - proposal_chi2
-        DP = 0.5 * (
-            (momentum_0 @ self._inv_mass @ momentum_0) - (momentum_t @ self._inv_mass @ momentum_t)
-        )
-        log_alpha = (DU + DP) / self.temperature
-
-        # Determine if proposal is accepted
-        accept = self.accept(log_alpha)
-
-        # Record result
-        self._accepted += accept
-        self._sampled += 1
-
-        return (
-            (proposal_state, proposal_score, proposal_chi2)
-            if accept
-            else (state, score, chi2)
-        )
+        if self.prior is None:
+            self.prior = dist.Normal(
+                self.current_state,
+                torch.ones_like(self.current_state) * 1e2
+                + torch.abs(self.current_state) * 1e2,
+            )
 
-    @property
-    def acceptance(self):
-        """
-        Returns the ratio of accepted states to total states sampled.
-        """
-        return self._accepted / self._sampled
+        # Set up the HMC sampler
+        hmc_kwargs = {
+            "jit_compile": True,
+            "ignore_jit_warnings": True,
+            "full_mass": True,
+            "step_size": self.epsilon,
+            "num_steps": self.leapfrog_steps,
+            "adapt_step_size": False,
+        }
+        hmc_kwargs.update(self.hmc_kwargs)
+        hmc_kernel = pyro_HMC(step, **hmc_kwargs)
+
+        # Provide an initial guess for the parameters
+        init_params = {"x": self.model.get_parameter_vector(as_representation=True)}
+
+        # Run MCMC with the HMC sampler and the initial guess
+        mcmc_kwargs = {
+            "num_samples": self.max_iter,
+            "warmup_steps": self.warmup,
+            "initial_params": init_params,
+            "disable_progbar": not self.progress_bar,
+        }
+        mcmc_kwargs.update(self.mcmc_kwargs)
+        mcmc = pyro_MCMC(hmc_kernel, **mcmc_kwargs)
+        
+        mcmc.run(self.model, self.prior)
+        self.iteration += self.max_iter
 
-    @property
-    def mass(self):
-        return self._mass
-    @mass.setter
-    def mass(self, value):
-        """Set the mass matrix for the HMC sampler
-
-        A note when setting the mass matrix it is often a good idea to
-        set it to `mass / mean(mass)` to normalize the matrix.
-        Otherise it is possible for the numerical stability to be off
-        if there is a huge discrepancy between the parameters and the
-        momentum. This can show up as requring a very small epsilon
-        for the chain to run, which then leaves a high
-        autocorrelation.
+        # Extract posterior samples
+        chain = mcmc.get_samples()["x"]
 
-        """
-        if value is None:
-            value = torch.eye(
-                len(self.current_state),
-                dtype = AP_config.ap_dtype,
-                device = AP_config.ap_device
-            )
-        self._mass = torch.as_tensor(value, dtype = AP_config.ap_dtype, device = AP_config.ap_device)
-        self._inv_mass = torch.linalg.inv(self._mass)
-        self._det_mass = torch.linalg.det(self._mass)
-
-    def iter_generator(self, N):
-        if self.progress_bar:
-            return tqdm(range(N))
-        return range(N)
+        with torch.no_grad():
+            for i in range(len(chain)):
+                chain[i] = self.model.transform(chain[i], to_representation = False)
+        self.chain = chain
         
-    def estimate_mass(self, chain = None):
-        if chain is None:
-            chain = self.chain
-
-        return np.cov(chain, rowvar = False)
+        return self
```

## autoprof/fit/lm.py

```diff
@@ -109,14 +109,15 @@
             self.ndf -= torch.sum(self.mask)
         self.L_history = []
         self.decision_history = []
         self.rho_history = []
         self._count_grad_step = 0
         self._count_converged = 0
         self.ndf = kwargs.get("ndf", self.ndf)
+        self._covariance_matrix = None
 
         # update attributes with constraints
         self.constraints = kwargs.get("constraints", None)
         if self.constraints is not None and isinstance(self.constraints, LM_Constraint):
             self.constraints = (self.constraints,)
 
         if self.constraints is not None:
@@ -403,28 +404,30 @@
             as_representation=True,
             parameters_identity=self.fit_parameters_identity,
         )
         if self.verbose > 1:
             AP_config.ap_logger.info(
                 f"LM Fitting complete in {time() - start_fit} sec with message: {self.message}"
             )
-        # set the uncertainty for each parameter
-        if self.use_broyden:
-            self.update_J_AD()
-            self.update_hess()
-        cov = self.covariance_matrix()
-        if torch.all(torch.isfinite(cov)):
-            self.model.set_uncertainty(
-                torch.sqrt(torch.abs(torch.diag(cov))),
-                as_representation=True,
-                parameters_identity=self.fit_parameters_identity,
-            )
 
         return self
 
+    def update_uncertainty(self):
+        # set the uncertainty for each parameter
+        cov = self.covariance_matrix
+        if torch.all(torch.isfinite(cov)):
+            try:
+                self.model.set_uncertainty(
+                    torch.sqrt(torch.abs(torch.diag(cov))),
+                    as_representation=False,
+                    parameters_identity=self.fit_parameters_identity,
+                )
+            except RuntimeError as e:
+                AP_config.ap_logger.warning(f"Unable to update uncertainty due to: {e}")
+        
     @torch.no_grad()
     def undo_step(self) -> None:
         AP_config.ap_logger.info("undoing step, trying to recover")
         assert (
             self.decision_history.count("accept") >= 2
         ), "cannot undo with not enough accepted steps, retry with new parameters"
         assert len(self.decision_history) == len(self.lambda_history)
@@ -572,14 +575,47 @@
 
         # Apply mask if needed
         if self.model.target.has_mask:
             self.J[self.mask] = 0.0
 
         # Note that the most recent jacobian was a full autograd jacobian
         self.full_jac = True
+        
+    def update_J_natural(self) -> None:
+        """
+        Update the jacobian using automatic differentiation, produces an accurate jacobian at the current state. Use this method to get the jacobian in the parameter space instead of representation space.
+        """
+        # Free up memory
+        del self.J
+        if "cpu" not in AP_config.ap_device:
+            torch.cuda.empty_cache()
+
+        # Compute jacobian on image
+        self.J = self.model.jacobian(
+            torch.clone(self.model.transform(
+                self.current_state,
+                to_representation=False,
+                parameters_identity=self.fit_parameters_identity,
+            )).detach(),
+            as_representation=False,
+            parameters_identity=self.fit_parameters_identity,
+            window=self.fit_window,
+        ).flatten("data")
+
+        # compute the constraint jacobian if needed
+        if self.constraints is not None:
+            for con in self.constraints:
+                self.J = torch.cat((self.J, con.jacobian(self.model)))
+
+        # Apply mask if needed
+        if self.model.target.has_mask:
+            self.J[self.mask] = 0.0
+
+        # Note that the most recent jacobian was a full autograd jacobian
+        self.full_jac = False
 
     @torch.no_grad()
     def update_J_Broyden(self, h, Yp, Yph) -> None:
         """
         Use the Broyden update to approximate the new Jacobian tensor at the current state. Less accurate, but far faster.
         """
 
@@ -610,26 +646,32 @@
             self.hess = torch.matmul(self.J.T, self.W.view(len(self.W), -1) * self.J)
         self.hess += self.epsilon5 * torch.eye(
             len(self.current_state),
             dtype=AP_config.ap_dtype,
             device=AP_config.ap_device,
         )
 
+    @property
     @torch.no_grad()
     def covariance_matrix(self) -> torch.Tensor:
+        if self._covariance_matrix is not None:
+            return self._covariance_matrix
+        self.update_J_natural()
+        self.update_hess()        
         try:
-            return torch.linalg.inv(-self.hess)
+            self._covariance_matrix = torch.linalg.inv(self.hess)
         except:
             AP_config.ap_logger.warning(
                 "WARNING: Hessian is singular, likely at least one model is non-physical. Will massage Hessian to continue but results should be inspected."
             )
             self.hess += torch.eye(
                 len(self.grad), dtype=AP_config.ap_dtype, device=AP_config.ap_device
             ) * (torch.diag(self.hess) == 0)
-            return torch.linalg.inv(-self.hess) # * self.res_loss()
+            self._covariance_matrix = torch.linalg.inv(self.hess)
+        return self._covariance_matrix
 
     @torch.no_grad()
     def update_grad(self, Yph) -> None:
         """
         Update the gradient using the model evaluation on all pixels
         """
         self.grad = torch.matmul(self.J.T, self.W * (self.Y - Yph))
```

## autoprof/fit/mhmcmc.py

```diff
@@ -32,24 +32,16 @@
         initial_state: Optional[Sequence] = None,
         max_iter: int = 1000,
         **kwargs
     ):
         super().__init__(model, initial_state, max_iter=max_iter, **kwargs)
 
         self.epsilon = kwargs.get("epsilon", 1e-2)
-
-        self.Y = self.model.target[self.model.window].flatten("data")
-        #        1 / sigma^2
-        self.W = (
-            1.0 / self.model.target[self.model.window].flatten("variance")
-            if model.target.has_variance
-            else 1.0
-        )
-        #          # pixels      # parameters
-        self.ndf = len(self.Y) - len(self.current_state)
+        self.progress_bar = kwargs.get("progress_bar", True)
+        self.report_after = kwargs.get("report_after", int(self.max_iter / 10))
 
         self.chain = []
         self._accepted = 0
         self._sampled = 0
 
     def fit(
         self,
@@ -68,63 +60,64 @@
             state = self.current_state
         chi2 = self.sample(state)
 
         if restart_chain:
             self.chain = []
         else:
             self.chain = list(self.chain)
-        for _ in tqdm(range(nsamples)):
+
+        iterator = tqdm(range(nsamples)) if self.progress_bar else range(nsamples)
+        for i in iterator:
             state, chi2 = self.step(state, chi2)
             self.append_chain(state)
+            if i % self.report_after == 0 and i > 0 and self.verbose > 0:
+                AP_config.ap_logger.info(f"Acceptance: {self.acceptance}")
+        if self.verbose > 0:
+            AP_config.ap_logger.info(f"Acceptance: {self.acceptance}")
         self.current_state = state
         self.chain = np.stack(self.chain)
         return self
 
     def append_chain(self, state: torch.Tensor):
         """
         Add a state vector to the MCMC chain
         """
 
-        self.model.set_parameters(state, as_representation=True)
-        chain_state = self.model.get_parameter_vector(as_representation=False)
-        self.chain.append(chain_state.detach().cpu().clone().numpy())
+        self.chain.append(
+            self.model.transform(
+                state,
+                to_representation=False,
+            ).detach().cpu().clone().numpy()
+        )
 
     @staticmethod
     def accept(log_alpha):
         """
         Evaluates randomly if a given proposal is accepted. This is done in log space which is more natural for the evaluation in the step.
         """
         return torch.log(torch.rand(log_alpha.shape)) < log_alpha
 
     @torch.no_grad()
     def sample(self, state: torch.Tensor):
         """
         Samples the model at the proposed state vector values
         """
-        Y = self.model(parameters=state, as_representation=True).flatten("data")
-        # Compute Chi^2
-        if self.model.target.has_mask:
-            loss = (
-                torch.sum(((self.Y - Y) ** 2 * self.W)[torch.logical_not(self.mask)])
-                / self.ndf
-            )
-        else:
-            loss = torch.sum((self.Y - Y) ** 2 * self.W) / self.ndf
-
-        return loss
+        return self.model.negative_log_likelihood(
+            parameters=state, as_representation=True
+        )
 
     @torch.no_grad()
     def step(self, state: torch.Tensor, chi2: torch.Tensor) -> torch.Tensor:
         """
         Takes one step of the HMC sampler by integrating along a path initiated with a random momentum.
         """
 
         proposal_state = torch.normal(mean=state, std=self.epsilon)
         proposal_chi2 = self.sample(proposal_state)
-        log_alpha = (chi2 - proposal_chi2) / 2
+        log_alpha = chi2 - proposal_chi2
         accept = self.accept(log_alpha)
         self._accepted += accept
         self._sampled += 1
         return proposal_state if accept else state, proposal_chi2 if accept else chi2
 
     @property
     def acceptance(self):
```

## autoprof/models/core_model.py

```diff
@@ -110,14 +110,39 @@
     def sample(self, image=None, window=None, *args, **kwargs):
         """Calling this function should fill the given image with values
         sampled from the given model.
 
         """
         pass
 
+    def negative_log_likelihood(
+        self,
+        parameters=None,
+        as_representation=True,
+        parameters_identity=None,
+    ):
+        if parameters is not None:
+            self.set_parameters(parameters, as_representation, parameters_identity)
+
+        model = self.sample()
+        data = self.target[self.window]
+        variance = data.variance
+        if self.target.has_mask:
+            mask = torch.logical_not(data.mask)
+            chi2 = torch.sum(
+                ((model - data).data ** 2 / variance)[mask]
+            ) / 2.
+        else:
+            chi2 = torch.sum(
+                ((model - data).data ** 2 / variance)
+            ) / 2.
+            
+        return chi2
+        
+
     def set_parameters(
         self,
         parameters,
         as_representation=True,
         parameters_identity=None,
     ):
         """
@@ -386,14 +411,48 @@
         ):
             for pid in self[P].identities:
                 if parameters_identity is None or pid in parameters_identity:
                     parameters.append(pid)
             vstart += V
         return parameters
 
+    def transform(self, in_parameters, to_representation = True, parameters_identity = None):
+        out_parameters = torch.zeros(
+            np.sum(self.parameter_vector_len(parameters_identity = parameters_identity)),
+            dtype=AP_config.ap_dtype,
+            device=AP_config.ap_device,
+        )
+        porder = self.parameter_order(parameters_identity = parameters_identity)
+        
+        # If vector is requested by identity, they are individually updated
+        if parameters_identity is not None:
+            pindex = 0
+            for P in porder:
+                for pid in self[P].identities:
+                    if pid in parameters_identity:
+                        if to_representation:
+                            out_parameters[pindex] = self[P].val_to_rep(in_parameters[pindex])
+                        else:
+                            out_parameters[pindex] = self[P].rep_to_val(in_parameters[pindex])
+                        pindex += 1
+            return out_parameters
+        
+        # If the full vector is requested, they are added in bulk
+        vstart = 0
+        for P, V in zip(
+            porder,
+            self.parameter_vector_len(),
+        ):
+            if to_representation:
+                out_parameters[vstart : vstart + V] = self[P].val_to_rep(in_parameters[vstart : vstart + V])
+            else:
+                out_parameters[vstart : vstart + V] = self[P].rep_to_val(in_parameters[vstart : vstart + V])
+            vstart += V
+        return out_parameters
+
     def get_uncertainty_vector(self, as_representation=False):
         uncertanty = torch.zeros(
             np.sum(self.parameter_vector_len()),
             dtype=AP_config.ap_dtype,
             device=AP_config.ap_device,
         )
         vstart = 0
```

## autoprof/models/model_object.py

```diff
@@ -246,26 +246,26 @@
             center_shift = (
                 self["center"].value
                 - (
                     torch.round(self["center"].value / working_pixelscale - align)
                     + align
                 )
                 * working_pixelscale
-            )
+            ).detach()
             working_window.shift_origin(center_shift)
             # Make the image object to which the samples will be tracked
             working_image = Model_Image(
                 pixelscale=working_pixelscale, window=working_window
             )
             # Evaluate the model at the current resolution
             working_image.data += self.evaluate_model(image = working_image)
             # If needed, super-resolve the image in areas of high curvature so pixels are properly sampled
             if self.integrate_mode == "none":
                 pass
-            if self.integrate_mode == "threshold":
+            elif self.integrate_mode == "threshold":
                 X, Y = working_image.get_coordinate_meshgrid_torch(self["center"].value[0], self["center"].value[1])
                 selective_integrate(
                     X = X,
                     Y = Y,
                     data = working_image.data,
                     image_header = working_image.header,
                     eval_brightness = self.evaluate_model,
@@ -304,15 +304,15 @@
             )
         else:
             # Create an image to store pixel samples
             working_image = Model_Image(
                 pixelscale=image.pixelscale, window=working_window
             )
             # Evaluate the model on the image
-            working_image.data += self.evaluate_model(working_image)
+            working_image.data += self.evaluate_model(image = working_image)
             # Super-resolve and integrate where needed
             if self.integrate_mode == "none":
                 pass
             elif self.integrate_mode == "threshold":
                 X, Y = working_image.get_coordinate_meshgrid_torch(self["center"].value[0], self["center"].value[1])
                 selective_integrate(
                     X = X,
```

## autoprof/models/parameter_object.py

```diff
@@ -239,14 +239,28 @@
             )
         if self.limits is None:
             return self.get_representation(index=index, identity=identity)
         return inv_boundaries(
             self.get_representation(index=index, identity=identity), self.limits
         )
 
+    def rep_to_val(self, rep):
+        if self.cyclic:
+            return cyclic_boundaries(rep, self.limits)
+        if self.limits is None:
+            return rep
+        return inv_boundaries(rep, self.limits)
+    
+    def val_to_rep(self, val):
+        if self.cyclic:
+            return cyclic_boundaries(val, self.limits)
+        if self.limits is None:
+            return val
+        return boundaries(val, self.limits)    
+
     def get_uncertainty(self, index=None, identity=None):
         if self._uncertainty is None:
             return None
 
         # Ensure the shape of uncertinty matches the value
         if (
             self._uncertainty.numel() == 1
```

## autoprof/plots/image.py

```diff
@@ -125,14 +125,15 @@
     model,
     target=None,
     sample_image=None,
     showcbar=True,
     window=None,
     center_residuals=False,
     clb_label=None,
+    normalize_residuals=False,
     **kwargs,
 ):
     if window is None:
         window = model.window
     if target is None:
         target = model.target
     if sample_image is None:
@@ -149,15 +150,18 @@
                 window=win,
                 showcbar=showcbar,
                 center_residuals=center_residuals,
                 **kwargs,
             )
         return fig, ax
 
-    residuals = (target[window] - sample_image[window]).data.detach().cpu().numpy()
+    residuals = (target[window] - sample_image[window]).data
+    if normalize_residuals:
+        residuals = residuals / torch.sqrt(target[window].variance)
+    residuals = residuals.detach().cpu().numpy()
 
     if target.has_mask:
         residuals[target[window].mask.detach().cpu().numpy()] = np.nan
     if center_residuals:
         residuals -= np.nanmedian(residuals)
     residuals = np.arctan(
         residuals / (iqr(residuals[np.isfinite(residuals)], rng=[10, 90]) * 2)
@@ -173,15 +177,19 @@
     }
     imshow_kwargs.update(kwargs)
     im = ax.imshow(
         residuals,
         **imshow_kwargs,
     )
     if showcbar:
-        clb = fig.colorbar(im, ax=ax, label=f"tan$^{{-1}}$(Target - {model.name})" if clb_label is None else clb_label)
+        if normalize_residuals:
+            default_label = f"tan$^{{-1}}$((Target - {model.name}) / $\\sigma$)"
+        else:
+            default_label = f"tan$^{{-1}}$(Target - {model.name})"
+        clb = fig.colorbar(im, ax=ax, label=default_label if clb_label is None else clb_label)
         clb.ax.set_yticks([])
         clb.ax.set_yticklabels([])
     return fig, ax
 
 
 def model_window(fig, ax, model, rectangle_linewidth=2, **kwargs):
     if isinstance(ax, np.ndarray):
```

## autoprof/plots/visuals.py

```diff
@@ -1,47 +1,56 @@
 import numpy as np
 import matplotlib.pyplot as plt
 from matplotlib.colors import LinearSegmentedColormap, ListedColormap
+import matplotlib
+import os
 
 __all__ = ["main_pallet", "cmap_grad", "cmap_div"]
 
 main_pallet = {
     "primary1": "#5FAD41",
     "primary2": "#46A057",
     "primary3": "#2D936C",
     "secondary1": "#595122",
     "secondary2": "#BFAE48",
     "pop": "#391463",
 }
 
-grad_list = [
-    "#000000",
-    "#1A1F16",
-    "#1E3F20",
-    "#335E31",  # "#294C28",
-    "#477641",  # "#345830",
-    "#5D986D",  # "#4A7856",
-    "#88BF9E",  # "#6FB28A",
-    "#94ECBE",
-    "#FFFFFF",
-]
-# grad_list = ["#000000", "#1A1F16", "#1E3F20", "#294C28", "#345830", "#4A7856", "#6FB28A", "#94ECBE", "#FFFFFF"]
-grad_cdict = {"red": [], "green": [], "blue": []}
-cpoints = np.linspace(0, 1, len(grad_list))
-for i in range(len(grad_list)):
-    grad_cdict["red"].append(
-        [cpoints[i], int(grad_list[i][1:3], 16) / 256, int(grad_list[i][1:3], 16) / 256]
-    )
-    grad_cdict["green"].append(
-        [cpoints[i], int(grad_list[i][3:5], 16) / 256, int(grad_list[i][3:5], 16) / 256]
-    )
-    grad_cdict["blue"].append(
-        [cpoints[i], int(grad_list[i][5:7], 16) / 256, int(grad_list[i][5:7], 16) / 256]
-    )
-cmap_grad = LinearSegmentedColormap("cmap_grad", grad_cdict)
+# grad_list = [
+#     "#000000",
+#     "#1A1F16",
+#     "#1E3F20",
+#     "#335E31",  # "#294C28",
+#     "#477641",  # "#345830",
+#     "#5D986D",  # "#4A7856",
+#     "#88BF9E",  # "#6FB28A",
+#     "#94ECBE",
+#     "#FFFFFF",
+# ]
+
+#grad_list = np.load(os.path.join(os.path.dirname(os.path.abspath(__file__)), "rgb_colours.npy"))
+# not proud of this but it works
+grad_list = [[0.02352941176470601, 0.05490196078431372, 0.03137254901960787], [0.025423221664412132, 0.057920953380312966, 0.033516620406216086], [0.027376785284830882, 0.06093685701603565, 0.035720426678078974], [0.02938891743876659, 0.0639504745699993, 0.03798295006291389], [0.03145841869575705, 0.06696255038243151, 0.04030317185736432], [0.033584075048444885, 0.06997377604547542, 0.04260833195845542], [0.03576465765226276, 0.07298479546414544, 0.044888919486002675], [0.03799892263356237, 0.0759962092973116, 0.0471477168479796], [0.04028561096212802, 0.07900857886908796, 0.049385189300118405], [0.04255435348091496, 0.08202242962578685, 0.05160177112762838], [0.04479522750797262, 0.0850382542012795, 0.053797868796876404], [0.047011290960205, 0.08805651514356005, 0.05597386374386699], [0.04920301153517722, 0.09107764734708199, 0.05813011485045571], [0.05137081101697757, 0.09410206022865564, 0.06026696065107289], [0.05351507020340438, 0.09713013967907827, 0.062384721306060466], [0.05563613318418619, 0.1001622498180042, 0.06448370037222773], [0.057734311075703586, 0.10319873457564618, 0.06656418639667772], [0.0598098852979892, 0.10623991912163352, 0.0686264543561699], [0.06186311046428467, 0.10928611115857814, 0.07067076696111699], [0.06389421694104419, 0.11233760209556845, 0.07269737584065203], [0.06590341312637987, 0.11539466811482368, 0.07470652262295904], [0.06789088748697142, 0.11845757114304345, 0.07669843992315992], [0.06985681038697011, 0.12152655973754478, 0.07867335224943414], [0.07180133573715267, 0.12460186989603428, 0.08063147683667268], [0.07372460248826435, 0.12768372579779083, 0.08257302441578668], [0.07562673598889558, 0.13077234048311664, 0.08449819992578214], [0.07750784922530482, 0.1338679164771084, 0.0864072031748393], [0.0793680439581338, 0.13697064636311304, 0.08830022945588598], [0.08120741176889373, 0.14008071331062474, 0.0901774701215026], [0.08302603502740363, 0.1431982915618533, 0.09203911312243368], [0.08482398778987404, 0.14632354688073884, 0.0938853435134896], [0.08660133663613406, 0.14945663696777384, 0.09571634393019346], [0.08835814145343088, 0.15259771184365092, 0.0975322950391592], [0.09009445617335096, 0.15574691420443426, 0.09933337596485287], [0.09181032946766504, 0.1589043797506794, 0.10111976469511005], [0.09350580540821188, 0.16207023749268656, 0.1028916384675223], [0.0951809240954313, 0.16524461003384922, 0.1046491741385914], [0.09683572225960263, 0.1684276138338816, 0.10639254853734492], [0.09847023383851092, 0.17161935945352033, 0.10812193880494256], [0.10008449053481877, 0.17481995178216395, 0.10983752272163866], [0.10167852235616967, 0.1780294902497653, 0.11153947902233724], [0.10325235814074307, 0.18124806902417712, 0.11322798770185102], [0.10480602607076109, 0.18447577719504085, 0.1149032303108651], [0.10633955417621349, 0.18771269894521686, 0.11656539024350993], [0.10785297083092218, 0.19095891371065837, 0.11821465301736292], [0.10934630524287259, 0.19421449632956456, 0.11985120654661363], [0.11081958794061691, 0.19747951718156898, 0.12147524140906224], [0.11227285125743197, 0.20075404231766003, 0.12308695110755152], [0.113706129814793, 0.2040381335814737, 0.12468653232637883], [0.11511946100664691, 0.20733184872254534, 0.12627418518317554], [0.1165128854858628, 0.21063524150205992, 0.12785011347669853], [0.11788644765418299, 0.21394836179160054, 0.12941452493092973], [0.11924019615691589, 0.21727125566535316, 0.13096763143583748], [0.12057418438355699, 0.2206039654861931, 0.13250964928512182], [0.12188847097547184, 0.2239465299860438, 0.13404079941121932], [0.12318312034172044, 0.2272989843408748, 0.13556130761782226], [0.12445820318406359, 0.23066136024067158, 0.13707140481012495], [0.12571379703214872, 0.2340336859546926, 0.13857132722298834], [0.12694998678982505, 0.23741598639230338, 0.14006131664718174], [0.1281668652935146, 0.2408082831596569, 0.14154162065383566], [0.12936453388351488, 0.24421059461247346, 0.14301249281721415], [0.13054310298908373, 0.24762293590515277, 0.14447419293589053], [0.13170269272811447, 0.25104531903643956, 0.1459269872523864], [0.1328434335221802, 0.25447775289184116, 0.14737114867131162], [0.1339654667276644, 0.2579202432829991, 0.14880695697601956], [0.13506894528368824, 0.26137279298418187, 0.15023469904377038], [0.1361540343774794, 0.26483540176607334, 0.15165466905937425], [0.13722091212776352, 0.2683080664270149, 0.15306716872726295], [0.1382697702867517, 0.271790780821844, 0.15447250748191957], [0.13930081496119553, 0.2752835358884738, 0.1558710026965733], [0.14031426735293703, 0.2787863196723416, 0.15726297989004664], [0.1413103645193169, 0.2822991173488483, 0.15864877293161908], [0.142289360153694, 0.28582191124391093, 0.16002872424375406], [0.1432515253862969, 0.2893546808527299, 0.16140318500251255], [0.1441971496054517, 0.29289740285688415, 0.16277251533545473], [0.14512654129921948, 0.29645005113984313, 0.16413708451681472], [0.1460400289172567, 0.3000125968009987, 0.1654972711597065], [0.14693796175266738, 0.30358500816829653, 0.16685346340510454], [0.14782071084339368, 0.3071672508095593, 0.16820605910731407], [0.14868866989260401, 0.31075928754257476, 0.16955546601563484], [0.14954225620729, 0.3143610784440312, 0.1709021019518895], [0.1503819116541449, 0.31797258085736924, 0.1722463949834796], [0.15120810363154275, 0.32159374939962293, 0.17358878359160151], [0.1520213260562527, 0.32522453596731304, 0.17492971683424066], [0.15282210036320543, 0.32886488974146083, 0.1762696545035385], [0.15361097651642713, 0.33251475719178275, 0.17760906727710982], [0.15438853402891373, 0.3361740820801234, 0.17894843686287198], [0.15515538298892081, 0.3398428054631865, 0.18028825613691796], [0.15591216508980174, 0.34352086569461787, 0.18162902927396304], [0.15665955466015927, 0.3472081984264961, 0.18297127186986703], [0.15739825969072788, 0.3509047366102775, 0.1843155110557227], [0.1581290228539065, 0.35461041049725495, 0.18566228560298584], [0.15885262251157475, 0.35832514763856854, 0.18701214601910962], [0.1595698737062211, 0.36204887288482673, 0.1883656546331343], [0.16028162913006866, 0.365781508385379, 0.18972338567067223], [0.1609887800663473, 0.3695229735872851, 0.19108592531772042], [0.1616922572963582, 0.3732731852340314, 0.19245387177272855], [0.1623930319654953, 0.3770320573640352, 0.19382783528634243], [0.1630921164008896, 0.38079950130898077, 0.1952084381882439], [0.16379056487278745, 0.38457542569203246, 0.19659631490050877], [0.16448947429132857, 0.38835973642596816, 0.19799211193690508], [0.16518998482987113, 0.39215233671127353, 0.19939648788756642], [0.1658932804655635, 0.3959531270342421, 0.20081011338847304], [0.166600589427405, 0.39976200516512433, 0.20223367107520046], [0.16731318454171665, 0.4035788661563645, 0.20366785552039823], [0.1680323834645103, 0.4074036023409737, 0.20511337315448636], [0.16875954879008526, 0.4112361033310768, 0.20657094216908256], [0.16949608802490335, 0.4150762560166792, 0.20804129240268987], [0.17024345341575386, 0.4189239445646968, 0.20952516520821696], [0.17100314162122876, 0.42277905041829333, 0.21102331330192792], [0.17177669321562006, 0.4266414522965662, 0.21253650059345497], [0.1725656920146934, 0.43051102619463094, 0.21406550199656194], [0.17337176421315095, 0.43438764538414765, 0.2156111032203738], [0.174196577324217, 0.4382711804143329, 0.21717410054084768], [0.1750418389125363, 0.4421614991135102, 0.21875530055231346], [0.1759092951125111, 0.44605846659124265, 0.22035551989895852], [0.17680072892538157, 0.4499619452410963, 0.22197558498620257], [0.17771795828963766, 0.4538717947440886, 0.2236163316719602], [0.178662833920936, 0.45778787207287114, 0.22527860493786028], [0.1796372369194738, 0.46171003149669404, 0.22696325854055394], [0.18064307614457878, 0.46563812458721304, 0.22867115464331578], [0.18168228535855538, 0.4695720002251926, 0.23040316342821293], [0.1827568201439772, 0.47351150460815455, 0.23216016268918455], [0.1838686546011176, 0.477456481259039, 0.233943037406457], [0.18501977783468404, 0.48140677103593116, 0.23575267930278093], [0.18621219024170063, 0.4853622121429166, 0.23758998638206003], [0.18744789961499414, 0.4893226401421262, 0.23945586245100942], [0.1887289170794073, 0.49328788796703654, 0.24135121662454895], [0.19005725288048025, 0.4972577859370923, 0.24327696281571337], [0.19143491204781923, 0.5012321617737128, 0.24523401921092142], [0.19286388995773657, 0.505210840617762, 0.2472233077315104], [0.1943461678218979, 0.509193645048545, 0.24924575348250605], [0.1958837081305836, 0.5131803951044065, 0.2513022841896479], [0.1974784500806806, 0.5171709083050143, 0.25339382962574203], [0.1991323050198725, 0.5211649996753929, 0.2555213210274589], [0.20084715193916947, 0.5251624817718009, 0.2576856905037352], [0.20262483304633028, 0.5291631647095255, 0.2598878704369637], [0.204467149452764, 0.5331668561926818, 0.26212879287818813], [0.206375857005755, 0.5371733615461064, 0.2644093889375338], [0.20835266229704946, 0.5411824837494301, 0.26673058817112216], [0.2103992188771397, 0.5451940234734288, 0.26909331796570707], [0.21251712370282538, 0.5492077791187413, 0.2714985029222856], [0.21470791384316384, 0.5532235468570559, 0.2739470642399031], [0.21697306346622774, 0.5572411206748591, 0.27643991910086557], [0.21931398112597572, 0.5612602924198622, 0.2789779800585389], [0.22173200736531357, 0.5652808518501962, 0.28156215442887567], [0.22422841264772203, 0.5693025866864954, 0.28419334368676863], [0.2268043956263523, 0.5733252826669735, 0.2868724428682829], [0.22946108175552465, 0.5773487236056106, 0.28960033997974594], [0.23219952224604845, 0.5813726914535655, 0.2923779154146281], [0.23502069336196157, 0.5853969663639336, 0.29520604137906364], [0.23792549605282798, 0.5894213267599773, 0.2980855813267848], [0.24091475591248918, 0.5934455494069475, 0.30101738940417244], [0.2439892234519782, 0.5974694094876285, 0.3040023099060248], [0.2471495746716238, 0.6014926806817401, 0.3070411767425731], [0.2503964119149868, 0.6055151352493269, 0.31013481291816875], [0.25373026498509416, 0.6095365441182762, 0.3132840300219835], [0.25715159250188796, 0.6135566769761028, 0.3164896277309624], [0.26066078347841287, 0.6175753023661407, 0.31975239332517896], [0.26425815909238537, 0.6215921877882963, 0.3230731012156454], [0.26794397462927927, 0.6256070998045035, 0.32645251248454027], [0.27171842157278525, 0.6296198041490337, 0.32989137443772115], [0.2755816298187335, 0.6336300658438219, 0.3333904201693042], [0.27953366998896156, 0.6376376493189533, 0.3369503681380034], [0.2835745558223273, 0.6416423185384807, 0.34057192175484835], [0.2877042466209889, 0.6456438371317278, 0.3442557689818123], [0.2919226497312409, 0.6496419685302498, 0.34800258194081274], [0.2962296230395252, 0.6536364761106029, 0.3518130165324921], [0.30062497746549316, 0.6576271233431101, 0.3556877120641009], [0.3051084794357496, 0.6616136739467778, 0.3596272908857725], [0.3096798533232689, 0.6655958920505426, 0.3636323580344192], [0.3143387838391119, 0.6695735423610237, 0.3677035008844355], [0.3190849183647877, 0.6735463903369483, 0.37184128880436657], [0.3239178692149385, 0.677514202370436, 0.37604627281865705], [0.3288372158217989, 0.6814767459753105, 0.38031898527358976], [0.33384250683409117, 0.6854337899826277, 0.3846599395064855], [0.33893326212463143, 0.68938510474359, 0.38906962951724855], [0.344108974702066, 0.6933304623400306, 0.39354852964131404], [0.34936911252340047, 0.697269636802651, 0.398097094223074], [0.354713120205116, 0.7012024043371861, 0.40271575728885467], [0.3601404206316701, 0.7051285435586824, 0.407404932218529], [0.365650416461051, 0.7090478357340677, 0.4121650114148717], [0.3712424915278934, 0.7129600650331871, 0.4169963659697734], [0.3769160121453231, 0.7168650187884974, 0.4218993453264664], [0.3826703283074528, 0.7207624877635708, 0.426874276936929], [0.38850477479467865, 0.724652266430624, 0.4319214659136745], [0.39441867218475135, 0.7285341532572063, 0.4370411946751511], [0.4004113277726398, 0.7324079510022498, 0.4422337225840276], [0.40648203640264285, 0.7362734670216392, 0.4474992855776477], [0.41263008121642264, 0.7401305135834749, 0.4528380957899934], [0.41885473432075093, 0.7439789081931996, 0.4582503411645124], [0.42515525737890464, 0.74781847392875, 0.4637361850571962], [0.4315309021296915, 0.7516490397859019, 0.4692957658293329], [0.4379809108381271, 0.755470441033972, 0.4749291964293609], [0.44450451668174257, 0.7592825195820302, 0.4806365639632952], [0.4511009440764859, 0.7630851243557921, 0.48641792925318267], [0.4577694089460426, 0.766878111685348, 0.49227332638307436], [0.46450911893842234, 0.7706613457038759, 0.49820276223198207], [0.47131927359337594, 0.7744346987575222, 0.5042062159932884], [0.47819906446422616, 0.7781980518265826, 0.5102836386800519], [0.4851476751974689, 0.7819512949581628, 0.5164349526156181], [0.4921642815733072, 0.7856943277104902, 0.5226600509088999], [0.49924805151023793, 0.7894270596090324, 0.528958796913626], [0.506398145036514, 0.7931494106146145, 0.5353310236707783], [0.5136137142311699, 0.7968613116037284, 0.5417765333333355], [0.5208939031371336, 0.8005627048612254, 0.5482950965723069], [0.528237847648764, 0.8042535445856207, 0.5548864519629064], [0.5356446753758648, 0.8079337974072546, 0.5615503053495099], [0.5431135054862372, 0.8116034429195591, 0.5682863291878316], [0.5506434485283842, 0.8152624742237489, 0.5750941618624987], [0.558233606235997, 0.8189108984872628, 0.5819734069778754], [0.5658830713155681, 0.8225487375163245, 0.588923632619649], [0.5735909272182081, 0.8261760283430954, 0.5959443705842361], [0.5813562478966682, 0.8297928238278868, 0.6030351155725882], [0.5891780975482949, 0.8333991932770318, 0.6101953243443609], [0.5970555303443447, 0.8369952230771135, 0.6174244148277456], [0.6049875901460008, 0.8405810173463196, 0.6247217651794131], [0.6129733102071075, 0.8441566986038703, 0.6320867127880987], [0.6210117128633796, 0.8477224084586106, 0.6395185532141892], [0.629101809207598, 0.8512783083180583, 0.6470165390563735], [0.6372425987500262, 0.8548245801194263, 0.6545798787348152], [0.6454330690629193, 0.8583614270844092, 0.6622077351784434], [0.6536721954076695, 0.861889074499868, 0.6698992244017301], [0.6619589403427637, 0.8654077705269353, 0.6776534139536473], [0.670292253310307, 0.868917787041518, 0.6854693212183214], [0.6786710701982759, 0.8724194205097873, 0.6933459115430326], [0.6870943128752733, 0.8759129929029018, 0.7012820961645896], [0.6955608886937588, 0.8793988526560546, 0.7092767298994272], [0.7040696899570745, 0.8828773756779759, 0.7173286085559195], [0.7126195933446154, 0.8863489664182553, 0.7254364660189113], [0.7212094592884835, 0.8898140590014009, 0.7335989709460631], [0.7298381312936156, 0.8932731184384776, 0.7418147230026438], [0.738504435191736, 0.8967266419295534, 0.7500822485452499], [0.7472071783175026, 0.900175160273203, 0.7583999956446169], [0.755945148592551, 0.9036192394031619, 0.7667663283120103], [0.7647171134997374, 0.9070594820771404, 0.7751795197609306], [0.7735218189254033, 0.9104965297491636, 0.783637744493875], [0.7823579878412776, 0.9139310646651907, 0.7921390689495337], [0.79122431878925, 0.9173638122327802, 0.8006814403748957], [0.8001194841201199, 0.9207955437304958, 0.8092626734934449], [0.8090421279203749, 0.9242270794429251, 0.817880434416763], [0.8179908635354565, 0.9276592923352225, 0.8265322210808196], [0.8269642705600306, 0.9310931124203533, 0.8352153392634705], [0.8359608911072839, 0.934529532028408, 0.8439268729323351], [0.844979225077973, 0.9379696122690698, 0.852663647247446], [0.8540177240040997, 0.9414144910996233, 0.8614221819497985], [0.863074782804233, 0.9448653935946189, 0.8701986320294973], [0.8721487283913724, 0.9483236452977989, 0.87898871137326], [0.8812378033992858, 0.9517906899876957, 0.8877875933734316], [0.890340142117086, 0.955268113920236, 0.8965897799935405], [0.8994537336219597, 0.9587576798305327, 0.9053889271758421], [0.908576363256908, 0.9622613760595616, 0.9141776092709066], [0.9177055163843063, 0.9657814898283584, 0.9229469978386258], [0.9268382144426769, 0.9693207202671751, 0.9316864204851563], [0.9359707258804478, 0.9728823589346071, 0.9403827547526942], [0.9450980392156855, 0.9764705882352952, 0.9490196078431373]]
+
+cmap_grad = LinearSegmentedColormap.from_list("cmap_grad", grad_list)
+
+# # grad_list = ["#000000", "#1A1F16", "#1E3F20", "#294C28", "#345830", "#4A7856", "#6FB28A", "#94ECBE", "#FFFFFF"]
+# grad_cdict = {"red": [], "green": [], "blue": []}
+# cpoints = np.linspace(0, 1, len(grad_list))
+# for i in range(len(grad_list)):
+#     grad_cdict["red"].append(
+#         [cpoints[i], int(grad_list[i][1:3], 16) / 256, int(grad_list[i][1:3], 16) / 256]
+#     )
+#     grad_cdict["green"].append(
+#         [cpoints[i], int(grad_list[i][3:5], 16) / 256, int(grad_list[i][3:5], 16) / 256]
+#     )
+#     grad_cdict["blue"].append(
+#         [cpoints[i], int(grad_list[i][5:7], 16) / 256, int(grad_list[i][5:7], 16) / 256]
+#     )
+# cmap_grad = LinearSegmentedColormap("cmap_grad", grad_cdict)
 
 div_list = [
     "#332A1F",
     "#514129",
     "#7C6527",
     "#A2862A",
     "#DAB944",
```

## Comparing `autoprof-0.6.3.dist-info/LICENSE` & `autoprof-0.7.0.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `autoprof-0.6.3.dist-info/METADATA` & `autoprof-0.7.0.dist-info/METADATA`

 * *Files 5% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: autoprof
-Version: 0.6.3
+Version: 0.7.0
 Summary: A fast, flexible, differentiable, and automated astronomical image modelling tool for precise parallel multi-wavelength photometry
 Home-page: https://github.com/ConnorStoneAstro/AutoProf
 Author: Connor Stone
 Author-email: connorstone628@gmail.com
 License: GPL-3.0 license
 Platform: UNKNOWN
 Classifier: Development Status :: 1 - Planning
@@ -12,14 +12,15 @@
 Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
 Classifier: Programming Language :: Python :: 3
 Description-Content-Type: text/markdown
 Requires-Dist: astropy
 Requires-Dist: h5py
 Requires-Dist: matplotlib
 Requires-Dist: numpy
+Requires-Dist: pyro-ppl
 Requires-Dist: pyyaml
 Requires-Dist: requests
 Requires-Dist: scipy
 Requires-Dist: torch
 Requires-Dist: tqdm
 
 <picture>
```

## Comparing `autoprof-0.6.3.dist-info/RECORD` & `autoprof-0.7.0.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,40 +1,41 @@
 autoprof/AP_config.py,sha256=fIxYQr2lxsjA3ONiSLuKMAKmL2BPM7Xz99L3gGab8AQ,3171
-autoprof/__init__.py,sha256=9mEKgo0IFTUS7QA0AdljQ-Vl1owMpttHHJqMUvwfjR4,5650
+autoprof/__init__.py,sha256=1OR5y4Er-gdnp0JnyBkOq-r5qAfSo3s6ILT-jC0mdiU,5755
 autoprof/__main__.py,sha256=Ce_QoW2_-x3o3ZXbErfBFzJ_M20JzoaB3dUvwzAhXx4,281
-autoprof/fit/__init__.py,sha256=uXsyhfhD3BguZRE0BrDexHqYU2FlGbBg9sLWgH4CSSM,1072
-autoprof/fit/base.py,sha256=xaA0lqrVvM7xoiSWYPPA2laz1VfpXaXLifM1iS5qbvE,6689
+autoprof/fit/__init__.py,sha256=vPVo-YP7Ns-Y9hsAI_riY93kTtIi1FD5yw6Mdw3bQUY,1092
+autoprof/fit/base.py,sha256=C_FhJa8dwD631vgD_Tansqn_E0ZNmA75KBu7bmT1DqQ,6698
 autoprof/fit/gp.py,sha256=PvMC6LeAIYWwDteVVo3AY7lb_TkQOY2-C5yWfK4CpUY,30
 autoprof/fit/gradient.py,sha256=N1RzlOkWjo0yoNxD483v6EVFtvNODiypLkCy7WoeEQw,6704
-autoprof/fit/hmc.py,sha256=LjvpNIdEfWQ9eSlqjxLuS2c7oT3ZlqhPVS5el34haB0,9877
+autoprof/fit/hmc.py,sha256=HX11L2YANm-Oyo4gqceMCPuy5zPb7acpK1gYvKINSjs,4884
 autoprof/fit/iterative.py,sha256=5Jaa5ks3gsr0u1stvMNSZbCpJ71Q4E1RGhVV8SGWnJA,13206
-autoprof/fit/lm.py,sha256=bv0B8RC8ePitca99wFCZji6EIh4oGBmmHli_jg5rCGA,29711
-autoprof/fit/mhmcmc.py,sha256=jwdU4lmte10YNyWexhhaShYIjavQSFrU-kwwvM2bCcg,4546
+autoprof/fit/lm.py,sha256=kNPkGjTv9izC5oMAV56EMkBP7vHf8XyYqhUIJUK-x4k,31364
+autoprof/fit/mhmcmc.py,sha256=-3f_LfXFzB1KivEhHqdfUT38CpLw5JE_7_6aKWNjOJY,4317
+autoprof/fit/nuts.py,sha256=Ahq4uFryFXc1KqJgwIj1EwiNdRVqV6tf18Sa-CrQuLY,5296
 autoprof/image/__init__.py,sha256=ksiEV0RXh6C7szKH1TQQm6kAWG_S2fx4-4NqAmAyMLc,1125
 autoprof/image/image_header.py,sha256=EZc6YXArsirO5TIYOHzAcQXQseINYGmQBK6RbU637cI,11195
 autoprof/image/image_object.py,sha256=qqZ6hKfp3c0et4om-AmagH7Np4vZmjxaROJ_ysq81z8,21644
 autoprof/image/jacobian_image.py,sha256=1s3eAq3xB353i_srThMZVaJ7Wp-aXu0POLfvZ1JQSgg,5269
 autoprof/image/model_image.py,sha256=kcGG1_d-mYMUmDsYcJ9bTA7XNecS1rbt4dkRbcPNoLk,6615
 autoprof/image/target_image.py,sha256=y-_69PPjh2kKO5C9MoF4F3GeJdRtEekhblb_4U_Bd18,15842
 autoprof/image/window_object.py,sha256=DbOnZQmbYMQKB1T_Z-Fpc6PdEENFIGxy7QSGLY0s4jI,22836
 autoprof/models/__init__.py,sha256=V5ziTIawNay5GoqHIz-x1rnIOZqARe5VmMyFPCWLOR8,654
 autoprof/models/_model_methods.py,sha256=rZh4fuVedfFmzaLJsrdZ_a4WG96JTk9E76SX8Pe0Ues,4277
 autoprof/models/_shared_methods.py,sha256=DPuQmdddnpnWIpRrGoh2eB2sxoa3pwI1U_YABxCogJo,20263
-autoprof/models/core_model.py,sha256=GB8zXtIEP_IVLFwmBWcpoRhfi8qQJ1sttEAIOYBONQk,18936
+autoprof/models/core_model.py,sha256=ckV_Xhg2OVlBIPGJ7cN3YuLHjhemkmQ5rujPONEcHaQ,21176
 autoprof/models/edgeon_model.py,sha256=N7tz7U0cV1jzX7RUgAGKngVFvm73aPJEO-0PCgEN05U,5837
 autoprof/models/exponential_model.py,sha256=P3H5OYQAQSdkqiX3QRosLZ8nBSKXmhupxyLO9L_PnZc,12642
 autoprof/models/flatsky_model.py,sha256=8WJ6YRTzepIO11S_Rd6OaL8XO1BopwNcgt5nT8UdSEU,1945
 autoprof/models/foureirellipse_model.py,sha256=SSbySZcb_GWQtdm1valPRVq41m8tvthDKKlt-vWw7BU,8266
 autoprof/models/galaxy_model_object.py,sha256=ZzqxORKqjkhqiYk_WopuaSe8RiSZOFBiE2kRH9F6sa4,4825
 autoprof/models/gaussian_model.py,sha256=ZYdjl1C5oOK5DVUFie57MNc8PJxB1q2Qala_W7ja_YU,11660
 autoprof/models/group_model_object.py,sha256=i5WwS6KZwFAYBwdYa2mebMvuJxXfeee9LAlV_mCIMyI,14753
-autoprof/models/model_object.py,sha256=P_Et84Dpnen4JiD52qjKrre3VMFBqZYZ9vSX5YnZCZ8,25820
+autoprof/models/model_object.py,sha256=v8PWY3tSeG2UYHXeTiApr_F2SMITHizn4aQyEqo-2Ck,25839
 autoprof/models/moffat_model.py,sha256=meLcfV_DFt39ESSP1P3dner_2ZjZZBb0Eq-Vi29f2yo,3380
 autoprof/models/nuker_model.py,sha256=KWx8lFVWhguLkEybcOAV3nMCBsiKOCdYGtNj7JzhYgU,17123
-autoprof/models/parameter_object.py,sha256=9kzWwOlzyvtkm0kf8rTyeN441xdnufJnJT4QMEWKHSE,17262
+autoprof/models/parameter_object.py,sha256=7IxvTaTbVl8B6nMsJOdwIYVNUMS6vQpAEz0Hpx9zO5E,17694
 autoprof/models/planesky_model.py,sha256=w2pQ25uvtgnWxydt5jMSX_wwMNysETPUOIfTeNVwc5M,2212
 autoprof/models/psf_model.py,sha256=EjdYEFzv1Tx82f0bdYTHf5NjV7IYYSmRC554UlF66Jc,3439
 autoprof/models/ray_model.py,sha256=7h9zfKopqiTbTEkyXh7FVC0SXRshriHv3tpNd_FtW_U,4600
 autoprof/models/sersic_model.py,sha256=atmSKyTJ_3b_K9lvuQoDYoLsjMn4d2JfPn1Ng5TrnMc,14516
 autoprof/models/sky_model_object.py,sha256=WD450x05pnwMDOzDo5yhQfqYXAHgE8QPkJLVyKJeFGQ,875
 autoprof/models/spline_model.py,sha256=SjZOm8tKIBWY2alXsgrBAOJN5vQQgIJ7YfBOgdsJJ6o,10193
 autoprof/models/star_model_object.py,sha256=kc57eQy02n02zCJKKqLDXoqoa0ZqgrzBONLpJUgRKZ4,1125
@@ -42,18 +43,18 @@
 autoprof/models/warp_model.py,sha256=5r_RniJvEDQFVm60FtXQsUucHIeBOY6AG3RXjof_oxo,4332
 autoprof/models/wedge_model.py,sha256=VKOs0mEYm3WUB-xj_CePEWDxZVj63hxmrKVmiqHtVqw,3546
 autoprof/parse_config/__init__.py,sha256=CT6gEcILfcEdz3I5nbgqKeno8tyGZsMtHmfXQUWejUA,57
 autoprof/parse_config/basic_config.py,sha256=amQEkpKoDp04RVYXNrO4lO0j4T_G1BK7MaLmemtaN-4,4147
 autoprof/parse_config/galfit_config.py,sha256=0HqkIJwgv3XKiW2EZAidXqneDQRaOkgfMxNksxKwuBY,4590
 autoprof/parse_config/shared_methods.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autoprof/plots/__init__.py,sha256=p8lf0eFHXeZ7-3s0SHRb3zIH1pR6x_jnMxIPUHaEPls,67
-autoprof/plots/image.py,sha256=0VqC5E619hIb1lZCzD4mZTXJO3cWo11hX-4Up5kmCro,6535
+autoprof/plots/image.py,sha256=ApzbOLJQWaHm8PAinkvuYF0Yg7UJG6Swx1iMdxaLO9I,6857
 autoprof/plots/profile.py,sha256=zyyFTrW-u44d9xt4bX0euAXz765KU7XGZfrmQvgYC8I,7559
 autoprof/plots/shared_elements.py,sha256=-9cuoMXOSZh7uB32Fkn2R5wCFecNU3WIGvVrjmP8ESY,3048
-autoprof/plots/visuals.py,sha256=v_DWht98km2HVHpwWBwNGHNW4Jc3_sHCWXhHhOZUKGU,3527
+autoprof/plots/visuals.py,sha256=NQPn6pQPxDUhETndE6AaXpeRCwl-dzuKuThaSlvH7Vs,19994
 autoprof/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autoprof/utils/angle_operations.py,sha256=oJp9g9v5v7SGzuDRXILg7B0d2CcXygT3GXAzoUbtPGk,800
 autoprof/utils/interpolate.py,sha256=LCI4utAYOay8IsNQIpq0FHhj6MUpigOYii5cEnEhADA,8609
 autoprof/utils/operations.py,sha256=LLO8LB02L3sJMsmlKiH3XS2m4dZBFxwq2aN8o8v0evs,6369
 autoprof/utils/optimization.py,sha256=VkAusKnyc4zyztbceazKADVy85g6VV7qqYIw6V7cQos,963
 autoprof/utils/parametric_profiles.py,sha256=JY6reeVkATmuPkhKY7cWPgrnpZaM--simb-8mS2xh1A,5990
 autoprof/utils/conversions/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
@@ -67,13 +68,13 @@
 autoprof/utils/initialize/construct_psf.py,sha256=DaIQhQOn9cqn7q_NGIK8zWXNL1WV4n2hiSTeDOlBIao,3296
 autoprof/utils/initialize/initialize.py,sha256=V1Epy8vudmquOxSB5S-moCcS7bvjdr5qze6IcqihsSs,3921
 autoprof/utils/initialize/segmentation_map.py,sha256=6fG5U4y5P6MDnX4_6ROBdRhUq6TxgbwVG1V3MaLf7-c,7036
 autoprof/utils/isophote/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 autoprof/utils/isophote/ellipse.py,sha256=p2SGzU067jBIBrKjhYKpnJQF7MSThMcnLajsaXeJ23k,1085
 autoprof/utils/isophote/extract.py,sha256=ousarHmkj7GrgAZ6mO3G-QY0tXjhd8bBh6lHzb--d6U,8531
 autoprof/utils/isophote/integrate.py,sha256=jNOCbSYC1dZO1pasEMcRUqZCpt43DEPVME4Hsa37DKQ,7012
-autoprof-0.6.3.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
-autoprof-0.6.3.dist-info/METADATA,sha256=PSpyI8O6WEiPRiq-W_pVZ9mJJ-mjYoDp6ZwEONFgPbQ,3788
-autoprof-0.6.3.dist-info/WHEEL,sha256=kGT74LWyRUZrL4VgLh6_g12IeVl_9u9ZVhadrgXZUEY,110
-autoprof-0.6.3.dist-info/entry_points.txt,sha256=CJw03tyO_XyE5_-xxRzbAg74ITHASY47DhUiRVsn67s,57
-autoprof-0.6.3.dist-info/top_level.txt,sha256=8N1I5eyEKnh1QOUENsiy7fDvtU-sfyRCYsUJPzrvPvc,9
-autoprof-0.6.3.dist-info/RECORD,,
+autoprof-0.7.0.dist-info/LICENSE,sha256=OXLcl0T2SZ8Pmy2_dmlvKuetivmyPd5m1q-Gyd-zaYY,35149
+autoprof-0.7.0.dist-info/METADATA,sha256=EqzXeEQgLzd0XQcApSWz0xfEhZchDYplC61btcBmHhk,3812
+autoprof-0.7.0.dist-info/WHEEL,sha256=kGT74LWyRUZrL4VgLh6_g12IeVl_9u9ZVhadrgXZUEY,110
+autoprof-0.7.0.dist-info/entry_points.txt,sha256=CJw03tyO_XyE5_-xxRzbAg74ITHASY47DhUiRVsn67s,57
+autoprof-0.7.0.dist-info/top_level.txt,sha256=8N1I5eyEKnh1QOUENsiy7fDvtU-sfyRCYsUJPzrvPvc,9
+autoprof-0.7.0.dist-info/RECORD,,
```

