# Comparing `tmp/dtlpy-1.75.8-py3-none-any.whl.zip` & `tmp/dtlpy-1.76.15-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 538789 bytes, number of entries: 239
--rw-rw-rw-  2.0 fat    20170 b- defN 23-Apr-04 16:44 dtlpy/__init__.py
--rw-rw-rw-  2.0 fat       20 b- defN 23-Apr-04 17:20 dtlpy/__version__.py
+Zip file size: 542050 bytes, number of entries: 240
+-rw-rw-rw-  2.0 fat    20229 b- defN 23-May-01 13:57 dtlpy/__init__.py
+-rw-rw-rw-  2.0 fat       21 b- defN 23-May-02 09:06 dtlpy/__version__.py
 -rw-rw-rw-  2.0 fat     2996 b- defN 22-Jun-14 12:29 dtlpy/exceptions.py
 -rw-rw-rw-  2.0 fat     5843 b- defN 23-Mar-20 13:48 dtlpy/new_instance.py
 -rw-rw-rw-  2.0 fat     1002 b- defN 23-Jan-25 12:50 dtlpy/assets/__init__.py
 -rw-rw-rw-  2.0 fat    24081 b- defN 21-Apr-13 14:29 dtlpy/assets/lock_open.png
--rw-rw-rw-  2.0 fat     1433 b- defN 22-Jun-14 12:29 dtlpy/assets/main.py
+-rw-rw-rw-  2.0 fat     1433 b- defN 23-Apr-09 15:56 dtlpy/assets/main.py
 -rw-rw-rw-  2.0 fat      285 b- defN 22-Jun-14 12:29 dtlpy/assets/main_partial.py
--rw-rw-rw-  2.0 fat      160 b- defN 22-Jun-14 12:29 dtlpy/assets/mock.json
+-rw-rw-rw-  2.0 fat      160 b- defN 23-Apr-09 15:56 dtlpy/assets/mock.json
 -rw-rw-rw-  2.0 fat     3251 b- defN 22-Nov-15 22:39 dtlpy/assets/model_adapter.py
 -rw-rw-rw-  2.0 fat     1320 b- defN 22-Jun-14 12:29 dtlpy/assets/package.json
 -rw-rw-rw-  2.0 fat      597 b- defN 21-Apr-13 14:29 dtlpy/assets/package_catalog.json
 -rw-rw-rw-  2.0 fat     4712 b- defN 21-Apr-13 14:29 dtlpy/assets/package_gitignore
 -rw-rw-rw-  2.0 fat    10807 b- defN 21-Apr-13 14:29 dtlpy/assets/project_dataset_recipe_ontology.png
 -rw-rw-rw-  2.0 fat      765 b- defN 22-Nov-30 15:08 dtlpy/assets/voc_annotation_template.xml
 -rw-rw-rw-  2.0 fat       50 b- defN 21-May-25 09:53 dtlpy/assets/.idea/.gitignore
@@ -20,14 +20,15 @@
 -rw-rw-rw-  2.0 fat      271 b- defN 21-May-25 09:53 dtlpy/assets/.idea/modules.xml
 -rw-rw-rw-  2.0 fat      191 b- defN 21-May-25 09:53 dtlpy/assets/.idea/vcs.xml
 -rw-rw-rw-  2.0 fat     1793 b- defN 21-May-25 09:53 dtlpy/assets/.idea/workspace.xml
 -rw-rw-rw-  2.0 fat     1112 b- defN 23-Feb-27 16:46 dtlpy/assets/__pycache__/__init__.cpython-310.pyc
 -rw-rw-rw-  2.0 fat     1702 b- defN 23-Mar-23 14:24 dtlpy/assets/__pycache__/__init__.cpython-311.pyc
 -rw-rw-rw-  2.0 fat     1100 b- defN 23-Jan-25 18:10 dtlpy/assets/__pycache__/__init__.cpython-37.pyc
 -rw-rw-rw-  2.0 fat     1068 b- defN 22-Jul-14 16:38 dtlpy/assets/__pycache__/__init__.cpython-39.pyc
+-rw-rw-rw-  2.0 fat     2475 b- defN 23-Apr-09 15:55 dtlpy/assets/__pycache__/main.cpython-311.pyc
 -rw-rw-rw-  2.0 fat     1719 b- defN 22-Jun-09 09:46 dtlpy/assets/__pycache__/main.cpython-37.pyc
 -rw-rw-rw-  2.0 fat      648 b- defN 22-Jun-09 09:46 dtlpy/assets/__pycache__/main_partial.cpython-37.pyc
 -rw-rw-rw-  2.0 fat     6376 b- defN 22-Jun-09 09:46 dtlpy/assets/__pycache__/model_adapter.cpython-37.pyc
 -rw-rw-rw-  2.0 fat       50 b- defN 23-Apr-04 17:16 dtlpy/assets/code_server/config.yaml
 -rw-rw-rw-  2.0 fat      862 b- defN 23-Apr-04 17:16 dtlpy/assets/code_server/installation.sh
 -rw-rw-rw-  2.0 fat      344 b- defN 23-Jan-25 12:50 dtlpy/assets/code_server/launch.json
 -rw-rw-rw-  2.0 fat       66 b- defN 23-Jan-25 12:50 dtlpy/assets/code_server/settings.json
@@ -56,39 +57,39 @@
 -rw-rw-rw-  2.0 fat      888 b- defN 21-May-24 14:31 dtlpy/dlp/__init__.py
 -rw-rw-rw-  2.0 fat    16404 b- defN 22-Jun-14 12:29 dtlpy/dlp/cli_utilities.py
 -rw-rw-rw-  2.0 fat    32280 b- defN 23-Jan-25 12:50 dtlpy/dlp/command_executor.py
 -rw-rw-rw-  2.0 fat       11 b- defN 21-May-24 14:31 dtlpy/dlp/dlp
 -rw-rw-rw-  2.0 fat       38 b- defN 21-May-24 14:31 dtlpy/dlp/dlp.bat
 -rw-rw-rw-  2.0 fat     4406 b- defN 21-May-24 14:31 dtlpy/dlp/dlp.py
 -rw-rw-rw-  2.0 fat    31202 b- defN 23-Jan-25 12:50 dtlpy/dlp/parser.py
--rw-rw-rw-  2.0 fat     4335 b- defN 23-Apr-04 16:44 dtlpy/entities/__init__.py
+-rw-rw-rw-  2.0 fat     4353 b- defN 23-May-01 13:57 dtlpy/entities/__init__.py
 -rw-rw-rw-  2.0 fat    11665 b- defN 22-Nov-15 22:39 dtlpy/entities/analytic.py
--rw-rw-rw-  2.0 fat    67925 b- defN 23-Apr-04 17:16 dtlpy/entities/annotation.py
--rw-rw-rw-  2.0 fat    30245 b- defN 23-Apr-04 17:20 dtlpy/entities/annotation_collection.py
+-rw-rw-rw-  2.0 fat    68750 b- defN 23-May-01 13:57 dtlpy/entities/annotation.py
+-rw-rw-rw-  2.0 fat    30099 b- defN 23-May-01 13:03 dtlpy/entities/annotation_collection.py
 -rw-rw-rw-  2.0 fat     4999 b- defN 23-Mar-20 13:48 dtlpy/entities/app.py
 -rw-rw-rw-  2.0 fat     5885 b- defN 22-Nov-15 22:39 dtlpy/entities/artifact.py
 -rw-rw-rw-  2.0 fat    14739 b- defN 22-Dec-12 10:20 dtlpy/entities/assignment.py
 -rw-rw-rw-  2.0 fat     7642 b- defN 23-Jan-25 12:50 dtlpy/entities/base_entity.py
 -rw-rw-rw-  2.0 fat     3932 b- defN 22-Jun-14 12:29 dtlpy/entities/bot.py
 -rw-rw-rw-  2.0 fat     9295 b- defN 23-Apr-04 17:16 dtlpy/entities/codebase.py
 -rw-rw-rw-  2.0 fat     5138 b- defN 23-Mar-20 13:48 dtlpy/entities/command.py
 -rw-rw-rw-  2.0 fat    45150 b- defN 23-Mar-20 13:48 dtlpy/entities/dataset.py
 -rw-rw-rw-  2.0 fat     1230 b- defN 22-Jun-14 12:29 dtlpy/entities/directory_tree.py
--rw-rw-rw-  2.0 fat    10737 b- defN 23-Mar-20 13:48 dtlpy/entities/dpk.py
--rw-rw-rw-  2.0 fat     7032 b- defN 23-Apr-04 16:44 dtlpy/entities/driver.py
+-rw-rw-rw-  2.0 fat    11002 b- defN 23-May-01 13:57 dtlpy/entities/dpk.py
+-rw-rw-rw-  2.0 fat     7419 b- defN 23-May-01 13:57 dtlpy/entities/driver.py
 -rw-rw-rw-  2.0 fat    12747 b- defN 23-Mar-20 13:48 dtlpy/entities/execution.py
 -rw-rw-rw-  2.0 fat     4518 b- defN 23-Mar-20 13:48 dtlpy/entities/feature.py
--rw-rw-rw-  2.0 fat     4530 b- defN 23-Mar-20 13:48 dtlpy/entities/feature_set.py
--rw-rw-rw-  2.0 fat    18869 b- defN 23-Apr-04 14:37 dtlpy/entities/filters.py
--rw-rw-rw-  2.0 fat     3621 b- defN 23-Apr-04 16:44 dtlpy/entities/integration.py
+-rw-rw-rw-  2.0 fat     4736 b- defN 23-May-01 13:03 dtlpy/entities/feature_set.py
+-rw-rw-rw-  2.0 fat    18988 b- defN 23-May-01 13:03 dtlpy/entities/filters.py
+-rw-rw-rw-  2.0 fat     5535 b- defN 23-May-01 13:57 dtlpy/entities/integration.py
 -rw-rw-rw-  2.0 fat    28708 b- defN 23-Mar-20 13:48 dtlpy/entities/item.py
 -rw-rw-rw-  2.0 fat     3992 b- defN 23-Jan-25 12:50 dtlpy/entities/label.py
 -rw-rw-rw-  2.0 fat     2601 b- defN 22-Jun-14 12:29 dtlpy/entities/links.py
--rw-rw-rw-  2.0 fat    17554 b- defN 23-Apr-04 17:14 dtlpy/entities/model.py
--rw-rw-rw-  2.0 fat    35390 b- defN 23-Mar-20 13:48 dtlpy/entities/node.py
+-rw-rw-rw-  2.0 fat    18684 b- defN 23-May-01 13:57 dtlpy/entities/model.py
+-rw-rw-rw-  2.0 fat    35459 b- defN 23-May-01 14:18 dtlpy/entities/node.py
 -rw-rw-rw-  2.0 fat    30018 b- defN 23-Mar-20 13:48 dtlpy/entities/ontology.py
 -rw-rw-rw-  2.0 fat    10180 b- defN 23-Mar-20 13:48 dtlpy/entities/organization.py
 -rw-rw-rw-  2.0 fat    26681 b- defN 23-Mar-20 13:48 dtlpy/entities/package.py
 -rw-rw-rw-  2.0 fat      216 b- defN 21-Apr-13 14:29 dtlpy/entities/package_defaults.py
 -rw-rw-rw-  2.0 fat     5947 b- defN 23-Jan-25 12:50 dtlpy/entities/package_function.py
 -rw-rw-rw-  2.0 fat     4134 b- defN 23-Jan-25 12:50 dtlpy/entities/package_module.py
 -rw-rw-rw-  2.0 fat     5867 b- defN 23-Jan-25 12:50 dtlpy/entities/package_slot.py
@@ -96,24 +97,24 @@
 -rw-rw-rw-  2.0 fat    18072 b- defN 23-Mar-20 13:48 dtlpy/entities/pipeline.py
 -rw-rw-rw-  2.0 fat     7417 b- defN 23-Apr-04 17:16 dtlpy/entities/pipeline_execution.py
 -rw-rw-rw-  2.0 fat    14686 b- defN 23-Apr-04 17:16 dtlpy/entities/project.py
 -rw-rw-rw-  2.0 fat     9793 b- defN 23-Mar-20 13:48 dtlpy/entities/recipe.py
 -rw-rw-rw-  2.0 fat     3375 b- defN 22-Oct-23 10:24 dtlpy/entities/reflect_dict.py
 -rw-rw-rw-  2.0 fat     5171 b- defN 23-Mar-20 13:48 dtlpy/entities/resource_execution.py
 -rw-rw-rw-  2.0 fat    28019 b- defN 23-Mar-20 13:48 dtlpy/entities/service.py
--rw-rw-rw-  2.0 fat     8911 b- defN 23-Jan-25 12:50 dtlpy/entities/setting.py
+-rw-rw-rw-  2.0 fat     8759 b- defN 23-May-01 13:57 dtlpy/entities/setting.py
 -rw-rw-rw-  2.0 fat     6318 b- defN 22-Jun-14 12:29 dtlpy/entities/similarity.py
--rw-rw-rw-  2.0 fat    19245 b- defN 22-Dec-12 10:20 dtlpy/entities/task.py
+-rw-rw-rw-  2.0 fat    19243 b- defN 23-May-01 13:03 dtlpy/entities/task.py
 -rw-rw-rw-  2.0 fat     4160 b- defN 22-Jun-14 12:29 dtlpy/entities/time_series.py
 -rw-rw-rw-  2.0 fat    14404 b- defN 23-Mar-20 13:48 dtlpy/entities/trigger.py
 -rw-rw-rw-  2.0 fat     3983 b- defN 22-Oct-23 10:23 dtlpy/entities/user.py
 -rw-rw-rw-  2.0 fat     3663 b- defN 23-Mar-20 13:48 dtlpy/entities/webhook.py
 -rw-rw-rw-  2.0 fat      589 b- defN 22-Nov-28 14:18 dtlpy/entities/annotation_definitions/__init__.py
 -rw-rw-rw-  2.0 fat     2291 b- defN 22-Oct-23 10:23 dtlpy/entities/annotation_definitions/base_annotation_definition.py
--rw-rw-rw-  2.0 fat     9313 b- defN 22-Oct-23 10:23 dtlpy/entities/annotation_definitions/box.py
+-rw-rw-rw-  2.0 fat     8874 b- defN 23-May-01 13:57 dtlpy/entities/annotation_definitions/box.py
 -rw-rw-rw-  2.0 fat     1626 b- defN 22-Jun-14 12:29 dtlpy/entities/annotation_definitions/classification.py
 -rw-rw-rw-  2.0 fat     1878 b- defN 22-Jun-14 12:29 dtlpy/entities/annotation_definitions/comparison.py
 -rw-rw-rw-  2.0 fat     8758 b- defN 22-Jun-14 12:29 dtlpy/entities/annotation_definitions/cube.py
 -rw-rw-rw-  2.0 fat     5954 b- defN 22-Jun-14 12:29 dtlpy/entities/annotation_definitions/cube_3d.py
 -rw-rw-rw-  2.0 fat      951 b- defN 22-Jun-14 12:29 dtlpy/entities/annotation_definitions/description.py
 -rw-rw-rw-  2.0 fat     4178 b- defN 22-Jun-14 12:29 dtlpy/entities/annotation_definitions/ellipse.py
 -rw-rw-rw-  2.0 fat     4244 b- defN 23-Mar-20 13:48 dtlpy/entities/annotation_definitions/note.py
@@ -152,54 +153,54 @@
 -rw-rw-rw-  2.0 fat      849 b- defN 22-Nov-15 22:39 dtlpy/miscellaneous/__init__.py
 -rw-rw-rw-  2.0 fat     3584 b- defN 22-Oct-23 10:23 dtlpy/miscellaneous/dict_differ.py
 -rw-rw-rw-  2.0 fat     8188 b- defN 23-Apr-04 17:16 dtlpy/miscellaneous/git_utils.py
 -rw-rw-rw-  2.0 fat      442 b- defN 22-Nov-15 22:39 dtlpy/miscellaneous/json_utils.py
 -rw-rw-rw-  2.0 fat     4913 b- defN 22-Jun-14 12:29 dtlpy/miscellaneous/list_print.py
 -rw-rw-rw-  2.0 fat     5245 b- defN 22-Nov-17 23:38 dtlpy/miscellaneous/zipping.py
 -rw-rw-rw-  2.0 fat      819 b- defN 22-Nov-15 22:39 dtlpy/ml/__init__.py
--rw-rw-rw-  2.0 fat    35225 b- defN 23-Apr-04 17:14 dtlpy/ml/base_model_adapter.py
+-rw-rw-rw-  2.0 fat    35227 b- defN 23-May-01 13:57 dtlpy/ml/base_model_adapter.py
 -rw-rw-rw-  2.0 fat    20498 b- defN 23-Apr-04 17:16 dtlpy/ml/metrics.py
 -rw-rw-rw-  2.0 fat    12758 b- defN 23-Apr-04 17:16 dtlpy/ml/predictions_utils.py
 -rw-rw-rw-  2.0 fat     2841 b- defN 22-Oct-23 10:23 dtlpy/ml/summary_writer.py
--rw-rw-rw-  2.0 fat     3545 b- defN 23-Apr-04 17:14 dtlpy/ml/train_utils.py
--rw-rw-rw-  2.0 fat     1893 b- defN 22-Nov-15 22:39 dtlpy/repositories/__init__.py
+-rw-rw-rw-  2.0 fat     2505 b- defN 23-May-01 13:57 dtlpy/ml/train_utils.py
+-rw-rw-rw-  2.0 fat     1902 b- defN 23-May-01 13:03 dtlpy/repositories/__init__.py
 -rw-rw-rw-  2.0 fat     3051 b- defN 23-Mar-20 13:48 dtlpy/repositories/analytics.py
 -rw-rw-rw-  2.0 fat    35948 b- defN 23-Mar-20 13:48 dtlpy/repositories/annotations.py
--rw-rw-rw-  2.0 fat    10396 b- defN 23-Mar-20 13:48 dtlpy/repositories/apps.py
+-rw-rw-rw-  2.0 fat    10547 b- defN 23-May-01 13:57 dtlpy/repositories/apps.py
 -rw-rw-rw-  2.0 fat    19986 b- defN 23-Mar-20 13:48 dtlpy/repositories/artifacts.py
 -rw-rw-rw-  2.0 fat    26008 b- defN 23-Mar-20 13:48 dtlpy/repositories/assignments.py
 -rw-rw-rw-  2.0 fat     8408 b- defN 23-Mar-20 13:48 dtlpy/repositories/bots.py
--rw-rw-rw-  2.0 fat    25740 b- defN 23-Apr-04 17:16 dtlpy/repositories/codebases.py
+-rw-rw-rw-  2.0 fat    25751 b- defN 23-May-01 13:03 dtlpy/repositories/codebases.py
 -rw-rw-rw-  2.0 fat     5374 b- defN 23-Apr-04 17:16 dtlpy/repositories/commands.py
--rw-rw-rw-  2.0 fat    43380 b- defN 23-Apr-04 17:16 dtlpy/repositories/datasets.py
+-rw-rw-rw-  2.0 fat    43380 b- defN 23-May-01 04:42 dtlpy/repositories/datasets.py
 -rw-rw-rw-  2.0 fat    41923 b- defN 23-Jan-25 12:50 dtlpy/repositories/downloader.py
--rw-rw-rw-  2.0 fat    15357 b- defN 23-Mar-20 13:48 dtlpy/repositories/dpks.py
--rw-rw-rw-  2.0 fat    10454 b- defN 23-Apr-04 17:16 dtlpy/repositories/drivers.py
--rw-rw-rw-  2.0 fat    30968 b- defN 23-Apr-04 14:37 dtlpy/repositories/executions.py
--rw-rw-rw-  2.0 fat     7724 b- defN 23-Mar-20 13:48 dtlpy/repositories/feature_sets.py
--rw-rw-rw-  2.0 fat     9919 b- defN 23-Mar-20 13:48 dtlpy/repositories/features.py
--rw-rw-rw-  2.0 fat    10118 b- defN 23-Apr-04 16:44 dtlpy/repositories/integrations.py
+-rw-rw-rw-  2.0 fat    15347 b- defN 23-May-01 13:57 dtlpy/repositories/dpks.py
+-rw-rw-rw-  2.0 fat    10554 b- defN 23-May-01 13:57 dtlpy/repositories/drivers.py
+-rw-rw-rw-  2.0 fat    31015 b- defN 23-May-01 13:03 dtlpy/repositories/executions.py
+-rw-rw-rw-  2.0 fat     8079 b- defN 23-May-01 13:57 dtlpy/repositories/feature_sets.py
+-rw-rw-rw-  2.0 fat    10323 b- defN 23-May-01 13:03 dtlpy/repositories/features.py
+-rw-rw-rw-  2.0 fat    11667 b- defN 23-May-01 13:57 dtlpy/repositories/integrations.py
 -rw-rw-rw-  2.0 fat    38664 b- defN 23-Mar-20 13:48 dtlpy/repositories/items.py
--rw-rw-rw-  2.0 fat    23472 b- defN 23-Apr-04 17:14 dtlpy/repositories/models.py
+-rw-rw-rw-  2.0 fat    29217 b- defN 23-May-01 13:57 dtlpy/repositories/models.py
 -rw-rw-rw-  2.0 fat     3141 b- defN 23-Mar-20 13:48 dtlpy/repositories/nodes.py
 -rw-rw-rw-  2.0 fat    20035 b- defN 23-Mar-29 12:32 dtlpy/repositories/ontologies.py
 -rw-rw-rw-  2.0 fat    23479 b- defN 23-Mar-20 13:48 dtlpy/repositories/organizations.py
--rw-rw-rw-  2.0 fat    88407 b- defN 23-Mar-20 13:48 dtlpy/repositories/packages.py
+-rw-rw-rw-  2.0 fat    88407 b- defN 23-Apr-30 11:40 dtlpy/repositories/packages.py
 -rw-rw-rw-  2.0 fat    12067 b- defN 23-Mar-20 13:48 dtlpy/repositories/pipeline_executions.py
 -rw-rw-rw-  2.0 fat    22927 b- defN 23-Mar-20 13:48 dtlpy/repositories/pipelines.py
 -rw-rw-rw-  2.0 fat    22418 b- defN 23-Mar-20 13:48 dtlpy/repositories/projects.py
--rw-rw-rw-  2.0 fat    15746 b- defN 23-Mar-20 13:48 dtlpy/repositories/recipes.py
+-rw-rw-rw-  2.0 fat    15746 b- defN 23-May-01 14:18 dtlpy/repositories/recipes.py
 -rw-rw-rw-  2.0 fat     5511 b- defN 23-Mar-20 13:48 dtlpy/repositories/resource_executions.py
 -rw-rw-rw-  2.0 fat    65527 b- defN 23-Feb-22 15:06 dtlpy/repositories/services.py
 -rw-rw-rw-  2.0 fat    12648 b- defN 22-Oct-25 07:58 dtlpy/repositories/settings.py
--rw-rw-rw-  2.0 fat    47331 b- defN 23-Apr-04 17:16 dtlpy/repositories/tasks.py
+-rw-rw-rw-  2.0 fat    47671 b- defN 23-May-01 13:03 dtlpy/repositories/tasks.py
 -rw-rw-rw-  2.0 fat    11698 b- defN 23-Mar-20 13:48 dtlpy/repositories/times_series.py
--rw-rw-rw-  2.0 fat    22432 b- defN 23-Apr-04 17:20 dtlpy/repositories/triggers.py
+-rw-rw-rw-  2.0 fat    22496 b- defN 23-May-01 13:03 dtlpy/repositories/triggers.py
 -rw-rw-rw-  2.0 fat     9476 b- defN 22-Oct-23 10:23 dtlpy/repositories/upload_element.py
--rw-rw-rw-  2.0 fat    31308 b- defN 23-Mar-20 13:48 dtlpy/repositories/uploader.py
+-rw-rw-rw-  2.0 fat    31308 b- defN 23-Apr-13 09:04 dtlpy/repositories/uploader.py
 -rw-rw-rw-  2.0 fat     9282 b- defN 23-Mar-20 13:48 dtlpy/repositories/webhooks.py
 -rw-rw-rw-  2.0 fat      926 b- defN 23-Mar-20 13:48 dtlpy/services/__init__.py
 -rw-rw-rw-  2.0 fat     5153 b- defN 21-Apr-13 14:29 dtlpy/services/aihttp_retry.py
 -rw-rw-rw-  2.0 fat    66643 b- defN 23-Mar-29 12:15 dtlpy/services/api_client.py
 -rw-rw-rw-  2.0 fat     1555 b- defN 22-Dec-12 10:20 dtlpy/services/api_reference.py
 -rw-rw-rw-  2.0 fat     2918 b- defN 22-Oct-23 10:23 dtlpy/services/async_utils.py
 -rw-rw-rw-  2.0 fat     1080 b- defN 21-Apr-13 14:29 dtlpy/services/calls_counter.py
@@ -208,34 +209,34 @@
 -rw-rw-rw-  2.0 fat     6488 b- defN 22-Oct-23 10:23 dtlpy/services/create_logger.py
 -rw-rw-rw-  2.0 fat     3770 b- defN 22-Nov-15 22:39 dtlpy/services/events.py
 -rw-rw-rw-  2.0 fat     8110 b- defN 23-Jan-25 12:50 dtlpy/services/logins.py
 -rw-rw-rw-  2.0 fat     9378 b- defN 23-Jan-25 12:50 dtlpy/services/reporter.py
 -rw-rw-rw-  2.0 fat     3951 b- defN 22-Oct-23 10:23 dtlpy/services/service_defaults.py
 -rw-rw-rw-  2.0 fat      918 b- defN 23-Jan-25 12:50 dtlpy/utilities/__init__.py
 -rw-rw-rw-  2.0 fat     7744 b- defN 22-Oct-23 10:23 dtlpy/utilities/base_package_runner.py
--rw-rw-rw-  2.0 fat    75334 b- defN 23-Mar-20 13:48 dtlpy/utilities/converter.py
+-rw-rw-rw-  2.0 fat    75300 b- defN 23-May-01 13:03 dtlpy/utilities/converter.py
 -rw-rw-rw-  2.0 fat      744 b- defN 21-Apr-13 14:29 dtlpy/utilities/annotations/__init__.py
 -rw-rw-rw-  2.0 fat    11065 b- defN 22-Jun-14 12:29 dtlpy/utilities/annotations/annotation_converters.py
 -rw-rw-rw-  2.0 fat       66 b- defN 22-Oct-23 10:23 dtlpy/utilities/dataset_generators/__init__.py
 -rw-rw-rw-  2.0 fat    32021 b- defN 23-Mar-20 13:48 dtlpy/utilities/dataset_generators/dataset_generator.py
 -rw-rw-rw-  2.0 fat      739 b- defN 22-Nov-15 22:39 dtlpy/utilities/dataset_generators/dataset_generator_tensorflow.py
 -rw-rw-rw-  2.0 fat      557 b- defN 22-Nov-15 22:39 dtlpy/utilities/dataset_generators/dataset_generator_torch.py
 -rw-rw-rw-  2.0 fat       71 b- defN 23-Jan-25 12:50 dtlpy/utilities/local_development/__init__.py
 -rw-rw-rw-  2.0 fat     6630 b- defN 23-Jan-25 12:50 dtlpy/utilities/local_development/local_session.py
 -rw-rw-rw-  2.0 fat      126 b- defN 22-Oct-23 10:23 dtlpy/utilities/reports/__init__.py
 -rw-rw-rw-  2.0 fat     6103 b- defN 22-Nov-17 23:38 dtlpy/utilities/reports/figures.py
 -rw-rw-rw-  2.0 fat     2710 b- defN 22-Oct-23 10:23 dtlpy/utilities/reports/report.py
 -rw-rw-rw-  2.0 fat      751 b- defN 21-Apr-13 14:29 dtlpy/utilities/videos/__init__.py
 -rw-rw-rw-  2.0 fat    24670 b- defN 22-Jun-14 12:29 dtlpy/utilities/videos/video_player.py
 -rw-rw-rw-  2.0 fat    22345 b- defN 22-Oct-23 10:23 dtlpy/utilities/videos/videos.py
--rw-rw-rw-  2.0 fat       11 b- defN 21-May-24 14:31 dtlpy-1.75.8.data/scripts/dlp
--rw-rw-rw-  2.0 fat       38 b- defN 21-May-24 14:31 dtlpy-1.75.8.data/scripts/dlp.bat
--rw-rw-rw-  2.0 fat     4406 b- defN 21-May-24 14:31 dtlpy-1.75.8.data/scripts/dlp.py
+-rw-rw-rw-  2.0 fat       11 b- defN 21-May-24 14:31 dtlpy-1.76.15.data/scripts/dlp
+-rw-rw-rw-  2.0 fat       38 b- defN 21-May-24 14:31 dtlpy-1.76.15.data/scripts/dlp.bat
+-rw-rw-rw-  2.0 fat     4406 b- defN 21-May-24 14:31 dtlpy-1.76.15.data/scripts/dlp.py
 -rw-rw-rw-  2.0 fat        0 b- defN 21-Apr-13 14:29 tests/features/__init__.py
 -rw-rw-rw-  2.0 fat     8810 b- defN 23-Mar-20 13:48 tests/features/environment.py
--rw-rw-rw-  2.0 fat    11556 b- defN 23-Apr-06 08:09 dtlpy-1.75.8.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     2867 b- defN 23-Apr-06 08:09 dtlpy-1.75.8.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 23-Apr-06 08:09 dtlpy-1.75.8.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       44 b- defN 23-Apr-06 08:09 dtlpy-1.75.8.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat       12 b- defN 23-Apr-06 08:09 dtlpy-1.75.8.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    21649 b- defN 23-Apr-06 08:09 dtlpy-1.75.8.dist-info/RECORD
-239 files, 2195790 bytes uncompressed, 504429 bytes compressed:  77.1%
+-rw-rw-rw-  2.0 fat    11556 b- defN 23-May-02 09:18 dtlpy-1.76.15.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     3020 b- defN 23-May-02 09:18 dtlpy-1.76.15.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 23-May-02 09:18 dtlpy-1.76.15.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       44 b- defN 23-May-02 09:18 dtlpy-1.76.15.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat       12 b- defN 23-May-02 09:18 dtlpy-1.76.15.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat    21761 b- defN 23-May-02 09:18 dtlpy-1.76.15.dist-info/RECORD
+240 files, 2210477 bytes uncompressed, 507506 bytes compressed:  77.1%
```

## zipnote {}

```diff
@@ -69,14 +69,17 @@
 
 Filename: dtlpy/assets/__pycache__/__init__.cpython-37.pyc
 Comment: 
 
 Filename: dtlpy/assets/__pycache__/__init__.cpython-39.pyc
 Comment: 
 
+Filename: dtlpy/assets/__pycache__/main.cpython-311.pyc
+Comment: 
+
 Filename: dtlpy/assets/__pycache__/main.cpython-37.pyc
 Comment: 
 
 Filename: dtlpy/assets/__pycache__/main_partial.cpython-37.pyc
 Comment: 
 
 Filename: dtlpy/assets/__pycache__/model_adapter.cpython-37.pyc
@@ -678,41 +681,41 @@
 
 Filename: dtlpy/utilities/videos/video_player.py
 Comment: 
 
 Filename: dtlpy/utilities/videos/videos.py
 Comment: 
 
-Filename: dtlpy-1.75.8.data/scripts/dlp
+Filename: dtlpy-1.76.15.data/scripts/dlp
 Comment: 
 
-Filename: dtlpy-1.75.8.data/scripts/dlp.bat
+Filename: dtlpy-1.76.15.data/scripts/dlp.bat
 Comment: 
 
-Filename: dtlpy-1.75.8.data/scripts/dlp.py
+Filename: dtlpy-1.76.15.data/scripts/dlp.py
 Comment: 
 
 Filename: tests/features/__init__.py
 Comment: 
 
 Filename: tests/features/environment.py
 Comment: 
 
-Filename: dtlpy-1.75.8.dist-info/LICENSE
+Filename: dtlpy-1.76.15.dist-info/LICENSE
 Comment: 
 
-Filename: dtlpy-1.75.8.dist-info/METADATA
+Filename: dtlpy-1.76.15.dist-info/METADATA
 Comment: 
 
-Filename: dtlpy-1.75.8.dist-info/WHEEL
+Filename: dtlpy-1.76.15.dist-info/WHEEL
 Comment: 
 
-Filename: dtlpy-1.75.8.dist-info/entry_points.txt
+Filename: dtlpy-1.76.15.dist-info/entry_points.txt
 Comment: 
 
-Filename: dtlpy-1.75.8.dist-info/top_level.txt
+Filename: dtlpy-1.76.15.dist-info/top_level.txt
 Comment: 
 
-Filename: dtlpy-1.75.8.dist-info/RECORD
+Filename: dtlpy-1.76.15.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## dtlpy/__init__.py

```diff
@@ -75,34 +75,37 @@
     # triggers
     TriggerResource, TriggerAction, TriggerExecutionMode, TriggerType,
     # faas
     FunctionIO, KubernetesAutuscalerType, KubernetesRabbitmqAutoscaler, KubernetesAutoscaler, KubernetesRuntime,
     InstanceCatalog, PackageInputType, ServiceType, ServiceModeType,
     PackageSlot, SlotPostAction, SlotPostActionType, SlotDisplayScope, SlotDisplayScopeResource, UiBindingPanel,
     # roberto
-    DatasetSubsetType, LogSample, ArtifactType, Artifact, ItemArtifact, LinkArtifact, LocalArtifact, EntityScopeLevel,
+    DatasetSubsetType, PlotSample, ArtifactType, Artifact, ItemArtifact, LinkArtifact, LocalArtifact, EntityScopeLevel,
+    # features
+    FeatureEntityType, FeatureDataType, Feature, FeatureSet,
     #
     RequirementOperator, PackageRequirement,
     Command, CommandsStatus,
     LocalCodebase, GitCodebase, ItemCodebase, FilesystemCodebase, PackageCodebaseType,
-    MemberRole, FeatureEntityType, FeatureDataType, MemberOrgRole,
+    MemberRole, MemberOrgRole,
     Webhook, HttpMethod,
     ViewAnnotationOptions, AnnotationStatus, AnnotationType,
     ItemStatus, ExecutionStatus, ExportMetadata,
     Similarity, SimilarityTypeEnum, MultiView,
     ItemLink, UrlLink, LinkTypeEnum,
     Modality, ModalityTypeEnum, ModalityRefTypeEnum,
     Workload, WorkloadUnit, ItemAction,
     PipelineExecution, PipelineExecutionNode, Pipeline, PipelineConnection,
     PipelineNode, TaskNode, CodeNode, PipelineStats, PipelineSettings,
     PipelineNodeType, PipelineNameSpace, PipelineResumeOption,
     FunctionNode, DatasetNode, PipelineConnectionPort, PipelineNodeIO, Organization, OrganizationsPlans, Integration,
     Driver, S3Driver, GcsDriver, AzureBlobDriver, CacheAction, PodType,
-    ExternalStorage, Role, PlatformEntityType, SettingsValueTypes, SettingsTypes, SettingsSectionNames, SettingScope, \
-    BaseSetting, UserSetting, Setting, ServiceSample, ExecutionSample, PipelineExecutionSample, ResourceExecution
+    ExternalStorage, IntegrationType, Role, PlatformEntityType, SettingsValueTypes, SettingsTypes, SettingsSectionNames,
+    SettingScope, BaseSetting, UserSetting, Setting, ServiceSample, ExecutionSample, PipelineExecutionSample,
+    ResourceExecution
 )
 from .ml import BaseModelAdapter
 from .utilities import Converter, BaseServiceRunner, Progress, Context, AnnotationFormat
 from .repositories import FUNCTION_END_LINE, PackageCatalog
 
 warnings.simplefilter('once', DeprecationWarning)
 
@@ -475,9 +478,7 @@
 
 TASK_PRIORITY_LOW = TaskPriority.LOW
 TASK_PRIORITY_MEDIUM = TaskPriority.MEDIUM
 TASK_PRIORITY_HIGH = TaskPriority.HIGH
 
 SERVICE_MODE_TYPE_REGULAR = ServiceModeType.REGULAR
 SERVICE_MODE_TYPE_DEBUG = ServiceModeType.DEBUG
-
-
```

## dtlpy/__version__.py

```diff
@@ -1 +1 @@
-version = '1.75.8'
+version = '1.76.15'
```

## dtlpy/entities/__init__.py

```diff
@@ -50,25 +50,25 @@
 from .assignment import Assignment, Workload, WorkloadUnit
 from .task import Task, ItemAction, TaskPriority
 from .directory_tree import DirectoryTree
 from .similarity import Similarity, MultiView, SimilarityItem, MultiViewItem, SimilarityTypeEnum
 from .user import User
 from .bot import Bot
 from .webhook import Webhook, HttpMethod
-from .model import Model, DatasetSubsetType, LogSample
+from .model import Model, DatasetSubsetType, PlotSample
 from .driver import Driver, S3Driver, GcsDriver, AzureBlobDriver
 from .pipeline import Pipeline, PipelineStats, PipelineResumeOption, PipelineSettings
 from .node import PipelineConnection, PipelineNode, PipelineConnectionPort, PipelineNodeIO, TaskNode, \
     CodeNode, FunctionNode, PipelineNodeType, PipelineNameSpace, DatasetNode
 from .pipeline_execution import PipelineExecution, PipelineExecutionNode
 from .feature import Feature, FeatureDataType
 from .feature_set import FeatureSet, FeatureEntityType
 from .organization import Organization, OrganizationsPlans, MemberOrgRole, CacheAction, PodType
 from .analytic import ServiceSample, ExecutionSample, PipelineExecutionSample
-from .integration import Integration
+from .integration import Integration, IntegrationType
 from .driver import Driver, ExternalStorage
 from .setting import Role, PlatformEntityType, SettingsValueTypes, SettingsTypes, SettingsSectionNames, SettingScope, \
     BaseSetting, UserSetting, Setting
 from .reflect_dict import ReflectDict
 from .dpk import Dpk, Panel, Toolbar, Components
 from .app import App
 from .resource_execution import ResourceExecution
```

## dtlpy/entities/annotation.py

```diff
@@ -1665,14 +1665,27 @@
     def label(self, label):
         self.annotation_definition.label = label
 
     @property
     def attributes(self):
         return self._recipe_2_attributes if self.annotation._use_attributes_2 else self.annotation_definition.attributes
 
+    @attributes.setter
+    def attributes(self, attributes):
+        if self.annotation._use_attributes_2:
+            if not isinstance(attributes, dict):
+                raise ValueError(
+                    'Attributes must be a dict. If you are using v1 attributes please use dl.use_attributes_2(False)')
+            self._recipe_2_attributes = attributes
+        else:
+            if not isinstance(attributes, list):
+                raise ValueError(
+                    'Attributes must be a list. If you are using v2 attributes please use dl.use_attributes_2(True)')
+            self.annotation_definition.attributes = attributes
+
     @property
     def geo(self):
         return self.annotation_definition.geo
 
     @property
     def top(self):
         return self.annotation_definition.top
@@ -1789,24 +1802,27 @@
         :param annotation: annotation
         :param annotation_definition: annotation type object - must be same type as annotation
         :param frame_num: frame number
         :param fixed: is fixed
         :param object_visible: does the annotated object is visible
         :return: FrameAnnotation object
         """
-        return cls(
+        frame = cls(
             # annotations
             annotation=annotation,
             annotation_definition=annotation_definition,
 
             # multi
             frame_num=frame_num,
             fixed=fixed,
-            object_visible=object_visible
+            object_visible=object_visible,
         )
+        if annotation_definition.attributes:
+            frame.attributes = annotation_definition.attributes
+        return frame
 
     @classmethod
     def from_snapshot(cls, annotation, _json, fps):
         """
         new frame state to annotation
 
         :param annotation: annotation
@@ -1836,17 +1852,18 @@
 
     def to_snapshot(self):
 
         snapshot_dict = {
             'frame': self.frame_num,
             'fixed': self.fixed,
             'label': self.label,
-            'attributes': self.attributes,
             'type': self.type,
             'objectVisible': self.object_visible,
             'data': self.coordinates
         }
 
-        if self._recipe_2_attributes:
+        if self.annotation._use_attributes_2:
             snapshot_dict['namedAttributes'] = self._recipe_2_attributes
+        else:
+            snapshot_dict['attributes'] = self.attributes
 
         return snapshot_dict
```

## dtlpy/entities/annotation_collection.py

```diff
@@ -105,16 +105,15 @@
                                                  start_time=start_time,
                                                  end_time=end_time)
             #  add frame if exists
             if (frame_num is not None or start_time is not None) and (
                     self.item is None or 'audio' not in self.item.metadata.get('system').get(
                     'mimetype', '')):
                 if object_id is None:
-                    raise ValueError('Video Annotation must have object_id. '
-                                     'for more information visit: https://dataloop.ai/docs/sdk-create-video-annotation#create-video-annotation')
+                    raise ValueError('Video Annotation must have object_id.')
                 else:
                     if isinstance(object_id, int):
                         object_id = '{}'.format(object_id)
                     elif not isinstance(object_id, str) or not object_id.isnumeric():
                         raise ValueError('Object id must be an int or a string containing only numbers.')
                 # find matching element_id
                 matched_ind = [i_annotation
```

## dtlpy/entities/dpk.py

```diff
@@ -77,23 +77,27 @@
 
 class Components(entities.DlEntity):
     panels: List[Panel] = entities.DlProperty(location=['panels'],
                                               _kls='Panel')
     modules: List[entities.PackageModule] = entities.DlProperty(location=['modules'],
                                                                 _kls='PackageModule')
     services: List[entities.Service] = entities.DlProperty(location=['services'],
-                                                           _kls='Service')
+                                                           # _kls='Service'
+                                                           )
     triggers: List[entities.Trigger] = entities.DlProperty(location=['triggers'],
-                                                           _kls='Trigger')
+                                                           # _kls='Trigger'
+                                                           )
     pipelines: List[entities.Pipeline] = entities.DlProperty(location=['pipelines'],
-                                                             _kls='Pipeline')
+                                                             # _kls='Pipeline'
+                                                             )
     toolbars: List[Toolbar] = entities.DlProperty(location=['toolbars'],
                                                   _kls='Toolbar')
     models: List[entities.Model] = entities.DlProperty(location=['models'],
-                                                       _kls='Model')
+                                                       # _kls='Model'
+                                                       )
 
     @panels.default
     def default_panels(self):
         self._dict['panels'] = list()
         return self._dict['panels']
 
     @modules.default
@@ -228,17 +232,17 @@
         """
         returns the available versions of the dpk.
 
         :return the available versions of the dpk.
 
         ** Example **
         ..code-block:: python
-            versions = dl.dpks.revisions(dpk_id='id')
+            versions = dl.dpks.revisions(dpk_name='name')
         """
-        return self.dpks.revisions(self.id)
+        return self.dpks.revisions(dpk_name=self.name)
 
     @staticmethod
     def _protected_from_json(_json, client_api, project, is_fetched=True):
         """
         Same as from_json but with try-except to catch if error
 
         :param _json:  platform json
```

## dtlpy/entities/driver.py

```diff
@@ -5,20 +5,37 @@
 from .. import entities, repositories
 from ..services.api_client import ApiClient
 
 logger = logging.getLogger(name='dtlpy')
 
 
 class ExternalStorage(str, Enum):
+    """ The type of the Integration.
+
+    .. list-table::
+       :widths: 15 150
+       :header-rows: 1
+
+       * - State
+         - Description
+       * - S3
+         - AWS S3 drivers
+       * - GCS
+         - Google GCS drivers
+       * - AZUREBLOB
+         - Microsoft AZURE BLOB drivers
+       * - AZURE_DATALAKE_GEN2
+         - Microsoft AZURE GEN2 drivers
+    """
     S3 = "s3"
     GCS = "gcs"
     AZUREBLOB = "azureblob"
+    AZURE_DATALAKE_GEN2 = 'azureDatalakeGen2'
     KEY_VALUE = "key_value"
     AWS_STS = 'aws-sts'
-    AZURE_DATALAKE_GEN2 = 'azureDatalakeGen2'
 
 
 @attr.s()
 class Driver(entities.BaseEntity):
     """
     Driver entity
     """
```

## dtlpy/entities/feature_set.py

```diff
@@ -42,16 +42,19 @@
     ################
     # repositories #
     ################
     @_repositories.default
     def set_repositories(self):
         reps = namedtuple('repositories',
                           field_names=['feature_sets', 'features'])
-        feature_sets_repo = repositories.FeatureSets(client_api=self._client_api)
-        features_repo = repositories.Features(client_api=self._client_api, feature_set=self)
+        feature_sets_repo = repositories.FeatureSets(client_api=self._client_api,
+                                                     project_id=self.project_id)
+        features_repo = repositories.Features(client_api=self._client_api,
+                                              project_id=self.project_id,
+                                              feature_set=self, )
         r = reps(feature_sets=feature_sets_repo,
                  features=features_repo)
         return r
 
     @property
     def feature_sets(self):
         assert isinstance(self._repositories.feature_sets, repositories.FeatureSets)
```

## dtlpy/entities/filters.py

```diff
@@ -41,14 +41,15 @@
     COMPOSITION = 'composition'
     FEATURE = 'feature_vectors'
     FEATURE_SET = 'feature_sets'
     ORGANIZATIONS = 'organizations'
     DRIVERS = 'drivers'
     SETTINGS = 'setting'
     RESOURCE_EXECUTION = 'resourceExecution'
+    METRICS = 'metrics'
 
 
 class FiltersOperations(str, Enum):
     OR = "or"
     AND = "and"
     IN = "in"
     NOT_EQUAL = "ne"
@@ -527,11 +528,12 @@
                 for i_value, value in enumerate(values):
                     values[i_value] = self.__add_recursive(value=value)
 
         if self.operator is None:
             _json[self.field] = values
         else:
             value = dict()
-            value['${}'.format(self.operator)] = values
+            op = self.operator.value if isinstance(self.operator, FiltersOperations) else self.operator
+            value['${}'.format(op)] = values
             _json[self.field] = value
 
         return _json
```

## dtlpy/entities/integration.py

```diff
@@ -1,29 +1,62 @@
+from enum import Enum
+
 import logging
 import attr
 
 from .. import entities, exceptions, repositories
 from ..services.api_client import ApiClient
 
 logger = logging.getLogger(name='dtlpy')
 
 
+class IntegrationType(str, Enum):
+    """ The type of the Integration.
+
+    .. list-table::
+       :widths: 15 150
+       :header-rows: 1
+
+       * - State
+         - Description
+       * - S3
+         - S3 Integration - for S3 drivers
+       * - AWS_CROSS_ACCOUNT
+         - AWS CROSS ACCOUNT Integration - for S3 drivers
+       * - AWS_STS
+         - AWS STS Integration - for S3 drivers
+       * - GCS
+         - GCS Integration - for GCS drivers
+       * - AZUREBLOB
+         - AZURE BLOB Integration - for S3 AZUREBLOB and AZURE_DATALAKE_GEN2 drivers
+       * - KEY_VALUE
+         - KEY VALUE Integration - for save secrets in the platform
+    """
+    S3 = "s3"
+    AWS_CROSS_ACCOUNT = 'aws-cross'
+    AWS_STS = 'aws-sts'
+    GCS = "gcs"
+    AZUREBLOB = "azureblob"
+    KEY_VALUE = "key_value"
+
+
 @attr.s
 class Integration(entities.BaseEntity):
     """
     Integration object
     """
     id = attr.ib()
     name = attr.ib()
     type = attr.ib()
     org = attr.ib()
     created_at = attr.ib()
     created_by = attr.ib()
     update_at = attr.ib()
     _client_api = attr.ib(type=ApiClient, repr=False)
+    meatadata = attr.ib(default=None, repr=False)
     _project = attr.ib(default=None, repr=False)
 
     @classmethod
     def from_json(cls,
                   _json: dict,
                   client_api: ApiClient,
                   is_fetched=True):
@@ -38,15 +71,16 @@
         inst = cls(id=_json.get('id', None),
                    name=_json.get('name', None),
                    created_by=_json.get('createdBy', None),
                    created_at=_json.get('createdAt', None),
                    update_at=_json.get('updateAt', None),
                    type=_json.get('type', None),
                    org=_json.get('org', None),
-                   client_api=client_api)
+                   client_api=client_api,
+                   meatadata=_json.get('metadata', None))
         inst.is_fetched = is_fetched
         return inst
 
     def to_json(self):
         """
         Returns platform _json format of object
 
@@ -63,31 +97,55 @@
 
     @project.setter
     def project(self, project):
         if not isinstance(project, entities.Project):
             raise ValueError('Must input a valid Project entity')
         self._project = project
 
-    def update(self, new_name: str):
+    def update(self,
+               new_name: str = None,
+               new_options: dict = None):
         """
-        Update the integrations name
+        Update the integration's name.
+
+        **Prerequisites**: You must be an *owner* in the organization.
 
         :param str new_name: new name
+        :param dict new_options: new value
+        :return: Integration object
+        :rtype: dtlpy.entities.integration.Integration
+
+        **Examples for options include**:
+        s3 - {key: "", secret: ""};
+        gcs - {key: "", secret: "", content: ""};
+        azureblob - {key: "", secret: "", clientId: "", tenantId: ""};
+        key_value - {key: "", value: ""}
+        aws-sts - {key: "", secret: "", roleArns: ""}
+        aws-cross - {roleArns: ""}
+
+        **Example**:
+
+        .. code-block:: python
+
+            project.integrations.update(integrations_id='integrations_id', new_name="new_integration_name")
         """
         if self.project is not None:
             identifier = self.project
         elif self.org is not None:
             identifier = repositories.organizations.Organizations(client_api=self._client_api).get(
                 organization_id=self.org)
         else:
             raise exceptions.PlatformException(
                 error='400',
                 message='Must provide an identifier in inputs')
 
-        identifier.integrations.update(new_name=new_name, integrations_id=self.id)
+        identifier.integrations.update(new_name=new_name,
+                                       integrations_id=self.id,
+                                       integration=self,
+                                       new_options=new_options)
 
     def delete(self,
                sure: bool = False,
                really: bool = False) -> bool:
         """
         Delete integrations from the Organization
```

## dtlpy/entities/model.py

```diff
@@ -1,7 +1,8 @@
+import json
 from collections import namedtuple
 from enum import Enum
 import traceback
 import logging
 
 import attr
 
@@ -14,15 +15,15 @@
 class DatasetSubsetType(str, Enum):
     """Available types for dataset subsets"""
     TRAIN = 'train'
     VALIDATION = 'validation'
     TEST = 'test'
 
 
-class LogSample:
+class PlotSample:
     def __init__(self, figure, legend, x, y):
         """
         Create a single metric sample for Model
 
         :param figure: figure name identifier
         :param legend: line name identifier
         :param x: x value for the current sample
@@ -37,14 +38,37 @@
         _json = {'figure': self.figure,
                  'legend': self.legend,
                  'data': {'x': self.x,
                           'y': self.y}}
         return _json
 
 
+# class MatrixSample:
+#     def __init__(self, figure, legend, x, y):
+#         """
+#         Create a single metric sample for Model
+#
+#         :param figure: figure name identifier
+#         :param legend: line name identifier
+#         :param x: x value for the current sample
+#         :param y: y value for the current sample
+#         """
+#         self.figure = figure
+#         self.legend = legend
+#         self.x = x
+#         self.y = y
+#
+#     def to_json(self) -> dict:
+#         _json = {'figure': self.figure,
+#                  'legend': self.legend,
+#                  'data': {'x': self.x,
+#                           'y': self.y}}
+#         return _json
+
+
 @attr.s
 class Model(entities.BaseEntity):
     """
     Model object
     """
     # platform
     id = attr.ib()
@@ -189,14 +213,15 @@
                                                         ))
         _json['packageId'] = self.package_id
         _json['datasetId'] = self.dataset_id
         _json['createdAt'] = self.created_at
         _json['updatedAt'] = self.updated_at
         _json['inputType'] = self.input_type
         _json['outputType'] = self.output_type
+
         model_artifacts = list()
         for artifact in self.model_artifacts:
             if artifact.type in ['file', 'dir']:
                 artifact = {'type': 'item',
                             'itemId': artifact.id}
             else:
                 artifact = artifact.to_json(as_artifact=True)
@@ -244,15 +269,16 @@
 
     ################
     # repositories #
     ################
     @_repositories.default
     def set_repositories(self):
         reps = namedtuple('repositories',
-                          field_names=['projects', 'datasets', 'packages', 'models', 'ontologies', 'artifacts'])
+                          field_names=['projects', 'datasets', 'packages', 'models', 'ontologies', 'artifacts',
+                                       'metrics'])
 
         r = reps(projects=repositories.Projects(client_api=self._client_api),
                  datasets=repositories.Datasets(client_api=self._client_api,
                                                 project=self._project),
                  models=repositories.Models(client_api=self._client_api,
                                             project=self._project,
                                             project_id=self.project_id,
@@ -261,15 +287,17 @@
                                                 project=self._project),
                  ontologies=repositories.Ontologies(client_api=self._client_api,
                                                     project=self._project,
                                                     dataset=self._dataset),
                  artifacts=repositories.Artifacts(client_api=self._client_api,
                                                   project=self._project,
                                                   project_id=self.project_id,
-                                                  model=self)
+                                                  model=self),
+                 metrics=repositories.Metrics(client_api=self._client_api,
+                                              model=self)
                  )
 
         return r
 
     @property
     def platform_url(self):
         return self._client_api._get_resource_url("projects/{}/model/{}".format(self.project_id, self.id))
@@ -301,14 +329,19 @@
 
     @property
     def artifacts(self):
         assert isinstance(self._repositories.artifacts, repositories.Artifacts)
         return self._repositories.artifacts
 
     @property
+    def metrics(self):
+        assert isinstance(self._repositories.metrics, repositories.Metrics)
+        return self._repositories.metrics
+
+    @property
     def id_to_label_map(self):
         if 'id_to_label_map' not in self.configuration:
             # default
             if self.ontology_id == 'null' or self.ontology_id is None:
                 self.configuration['id_to_label_map'] = {int(idx): lbl for idx, lbl in enumerate(self.labels)}
             else:
                 self.configuration['id_to_label_map'] = {int(idx): lbl.tag for idx, lbl in
@@ -367,40 +400,46 @@
               configuration: dict = None,
               status=None,
               scope=None,
               project_id: str = None,
               labels: list = None,
               description: str = None,
               tags: list = None,
+              train_filter: entities.Filters = None,
+              validation_filter: entities.Filters = None,
               ):
         """
         Clones and creates a new model out of existing one
 
         :param str model_name: `str` new model name
         :param str dataset: dataset object for the cloned model
         :param dict configuration: `dict` (optional) if passed replaces the current configuration
         :param str status: `str` (optional) set the new status
         :param str scope: `str` (optional) set the new scope. default is "project"
         :param str project_id: `str` specify the project id to create the new model on (if other than the source model)
         :param list labels:  `list` of `str` - label of the model
         :param str description: `str` description of the new model
         :param list tags:  `list` of `str` - label of the model
+        :param dtlpy.entities.filters.Filters train_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model train
+        :param dtlpy.entities.filters.Filters validation_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model validation
 
         :return: dl.Model which is a clone version of the existing model
         """
         return self.models.clone(from_model=self,
                                  model_name=model_name,
                                  project_id=project_id,
                                  dataset=dataset,
                                  scope=scope,
                                  status=status,
                                  configuration=configuration,
                                  labels=labels,
                                  description=description,
-                                 tags=tags
+                                 tags=tags,
+                                 train_filter=train_filter,
+                                 validation_filter=validation_filter,
                                  )
 
     def train(self):
         """
         Train the model in the cloud. This will create a service and will run the adapter's train function as an execution
         :return:
         """
@@ -420,19 +459,7 @@
         Deploy a trained model. This will create a service that will execute predictions
 
         :param dict service_config : Service object as dict. Contains the spec of the default service to create.
 
         :return:
         """
         return self.models.deploy(model_id=self.id, service_config=service_config)
-
-    def add_log_samples(self, samples, dataset_id) -> bool:
-        """
-        Add Samples for dataset analytics and metrics
-
-        :param samples: list of dl.LogSample - must contain: figure, legend, x, y
-        :param dataset_id: id to dataset related to the metrics
-        :return: bool: True if success
-        """
-        return self.models.add_log_samples(model_id=self.id,
-                                           dataset_id=dataset_id,
-                                           samples=samples)
```

## dtlpy/entities/node.py

```diff
@@ -561,29 +561,29 @@
         :param list actions: list of task actions
         :param bool repeatable: can repeat in the item
         :param int groups: groups to assign the task to
         :param int batch_size: Pulling batch size (items) . Restrictions - Min 3, max 100 - for create pulling task
         :param int max_batch_workload: Max items in assignment . Restrictions - Min batchSize + 2 , max batchSize * 2 - for create pulling task
         :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
         :param float due_date: date by which the task should be finished; for example, due_date = datetime.datetime(day= 1, month= 1, year= 2029).timestamp()
-        :param int consensus_percentage: the consensus percentage ber task
-        :param int consensus_assignees: the consensus assignees number of the task
+        :param int consensus_percentage: percentage of items to be copied to multiple annotators (consensus items)
+        :param int consensus_assignees: the number of different annotators per item (number of copies per item)
         """
         if actions is None or actions == []:
             actions = []
             if task_type == 'qa':
                 if 'approve' not in actions:
                     actions.append('approve')
             else:
                 if 'complete' not in actions:
                     actions.append('complete')
             actions.append('discard')
         else:
             logger.warning(
-                "The 'actions' field was updated to override the system default actions for task (Complete/Approve, Discard) if provided, due to a bug fix.")
+                "The 'actions' field was updated to override the system default actions for task (complete/approve, discard) if provided, due to a bug fix.")
 
         inputs = [self._default_io()]
 
         outputs = [self._default_io(action=action) for action in actions]
 
         if groups is not None:
             if not isinstance(groups, list) or not all(isinstance(group, str) for group in groups):
```

## dtlpy/entities/setting.py

```diff
@@ -268,17 +268,14 @@
             id: str = None,
             sub_section_name: str = None,
             hint=None,
             client_api=None,
             project=None,
             org=None
     ):
-        warnings.warn(
-            message='UserSetting will be Deprecation from version 1.62 use Setting',
-            category=DeprecationWarning)
         super().__init__(
             default_value=default_value,
             value=value,
             name=name,
             value_type=value_type,
             scope=scope,
             metadata=metadata,
```

## dtlpy/entities/task.py

```diff
@@ -333,15 +333,15 @@
         :param float due_date: date by which the QA task should be finished; for example, due_date=datetime.datetime(day=1, month=1, year=2029).timestamp()
         :param list assignee_ids: list the QA task assignees (contributors) that should be working on the task. Provide a list of users' emails
         :param entities.Filters filters: dl.Filters entity to filter items for the task
         :param List[entities.Item] items: list of items (item Id or objects) to insert to the task
         :param dict DQL query: filter items for the task
         :param List[WorkloadUnit] workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
         :param dict metadata: metadata for the task
-        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "Approved" and "Discarded"
+        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "approved" and "discard"
         :param bool wait: wait until create task finish
         :param int batch_size: Pulling batch size (items), use with pulling allocation method. Restrictions - Min 3, max 100
         :param int max_batch_workload: Max items in assignment, use with pulling allocation method. Restrictions - Min batchSize + 2, max batchSize * 2
         :param list allowed_assignees: list the task assignees (contributors) that should be working on the task. Provide a list of users' emails
         :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
         :return: task object
         :rtype: dtlpy.entities.task.Task
```

## dtlpy/entities/annotation_definitions/box.py

```diff
@@ -67,20 +67,14 @@
         return [self.top, self.bottom]
 
     @property
     def geo(self):
         if self._box_points_setting():
             res = self._four_points
         else:
-            warnings.warn(
-                message='annotation.geo() for box representation will be changed from 2 points to 4 points, '
-                        'starting from version 1.62.0. '
-                        'We recommend switching to the new format before the deprecation. '
-                        'For more info: https://dataloop.ai/docs/SDK/sdk-annotationgeo-deprecation ',
-                category=DeprecationWarning)
             res = [
                 [self.left, self.top],
                 [self.right, self.bottom]
             ]
         return res
 
     def _box_points_setting(self):
```

## dtlpy/ml/base_model_adapter.py

```diff
@@ -202,19 +202,19 @@
             self.logger.warning("Data path directory ({}) is not empty..".format(data_path))
 
         annotation_options = entities.ViewAnnotationOptions.JSON
         if self.model_entity.output_type in [entities.AnnotationType.SEGMENTATION]:
             annotation_options = entities.ViewAnnotationOptions.INSTANCE
 
         # Download the subset items
-        subsets = dataset.metadata.get("system", dict()).get("subsets", None)
+        subsets = self.model_entity.metadata.get("system", dict()).get("subsets", None)
         if subsets is None:
-            raise ValueError("Dataset (id: {}) must have subsets in metadata.system.subsets".format(dataset.id))
-        for subset, filters_string in subsets.items():
-            filters = entities.Filters(custom_filter=json.loads(filters_string))
+            raise ValueError("Model (id: {}) must have subsets in metadata.system.subsets".format(self.model_entity.id))
+        for subset, filters_dict in subsets.items():
+            filters = entities.Filters(custom_filter=filters_dict)
             data_subset_base_path = os.path.join(data_path, subset)
             if os.path.isdir(data_subset_base_path) and not overwrite:
                 # existing and dont overwrite
                 self.logger.debug("Subset {!r} Existing (and overwrite=False). Skipping.".format(subset))
             else:
                 self.logger.debug("Downloading subset {!r} of {}".format(subset,
                                                                          self.model_entity.dataset.name))
```

## dtlpy/ml/train_utils.py

```diff
@@ -52,30 +52,10 @@
     logger.info("clone complete: {!r} in {:1.1f}[s]".format(cloned_dataset.name, toc - tic))
 
     assert cloned_dataset.name is not None, ('unable to get new ds {}'.format(clone_name))
     cloned_dataset.metadata['system']['clone_info'] = {'date': now,
                                                        'originalDatasetId': dataset.id}
     if filters is not None:
         cloned_dataset.metadata['system']['clone_info'].update({'filters': json.dumps(filters.prepare())})
-
-    dataset_subsets = cloned_dataset.metadata['system'].get("subsets", None)
-    if dataset_subsets is not None:
-        logger.warning(
-            "Dataset {} ({!r}) already have subsets in dataset.system.subsets".format(dataset.name, dataset.id))
-    else:
-        subsets_dict = dict()
-        for subset_name, subset_filter in subsets.items():
-            if isinstance(subset_filter, entities.Filters):
-                subset_filter_str = json.dumps(subset_filter.prepare())
-            elif isinstance(subset_filter, dict):
-                subset_filter_str = json.dumps(subset_filter)
-            elif isinstance(subset_filter, str):
-                subset_filter_str = subset_filter
-            else:
-                raise ValueError(
-                    'Input value `subsets` should be a dictionary with dl.Filter as values. got: {}'.format(
-                        subset_filter))
-            subsets_dict[subset_name] = subset_filter_str
-        cloned_dataset.metadata['system']['subsets'] = subsets_dict
     cloned_dataset.update(system_metadata=True)
     return cloned_dataset
     # cloned_dataset.set_readonly(True)
```

## dtlpy/repositories/__init__.py

```diff
@@ -28,15 +28,15 @@
 from .times_series import TimesSeries
 from .services import Services, FUNCTION_END_LINE
 from .executions import Executions
 from .assignments import Assignments
 from .tasks import Tasks
 from .bots import Bots
 from .webhooks import Webhooks
-from .models import Models
+from .models import Models, Metrics
 from .analytics import Analytics
 from .drivers import Drivers
 from .commands import Commands
 from .pipelines import Pipelines
 from .nodes import Nodes
 from .pipeline_executions import PipelineExecutions
 from .features import Features
```

## dtlpy/repositories/apps.py

```diff
@@ -176,32 +176,35 @@
         success, response = self._client_api.gen_request(req_type='put',
                                                          path=f"/apps/{app.id}",
                                                          json_req=app.to_json())
         if success:
             return success
         raise exceptions.PlatformException(response)
 
-    def install(self, dpk: entities.Dpk, organization_id: str = None) -> entities.App:
+    def install(self, dpk: entities.Dpk, app_name: str = None, organization_id: str = None) -> entities.App:
         """
         Install the specified app in the project.
 
         Note: You must pass either the app_id or app_name
         :param entities.App dpk: the app entity
+        :param str app_name: installed app name. default is the dpk name
         :param str organization_id: the organization which you want to apply on the filter.
         :return the installed app.
         :rtype entities.App
 
         **Example**
         .. code-block:: python
             app = dl.apps.install(dpk=dpk)
         """
         if dpk is None:
             raise exceptions.PlatformException(error='400', message='You must provide an app')
 
-        app = entities.App.from_json(_json={'name': dpk.display_name,
+        if app_name is None:
+            app_name = dpk.name
+        app = entities.App.from_json(_json={'name': app_name,
                                             'projectId': self.project.id,
                                             'orgId': organization_id,
                                             'dpkName': dpk.name,
                                             'dpkVersion': dpk.version,
                                             'scope': dpk.scope
                                             },
                                      client_api=self._client_api,
```

## dtlpy/repositories/codebases.py

```diff
@@ -296,15 +296,16 @@
 
         try:
             if not os.path.isdir(directory):
                 raise PlatformException(error='400', message='Not a directory: {}'.format(directory))
             directory = os.path.abspath(directory)
 
             # create zipfile
-            miscellaneous.Zipping.zip_directory(zip_filename=zip_filename, directory=directory,
+            miscellaneous.Zipping.zip_directory(zip_filename=zip_filename,
+                                                directory=directory,
                                                 ignore_directories=ignore_directories)
             zip_md = self.__file_hash(zip_filename)
 
             # get latest version
             same_version_found = None
             try:
                 all_versions_pages = self.get(codebase_name=name, version='all')
@@ -464,15 +465,15 @@
                                             tag=codebase.git_tag,
                                             username=username,
                                             password=password)
         if response:
             logger.info('Source code was cloned from {}(Git) to: {}'.format(codebase.git_url, local_path))
         else:
             raise RuntimeError('Failed cloning. See above for full log. codebase: {}'.format(codebase))
-        return os.path.join(local_path, codebase.git_repo_name)
+        return local_path
 
     def pull_git(self, codebase, local_path):
         """
         Pull (download) a codebase.
 
         **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a package.
```

## dtlpy/repositories/dpks.py

```diff
@@ -97,20 +97,17 @@
             raise ValueError('Not a directory: {}'.format(directory))
         if subpaths_to_append is None:
             subpaths_to_append = []
 
         try:
             directory = os.path.abspath(directory)
             # create zipfile
-            miscellaneous.Zipping.zip_directory_inclusive(zip_filename=dpk_filename,
-                                                          directory=directory,
-                                                          subpaths=['functions',
-                                                                    'panels',
-                                                                    'dataloop.json'] + subpaths_to_append
-                                                          )
+            miscellaneous.Zipping.zip_directory(zip_filename=dpk_filename,
+                                                directory=directory,
+                                                ignore_directories=['artifacts'])
             return dpk_filename
         except Exception:
             logger.error('Error when packing:')
             raise
 
     def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Dpk]:
         pool = self._client_api.thread_pools(pool_name='entity.create')
@@ -152,19 +149,21 @@
                 "dpks",
                 dpk.name,
                 str(dpk.version))
 
         dpk.codebases.unpack(codebase=dpk.codebase, local_path=local_path)
         return local_path
 
-    def __get_by_name(self, dpk_name: str):
+    def __get_by_name(self, dpk_name: str, dpk_version: str = None):
         filters = entities.Filters(field='name',
                                    values=dpk_name,
                                    resource=entities.FiltersResource.DPK,
                                    use_defaults=False)
+        if dpk_version is not None:
+            filters.add(field='version', values=dpk_version)
         dpks = self.list(filters=filters)
         if dpks.items_count == 0:
             raise exceptions.PlatformException(
                 error='404',
                 message='Dpk not found. Name: {}'.format(dpk_name))
         elif dpks.items_count > 1:
             raise exceptions.PlatformException(
@@ -306,22 +305,23 @@
         success, response = self._client_api.gen_request(req_type='post',
                                                          path=url,
                                                          json_req=filters.prepare())
         if not success:
             raise exceptions.PlatformException(response)
         return response.json()
 
-    def get(self, dpk_name: str = None, dpk_id: str = None) -> entities.Dpk:
+    def get(self, dpk_name: str = None, dpk_version: str = None, dpk_id: str = None) -> entities.Dpk:
         """
         Get a specific dpk from the platform.
 
         Note: you must pass either dpk_id or dpk_name.
 
         :param str dpk_id: the id of the dpk to get.
         :param str dpk_name: the name of the dpk to get.
+        :param str dpk_version: options - to get a specific dpk version
         :return the entity of the dpk
         :rtype entities.Dpk
 
         ** Example **
         ..coed-block:: python
             dpk = dl.dpks.get(dpk_name='name')
         """
@@ -337,10 +337,10 @@
                 raise exceptions.PlatformException(response)
 
             dpk = entities.Dpk.from_json(_json=response.json(),
                                          client_api=self._client_api,
                                          project=self._project,
                                          is_fetched=False)
         else:
-            dpk = self.__get_by_name(dpk_name)
+            dpk = self.__get_by_name(dpk_name=dpk_name, dpk_version=dpk_version)
 
         return dpk
```

## dtlpy/repositories/drivers.py

```diff
@@ -6,15 +6,15 @@
 logger = logging.getLogger(name='dtlpy')
 
 
 class Drivers:
     """
     Drivers Repository
     
-    The Drivers class allows users to manage drivers that are used to connect with external storage. Read more about external storage in our `documentation <https://dataloop.ai/docs/storage>`_ and `SDK documentation <https://dataloop.ai/docs/sdk-sync-storage>`_.
+    The Drivers class allows users to manage drivers that are used to connect with external storage. Read more about external storage in our `documentation <https://dataloop.ai/docs/storage>`_ and `SDK documentation <https://sdk-docs.dataloop.ai/en/latest/tutorials/data_management/cloud_storage/create_an_external_dataset/chapter.html#external-storage-dataset>`_.
     """
 
     def __init__(self, client_api: ApiClient, project: entities.Project = None):
         self._client_api = client_api
         self._project = project
 
     ############
@@ -141,34 +141,34 @@
 
     @_api_reference.add(path='/drivers', method='post')
     def create(self,
                name: str,
                driver_type: entities.ExternalStorage,
                integration_id: str,
                bucket_name: str,
-               integration_type: entities.ExternalStorage,
+               integration_type: entities.IntegrationType,
                project_id: str = None,
                allow_external_delete: bool = True,
                region: str = None,
                storage_class: str = "",
                path: str = ""):
         """
         Create a storage driver.
 
         **Prerequisites**: You must be in the role of an *owner* or *developer*.
 
         :param str name: the driver name
         :param ExternalStorage driver_type: dl.ExternalStorage (Enum). For all options run: list(dl.ExsternalStorage)
         :param str integration_id: the integration id
         :param str bucket_name: the external bucket name
-        :param ExternalStorage integration_type: dl.ExternalStorage (Enum). For all options run: list(dl.ExsternalStorage)
+        :param IntegrationType integration_type: dl.IntegrationType (Enum). For all options run: list(dl.IntegrationType)
         :param str project_id: project id
         :param bool allow_external_delete: true to allow deleting files from external storage when files are deleted in your Dataloop storage
         :param str region: relevant only for s3 - the bucket region
-        :param str storage_class: rilevante only for s3
+        :param str storage_class: relevant only for s3
         :param str path: Optional. By default path is the root folder. Path is case sensitive integration
         :return: driver object
         :rtype: dtlpy.entities.driver.Driver
 
         **Example**:
 
         .. code-block:: python
```

## dtlpy/repositories/executions.py

```diff
@@ -62,14 +62,15 @@
 
     def __get_from_entity(self, name, value):
         project_id = None
         try:
             from_dataset = False
             entity_obj = None
             for input_type in entities.FunctionIO.INPUT_TYPES:
+                input_type = input_type.value
                 entity = input_type.lower()
                 param = '{}_id'.format(entity)
                 if isinstance(value, dict):
                     if param in value:
                         repo = getattr(repositories, '{}s'.format(input_type))(client_api=self._client_api)
                         entity_obj = repo.get(**{param: value[param]})
                         if param in ['annotation_id', 'item_id']:
```

## dtlpy/repositories/feature_sets.py

```diff
@@ -1,29 +1,35 @@
 import logging
-from .. import exceptions, entities, miscellaneous, _api_reference
+from .. import exceptions, entities, miscellaneous, _api_reference, repositories
 from ..services.api_client import ApiClient
 
 logger = logging.getLogger(name='dtlpy')
 
 
 class FeatureSets:
     """
     Feature Sets repository
     """
     URL = '/features/sets'
 
-    def __init__(self, client_api: ApiClient, project: entities.Project = None):
+    def __init__(self, client_api: ApiClient,
+                 project_id: str = None,
+                 project: entities.Project = None):
         self._project = project
+        self._project_id = project_id
         self._client_api = client_api
 
     ############
     # entities #
     ############
     @property
     def project(self) -> entities.Project:
+        if self._project is None and self._project_id is not None:
+            # get from id
+            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self._project_id)
         if self._project is None:
             # try get checkout
             project = self._client_api.state_io.get('project')
             if project is not None:
                 self._project = entities.Project.from_json(_json=project, client_api=self._client_api)
         if self._project is None:
             raise exceptions.PlatformException(
@@ -96,30 +102,30 @@
                 message='Must provide an identifier in inputs, feature_set_name or feature_set_id')
         return feature_set
 
     @_api_reference.add(path='/features/sets', method='post')
     def create(self, name: str,
                size: int,
                set_type: str,
-               data_type: entities.FeatureDataType,
                entity_type: entities.FeatureEntityType,
+               data_type: entities.FeatureDataType = None,
                project_id: str = None,
                tags: list = None,
                org_id: str = None):
         """
         Create a new Feature Set
 
         :param str name: the Feature name
-        :param int size: the set size
+        :param int size: the length of a single vector in the set
         :param str set_type: string of the feature type: 2d, 3d, modelFC, TSNE,PCA,FFT
         :param entity_type: the entity that feature vector is linked to. Use the enum dl.FeatureEntityType
-        :param str project_id: the Id of the project where feature set will be created
+        :param str project_id: the ID of the project where feature set will be created
         :param list tags: optional tag per feature  - matched by index
-        :param str org_id: the Id of the org where feature set will be created
-        :param data_type: the type of feature vectors that relate to this set. Use the enum dl.FeatureDataType
+        :param str org_id: the ID of the org where feature set will be created
+        :param data_type: optional, only when using feature vector as scores. Use the enum dl.FeatureDataType
         :return: Feature Set object
         """
         if tags is None:
             tags = list()
         if project_id is None:
             if self._project is None:
                 raise ValueError('Must input a project id')
```

## dtlpy/repositories/features.py

```diff
@@ -1,41 +1,46 @@
 import logging
 
-from .. import exceptions, entities, miscellaneous, _api_reference
+from .. import exceptions, entities, miscellaneous, _api_reference, repositories
 from ..services.api_client import ApiClient
 
 logger = logging.getLogger(name='dtlpy')
 
 
 class Features:
     """
     Features repository
     """
     URL = '/features/vectors'
 
     def __init__(self, client_api: ApiClient,
                  project: entities.Project = None,
+                 project_id: str = None,
                  item: entities.Item = None,
                  annotation: entities.Annotation = None,
                  feature_set: entities.FeatureSet = None):
         self._project = project
+        self._project_id = project_id
         self._item = item
         self._annotation = annotation
         self._feature_set = feature_set
         self._client_api = client_api
 
     ############
     # entities #
     ############
     @property
     def feature_set(self) -> entities.FeatureSet:
         return self._feature_set
 
     @property
     def project(self) -> entities.Project:
+        if self._project is None and self._project_id is not None:
+            # get from id
+            self._project = repositories.Projects(client_api=self._client_api).get(project_id=self._project_id)
         if self._project is None:
             # try get checkout
             project = self._client_api.state_io.get('project')
             if project is not None:
                 self._project = entities.Project.from_json(_json=project, client_api=self._client_api)
         if self._project is None:
             raise exceptions.PlatformException(
@@ -137,18 +142,21 @@
         :param str version: version of the featureSet generator
         :param str parent_id: optional: parent FeatureSet id - used when FeatureVector is a subFeature
         :param str org_id: the id of the org where featureVector will be created
         :param str refs: the context of the featureVector (feautureSet must be defined with dataType)
         :return: Feature vector: 
         """
         if project_id is None:
-            if self._project is None:
-                raise ValueError('Must insert a project id')
-            else:
+            if self._project is not None:
                 project_id = self._project.id
+            elif self._project_id is not None:
+                project_id = self._project_id
+            else:
+                raise ValueError('Must insert a project id')
+
         if feature_set_id is None:
             if self._feature_set is None:
                 raise ValueError(
                     'Missing feature_set_id. Must insert the variable or create from context - feature_set.features.create()')
             feature_set_id = self._feature_set.id
 
         payload = {'project': project_id,
```

## dtlpy/repositories/integrations.py

```diff
@@ -91,40 +91,41 @@
         else:
             raise exceptions.PlatformException(
                 error='403',
                 message='Cant delete integrations from SDK. Please login to platform to delete')
 
     @_api_reference.add(path='/orgs/{org_id}/integrations', method='post')
     def create(self,
-               integrations_type: entities.ExternalStorage,
+               integrations_type: entities.IntegrationType,
                name: str,
                options: dict):
         """
         Create an integration between an external storage and the organization.
 
         **Examples for options include**:
         s3 - {key: "", secret: ""};
         gcs - {key: "", secret: "", content: ""};
         azureblob - {key: "", secret: "", clientId: "", tenantId: ""};
         key_value - {key: "", value: ""}
         aws-sts - {key: "", secret: "", roleArns: ""}
+        aws-cross - {}
 
         **Prerequisites**: You must be an *owner* in the organization.
 
-        :param str integrations_type: integrations type dl.ExternalStorage
+        :param IntegrationType integrations_type: integrations type dl.IntegrationType
         :param str name: integrations name
         :param dict options: dict of storage secrets
         :return: success
         :rtype: bool
 
         **Example**:
 
         .. code-block:: python
 
-            project.integrations.create(integrations_type=dl.ExternalStorage.S3,
+            project.integrations.create(integrations_type=dl.IntegrationType.S3,
                             name='S3ntegration',
                             options={key: "Access key ID", secret: "Secret access key"})
         """
 
         if self.project is None and self.org is None:
             raise exceptions.PlatformException(
                 error='400',
@@ -139,48 +140,76 @@
         payload = {"type": integrations_type, 'name': name, 'options': options}
         success, response = self._client_api.gen_request(req_type='post',
                                                          path=url_path,
                                                          json_req=payload)
         if not success:
             raise exceptions.PlatformException(response)
         else:
-            return entities.Integration.from_json(_json=response.json(), client_api=self._client_api)
+            integration = entities.Integration.from_json(_json=response.json(), client_api=self._client_api)
+        if integration.meatadata and isinstance(integration.meatadata, list) and len(integration.meatadata) > 0:
+            for metadata in integration.meatadata:
+                if metadata['name'] == 'status':
+                    integration_status = metadata['value']
+                    logger.info('Integration status: {}'.format(integration_status))
+        return integration
 
     @_api_reference.add(path='/orgs/{org_id}/integrations', method='patch')
     def update(self,
-               new_name: str,
-               integrations_id: str):
+               new_name: str = None,
+               integrations_id: str = None,
+               integration: entities.Integration = None,
+               new_options: dict = None):
         """
         Update the integration's name.
 
         **Prerequisites**: You must be an *owner* in the organization.
 
         :param str new_name: new name
         :param str integrations_id: integrations id
+        :param Integration integration: integration object
+        :param dict new_options: new value
         :return: Integration object
         :rtype: dtlpy.entities.integration.Integration
 
+        **Examples for options include**:
+        s3 - {key: "", secret: ""};
+        gcs - {key: "", secret: "", content: ""};
+        azureblob - {key: "", secret: "", clientId: "", tenantId: ""};
+        key_value - {key: "", value: ""}
+        aws-sts - {key: "", secret: "", roleArns: ""}
+        aws-cross - {roleArn: ""}
+
         **Example**:
 
         .. code-block:: python
 
-            project.integrations.update(integrations_id='integrations_id', new_name="new_integration_name")
+            project.integrations.update(integrations_id='integrations_id', new_options={roleArn: ""})
         """
         if self.project is None and self.org is None:
             raise exceptions.PlatformException(
                 error='400',
                 message='Must have an organization or project')
+        if integrations_id is None and integration is None:
+            raise exceptions.PlatformException(
+                error='400',
+                message='Must have an integrations_id or integration')
 
         if self.project is not None:
             organization_id = self.project.org.get('id')
         else:
             organization_id = self.org.id
 
         url_path = '/orgs/{}/integrations/'.format(organization_id)
-        payload = dict(name=new_name, id=integrations_id)
+        payload = dict(integrationId=integrations_id if integrations_id is not None else integration.id)
+        if new_name is not None:
+            payload['name'] = new_name
+        if new_options is not None:
+            if integration is None:
+                integration = self.get(integrations_id=integrations_id)
+            payload['credentials'] = dict(options=new_options, type=integration.type)
 
         success, response = self._client_api.gen_request(req_type='patch',
                                                          path=url_path,
                                                          json_req=payload)
         if not success:
             raise exceptions.PlatformException(response)
```

## dtlpy/repositories/models.py

```diff
@@ -1,7 +1,9 @@
+import json
+
 from typing import List
 import logging
 import os
 
 from .. import entities, repositories, exceptions, miscellaneous
 from ..services.api_client import ApiClient
 
@@ -188,14 +190,32 @@
                                        filters=filters,
                                        page_offset=filters.page,
                                        page_size=filters.page_size,
                                        client_api=self._client_api)
         paged.get_page()
         return paged
 
+    def _set_model_filter(self,
+                          metadata: dict,
+                          train_filter: entities.Filters = None,
+                          validation_filter: entities.Filters = None, ):
+        if metadata is None:
+            metadata = {}
+        if 'system' not in metadata:
+            metadata['system'] = {}
+        if 'subsets' not in metadata['system']:
+            metadata['system']['subsets'] = {}
+        if train_filter is not None:
+            metadata['system']['subsets']['train'] = train_filter.prepare() if isinstance(train_filter,
+                                                                                          entities.Filters) else train_filter
+        if validation_filter is not None:
+            metadata['system']['subsets']['validation'] = validation_filter.prepare() if isinstance(validation_filter,
+                                                                                                    entities.Filters) else validation_filter
+        return metadata
+
     def create(
             self,
             model_name: str,
             dataset_id: str,
             labels: list = None,
             ontology_id: str = None,
             description: str = None,
@@ -204,15 +224,17 @@
             tags: List[str] = None,
             package: entities.Package = None,
             configuration: dict = None,
             status: str = None,
             scope: entities.EntityScopeLevel = entities.EntityScopeLevel.PROJECT,
             version: str = '1.0.0',
             input_type=None,
-            output_type=None
+            output_type=None,
+            train_filter: entities.Filters = None,
+            validation_filter: entities.Filters = None,
     ) -> entities.Model:
         """
         Create a Model entity
 
         :param str model_name: name of the model
         :param str dataset_id: dataset id
         :param list labels: list of labels from ontology (must mach ontology id) can be a subset
@@ -224,15 +246,24 @@
         :param package: optional - Package object
         :param dict configuration: optional - model configuration - dict
         :param str status: `str` of the optional values of
         :param str scope: the scope level of the model dl.EntityScopeLevel
         :param str version: version of the model
         :param str input_type: the file type the model expect as input (image, video, txt, etc)
         :param str output_type: dl.AnnotationType - the type of annotations the model produces (class, box segment, text, etc)
+        :param dtlpy.entities.filters.Filters train_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model train
+        :param dtlpy.entities.filters.Filters validation_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model validation
         :return: Model Entity
+
+        **Example**:
+
+        .. code-block:: python
+
+            project.models.create(model_name='model_name', dataset_id='dataset_id', labels=['label1', 'label2'], train_filter={filter: {$and: [{dir: "/10K short videos"}]},page: 0,pageSize: 1000,resource: "items"}})
+
         """
 
         if ontology_id is not None:
             # take labels from ontology
             ontologies = repositories.Ontologies(client_api=self._client_api)
             labels = [label.tag for label in ontologies.get(ontology_id=ontology_id).labels]
 
@@ -290,14 +321,19 @@
 
         if description is not None:
             payload['description'] = description
 
         if status is not None:
             payload['status'] = status
 
+        if train_filter or validation_filter:
+            metadata = self._set_model_filter(metadata={}, train_filter=train_filter,
+                                              validation_filter=validation_filter)
+            payload['metadata'] = metadata
+
         # request
         success, response = self._client_api.gen_request(req_type='post',
                                                          path='/ml/models',
                                                          json_req=payload)
 
         # exception handling
         if not success:
@@ -326,33 +362,38 @@
               configuration: dict = None,
               status=None,
               scope=None,
               project_id: str = None,
               labels: list = None,
               description: str = None,
               tags: list = None,
+              train_filter: entities.Filters = None,
+              validation_filter: entities.Filters = None,
               ) -> entities.Model:
         """
         Clones and creates a new model out of existing one
 
         :param from_model: existing model to clone from
         :param str model_name: `str` new model name
         :param str dataset: dataset object for the cloned model
         :param dict configuration: `dict` (optional) if passed replaces the current configuration
         :param str status: `str` (optional) set the new status
         :param str scope: `str` (optional) set the new scope. default is "project"
         :param str project_id: `str` specify the project id to create the new model on (if other than the source model)
         :param list labels:  `list` of `str` - label of the model
         :param str description: `str` description of the new model
         :param list tags:  `list` of `str` - label of the model
+        :param dtlpy.entities.filters.Filters train_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model train
+        :param dtlpy.entities.filters.Filters validation_filter: Filters entity or a dictionary to define the items' scope in the specified dataset_id for the model validation
         :return: dl.Model which is a clone version of the existing model
         """
         from_json = {"name": model_name,
                      "packageId": from_model.package_id,
-                     "configuration": from_model.configuration}
+                     "configuration": from_model.configuration,
+                     "metadata": from_model.metadata}
         if project_id is not None:
             from_json['projectId'] = project_id
         if dataset is not None:
             if labels is None:
                 labels = list(dataset.labels_flat_dict.keys())
             from_json['datasetId'] = dataset.id
         if labels is not None:
@@ -366,14 +407,20 @@
             from_json['description'] = description
         if tags is not None:
             from_json['tags'] = tags
         if scope is not None:
             from_json['scope'] = scope
         if status is not None:
             from_json['status'] = status
+
+        metadata = self._set_model_filter(metadata=from_model.metadata,
+                                          train_filter=train_filter,
+                                          validation_filter=validation_filter)
+        if metadata['system']:
+            from_json['metadata'] = metadata
         success, response = self._client_api.gen_request(req_type='post',
                                                          path='/ml/models/{}/clone'.format(from_model.id),
                                                          json_req=from_json)
         if not success:
             raise exceptions.PlatformException(response)
         new_model = entities.Model.from_json(_json=response.json(),
                                              client_api=self._client_api,
@@ -525,36 +572,101 @@
                                                          path=f"/ml/models/{model_id}/deploy",
                                                          json_req=payload)
         if not success:
             raise exceptions.PlatformException(response)
 
         return True
 
-    def add_log_samples(self, samples, model_id, dataset_id) -> bool:
+
+class Metrics:
+    def __init__(self, client_api, model=None, model_id=None):
+        self._client_api = client_api
+        self._model_id = model_id
+        self._model = model
+
+    @property
+    def model(self):
+        return self._model
+
+    def create(self, samples, dataset_id) -> bool:
         """
         Add Samples for model analytics and metrics
 
-        :param samples: list of dl.LogSample - must contain: model_id, figure, legend, x, y
+        :param samples: list of dl.PlotSample - must contain: model_id, figure, legend, x, y
         :param model_id: model id to save samples on
         :param dataset_id:
         :return: bool: True if success
         """
         if not isinstance(samples, list):
             samples = [samples]
 
         payload = list()
         for sample in samples:
             _json = sample.to_json()
-            _json['modelId'] = model_id
+            _json['modelId'] = self.model.id
             _json['datasetId'] = dataset_id
             payload.append(_json)
         # request
         success, response = self._client_api.gen_request(req_type='post',
                                                          path='/ml/metrics/publish',
                                                          json_req=payload)
 
         # exception handling
         if not success:
             raise exceptions.PlatformException(response)
 
         # return entity
         return True
+
+    def _list(self, filters: entities.Filters):
+        # request
+        success, response = self._client_api.gen_request(req_type='POST',
+                                                         path='/ml/metrics/query',
+                                                         json_req=filters.prepare())
+        if not success:
+            raise exceptions.PlatformException(response)
+        return response.json()
+
+    def _build_entities_from_response(self, response_items) -> miscellaneous.List[entities.Model]:
+        jobs = [None for _ in range(len(response_items))]
+        pool = self._client_api.thread_pools(pool_name='entity.create')
+
+        # return triggers list
+        for i_service, sample in enumerate(response_items):
+            jobs[i_service] = pool.submit(entities.PlotSample,
+                                          **{'x': sample.get('data', dict()).get('x', None),
+                                             'y': sample.get('data', dict()).get('y', None),
+                                             'legend': sample.get('legend', ''),
+                                             'figure': sample.get('figure', '')})
+
+        # get all results
+        results = [j.result() for j in jobs]
+        # return good jobs
+        return miscellaneous.List(results)
+
+    def list(self, filters=None) -> entities.PagedEntities:
+        """
+        List Samples for model analytics and metrics
+
+        :param filters: dl.Filter query entity
+        """
+        if filters is None:
+            filters = entities.Filters(resource=entities.FiltersResource.METRICS)
+            if self._model is not None:
+                filters.add(field='modelId', values=self._model.id)
+        # assert type filters
+        if not isinstance(filters, entities.Filters):
+            raise exceptions.PlatformException(error='400',
+                                               message='Unknown filters type: {!r}'.format(type(filters)))
+
+        if filters.resource != entities.FiltersResource.METRICS:
+            raise exceptions.PlatformException(
+                error='400',
+                message='Filters resource must to be FiltersResource.METRICS. Got: {!r}'.format(filters.resource))
+
+        paged = entities.PagedEntities(items_repository=self,
+                                       filters=filters,
+                                       page_offset=filters.page,
+                                       page_size=filters.page_size,
+                                       client_api=self._client_api)
+        paged.get_page()
+        return paged
```

## dtlpy/repositories/tasks.py

```diff
@@ -475,15 +475,15 @@
         :param list assignee_ids: list the QA task assignees (contributors) that should be working on the task. Provide a list of users' emails
         :param float due_date: date by which the QA task should be finished; for example, due_date=datetime.datetime(day=1, month=1, year=2029).timestamp()
         :param entities.Filters filters: dl.Filters entity to filter items for the task
         :param List[entities.Item] items: list of items (item Id or objects) to insert to the task
         :param dict DQL query: filter items for the task
         :param List[WorkloadUnit] workload: list of WorkloadUnit objects. Customize distribution (percentage) between the task assignees. For example: [dl.WorkloadUnit(annotator@hi.com, 80), dl.WorkloadUnit(annotator2@hi.com, 20)]
         :param dict metadata: metadata for the task
-        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "Approved" and "Discarded"
+        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "approved" and "discard"
         :param bool wait: wait until create task finish
         :param int batch_size: Pulling batch size (items), use with pulling allocation method. Restrictions - Min 3, max 100
         :param int max_batch_workload: Max items in assignment, use with pulling allocation method. Restrictions - Min batchSize + 2, max batchSize * 2
         :param list allowed_assignees: list the task assignees (contributors) that should be working on the task. Provide a list of users' emails
         :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
         :return: task object
         :rtype: dtlpy.entities.task.Task
@@ -600,24 +600,24 @@
         :param str project_id: the Id of the project where task will be created
         :param str recipe_id: recipe id for the task
         :param list assignments_ids: assignments ids to the task
         :param dict metadata: metadata for the task
         :param entities.Filters filters: dl.Filters entity to filter items for the task
         :param List[entities.Item] items: list of items (item Id or objects) to insert to the task
         :param dict DQL query: filter items for the task
-        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "Completed" and "Discarded"
+        :param list available_actions: list of available actions (statuses) that will be available for the task items; The default statuses are: "completed" and "discard"
         :param bool wait: wait until create task finish
         :param entities.Filters check_if_exist: dl.Filters check if task exist according to filter
         :param int limit: the limit items that the task can include
         :param int  batch_size: Pulling batch size (items), use with pulling allocation method. Restrictions - Min 3, max 100
         :param int max_batch_workload: max_batch_workload: Max items in assignment, use with pulling allocation method. Restrictions - Min batchSize + 2, max batchSize * 2
         :param list allowed_assignees: list the task assignees (contributors) that should be working on the task. Provide a list of users' emails
         :param entities.TaskPriority priority: priority of the task options in entities.TaskPriority
-        :param int consensus_percentage: the consensus percentage ber task
-        :param int consensus_assignees: the consensus assignees number of the task
+        :param int consensus_percentage: percentage of items to be copied to multiple annotators (consensus items)
+        :param int consensus_assignees: the number of different annotators per item (number of copies per item)
         :return: Task object
         :rtype: dtlpy.entities.task.Task
 
         **Example**:
 
         .. code-block:: python
 
@@ -709,20 +709,25 @@
         if any([is_pulling, is_consensus]):
             if metadata is None:
                 metadata = {}
             if 'system' not in metadata:
                 metadata['system'] = {}
             if allowed_assignees is not None or assignee_ids is not None:
                 metadata['system']['allowedAssignees'] = allowed_assignees if allowed_assignees else assignee_ids
-            metadata = self._add_task_metadata_params(metadata=metadata, input_value=batch_size, input_name='batchSize')
-            metadata = self._add_task_metadata_params(metadata=metadata, input_value=max_batch_workload,
+            metadata = self._add_task_metadata_params(metadata=metadata,
+                                                      input_value=batch_size,
+                                                      input_name='batchSize')
+            metadata = self._add_task_metadata_params(metadata=metadata,
+                                                      input_value=max_batch_workload,
                                                       input_name='maxBatchWorkload')
-            metadata = self._add_task_metadata_params(metadata=metadata, input_value=consensus_percentage,
+            metadata = self._add_task_metadata_params(metadata=metadata,
+                                                      input_value=consensus_percentage,
                                                       input_name='consensusPercentage')
-            metadata = self._add_task_metadata_params(metadata=metadata, input_value=consensus_assignees,
+            metadata = self._add_task_metadata_params(metadata=metadata,
+                                                      input_value=consensus_assignees,
                                                       input_name='consensusAssignees')
 
         if metadata is not None:
             payload['metadata'] = metadata
 
         success, response = self._client_api.gen_request(req_type='post',
                                                          path=URL_PATH,
```

## dtlpy/repositories/triggers.py

```diff
@@ -339,16 +339,17 @@
                     " {!r} != {!r}".format(
                         trigger_name,
                         trigger.name))
         else:
             if trigger_name is None:
                 raise exceptions.PlatformException('400', 'Must provide either trigger name or trigger id')
             else:
-                triggers = self.list(filters=entities.Filters(field='name', values=trigger_name,
-                                                              resource=entities.FiltersResource.TRIGGER))
+                filters = self.__generate_default_filter()
+                filters.add(field='name', values=trigger_name)
+                triggers = self.list(filters)
                 if triggers.items_count == 0:
                     raise exceptions.PlatformException('404', 'Trigger not found')
                 elif triggers.items_count == 1:
                     trigger = triggers.items[0]
                 else:
                     raise exceptions.PlatformException('404',
                                                        'More than one trigger by name {} exist'.format(trigger_name))
@@ -452,14 +453,25 @@
         success, response = self._client_api.gen_request(req_type='POST',
                                                          path=url,
                                                          json_req=filters.prepare())
         if not success:
             raise exceptions.PlatformException(response)
         return response.json()
 
+    def __generate_default_filter(self):
+        filters = entities.Filters(resource=entities.FiltersResource.TRIGGER)
+        if self._project is not None:
+            filters.add(field='projectId', values=self._project.id)
+        if self._service is not None:
+            filters.add(field='spec.operation.serviceId', values=self._service.id)
+        if self._pipeline is not None:
+            filters.add(field='spec.operation.id', values=self._pipeline.id)
+
+        return filters
+
     @_api_reference.add(path='/query/faas', method='post')
     def list(self, filters: entities.Filters = None) -> entities.PagedEntities:
         """
         List triggers of a project, package, or service.
 
         **Prerequisites**: You must be in the role of an *owner* or *developer*. You must have a service.
 
@@ -470,21 +482,15 @@
         **Example**:
 
         .. code-block:: python
 
             service.triggers.list()
         """
         if filters is None:
-            filters = entities.Filters(resource=entities.FiltersResource.TRIGGER)
-            if self._project is not None:
-                filters.add(field='projectId', values=self._project.id)
-            if self._service is not None:
-                filters.add(field='spec.operation.serviceId', values=self._service.id)
-            if self._pipeline is not None:
-                filters.add(field='spec.operation.id', values=self._pipeline.id)
+            filters = self.__generate_default_filter()
         # assert type filters
         elif not isinstance(filters, entities.Filters):
             raise exceptions.PlatformException(error='400',
                                                message='Unknown filters type: {!r}'.format(type(filters)))
 
         if filters.resource != entities.FiltersResource.TRIGGER:
             raise exceptions.PlatformException(
```

## dtlpy/utilities/converter.py

```diff
@@ -970,16 +970,15 @@
 
         pool = ThreadPool(processes=self.concurrency)
         i_item = 0
         for path, subdirs, files in os.walk(local_path):
             for name in files:
                 save_to = os.path.join(os.path.split(local_path)[0], to_format)
                 save_to = path.replace(local_path, save_to)
-                if not os.path.isdir(save_to):
-                    os.mkdir(save_to)
+                os.makedirs(save_to, exist_ok=True)
                 file_path = os.path.join(path, name)
                 pool.apply_async(
                     func=self._convert_and_report,
                     kwds={
                         "to_format": to_format,
                         "from_format": from_format,
                         "file_path": file_path,
```

## Comparing `dtlpy-1.75.8.data/scripts/dlp.py` & `dtlpy-1.76.15.data/scripts/dlp.py`

 * *Files identical despite different names*

## Comparing `dtlpy-1.75.8.dist-info/LICENSE` & `dtlpy-1.76.15.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `dtlpy-1.75.8.dist-info/METADATA` & `dtlpy-1.76.15.dist-info/METADATA`

 * *Files 4% similar despite different names*

```diff
@@ -1,24 +1,25 @@
 Metadata-Version: 2.1
 Name: dtlpy
-Version: 1.75.8
+Version: 1.76.15
 Summary: SDK and CLI for Dataloop platform
 Home-page: https://github.com/dataloop-ai/dtlpy
 Author: Dataloop Team
 Author-email: info@dataloop.ai
 License: Apache License 2.0
 Platform: UNKNOWN
 Classifier: Programming Language :: Python
 Classifier: Programming Language :: Python :: 3
 Classifier: Programming Language :: Python :: 3 :: Only
 Classifier: Programming Language :: Python :: 3.6
 Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
 Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
 Requires-Python: >=3.6
 Description-Content-Type: text/markdown
 Requires-Dist: urllib3 (>=1.26)
 Requires-Dist: tqdm (>=4.63)
 Requires-Dist: certifi (>=2020.12.5)
 Requires-Dist: webvtt-py (==0.4.3)
 Requires-Dist: aiohttp (>=3.8)
@@ -55,14 +56,15 @@
 
 For full SDK documentation click [here](https://console.dataloop.ai/sdk-docs/latest)
 
 ### Python Support
 
 #### Dtlpy supports these Python versions.
 
-| Python            | 3.10 | 3.9 | 3.8 | 3.7 | 3.6 | 3.5 |
-|-------------------|------|-----|-----|-----|-----|-----|
-| dtlpy >= 1.61     | Yes  | Yes | Yes | Yes | Yes |     |
-| dtlpy 1.60 - 1.50 |      | Yes | Yes | Yes | Yes |     |
-| dtlpy <= 1.49     |      | Yes | Yes | Yes | Yes | Yes |           
+| Python            | 3.11 | 3.10 | 3.9 | 3.8 | 3.7 | 3.6 | 3.5 |
+|-------------------|------|------|-----|-----|-----|-----|-----|
+| dtlpy >= 1.76     | Yes  | Yes  | Yes | Yes | Yes | Yes |     |
+| dtlpy >= 1.61     |      | Yes  | Yes | Yes | Yes | Yes |     |
+| dtlpy 1.60 - 1.50 |      |      | Yes | Yes | Yes | Yes |     |
+| dtlpy <= 1.49     |      |      | Yes | Yes | Yes | Yes | Yes |
```

## Comparing `dtlpy-1.75.8.dist-info/RECORD` & `dtlpy-1.76.15.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,9 +1,9 @@
-dtlpy/__init__.py,sha256=hH2m7d5RptSR35v2Cmq9ZY-3XqauZnNrQXCVI0r-S20,20170
-dtlpy/__version__.py,sha256=g1OcNHvULcwTKbiSCvxDGZ-tWYtJvooZf4AJm4WiCqY,20
+dtlpy/__init__.py,sha256=lBpQmPlqGmraAJY83Wa1x70G_IdB7BgZyJzAgYJNYqw,20229
+dtlpy/__version__.py,sha256=tm70wZCS_27bzuvKgXIyNKAQOwevmp_S8d430WFHbEk,21
 dtlpy/exceptions.py,sha256=3-QaX1marJ1oCNNnEgnvqPlhHq8mVKjufI157pzxGZU,2996
 dtlpy/new_instance.py,sha256=5gyea5WNez5CN3ipn7BLWKYaTlayw_C21vDGzACSj3c,5843
 dtlpy/assets/__init__.py,sha256=AzcOoxymAF5O2ujLGtBIXWcA_AZx20Z0BzKLQsQWw94,1002
 dtlpy/assets/lock_open.png,sha256=vXHune4YF__fINPQ2l61G2zI3BeJPX_z5gkwzUNFAxs,24081
 dtlpy/assets/main.py,sha256=ZzihXeha0F56LdKj3fi-WSKZSgjPFtTcV7lOsmFWT-A,1433
 dtlpy/assets/main_partial.py,sha256=Bd76_Iebp_AlDgQtSQr7jyknsetCem359Ng4_l77dv8,285
 dtlpy/assets/mock.json,sha256=5Qeb71I7vHO14WWpnpOsogMgI_O3ubiadhuFjXEu-jY,160
@@ -19,14 +19,15 @@
 dtlpy/assets/.idea/modules.xml,sha256=VbBExagJjYlkAg5oUUSwUBzdfPCmSVyOO-w3sHivUgM,271
 dtlpy/assets/.idea/vcs.xml,sha256=2HygA1oRAwc3VBf-irxHrX5JJG9DXuQwrN0BlubhoKY,191
 dtlpy/assets/.idea/workspace.xml,sha256=FtTtBmFX3q5vvUkKsqVG3PP2wsgjIkvCiBf9FWpT8Yw,1793
 dtlpy/assets/__pycache__/__init__.cpython-310.pyc,sha256=F4kLByEgzzoWTqUPF7wUuH41CN-1k8vSeto2v3dQDew,1112
 dtlpy/assets/__pycache__/__init__.cpython-311.pyc,sha256=ReXjOZwo_AtLz_x-PhOXRSC3v_Sn7SvymglCbxYPwMw,1702
 dtlpy/assets/__pycache__/__init__.cpython-37.pyc,sha256=tnmt4-FtMW5IKzHRE0Fr8p34FkRRKlU9wgPaGQdXmIU,1100
 dtlpy/assets/__pycache__/__init__.cpython-39.pyc,sha256=dBWH2V3DPo_pvhkQbe2nvHI-PamyfSKPXP2RJ57dACs,1068
+dtlpy/assets/__pycache__/main.cpython-311.pyc,sha256=rhvoZDm1VnuR71wPzQCm0j2z2J8U0MvEAK6g1P2ZIQM,2475
 dtlpy/assets/__pycache__/main.cpython-37.pyc,sha256=BnjE5oj43tqYyuzWMe-IUC3WlvYXsRFVsKtYuptD_C8,1719
 dtlpy/assets/__pycache__/main_partial.cpython-37.pyc,sha256=OcaZK7mBjHp0QTwgyY3p2rZtKOfkQc9ucePdZGtfdGo,648
 dtlpy/assets/__pycache__/model_adapter.cpython-37.pyc,sha256=y9f0gs-R193pPIp9PZOBG7rYfK1eGSmJwqK4tDSVrg4,6376
 dtlpy/assets/code_server/config.yaml,sha256=TZqkKBnpkDdi7g4Zy3lpoio9uFDltC_hr1eitKjFTO8,50
 dtlpy/assets/code_server/installation.sh,sha256=kDC6T5KMS_Rx8vJ9tT8v5WLKsldhRq7LQjsGnbiNTvc,862
 dtlpy/assets/code_server/launch.json,sha256=Stq7OBIgI_34fsdZBwY_pyat7Tbc3NYN9kcr6dbeSDk,344
 dtlpy/assets/code_server/settings.json,sha256=LCiQpqk7kK-AQo9xQK9b4jT6NWpT8MVbJD00BddmR5c,66
@@ -55,39 +56,39 @@
 dtlpy/dlp/__init__.py,sha256=By841bihrX1JLnYMgx_wQJSrJTjUKavpTPq_CI5eWuA,888
 dtlpy/dlp/cli_utilities.py,sha256=7xWCtxW_MzdtZxOS1DSXnMS8yEHjLodDXNElr3LNPHo,16404
 dtlpy/dlp/command_executor.py,sha256=WmSv1RO8W5mLaTbUKjfo_GizWupQK9Pr_1IavmnwW0k,32280
 dtlpy/dlp/dlp,sha256=R99eKZP_pZ7egsYawtuw2lRL_6b0P7spq7JpGAY3H7Y,11
 dtlpy/dlp/dlp.bat,sha256=2V2sDEhvV2Tm7nMoyFdIxo5ODq-msgt9LJesLoPVmX0,38
 dtlpy/dlp/dlp.py,sha256=MBtdqt1XbWtZmafolUiFa5j2Wi41Y5fMtMy-Xntu0aE,4406
 dtlpy/dlp/parser.py,sha256=GqgF4UMFTjTQH382ZpXME3NfW7rAq4Dq544BvUfm6rA,31202
-dtlpy/entities/__init__.py,sha256=Lhh3e5qBXk_CxLmWSTK6koTo-n0J_U3eyQcfu0_9nRU,4335
+dtlpy/entities/__init__.py,sha256=IvwheJw2E0bBdIFa10_yA6jo9r2HCvUgg0S4J0EdXvU,4353
 dtlpy/entities/analytic.py,sha256=qQCmJlijDYGIXvvmLAjb1zWTNKn8rXMLQS_34wZyw4I,11665
-dtlpy/entities/annotation.py,sha256=cOmSp-6ApkeQ93pM87Z7668yxCG-shu8rZXaCodV0AQ,67925
-dtlpy/entities/annotation_collection.py,sha256=XoXfPkD6SyZzYyv1j62w_kLlYhmeYjpx-OxPrtrqdo0,30245
+dtlpy/entities/annotation.py,sha256=-ZpVU0_grrbXcPvPB88qIRduB__AkV1OFnygDab38dE,68750
+dtlpy/entities/annotation_collection.py,sha256=vXMfFpm8vsbfW4781lV03Bvu3oemOtJXuPmelo5261M,30099
 dtlpy/entities/app.py,sha256=iA1E4iFEUrwaZPcdZvz1zTfk1w_z1PSueXSK_EHoHAE,4999
 dtlpy/entities/artifact.py,sha256=s23DzZpVc6QMAGvNRAuN4K3U2nKuSrY8gVD49XdyUhw,5885
 dtlpy/entities/assignment.py,sha256=Glpn91Eo6zBYt-iugcR0vxNujcu9FYlX37aoWGJFUvI,14739
 dtlpy/entities/base_entity.py,sha256=DJjT5aaGnpmy_YKPO_FzfCRRh4axifL4qK10imaDqM8,7642
 dtlpy/entities/bot.py,sha256=dfuw0CkGzcwjb1Ov0KpIs0mIW5d6piHHzI0ZHjL7Vek,3932
 dtlpy/entities/codebase.py,sha256=fGt8KQVc1nASxsDXdIyQ7vCtfZNbxAdZvTy6zXdXpso,9295
 dtlpy/entities/command.py,sha256=n1cDEkoKPRAnU15YCwXUuZf-Qut_FP-2rvf7ephAGJ0,5138
 dtlpy/entities/dataset.py,sha256=3EJcTW5TO7qDHIJBL1ZcK7iZ02a22kCY5D2Osl0UHU4,45150
 dtlpy/entities/directory_tree.py,sha256=j0-kIa--HBaAZKE14b-SoI3JaZkmYnvtpkBt4vSo8_Q,1230
-dtlpy/entities/dpk.py,sha256=4LQBqOmu8xTqAs42esMXZfuFbN3t-UhjAo3omDk7zv0,10737
-dtlpy/entities/driver.py,sha256=iExPMC-PO8B0gWluYEOXTKWYsfgAL0AvFrbGnd1gI-Y,7032
+dtlpy/entities/dpk.py,sha256=d5AnTPbCILmj0Wpq-jdGG7cb4zqBZOclgDBeBkyQlj4,11002
+dtlpy/entities/driver.py,sha256=LR9ctjTuG3_kp4hU0NJliBTTyqaX5hECQv4sqhqMTxI,7419
 dtlpy/entities/execution.py,sha256=HZRM1SVi9gXe6CEAJckRD8nRbIcSmAXcD-7jo014AOc,12747
 dtlpy/entities/feature.py,sha256=vyhxlWrCjFmG_cJtp7eaLg8c_AC7yDkFgJzllwpeDy8,4518
-dtlpy/entities/feature_set.py,sha256=lo3sUuc1jyy6HQBKMTPAcTb1gofEMpXDARNjNHI6bKA,4530
-dtlpy/entities/filters.py,sha256=JeGO3ijaK5mYruj1lTwdYlcS9YOl8AmBH9QjaRfm5DI,18869
-dtlpy/entities/integration.py,sha256=1av5XUUn70Mhu8lISqFUX_4r0Sl-Z3qHjA1NC3948uY,3621
+dtlpy/entities/feature_set.py,sha256=szNEyIce1MDjRaz52-YG-DJsdUR5QoBKh4p9YMzZeXw,4736
+dtlpy/entities/filters.py,sha256=FxYdyKNrIpz3bHW0vwTQKxmbc-g4jE8dVNzaRTMLV3I,18988
+dtlpy/entities/integration.py,sha256=Nb6hU9d1-l_ZVpA7doZ8IPn50S__Gdl5Ib-rubCK-HE,5535
 dtlpy/entities/item.py,sha256=Z26Kx2ojMDfYj-RKy-qmyqnPbbfvX59v0PqlK4HxARM,28708
 dtlpy/entities/label.py,sha256=y7kiq-FmsEc3LrXEXUPf53EZ3ijhhobAmLeJ6UUvb2c,3992
 dtlpy/entities/links.py,sha256=_WIIbRQJPcpKU_PgZknncJ_ktQINaHa2EpUxREDsXm8,2601
-dtlpy/entities/model.py,sha256=aPlM54_VG-0XnNpcJWyc7KfQep1E_5AL0EPjdzhJ3EA,17554
-dtlpy/entities/node.py,sha256=cp8yEvD8MyByaN_Hb4gpc1SJZgMtonjq4Vy3RvcLkSA,35390
+dtlpy/entities/model.py,sha256=14XBtA2RheaxKtiEa1wgmkcU_H819N0Th0f4fF0x0Q8,18684
+dtlpy/entities/node.py,sha256=7NEZ6CYOissAq2hYeci9Zdf-kpgUQKyp0sO8drsYoT4,35459
 dtlpy/entities/ontology.py,sha256=oeYOzMRY127Wkg0oUAflBCFUDnrISIVdG6Wq9ZakmpE,30018
 dtlpy/entities/organization.py,sha256=1w3zUxhI5vZR6PBEHWjTAAue4AgUoWevhdvEfY9SHWo,10180
 dtlpy/entities/package.py,sha256=Taw_bR28a1gNnE06On4mFPNbA78DGshBrJUqjBCMwwA,26681
 dtlpy/entities/package_defaults.py,sha256=HGrGoZmtWVV4K8746BOrmqwmLV8gqbJfo-eQg5KHKcQ,216
 dtlpy/entities/package_function.py,sha256=GxEeRltHZtSXLh9SKC3K4dwoLD0yOgQfsV_W2mCod7o,5947
 dtlpy/entities/package_module.py,sha256=XGtKftRP0Z4fziygZx7dWYemQkBsF4ANc7GqF1NMMGs,4134
 dtlpy/entities/package_slot.py,sha256=lvnwGAOENDGBMKRMHTSfYqXWhqoW7JqgrO9ieKQ-mPw,5867
@@ -95,24 +96,24 @@
 dtlpy/entities/pipeline.py,sha256=d24RvMNoJzZku4azZD2y1g2fLoDvjow6gTBQdwZ5EOc,18072
 dtlpy/entities/pipeline_execution.py,sha256=HiHLejgd6kP-tN3_AwNfDUgvJfxOvneV6ao2ZNVBivQ,7417
 dtlpy/entities/project.py,sha256=Emjbp-2Gvq8YNO-dz29Uee93aDAf47pMxEdCvq92wOQ,14686
 dtlpy/entities/recipe.py,sha256=NbB0TN0YtQzqPMXApR8dUi7-0s9od4P7sbmk9dB_5l0,9793
 dtlpy/entities/reflect_dict.py,sha256=11pCUyRsTfwSRP2vErSvhq-CdUMMEwfkGUvcDxek94k,3375
 dtlpy/entities/resource_execution.py,sha256=93Knb8xZXTOvEWks4BZSXz7ROnheNdIsD2tQ-iF3m14,5171
 dtlpy/entities/service.py,sha256=qaGDhqoQsZ6R5ijVRZKMTrDJbOEEMB9itirRVI-rnSE,28019
-dtlpy/entities/setting.py,sha256=A0sdmJ_2lDPhUcLTUfu8Uw9P8cZoUymAyS2TES3OYAc,8911
+dtlpy/entities/setting.py,sha256=IXyo0DtNNf2V5UgiEuWlNKTfA3lxYOhlKf5U1HCkx00,8759
 dtlpy/entities/similarity.py,sha256=wcdwvDdGCWr5e9P0ZMD-X4vOlV2ILCWxeRs76kLM7sI,6318
-dtlpy/entities/task.py,sha256=fB74q2Yq6vhylKGZi-vx4I4X0ioIcPuQtVMYP1UXy_o,19245
+dtlpy/entities/task.py,sha256=OjEkNI0N-dCkEXbZOl6xNEYCHENhZLM3PVQqz16ttNA,19243
 dtlpy/entities/time_series.py,sha256=pVyxlv3KwnEvmfo0WrxbB8zTJB9cOHzJmK3U92ufC8g,4160
 dtlpy/entities/trigger.py,sha256=1Qz4icaECd32129e8K8mc3Z9VprrVWvIYCtGVuEIOhc,14404
 dtlpy/entities/user.py,sha256=J5fIx0txKtFsBWhY56CFggNYxmfxvy1nycR6E6yBulc,3983
 dtlpy/entities/webhook.py,sha256=cB4ZsFiEK-QWqlIvQUth2-lQEcDbHzPQJWi1BmaX2GQ,3663
 dtlpy/entities/annotation_definitions/__init__.py,sha256=o30cN2FXf6zZ2nNwv7-8Ts9mafY9pLl5oLoc30ooUqo,589
 dtlpy/entities/annotation_definitions/base_annotation_definition.py,sha256=qIjDIO6as20LjXm-TsQGiS8Y1uW_btukqpYbeODRrb0,2291
-dtlpy/entities/annotation_definitions/box.py,sha256=kIZBBYH0Od59dNw3hS9IkpyUVa1G6VWkAvPZBsyvzNs,9313
+dtlpy/entities/annotation_definitions/box.py,sha256=rbRIIdCwJEuGYzr357snnGwtusF1Rg5-G7T8rw1Yf0M,8874
 dtlpy/entities/annotation_definitions/classification.py,sha256=MSJbbaOiZBlLpUkNvMEGMVkupqfchIxBqhoobziiUho,1626
 dtlpy/entities/annotation_definitions/comparison.py,sha256=fC3AQx-yM1mzlcWgqoI1_NAZuF_AwnjC_WaSjtMVgxw,1878
 dtlpy/entities/annotation_definitions/cube.py,sha256=WhjSCQcUAmTgfJObyjTALkhzC8EolZdSqtYQIYEXERI,8758
 dtlpy/entities/annotation_definitions/cube_3d.py,sha256=6A1Vvh7zUDo_fDgz6DAnFYdF1JDmvnKuG0zCxGBON4Y,5954
 dtlpy/entities/annotation_definitions/description.py,sha256=AMlnhvHkqScRYGSHhwKYcZcLUcZmy1F1Mxj0Ydl0Qr0,951
 dtlpy/entities/annotation_definitions/ellipse.py,sha256=k6I21mMioZ20gkYDxa9JsnhWkJYKL1zfo0Ve1l0-FaA,4178
 dtlpy/entities/annotation_definitions/note.py,sha256=VB2ructTUTNQzhwqO02etd2TWlYhQL4XrO8iOE9LDTg,4244
@@ -151,52 +152,52 @@
 dtlpy/miscellaneous/__init__.py,sha256=Wi71lvQ7lqKd34QykWZ2NAP-bfcpMzImJaPso-EMTWM,849
 dtlpy/miscellaneous/dict_differ.py,sha256=ZVQrgj2X3yjutu9X2J84YP7eDp__erglPXWaUUPY-9g,3584
 dtlpy/miscellaneous/git_utils.py,sha256=qU9RM6H794331A9_kMSgzhOqFuTMSqUaOnItTyEWJ6E,8188
 dtlpy/miscellaneous/json_utils.py,sha256=meDnMsOtGZlhUfogrCsg-RKd5FLCfeKgji_pdAyi2vY,442
 dtlpy/miscellaneous/list_print.py,sha256=xopFGpg20HrEuicUZQDFLwQdh7qHAn-vyAQqFXMS0Kk,4913
 dtlpy/miscellaneous/zipping.py,sha256=OHROy2huF1Rf1IERC0tZwn6DzmNlQ1529lyQICxuoZM,5245
 dtlpy/ml/__init__.py,sha256=coDyt1yOZrmc3FIFPnQ_GGUVtcr2U1OqBAcP2u8p63U,819
-dtlpy/ml/base_model_adapter.py,sha256=6DAw1nMm20NzF9c6NUaDDUUItBYSXFRjn8IO-heI8Mk,35225
+dtlpy/ml/base_model_adapter.py,sha256=c3P9pWn_P4wYUk1ZMQ-5nobaAe3tc_NhSzrB_ngynb8,35227
 dtlpy/ml/metrics.py,sha256=CEavVQ9FoF_iHHaQCs2tPOdA2QvRvVSY11by4wIpeyA,20498
 dtlpy/ml/predictions_utils.py,sha256=eNUaReXLDCex1y1TAxuFfQV_sCGk4iT77UJFMHMw6zI,12758
 dtlpy/ml/summary_writer.py,sha256=JuFlqJsUqTBfLlRI1HtffiJA5tcCqDbarAKuoRPnZew,2841
-dtlpy/ml/train_utils.py,sha256=uMPBxaGluJwVm5l6QwzV11K73DHA2Wr_SbcwaI7ctmM,3545
-dtlpy/repositories/__init__.py,sha256=q9W7VZunScJcMVyd6gqA_aMkAf5l8n4-cx17w5iXu2Q,1893
+dtlpy/ml/train_utils.py,sha256=fyOBP9PAXRcL5t75bYQbkDetvpDHnRuvL3ezD5qYZgw,2505
+dtlpy/repositories/__init__.py,sha256=fH0_qiXD-jf6b0ZZvl1R6C5ztsU-iUk3SIdp2foC5YQ,1902
 dtlpy/repositories/analytics.py,sha256=s9_VULCtbMmAylB21rnUvyc2VWc6th8Z9ZZe_zS00rs,3051
 dtlpy/repositories/annotations.py,sha256=RBNW6rxxbcqvmZ4L8r6Utidgn9CTb_iX8bNaIEL2Jhk,35948
-dtlpy/repositories/apps.py,sha256=_DokYeVRYi4TLEg9tOy4fA48fkaeKak6rUYmLrINY08,10396
+dtlpy/repositories/apps.py,sha256=ttwVsiJg1AVd-GEvzjlr5e_yws168ZVqdVHhh5-LY7I,10547
 dtlpy/repositories/artifacts.py,sha256=iADx6boe75yu1Ne600BvTcxQhtb3fMPwxA0CFuh7gVQ,19986
 dtlpy/repositories/assignments.py,sha256=QCoYZudCRveDqR8Tm2V4OjEeJeq2JthnepqDO3u9Xok,26008
 dtlpy/repositories/bots.py,sha256=Wx-L6uGK4x94SaYdHmGKSIZLO-91ohayn6gKoZvkzTY,8408
-dtlpy/repositories/codebases.py,sha256=gwj7ofYGSVJjnrIbTgm8ZSRNIzSFZKYnTAL3Ik-AxdY,25740
+dtlpy/repositories/codebases.py,sha256=t7rDJiHeGo8Q9oBZeEUmn4uf96yVjZzVDNQ54wT4aV0,25751
 dtlpy/repositories/commands.py,sha256=mePaVTMs9e7aMGanmHn8g4wR0nF4ipEt4JafMSD039I,5374
 dtlpy/repositories/datasets.py,sha256=iA_qO23FrYGGJCSiBipOteLxv1xgto3dezvExa10Dls,43380
 dtlpy/repositories/downloader.py,sha256=MoK1uIecwB2w2SuH3FH8hoGviCBj59Spr_6vEnlrnPw,41923
-dtlpy/repositories/dpks.py,sha256=pKqdwKp3wr_2qb_agFpXL3qIPeEH-BiBEsJbqKXm2Kc,15357
-dtlpy/repositories/drivers.py,sha256=_scPrgJxMD9kQX3o3224jRtYJwpqMqFmlCEIgUlgBuM,10454
-dtlpy/repositories/executions.py,sha256=zuXddpHxwXJDJEzlTFcdtmaJgHRS4tG5x2oSmJSymFU,30968
-dtlpy/repositories/feature_sets.py,sha256=kTDhq53DQ6G4oLIW3lGa9oqsxBuMXbSCHv7DOj80EN8,7724
-dtlpy/repositories/features.py,sha256=Vb5hG_rLtXPkmMg4hSoqg7XNDSgsS_v7OgUpK_uy4BQ,9919
-dtlpy/repositories/integrations.py,sha256=N5vMPmSjzm1rOgLVZ5MDQ078yjWS_W0KTetIQTUBLhc,10118
+dtlpy/repositories/dpks.py,sha256=lLwn0DH0-8Cya-WxhvjbPu6HADd0ffP3kJxNlRvbJic,15347
+dtlpy/repositories/drivers.py,sha256=VkeAJyvw5H6Nree3b-U0fB8Vb2b1fiGAVVWlEfb4OyE,10554
+dtlpy/repositories/executions.py,sha256=ntdnVEjEyOAvC9N-3IJmF8bx_U4O_T1o8VeR7E9E7XE,31015
+dtlpy/repositories/feature_sets.py,sha256=Z0rRWK1JiHntXKr9qXr4PE1FC5b2j7KbFe04mGfkCiY,8079
+dtlpy/repositories/features.py,sha256=sXeQ37r9Au0FN2PS14zccz98yN8bjLsuXSWlmVX1VEQ,10323
+dtlpy/repositories/integrations.py,sha256=2WnZDwO6o2SHgufB71Oa0RY3plLB7aiTYJ7ml2vgPsU,11667
 dtlpy/repositories/items.py,sha256=pjBRcp9MRF10BXCMDlr-YZPxSZOL6Ry5nYhcQ1r8jKo,38664
-dtlpy/repositories/models.py,sha256=lPIz67LZXkSwUO9bNxqet1iZzvlT_sLERIk6XbYm8vU,23472
+dtlpy/repositories/models.py,sha256=S188lioHygaSX68OoqYdpUWxJIrePmU-dsurRJkQVXU,29217
 dtlpy/repositories/nodes.py,sha256=wvn8sqqkmLNkGl_ob1lZPt9afzDebA-QVvs1hvJCelM,3141
 dtlpy/repositories/ontologies.py,sha256=gKXki3m7D5qvBD68oE-OYHnlXG6NJ4eVfDw9G5wbhQw,20035
 dtlpy/repositories/organizations.py,sha256=ZtP70OqA8nwc5kdB2ool-vVAzX-WNQAK6BxmQRvbusE,23479
 dtlpy/repositories/packages.py,sha256=Udbk8RN6H6VMNQUjxP9XDaBllPQHSQpR3JqtC_pBxew,88407
 dtlpy/repositories/pipeline_executions.py,sha256=vYZo8LfggbFxc8cS9Ch36byVGNSOUumnbgV3BPNgBQE,12067
 dtlpy/repositories/pipelines.py,sha256=KvbmoqmL1swUh-vFYaQwuYGaZtbd1z2mIiwZGJt5KLo,22927
 dtlpy/repositories/projects.py,sha256=qccKbfLHR0e9LPDsOWS_BvcJ1XyAFQjv7Hy0dwgop80,22418
 dtlpy/repositories/recipes.py,sha256=3ver_U82kso6GiVdETr55G_9b0wZaMhNO56vLC9PlN4,15746
 dtlpy/repositories/resource_executions.py,sha256=dFhSo1IxehzUND97GCRRLdA70jPm8XY3g9Hl8VIXfg0,5511
 dtlpy/repositories/services.py,sha256=TBBO8bkyVTljLsjs2gvNJKvARBufQG16wu3HG0kIy2U,65527
 dtlpy/repositories/settings.py,sha256=vMjVIGnlA6_h6pm71PhCt_79PeL2iRU2vUsfAPaktZg,12648
-dtlpy/repositories/tasks.py,sha256=Tt8eaB1oR2CYTkkqS7Abz11JQ-wGvHWhk0P3ob1wuBY,47331
+dtlpy/repositories/tasks.py,sha256=tvfjuuiMP5p1KnsCidDtTlFoM50zHFglrKawsYw1T1g,47671
 dtlpy/repositories/times_series.py,sha256=xpbs3H3FTUolPy157xxOSlI7KHfqATtENVOGMhGw5KM,11698
-dtlpy/repositories/triggers.py,sha256=C5rPM_IUDO8zj_WY9Ky4HkYfLKKt0eeUehKtltfJxUk,22432
+dtlpy/repositories/triggers.py,sha256=czDxjXMYQoLBqbo5oFduQJLMmbE0j_c2Msk0EccPfV8,22496
 dtlpy/repositories/upload_element.py,sha256=3wIaPHT9Ieu2xgsTZyFJin8YcGAEa5EhMCzifof4WJ0,9476
 dtlpy/repositories/uploader.py,sha256=KttmroBQrXmdR6CmOhduX8luTSTLJu2C_yoTXlq7myA,31308
 dtlpy/repositories/webhooks.py,sha256=rdjsqt8geo1Dp-xMIAtk3SRe7uAWw9JZ-FjS45v2C30,9282
 dtlpy/services/__init__.py,sha256=BA5Wr0xWhBqSy8ayz1lu0aQnhH1JP6LA8e2UokJPP4M,926
 dtlpy/services/aihttp_retry.py,sha256=k27de_S2sKAWTGjOm6qfiipwLDh0iislksbzTE2voaw,5153
 dtlpy/services/api_client.py,sha256=jGt4I0iW8xYWPHKCkdApNCuMrhu6jh4fH3v9u2Gie1A,66643
 dtlpy/services/api_reference.py,sha256=jlP-7JvxILf3_fNni7aSXf8iFPPXRks3KDxmQht4qpE,1555
@@ -207,33 +208,33 @@
 dtlpy/services/create_logger.py,sha256=LQY8AGUqSV2NNikOEmUQujEqfamQ-_R-Yd9SD8vOA7o,6488
 dtlpy/services/events.py,sha256=hy47MkkBfzeNxd-pKW8svR3qzF80oKzl1XFdmf6hl7A,3770
 dtlpy/services/logins.py,sha256=k6fEKS2SZpQ5ODAUnNLpootYW7iSjJO-42Wp9k9kgQc,8110
 dtlpy/services/reporter.py,sha256=m8TEXgmFLsm_PY56xW9n51GpfnO3XPD3X21edhsfsIE,9378
 dtlpy/services/service_defaults.py,sha256=QPhpQq6AvFi2XmOfSx-2asymrRaZ8kRTDq014ofI5UM,3951
 dtlpy/utilities/__init__.py,sha256=nsADzasz6BmK6YIhblbqHYm6_WT467QvSCP_03iG7yM,918
 dtlpy/utilities/base_package_runner.py,sha256=_P7c1PrJrnASvzGkdalCuZxSkdUa1JynjQS_BiWpBL8,7744
-dtlpy/utilities/converter.py,sha256=ddboQ9bXmqS26kePnmNwpFWaDfBCFZ43P_dRMivG_u8,75334
+dtlpy/utilities/converter.py,sha256=mlAg4d_oJmgA46NFnKzeNACNTGfgpu8h2eJTYrVkoxA,75300
 dtlpy/utilities/annotations/__init__.py,sha256=FhfCWdhssPoqQm_UqETGJeteMJA-PzwGcnSu938uLLQ,744
 dtlpy/utilities/annotations/annotation_converters.py,sha256=WpfQLz0kZpUF91iy4mf22VVPK4NfHZBQukpXi_5rTxY,11065
 dtlpy/utilities/dataset_generators/__init__.py,sha256=1g_AxbvHQxi1uF51iG55O2coJbDdIDcgpamrNbRMwcc,66
 dtlpy/utilities/dataset_generators/dataset_generator.py,sha256=rKmVk4es55gRdrG5-Osb6Wks_yeCLoPrZhGaIcf995c,32021
 dtlpy/utilities/dataset_generators/dataset_generator_tensorflow.py,sha256=bcCOGm437-CjhZTMmvwtTKF_W6nReu4A-XJdBqN-MuA,739
 dtlpy/utilities/dataset_generators/dataset_generator_torch.py,sha256=YTUwI_J6odEsN64XBHNWgDAghjMf9J3lpqxQK1BSWx4,557
 dtlpy/utilities/local_development/__init__.py,sha256=6mvaU2KelS75j4gimx07sMGlglqX4VBjXZPQ9Sqga8E,71
 dtlpy/utilities/local_development/local_session.py,sha256=uO10jFfO1hEMorSqnQK2AwSjkyhZjCyZS1jFcNwegZo,6630
 dtlpy/utilities/reports/__init__.py,sha256=-Cw_3KGxX2s7oJv_5IOhOgdBah8uIEyDKsAIYVNVre4,126
 dtlpy/utilities/reports/figures.py,sha256=huuHvslytzT_DHCeztJDPAMamEirJPxMNBPYbqW3TX0,6103
 dtlpy/utilities/reports/report.py,sha256=zer2AqDmmuuu_A31TjHI9yS4RnjAlIeQZ5xWzOM6djc,2710
 dtlpy/utilities/videos/__init__.py,sha256=erjgtnnSJYk3k9j4PGzJwu3Ohv7H7DMnKySvJUQoaQs,751
 dtlpy/utilities/videos/video_player.py,sha256=KJiMcrGo17qwaSnOF-SsccAR8BVChC_HMgTV6zW1bJ8,24670
 dtlpy/utilities/videos/videos.py,sha256=PjPx_2hwAxctoz96XBd407ds8JZpzur4Z54rtJlVO-8,22345
-dtlpy-1.75.8.data/scripts/dlp,sha256=R99eKZP_pZ7egsYawtuw2lRL_6b0P7spq7JpGAY3H7Y,11
-dtlpy-1.75.8.data/scripts/dlp.bat,sha256=2V2sDEhvV2Tm7nMoyFdIxo5ODq-msgt9LJesLoPVmX0,38
-dtlpy-1.75.8.data/scripts/dlp.py,sha256=MBtdqt1XbWtZmafolUiFa5j2Wi41Y5fMtMy-Xntu0aE,4406
+dtlpy-1.76.15.data/scripts/dlp,sha256=R99eKZP_pZ7egsYawtuw2lRL_6b0P7spq7JpGAY3H7Y,11
+dtlpy-1.76.15.data/scripts/dlp.bat,sha256=2V2sDEhvV2Tm7nMoyFdIxo5ODq-msgt9LJesLoPVmX0,38
+dtlpy-1.76.15.data/scripts/dlp.py,sha256=MBtdqt1XbWtZmafolUiFa5j2Wi41Y5fMtMy-Xntu0aE,4406
 tests/features/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/features/environment.py,sha256=IknISJDbjati9A9gIcmNGh8De3llxp6-mjALpnVe1mc,8810
-dtlpy-1.75.8.dist-info/LICENSE,sha256=WtjCEwlcVzkh1ziO35P2qfVEkLjr87Flro7xlHz3CEY,11556
-dtlpy-1.75.8.dist-info/METADATA,sha256=lar6EzdbEbMA3d4_qikVz9mJ_Wcjxg9trjmq4E7lnmk,2867
-dtlpy-1.75.8.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-dtlpy-1.75.8.dist-info/entry_points.txt,sha256=l6nDiLzvGNTQgFsKbEbd9_9rETJwki14EupJCnfrhn4,44
-dtlpy-1.75.8.dist-info/top_level.txt,sha256=ZWuLmQGUOtWAdgTf4Fbx884w1o0vBYq9dEc1zLv9Mig,12
-dtlpy-1.75.8.dist-info/RECORD,,
+dtlpy-1.76.15.dist-info/LICENSE,sha256=WtjCEwlcVzkh1ziO35P2qfVEkLjr87Flro7xlHz3CEY,11556
+dtlpy-1.76.15.dist-info/METADATA,sha256=VvOCOJTxM6_1WSP2c4qvL1qu1MYJ3P3W3o5wwxRvFmM,3020
+dtlpy-1.76.15.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+dtlpy-1.76.15.dist-info/entry_points.txt,sha256=l6nDiLzvGNTQgFsKbEbd9_9rETJwki14EupJCnfrhn4,44
+dtlpy-1.76.15.dist-info/top_level.txt,sha256=ZWuLmQGUOtWAdgTf4Fbx884w1o0vBYq9dEc1zLv9Mig,12
+dtlpy-1.76.15.dist-info/RECORD,,
```

